{"2023-10-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.05919v1","updated":"2023-10-09T17:59:21Z","published":"2023-10-09T17:59:21Z","title":"Few-Shot Spoken Language Understanding via Joint Speech-Text Models","summary":"  Recent work on speech representation models jointly pre-trained with text has\ndemonstrated the potential of improving speech representations by encoding\nspeech and text in a shared space. In this paper, we leverage such shared\nrepresentations to address the persistent challenge of limited data\navailability in spoken language understanding tasks. By employing a pre-trained\nspeech-text model, we find that models fine-tuned on text can be effectively\ntransferred to speech testing data. With as little as 1 hour of labeled speech\ndata, our proposed approach achieves comparable performance on spoken language\nunderstanding tasks (specifically, sentiment analysis and named entity\nrecognition) when compared to previous methods using speech-only pre-trained\nmodels fine-tuned on 10 times more data. Beyond the proof-of-concept study, we\nalso analyze the latent representations. We find that the bottom layers of\nspeech-text models are largely task-agnostic and align speech and text\nrepresentations into a shared space, while the top layers are more\ntask-specific.\n","authors":["Chung-Ming Chien","Mingjiamei Zhang","Ju-Chieh Chou","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2310.05919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05915v1","updated":"2023-10-09T17:58:38Z","published":"2023-10-09T17:58:38Z","title":"FireAct: Toward Language Agent Fine-tuning","summary":"  Recent efforts have augmented language models (LMs) with external tools or\nenvironments, leading to the development of language agents that can reason and\nact. However, most of these agents rely on few-shot prompting techniques with\noff-the-shelf LMs. In this paper, we investigate and argue for the overlooked\ndirection of fine-tuning LMs to obtain language agents. Using a setup of\nquestion answering (QA) with a Google search API, we explore a variety of base\nLMs, prompting methods, fine-tuning data, and QA tasks, and find language\nagents are consistently improved after fine-tuning their backbone LMs. For\nexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4\nleads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,\na novel approach to fine-tuning LMs with trajectories from multiple tasks and\nprompting methods, and show having more diverse fine-tuning data can further\nimprove agents. Along with other findings regarding scaling effects,\nrobustness, generalization, efficiency and cost, our work establishes\ncomprehensive benefits of fine-tuning LMs for agents, and provides an initial\nset of experimental designs, insights, as well as open questions toward\nlanguage agent fine-tuning.\n","authors":["Baian Chen","Chang Shu","Ehsan Shareghi","Nigel Collier","Karthik Narasimhan","Shunyu Yao"],"pdf_url":"https://arxiv.org/pdf/2310.05915v1.pdf","comment":"Code, data, and models are available at\n  https://fireact-agent.github.io"},{"id":"http://arxiv.org/abs/2310.05914v1","updated":"2023-10-09T17:58:34Z","published":"2023-10-09T17:58:34Z","title":"NEFTune: Noisy Embeddings Improve Instruction Finetuning","summary":"  We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.\n","authors":["Neel Jain","Ping-yeh Chiang","Yuxin Wen","John Kirchenbauer","Hong-Min Chu","Gowthami Somepalli","Brian R. Bartoldson","Bhavya Kailkhura","Avi Schwarzschild","Aniruddha Saha","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2310.05914v1.pdf","comment":"25 pages, Code is available on Github:\n  https://github.com/neelsjain/NEFTune"},{"id":"http://arxiv.org/abs/2310.05910v1","updated":"2023-10-09T17:56:53Z","published":"2023-10-09T17:56:53Z","title":"SALMON: Self-Alignment with Principle-Following Reward Models","summary":"  Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON (Self-ALignMent with\nprinciple-fOllowiNg reward models), to align base language models with minimal\nhuman supervision, using only a small set of human-defined principles, yet\nachieving superior performance. Central to our approach is a\nprinciple-following reward model. Trained on synthetic preference data, this\nmodel can generate reward scores based on arbitrary human-defined principles.\nBy merely adjusting these principles during the RL training phase, we gain full\ncontrol over the preferences with the reward model, subsequently influencing\nthe behavior of the RL-trained policies, and eliminating the reliance on the\ncollection of online human preferences. Applying our method to the LLaMA-2-70b\nbase language model, we developed an AI assistant named Dromedary-2. With only\n6 exemplars for in-context learning and 31 human-defined principles,\nDromedary-2 significantly surpasses the performance of several state-of-the-art\nAI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have\nopen-sourced the code and model weights to encourage further research into\naligning LLM-based AI agents with enhanced supervision efficiency, improved\ncontrollability, and scalable oversight.\n","authors":["Zhiqing Sun","Yikang Shen","Hongxin Zhang","Qinhong Zhou","Zhenfang Chen","David Cox","Yiming Yang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2310.05910v1.pdf","comment":"Project page: https://github.com/IBM/SALMON"},{"id":"http://arxiv.org/abs/2310.05884v1","updated":"2023-10-09T17:27:36Z","published":"2023-10-09T17:27:36Z","title":"A Meta-Learning Perspective on Transformers for Causal Language Modeling","summary":"  The Transformer architecture has become prominent in developing large causal\nlanguage models. However, mechanisms to explain its capabilities are not well\nunderstood. Focused on the training process, here we establish a meta-learning\nview of the Transformer architecture when trained for the causal language\nmodeling task, by explicating an inner optimization process that may happen\nwithin the Transformer. Further, from within the inner optimization, we\ndiscover and theoretically analyze a special characteristic of the norms of\nlearned token representations within Transformer-based causal language models.\nOur analysis is supported by experiments conducted on pre-trained large\nlanguage models and real-world data.\n","authors":["Xinbo Wu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.05884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05881v1","updated":"2023-10-09T17:22:58Z","published":"2023-10-09T17:22:58Z","title":"Controllable Chest X-Ray Report Generation from Longitudinal\n  Representations","summary":"  Radiology reports are detailed text descriptions of the content of medical\nscans. Each report describes the presence/absence and location of relevant\nclinical findings, commonly including comparison with prior exams of the same\npatient to describe how they evolved. Radiology reporting is a time-consuming\nprocess, and scan results are often subject to delays. One strategy to speed up\nreporting is to integrate automated reporting systems, however clinical\ndeployment requires high accuracy and interpretability. Previous approaches to\nautomated radiology reporting generally do not provide the prior study as\ninput, precluding comparison which is required for clinical accuracy in some\ntypes of scans, and offer only unreliable methods of interpretability.\nTherefore, leveraging an existing visual input format of anatomical tokens, we\nintroduce two novel aspects: (1) longitudinal representation learning -- we\ninput the prior scan as an additional input, proposing a method to align,\nconcatenate and fuse the current and prior visual information into a joint\nlongitudinal representation which can be provided to the multimodal report\ngeneration model; (2) sentence-anatomy dropout -- a training strategy for\ncontrollability in which the report generator model is trained to predict only\nsentences from the original report which correspond to the subset of anatomical\nregions given as input. We show through in-depth experiments on the MIMIC-CXR\ndataset how the proposed approach achieves state-of-the-art results while\nenabling anatomy-wise controllable report generation.\n","authors":["Francesco Dalla Serra","Chaoyang Wang","Fani Deligianni","Jeffrey Dalton","Alison Q O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.05881v1.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05872v1","updated":"2023-10-09T17:10:35Z","published":"2023-10-09T17:10:35Z","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models","summary":"  In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.\n","authors":["Kaiwen Zhou","Kwonjoon Lee","Teruhisa Misu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14761v2","updated":"2023-10-09T17:08:51Z","published":"2023-05-24T06:11:17Z","title":"UniChart: A Universal Vision-language Pretrained Model for Chart\n  Comprehension and Reasoning","summary":"  Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n","authors":["Ahmed Masry","Parsa Kavehzadeh","Xuan Long Do","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2305.14761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05861v1","updated":"2023-10-09T16:57:57Z","published":"2023-10-09T16:57:57Z","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models","summary":"  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to a LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on two visual question answering tasks, we show that RepARe can result\nin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%\npoint increase on A-OKVQA. Additionally, we find that using gold answers for\noracle question candidate selection achieves a substantial gain in VQA accuracy\nby up to 14.41%. Through extensive analysis, we demonstrate that outputs from\nRepARe increase syntactic complexity, and effectively utilize vision-language\ninteraction and the frozen language model in LVLMs.\n","authors":["Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.05861v1.pdf","comment":"22 pages, 4 figures, Code: https://github.com/archiki/RepARe"},{"id":"http://arxiv.org/abs/2305.08732v2","updated":"2023-10-09T16:55:39Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05857v1","updated":"2023-10-09T16:52:07Z","published":"2023-10-09T16:52:07Z","title":"Improving Summarization with Human Edits","summary":"  Recent work has shown the promise of learning with human feedback paradigms\nto produce human-determined high-quality text. Existing works use human\nfeedback to train large language models (LLMs) in general domain abstractive\nsummarization and have obtained summary quality exceeding traditional\nlikelihood training. In this paper, we focus on a less explored form of human\nfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training\n(SALT), a novel technique to use both the human-edited and model-generated data\ntogether in the training loop. In addition, we demonstrate simulating Human\nEdits with ground truth summaries coming from existing training data --\nImitation edits, along with the model-generated summaries obtained after the\ntraining, to reduce the need for expensive human-edit data. In our experiments,\nwe extend human feedback exploration from general domain summarization to\nmedical domain summarization. Our results demonstrate the effectiveness of SALT\nto improve the summary quality with Human and Imitation Edits.\n","authors":["Zonghai Yao","Benjamin J Schloss","Sai P. Selvaraj"],"pdf_url":"https://arxiv.org/pdf/2310.05857v1.pdf","comment":"To appear in proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2023"},{"id":"http://arxiv.org/abs/2310.05845v1","updated":"2023-10-09T16:42:00Z","published":"2023-10-09T16:42:00Z","title":"GraphLLM: Boosting Graph Reasoning Ability of Large Language Model","summary":"  The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.\n","authors":["Ziwei Chai","Tianjie Zhang","Liang Wu","Kaiqiao Han","Xiaohai Hu","Xuanwen Huang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19187v2","updated":"2023-10-09T16:30:08Z","published":"2023-05-30T16:31:26Z","title":"Generating with Confidence: Uncertainty Quantification for Black-box\n  Large Language Models","summary":"  Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for black-box LLMs. We first differentiate\nuncertainty vs confidence: the former refers to the \"dispersion\" of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty metrics, applying them to selective NLG where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple metric for the\nsemantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.\n","authors":["Zhen Lin","Shubhendu Trivedi","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2305.19187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12284v3","updated":"2023-10-09T16:22:17Z","published":"2023-09-21T17:45:42Z","title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language\n  Models","summary":"  Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\naway from satisfactory for solving mathematical problem due to the complex\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\nMetaMathQA dataset, the MetaMath models with different model sizes and the\ntraining code for public use.\n","authors":["Longhui Yu","Weisen Jiang","Han Shi","Jincheng Yu","Zhengying Liu","Yu Zhang","James T. Kwok","Zhenguo Li","Adrian Weller","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2309.12284v3.pdf","comment":"Technical Report, Work in Progress. Project Page:\n  https://meta-math.github.io/"},{"id":"http://arxiv.org/abs/2310.05824v1","updated":"2023-10-09T16:08:23Z","published":"2023-10-09T16:08:23Z","title":"Terminology-Aware Translation with Constrained Decoding and Large\n  Language Model Prompting","summary":"  Terminology correctness is important in the downstream application of machine\ntranslation, and a prevalent way to ensure this is to inject terminology\nconstraints into a translation system. In our submission to the WMT 2023\nterminology translation task, we adopt a translate-then-refine approach which\ncan be domain-independent and requires minimal manual efforts. We annotate\nrandom source words with pseudo-terminology translations obtained from word\nalignment to first train a terminology-aware model. Further, we explore two\npost-processing methods. First, we use an alignment process to discover whether\na terminology constraint has been violated, and if so, we re-decode with the\nviolating word negatively constrained. Alternatively, we leverage a large\nlanguage model to refine a hypothesis by providing it with terminology\nconstraints. Results show that our terminology-aware model learns to\nincorporate terminologies effectively, and the large language model refinement\nprocess can further improve terminology recall.\n","authors":["Nikolay Bogoychev","Pinzhen Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05824v1.pdf","comment":"WMT 2023 Terminology Translation Task"},{"id":"http://arxiv.org/abs/2310.05818v1","updated":"2023-10-09T16:03:22Z","published":"2023-10-09T16:03:22Z","title":"SC-Safety: A Multi-round Open-ended Question Adversarial Safety\n  Benchmark for Large Language Models in Chinese","summary":"  Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated\nremarkable abilities in natural language understanding and generation. However,\nalongside their positive impact on our daily tasks, they can also produce\nharmful content that negatively affects societal perceptions. To systematically\nassess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -\na multi-round adversarial benchmark with 4912 open-ended questions covering\nmore than 20 safety sub-dimensions. Adversarial human-model interactions and\nconversations significantly increase the challenges compared to existing\nmethods. Experiments on 13 major LLMs supporting Chinese yield the following\ninsights: 1) Closed-source models outperform open-sourced ones in terms of\nsafety; 2) Models released from China demonstrate comparable safety levels to\nLLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can\ncompete effectively in terms of safety. By introducing SC-Safety, we aim to\npromote collaborative efforts to create safer and more trustworthy LLMs. The\nbenchmark and findings provide guidance on model selection. Our benchmark can\nbe found at https://www.CLUEbenchmarks.com\n","authors":["Liang Xu","Kangkang Zhao","Lei Zhu","Hang Xue"],"pdf_url":"https://arxiv.org/pdf/2310.05818v1.pdf","comment":"20 pages, 8 tables, 16 figures"},{"id":"http://arxiv.org/abs/2106.01810v3","updated":"2023-10-09T15:55:36Z","published":"2021-06-03T13:00:28Z","title":"Defending Against Backdoor Attacks in Natural Language Generation","summary":"  The frustratingly fragile nature of neural network models make current\nnatural language generation (NLG) systems prone to backdoor attacks and\ngenerate malicious sequences that could be sexist or offensive. Unfortunately,\nlittle effort has been invested to how backdoor attacks can affect current NLG\nmodels and how to defend against these attacks. In this work, by giving a\nformal definition of backdoor attack and defense, we investigate this problem\non two important NLG tasks, machine translation and dialog generation. Tailored\nto the inherent nature of NLG models (e.g., producing a sequence of coherent\nwords given contexts), we design defending strategies against attacks. We find\nthat testing the backward probability of generating sources given targets\nyields effective defense performance against all different types of attacks,\nand is able to handle the {\\it one-to-many} issue in many NLG tasks such as\ndialog generation. We hope that this work can raise the awareness of backdoor\nrisks concealed in deep NLG systems and inspire more future work (both attack\nand defense) towards this direction.\n","authors":["Xiaofei Sun","Xiaoya Li","Yuxian Meng","Xiang Ao","Lingjuan Lyu","Jiwei Li","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2106.01810v3.pdf","comment":"To appear at AAAI 2023"},{"id":"http://arxiv.org/abs/2305.08377v3","updated":"2023-10-09T15:52:30Z","published":"2023-05-15T06:24:45Z","title":"Text Classification via Large Language Models","summary":"  Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.\n","authors":["Xiaofei Sun","Xiaoya Li","Jiwei Li","Fei Wu","Shangwei Guo","Tianwei Zhang","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2305.08377v3.pdf","comment":"Pre-print Version"},{"id":"http://arxiv.org/abs/2306.09719v2","updated":"2023-10-09T15:48:23Z","published":"2023-06-16T09:40:05Z","title":"Pushing the Limits of ChatGPT on NLP Tasks","summary":"  Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n  In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n  We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.\n","authors":["Xiaofei Sun","Linfeng Dong","Xiaoya Li","Zhen Wan","Shuhe Wang","Tianwei Zhang","Jiwei Li","Fei Cheng","Lingjuan Lyu","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2306.09719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05804v1","updated":"2023-10-09T15:43:07Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11534v3","updated":"2023-10-09T15:39:21Z","published":"2023-08-21T06:51:56Z","title":"PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator","summary":"  The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, due to challenges\nin gathering conversations involving human participation, current endeavors\nlike Baize and UltraChat aim to automatically generate conversational data.\nThey primarily rely on ChatGPT conducting roleplay to simulate human behaviors\nbased on instructions rather than genuine learning from humans, resulting in\nlimited scope, diminished diversity, and an absence of genuine multi-round\nconversational dynamics. To address the above issues, we target human questions\nextracted from genuine human-machine conversations as a learning goal and train\na user simulator called `Socratic' to produce a high-quality human-centric\nsynthetic conversation dataset. Subsequently, this dataset was used to train\nour assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline\nmodels in both Vicuna-Bench and MT-Bench by pairwise comparison when\nconsidering equivalent training set sizes, and manual evaluation also shows\nthat our model is highly competitive. Impressively, when fine-tuned with the\nlatest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models\n(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in\nAlpaca-Eval benchmark, it ranks second among 7B models, even beating some\nlarger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth\nanalysis demonstrates the scalability and transferability of our approach. The\ncode is available at https://github.com/FreedomIntelligence/PlatoLM.\n","authors":["Chuyi Kong","Yaxin Fan","Xiang Wan","Feng Jiang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11534v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01933v3","updated":"2023-10-09T15:38:46Z","published":"2023-04-04T16:31:37Z","title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\n  Large Language Models","summary":"  The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.\n","authors":["Zhiqiang Hu","Lei Wang","Yihuai Lan","Wanyu Xu","Ee-Peng Lim","Lidong Bing","Xing Xu","Soujanya Poria","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2304.01933v3.pdf","comment":"EMNLP 2023. The code of our framework can be found at\n  https://github.com/AGI-Edgerunners/LLM-Adapters. We will keep all of the code\n  open-source and continue to update the framework with new adapters, LLMs, and\n  tasks"},{"id":"http://arxiv.org/abs/2308.10792v4","updated":"2023-10-09T15:36:49Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v4.pdf","comment":"A Survey paper, Pre-print"},{"id":"http://arxiv.org/abs/2310.05797v1","updated":"2023-10-09T15:31:03Z","published":"2023-10-09T15:31:03Z","title":"Are Large Language Models Post Hoc Explainers?","summary":"  Large Language Models (LLMs) are increasingly used as powerful tools for a\nplethora of natural language processing (NLP) applications. A recent\ninnovation, in-context learning (ICL), enables LLMs to learn new tasks by\nsupplying a few examples in the prompt during inference time, thereby\neliminating the need for model fine-tuning. While LLMs have been utilized in\nseveral applications, their applicability in explaining the behavior of other\nmodels remains relatively unexplored. Despite the growing number of new\nexplanation techniques, many require white-box access to the model and/or are\ncomputationally expensive, highlighting a need for next-generation post hoc\nexplainers. In this work, we present the first framework to study the\neffectiveness of LLMs in explaining other predictive models. More specifically,\nwe propose a novel framework encompassing multiple prompting strategies: i)\nPerturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,\nand iv) Explanation-based ICL, with varying levels of information about the\nunderlying ML model and the local neighborhood of the test sample. We conduct\nextensive experiments with real-world benchmark datasets to demonstrate that\nLLM-generated explanations perform on par with state-of-the-art post hoc\nexplainers using their ability to leverage ICL examples and their internal\nknowledge in generating model explanations. On average, across four datasets\nand two ML models, we observe that LLMs identify the most important feature\nwith 72.19% accuracy, opening up new frontiers in explainable artificial\nintelligence (XAI) to explore LLM-based explanation frameworks.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05793v1","updated":"2023-10-09T15:29:10Z","published":"2023-10-09T15:29:10Z","title":"DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for\n  Accelerated Seq2Seq Diffusion Models","summary":"  Diffusion models have gained prominence in generating high-quality sequences\nof text. Nevertheless, current approaches predominantly represent discrete text\nwithin a continuous diffusion space, which incurs substantial computational\noverhead during training and results in slower sampling speeds. In this paper,\nwe introduce a soft absorbing state that facilitates the diffusion model in\nlearning to reconstruct discrete mutations based on the underlying Gaussian\nspace, thereby enhancing its capacity to recover conditional signals. During\nthe sampling phase, we employ state-of-the-art ODE solvers within the\ncontinuous space to expedite the sampling process. Comprehensive experimental\nevaluations reveal that our proposed method effectively accelerates the\ntraining convergence by 4x and generates samples of similar quality 800x\nfaster, rendering it significantly closer to practical application.\n\\footnote{The code is released at \\url{https://github.com/Shark-NLP/DiffuSeq}\n","authors":["Shansan Gong","Mukai Li","Jiangtao Feng","Zhiyong Wu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2310.05793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05791v1","updated":"2023-10-09T15:26:07Z","published":"2023-10-09T15:26:07Z","title":"Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for\n  Competitive Programming Problems","summary":"  The recent program development industries have required problem-solving\nabilities for engineers, especially application developers. However, AI-based\neducation systems to help solve computer algorithm problems have not yet\nattracted attention, while most big tech companies require the ability to solve\nalgorithm problems including Google, Meta, and Amazon. The most useful guide to\nsolving algorithm problems might be guessing the category (tag) of the facing\nproblems. Therefore, our study addresses the task of predicting the algorithm\ntag as a useful tool for engineers and developers. Moreover, we also consider\npredicting the difficulty levels of algorithm problems, which can be used as\nuseful guidance to calculate the required time to solve that problem. In this\npaper, we present a real-world algorithm problem multi-task dataset, AMT, by\nmainly collecting problem samples from the most famous and large competitive\nprogramming website Codeforces. To the best of our knowledge, our proposed\ndataset is the most large-scale dataset for predicting algorithm tags compared\nto previous studies. Moreover, our work is the first to address predicting the\ndifficulty levels of algorithm problems. We present a deep learning-based novel\nmethod for simultaneously predicting algorithm tags and the difficulty levels\nof an algorithm problem given. All datasets and source codes are available at\nhttps://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.\n","authors":["Juntae Kim","Eunjung Cho","Dongwoo Kim","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2310.05791v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.05782v1","updated":"2023-10-09T15:15:05Z","published":"2023-10-09T15:15:05Z","title":"Aligning Language Models with Human Preferences via a Bayesian Approach","summary":"  In the quest to advance human-centric natural language generation (NLG)\nsystems, ensuring alignment between NLG models and human preferences is\ncrucial. For this alignment, current popular methods leverage a reinforcement\nlearning (RL) approach with a reward model trained on feedback from humans.\nHowever, inherent disagreements due to the subjective nature of human\npreferences pose a significant challenge for training the reward model,\nresulting in a deterioration of the NLG performance. To tackle this issue,\nprevious approaches typically rely on majority voting or averaging to\nconsolidate multiple inconsistent preferences into a merged one. Although\nstraightforward to understand and execute, such methods suffer from an\ninability to capture the nuanced degrees of disaggregation among humans and may\nonly represent a specialized subset of individuals, thereby lacking the ability\nto quantitatively disclose the universality of human preferences. To address\nthis challenge, this paper proposes a novel approach, which employs a Bayesian\nframework to account for the distribution of disagreements among human\npreferences as training a preference model, and names it as d-PM. Besides,\nconsidering the RL strategy's inefficient and complex training process over the\ntraining efficiency, we further propose utilizing the contrastive learning\nstrategy to train the NLG model with the preference scores derived from the\nd-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,\nemotional support conversation and integrity \"Rule-of-Thumb\" generation, show\nthat our method consistently exceeds previous SOTA models in both automatic and\nhuman evaluations.\n","authors":["Jiashuo Wang","Haozhao Wang","Shichao Sun","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2310.05782v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2302.04054v7","updated":"2023-10-09T15:05:36Z","published":"2023-02-08T13:47:00Z","title":"Towards Inferential Reproducibility of Machine Learning Research","summary":"  Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n","authors":["Michael Hagmann","Philipp Meier","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2302.04054v7.pdf","comment":"Published at ICLR 2023"},{"id":"http://arxiv.org/abs/2307.01379v2","updated":"2023-10-09T14:26:59Z","published":"2023-07-03T22:17:16Z","title":"Shifting Attention to Relevance: Towards the Uncertainty Estimation of\n  Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated remarkable potential in\nnatural language generation and instruction following, a persistent challenge\nlies in their susceptibility to \"hallucinations\", which erodes trust in their\noutputs. Although Uncertainty Quantification (UQ) presents a promising\nsolution, its accurate implementation within the context of LLMs remains a\nsignificant hurdle. To address this critical roadblock, our research originates\nfrom a fundamental heuristic insight: tokens within auto-regressive\nLLM-generated text do not equally reflect the underlying meaning. Some tokens\ncarry greater relevance and representativeness than others, owing to the\nphenomenon of \"linguistic redundancy\", wherein a select few keywords suffice to\nconvey the essence of lengthy sentences. Regrettably, existing methodologies\ntreat all tokens with equal importance when estimating uncertainty,\ndisregarding these inherent generative inequalities. Our analysis reveals a\nsignificant issue with state-of-the-art: numerous tokens (and sentences) of\nlimited semantic significance receive equal or even excessive weighting during\nuncertainty estimation. To rectify this bias, we propose to jointly Shifting\nAttention to more Relevant (SAR) components, at both the token- and the\nsentence-levels for accurate uncertainty estimation. We conduct extensive\nexperiments involving a range of popular \"off-the-shelf\" LLMs, including\ninstruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as\npretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B\nparameters. We carry out evaluation across various free-form question-answering\ntasks, encompassing domains such as reading comprehension, science Q&A, and\nmedical Q&A. Our experimental results demonstrate the superior performance of\nSAR in addressing the challenges of uncertainty estimation within the realm of\nLLMs.\n","authors":["Jinhao Duan","Hao Cheng","Shiqi Wang","Alex Zavalny","Chenan Wang","Renjing Xu","Bhavya Kailkhura","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2307.01379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05746v1","updated":"2023-10-09T14:22:09Z","published":"2023-10-09T14:22:09Z","title":"Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena","summary":"  Can Large Language Models (LLMs) simulate human behavior in complex\nenvironments? LLMs have recently been shown to exhibit advanced reasoning\nskills but much of NLP evaluation still relies on static benchmarks. Answering\nthis requires evaluation environments that probe strategic reasoning in\ncompetitive, dynamic scenarios that involve long-term planning. We introduce\nAucArena, a novel simulation environment for evaluating LLMs within auctions, a\nsetting chosen for being highly unpredictable and involving many skills related\nto resource and risk management, while also being easy to evaluate. We conduct\nseveral controlled simulations using state-of-the-art LLMs as bidding agents.\nWe find that through simple prompting, LLMs do indeed demonstrate many of the\nskills needed for effectively engaging in auctions (e.g., managing budget,\nadhering to long-term goals and priorities), skills that we find can be\nsharpened by explicitly encouraging models to be adaptive and observe\nstrategies in past auctions. These results are significant as they show the\npotential of using LLM agents to model intricate social dynamics, especially in\ncompetitive settings. However, we also observe considerable variability in the\ncapabilities of individual LLMs. Notably, even our most advanced models (GPT-4)\nare occasionally surpassed by heuristic baselines and human agents,\nhighlighting the potential for further improvements in the design of LLM agents\nand the important role that our simulation environment can play in further\ntesting and refining agent architectures.\n","authors":["Jiangjie Chen","Siyu Yuan","Rong Ye","Bodhisattwa Prasad Majumder","Kyle Richardson"],"pdf_url":"https://arxiv.org/pdf/2310.05746v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.05736v1","updated":"2023-10-09T14:10:21Z","published":"2023-10-09T14:10:21Z","title":"LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language Models","summary":"  Large language models (LLMs) have been applied in various applications due to\ntheir astonishing capabilities. With advancements in technologies such as\nchain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed\nto LLMs are becoming increasingly lengthy, even exceeding tens of thousands of\ntokens. To accelerate model inference and reduce cost, this paper presents\nLLMLingua, a coarse-to-fine prompt compression method that involves a budget\ncontroller to maintain semantic integrity under high compression ratios, a\ntoken-level iterative compression algorithm to better model the interdependence\nbetween compressed contents, and an instruction tuning based method for\ndistribution alignment between language models. We conduct experiments and\nanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that the proposed approach yields\nstate-of-the-art performance and allows for up to 20x compression with little\nperformance loss. Our code is available at https://aka.ms/LLMLingua.\n","authors":["Huiqiang Jiang","Qianhui Wu","Chin-Yew Lin","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.05736v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2308.04948v2","updated":"2023-10-09T14:08:40Z","published":"2023-08-09T13:32:06Z","title":"Extrapolating Large Language Models to Non-English by Aligning Languages","summary":"  Existing large language models show disparate capability across different\nlanguages, due to the imbalance in the training data. Their performances on\nEnglish tasks are often stronger than on tasks of other languages. In this\npaper, we empower pre-trained LLMs on non-English languages by building\nsemantic alignment across languages. We start from targeting individual\nlanguages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e.\ntuning it with translation task data and cross-lingual general task data to\nobtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws\nto investigate the advantages of using scalable translation data. Then we\nperform multilingual instruction-tuning (MuIT) with mixed resources to build\nmultilingual m-LLaMA. We also illustrate how we leverage the scaling laws to\noptimize data allocation in a resource-constrained setting. Experiment results\non cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the\nEnglish instruction-tuned counterpart (Alpaca) by an average of 27.83% across\nsix non-English languages. Evaluation results on translation dataset Flores-101\nshow that x-LLaMAs outperform previous LLaMA-based models by an average of\n18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on\nindividual languages and demonstrates the ability to follow multilingual\ninstructions. Further analysis on response content and representation space\nreveals the alignment of the multilingual semantic space within the middle\nlayers of m-LLaMA.\n","authors":["Wenhao Zhu","Yunzhe Lv","Qingxiu Dong","Fei Yuan","Jingjing Xu","Shujian Huang","Lingpeng Kong","Jiajun Chen","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2308.04948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05727v1","updated":"2023-10-09T13:55:45Z","published":"2023-10-09T13:55:45Z","title":"The Program Testing Ability of Large Language Models for Code","summary":"  Recent development of large language models (LLMs) for code like CodeX and\nCodeT5+ demonstrates tremendous promise in achieving code intelligence. Their\nability of synthesizing code that completes a program for performing a\npre-defined task has been intensively tested and verified on benchmark datasets\nincluding HumanEval and MBPP. Yet, evaluation of these LLMs from more\nperspectives (than just program synthesis) is also anticipated, considering\ntheir broad scope of applications in software engineering. In this paper, we\nexplore the ability of LLMs for testing programs/code. By performing thorough\nanalyses of recent LLMs for code in program testing, we show a series of\nintriguing properties of these models and demonstrate how program testing\nability of LLMs can be improved. Following recent work which utilizes generated\ntest cases to enhance program synthesis, we further leverage our findings in\nimproving the quality of the synthesized programs and show +11.77% and +4.22%\nhigher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline\nand the recent state-of-the-art, respectively.\n","authors":["Weimin Xiong","Yiwen Guo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05707v1","updated":"2023-10-09T13:29:37Z","published":"2023-10-09T13:29:37Z","title":"Guiding Language Model Reasoning with Planning Tokens","summary":"  Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\nreasoning. However, most of the existing approaches to enhance this ability\nrely heavily on data-driven methods, while neglecting the structural aspects of\nthe model's reasoning capacity. We find that while LLMs can manage individual\nreasoning steps well, they struggle with maintaining consistency across an\nentire reasoning chain. To solve this, we introduce 'planning tokens' at the\nstart of each reasoning step, serving as a guide for the model. These token\nembeddings are then fine-tuned along with the rest of the model parameters. Our\napproach requires a negligible increase in trainable parameters (just 0.001%)\nand can be applied through either full fine-tuning or a more\nparameter-efficient scheme. We demonstrate our method's effectiveness by\napplying it to three different LLMs, showing notable accuracy improvements\nacross three math word problem datasets w.r.t. plain chain-of-thought\nfine-tuning baselines.\n","authors":["Xinyi Wang","Lucas Caccia","Oleksiy Ostapenko","Xingdi Yuan","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2310.05707v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05703v1","updated":"2023-10-09T13:24:44Z","published":"2023-10-09T13:24:44Z","title":"An Attribution Method for Siamese Encoders","summary":"  Despite the success of Siamese encoder models such as sentence transformers\n(ST), little is known about the aspects of inputs they pay attention to. A\nbarrier is that their predictions cannot be attributed to individual features,\nas they compare two inputs rather than processing a single one. This paper\nderives a local attribution method for Siamese encoders by generalizing the\nprinciple of integrated gradients to models with multiple inputs. The solution\ntakes the form of feature-pair attributions, and can be reduced to a\ntoken-token matrix for STs. Our method involves the introduction of integrated\nJacobians and inherits the advantageous formal properties of integrated\ngradients: it accounts for the model's full computation graph and is guaranteed\nto converge to the actual prediction. A pilot study shows that in an ST few\ntoken-pairs can often explain large fractions of predictions, and it focuses on\nnouns and verbs. For accurate predictions, it however needs to attend to the\nmajority of tokens and parts of speech.\n","authors":["Lucas Mller","Dmitry Nikolaev","Sebastian Pad"],"pdf_url":"https://arxiv.org/pdf/2310.05703v1.pdf","comment":"Accepted to EMNLP'23"},{"id":"http://arxiv.org/abs/2306.17175v2","updated":"2023-10-09T13:23:53Z","published":"2023-06-17T23:35:51Z","title":"RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19\n  Assessment in Primary Care","summary":"  Clinical decision-making is a fundamental stage in delivering appropriate\ncare to patients. In recent years several decision-making systems designed to\naid the clinician in this process have been developed. However, technical\nsolutions currently in use are based on simple regression models and are only\nable to take into account simple pre-defined multiple-choice features, such as\npatient age, pre-existing conditions, smoker status, etc. One particular source\nof patient data, that available decision-making systems are incapable of\nprocessing is the collection of patient consultation GP notes. These contain\ncrucial signs and symptoms - the information used by clinicians in order to\nmake a final decision and direct the patient to the appropriate care.\nExtracting information from GP notes is a technically challenging problem, as\nthey tend to include abbreviations, typos, and incomplete sentences.\n  This paper addresses this open challenge. We present a framework that\nperforms knowledge graph construction from raw GP medical notes written during\nor after patient consultations. By relying on support phrases mined from the\nSNOMED ontology, as well as predefined supported facts from values used in the\nRECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction\ntool, our graph generative framework is able to extract structured knowledge\ngraphs from the highly unstructured and inconsistent format that consultation\nnotes are written in. Our knowledge graphs include information about existing\npatient symptoms, their duration, and their severity.\n  We apply our framework to consultation notes of COVID-19 patients in the UK\nCOVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a\nquantitative evaluation of the performance of our framework, demonstrating that\nour approach has better accuracy than traditional NLP methods when answering\nquestions about patients.\n","authors":["Rakhilya Lee Mekhtieva","Brandon Forbes","Dalal Alrajeh","Brendan Delaney","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2306.17175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05694v1","updated":"2023-10-09T13:15:23Z","published":"2023-10-09T13:15:23Z","title":"A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics","summary":"  The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to datacentered methodologies.\n","authors":["Kai He","Rui Mao","Qika Lin","Yucheng Ruan","Xiang Lan","Mengling Feng","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2310.05694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05688v1","updated":"2023-10-09T12:56:08Z","published":"2023-10-09T12:56:08Z","title":"Larth: Dataset and Machine Translation for Etruscan","summary":"  Etruscan is an ancient language spoken in Italy from the 7th century BC to\nthe 1st century AD. There are no native speakers of the language at the present\nday, and its resources are scarce, as there exist only around 12,000 known\ninscriptions. To the best of our knowledge, there are no publicly available\nEtruscan corpora for natural language processing. Therefore, we propose a\ndataset for machine translation from Etruscan to English, which contains 2891\ntranslated examples from existing academic sources. Some examples are extracted\nmanually, while others are acquired in an automatic way. Along with the\ndataset, we benchmark different machine translation models observing that it is\npossible to achieve a BLEU score of 10.1 with a small transformer model.\nReleasing the dataset can help enable future research on this language, similar\nlanguages or other languages with scarce resources.\n","authors":["Gianluca Vico","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2310.05688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05686v1","updated":"2023-10-09T12:54:58Z","published":"2023-10-09T12:54:58Z","title":"The potential of large language models for improving probability\n  learning: A study on ChatGPT3.5 and first-year computer engineering students","summary":"  In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a\nlarge-scale language model, in solving probability problems typically presented\nin introductory computer engineering exams. Our study comprised a set of 23\nprobability exercises administered to students at Rey Juan Carlos University\n(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group\nof five statistics professors, who assessed them qualitatively and assigned\ngrades based on the same criteria used for students. Our results indicate that\nChatGPT surpasses the average student in terms of phrasing, organization, and\nlogical reasoning. The model's performance remained consistent for both the\nSpanish and English versions of the exercises. However, ChatGPT encountered\ndifficulties in executing basic numerical operations. Our experiments\ndemonstrate that requesting ChatGPT to provide the solution in the form of an R\nscript proved to be an effective approach for overcoming these limitations. In\nsummary, our results indicate that ChatGPT surpasses the average student in\nsolving probability problems commonly presented in introductory computer\nengineering exams. Nonetheless, the model exhibits limitations in reasoning\naround certain probability concepts. The model's ability to deliver\nhigh-quality explanations and illustrate solutions in any programming language,\ncoupled with its performance in solving probability exercises, suggests that\nlarge language models have the potential to serve as learning assistants.\n","authors":["Angel Udias","Antonio Alonso-Ayuso","Ignacio Sanchez","Sonia Hernandez","Maria Eugenia Castellanos","Raquel Montes Diez","Emilio Lopez Cano"],"pdf_url":"https://arxiv.org/pdf/2310.05686v1.pdf","comment":"10 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.05657v1","updated":"2023-10-09T12:12:55Z","published":"2023-10-09T12:12:55Z","title":"A Closer Look into Automatic Evaluation Using Large Language Models","summary":"  Using large language models (LLMs) to evaluate text quality has recently\ngained popularity. Some prior works explore the idea of using LLMs for\nevaluation, while they differ in some details of the evaluation process. In\nthis paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et\nal., 2023), and we discuss how those details in the evaluation process change\nhow well the ratings given by LLMs correlate with human ratings. We find that\nthe auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more\naligned with human ratings. We also show that forcing the LLM to output only a\nnumeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the\nLLM to explain its own ratings consistently improves the correlation between\nthe ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations\non two meta-evaluation datasets.\n","authors":["Cheng-Han Chiang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05657v1.pdf","comment":"EMNLP 2023 findings (short paper). Code:\n  https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/"},{"id":"http://arxiv.org/abs/2310.05650v1","updated":"2023-10-09T12:01:26Z","published":"2023-10-09T12:01:26Z","title":"RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for\n  Hate Speech","summary":"  The Counter Narrative (CN) is a promising approach to combat online hate\nspeech (HS) without infringing on freedom of speech. In recent years, there has\nbeen a growing interest in automatically generating CNs using natural language\ngeneration techniques. However, current automatic CN generation methods mainly\nrely on expert-authored datasets for training, which are time-consuming and\nlabor-intensive to acquire. Furthermore, these methods cannot directly obtain\nand extend counter-knowledge from external statistics, facts, or examples. To\naddress these limitations, we propose Retrieval-Augmented Unsupervised Counter\nNarrative Generation (RAUCG) to automatically expand external counter-knowledge\nand map it into CNs in an unsupervised paradigm. Specifically, we first\nintroduce an SSF retrieval method to retrieve counter-knowledge from the\nmultiple perspectives of stance consistency, semantic overlap rate, and fitness\nfor HS. Then we design an energy-based decoding mechanism by quantizing\nknowledge injection, countering and fluency constraints into differentiable\nfunctions, to enable the model to build mappings from counter-knowledge to CNs\nwithout expert-authored CN data. Lastly, we comprehensively evaluate model\nperformance in terms of language quality, toxicity, persuasiveness, relevance,\nand success rate of countering HS, etc. Experimental results show that RAUCG\noutperforms strong baselines on all metrics and exhibits stronger\ngeneralization capabilities, achieving significant improvements of +2.0% in\nrelevance and +4.5% in success rate of countering metrics. Moreover, RAUCG\nenabled GPT2 to outperform T0 in all metrics, despite the latter being\napproximately eight times larger than the former. Warning: This paper may\ncontain offensive or upsetting content!\n","authors":["Shuyu Jiang","Wenyi Tang","Xingshu Chen","Rui Tanga","Haizhou Wang","Wenxian Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05640v3","updated":"2023-10-09T11:58:55Z","published":"2023-05-09T17:39:45Z","title":"Representation Learning for Person or Entity-centric Knowledge Graphs:\n  An Application in Healthcare","summary":"  Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.\n","authors":["Christos Theodoropoulos","Natasha Mulligan","Thaddeus Stappenbeck","Joao Bettencourt-Silva"],"pdf_url":"https://arxiv.org/pdf/2305.05640v3.pdf","comment":"Accepted into the Twelfth International Conference on Knowledge\n  Capture (K-CAP 2023)"},{"id":"http://arxiv.org/abs/2310.05634v1","updated":"2023-10-09T11:45:59Z","published":"2023-10-09T11:45:59Z","title":"Towards Verifiable Generation: A Benchmark for Knowledge-aware Language\n  Model Attribution","summary":"  Although achieving great success, Large Language Models (LLMs) usually suffer\nfrom unreliable hallucinations. In this paper, we define a new task of\nKnowledge-aware Language Model Attribution (KaLMA) that improves upon three\ncore concerns on conventional attributed LMs. First, we extend attribution\nsource from unstructured texts to Knowledge Graph (KG), whose rich structures\nbenefit both the attribution performance and working scenarios. Second, we\npropose a new ``Conscious Incompetence\" setting considering the incomplete\nknowledge repository, where the model identifies the need for supporting\nknowledge beyond the provided KG. Third, we propose a comprehensive automatic\nevaluation metric encompassing text quality, citation quality, and text\ncitation alignment. To implement the above innovations, we build a dataset in\nbiography domain BioKaLMA via a well-designed evolutionary question generation\nstrategy, to control the question complexity and necessary knowledge to the\nanswer. For evaluation, we develop a baseline solution and demonstrate the room\nfor improvement in LLMs' citation generation, emphasizing the importance of\nincorporating the \"Conscious Incompetence\" setting, and the critical role of\nretrieval accuracy.\n","authors":["Xinze Li","Yixin Cao2","Liangming Pan","Yubo Ma","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2310.05634v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.05628v1","updated":"2023-10-09T11:34:41Z","published":"2023-10-09T11:34:41Z","title":"Glitter or Gold? Deriving Structured Insights from Sustainability\n  Reports via Large Language Models","summary":"  Over the last decade, several regulatory bodies have started requiring the\ndisclosure of non-financial information from publicly listed companies, in\nlight of the investors' increasing attention to Environmental, Social, and\nGovernance (ESG) issues. Such information is publicly released in a variety of\nnon-structured and multi-modal documentation. Hence, it is not straightforward\nto aggregate and consolidate such data in a cohesive framework to further\nderive insights about sustainability practices across companies and markets.\nThus, it is natural to resort to Information Extraction (IE) techniques to\nprovide concise, informative and actionable data to the stakeholders. Moving\nbeyond traditional text processing techniques, in this work we leverage Large\nLanguage Models (LLMs), along with prominent approaches such as Retrieved\nAugmented Generation and in-context learning, to extract semantically\nstructured information from sustainability reports. We then adopt graph-based\nrepresentations to generate meaningful statistical, similarity and correlation\nanalyses concerning the obtained findings, highlighting the prominent\nsustainability actions undertaken across industries and discussing emerging\nsimilarity and disclosing patterns at company, sector and region levels.\nLastly, we investigate which factual aspects impact the most on companies' ESG\nscores using our findings and other company information.\n","authors":["Marco Bronzini","Carlo Nicolini","Bruno Lepri","Andrea Passerini","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2310.05628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05627v1","updated":"2023-10-09T11:34:18Z","published":"2023-10-09T11:34:18Z","title":"Integrating Stock Features and Global Information via Large Language\n  Models for Enhanced Stock Return Prediction","summary":"  The remarkable achievements and rapid advancements of Large Language Models\n(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in\nquantitative investment. Traders can effectively leverage these LLMs to analyze\nfinancial news and predict stock returns accurately. However, integrating LLMs\ninto existing quantitative models presents two primary challenges: the\ninsufficient utilization of semantic information embedded within LLMs and the\ndifficulties in aligning the latent information within LLMs with pre-existing\nquantitative stock features. We propose a novel framework consisting of two\ncomponents to surmount these challenges. The first component, the Local-Global\n(LG) model, introduces three distinct strategies for modeling global\ninformation. These approaches are grounded respectively on stock features, the\ncapabilities of LLMs, and a hybrid method combining the two paradigms. The\nsecond component, Self-Correlated Reinforcement Learning (SCRL), focuses on\naligning the embeddings of financial news generated by LLMs with stock features\nwithin the same semantic space. By implementing our framework, we have\ndemonstrated superior performance in Rank Information Coefficient and returns,\nparticularly compared to models relying only on stock features in the China\nA-share market.\n","authors":["Yujie Ding","Shuai Jia","Tianyi Ma","Bingcheng Mao","Xiuze Zhou","Liuliu Li","Dongming Han"],"pdf_url":"https://arxiv.org/pdf/2310.05627v1.pdf","comment":"8 pages, International Joint Conferences on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2310.05620v1","updated":"2023-10-09T11:19:55Z","published":"2023-10-09T11:19:55Z","title":"LAiW: A Chinese Legal Large Language Models Benchmark (A Technical\n  Report)","summary":"  With the emergence of numerous legal LLMs, there is currently a lack of a\ncomprehensive benchmark for evaluating their legal abilities. In this paper, we\npropose the first Chinese Legal LLMs benchmark based on legal capabilities.\nThrough the collaborative efforts of legal and artificial intelligence experts,\nwe divide the legal capabilities of LLMs into three levels: basic legal NLP\ncapability, basic legal application capability, and complex legal application\ncapability. We have completed the first phase of evaluation, which mainly\nfocuses on the capability of basic legal NLP. The evaluation results show that\nalthough some legal LLMs have better performance than their backbones, there is\nstill a gap compared to ChatGPT. Our benchmark can be found at URL.\n","authors":["Yongfu Dai","Duanyu Feng","Jimin Huang","Haochen Jia","Qianqian Xie","Yifang Zhang","Weiguang Han","Wei Tian","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05619v1","updated":"2023-10-09T11:19:33Z","published":"2023-10-09T11:19:33Z","title":"Dynamic Top-k Estimation Consolidates Disagreement between Feature\n  Attribution Methods","summary":"  Feature attribution scores are used for explaining the prediction of a text\nclassifier to users by highlighting a k number of tokens. In this work, we\npropose a way to determine the number of optimal k tokens that should be\ndisplayed from sequential properties of the attribution scores. Our approach is\ndynamic across sentences, method-agnostic, and deals with sentence length bias.\nWe compare agreement between multiple methods and humans on an NLI task, using\nfixed k and dynamic k. We find that perturbation-based methods and Vanilla\nGradient exhibit highest agreement on most method--method and method--human\nagreement metrics with a static k. Their advantage over other methods\ndisappears with dynamic ks which mainly improve Integrated Gradient and\nGradientXInput. To our knowledge, this is the first evidence that sequential\nproperties of attribution scores are informative for consolidating attribution\nsignals for human interpretation.\n","authors":["Jonathan Kamp","Lisa Beinborn","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2310.05619v1.pdf","comment":"Short paper accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2211.07624v2","updated":"2023-10-09T10:38:04Z","published":"2022-11-14T18:47:26Z","title":"Semantic Similarity Models for Depression Severity Estimation","summary":"  Depressive disorders constitute a severe public health issue worldwide.\nHowever, public health systems have limited capacity for case detection and\ndiagnosis. In this regard, the widespread use of social media has opened up a\nway to access public information on a large scale. Computational methods can\nserve as support tools for rapid screening by exploiting this user-generated\nsocial media content. This paper presents an efficient semantic pipeline to\nstudy depression severity in individuals based on their social media writings.\nWe select test user sentences for producing semantic rankings over an index of\nrepresentative training sentences corresponding to depressive symptoms and\nseverity levels. Then, we use the sentences from those results as evidence for\npredicting users' symptom severity. For that, we explore different aggregation\nmethods to answer one of four Beck Depression Inventory (BDI) options per\nsymptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\\%\nimprovement over state of the art in terms of measuring depression severity.\n","authors":["Anxo Prez","Neha Warikoo","Kexin Wang","Javier Parapar","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2211.07624v2.pdf","comment":"Accepted at the EMNLP 2023 conference"},{"id":"http://arxiv.org/abs/2310.05597v1","updated":"2023-10-09T10:34:38Z","published":"2023-10-09T10:34:38Z","title":"Can language models learn analogical reasoning? Investigating training\n  objectives and comparisons to human performance","summary":"  While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.\n","authors":["Molly R. Petersen","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2310.05597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05592v1","updated":"2023-10-09T10:27:26Z","published":"2023-10-09T10:27:26Z","title":"InterroLang: Exploring NLP Models and Datasets through Dialogue-based\n  Explanations","summary":"  While recently developed NLP explainability methods let us open the black box\nin various ways (Madsen et al., 2022), a missing ingredient in this endeavor is\nan interactive tool offering a conversational interface. Such a dialogue system\ncan help users explore datasets and models with explanations in a\ncontextualized manner, e.g. via clarification or follow-up questions, and\nthrough a natural language interface. We adapt the conversational explanation\nframework TalkToModel (Slack et al., 2022) to the NLP domain, add new\nNLP-specific operations such as free-text rationalization, and illustrate its\ngeneralizability on three NLP tasks (dialogue act classification, question\nanswering, hate speech detection). To recognize user queries for explanations,\nwe evaluate fine-tuned and few-shot prompting models and implement a novel\nAdapter-based approach. We then conduct two user studies on (1) the perceived\ncorrectness and helpfulness of the dialogues, and (2) the simulatability, i.e.\nhow objectively helpful dialogical explanations are for humans in figuring out\nthe model's predicted label when it's not shown. We found rationalization and\nfeature attribution were helpful in explaining the model behavior. Moreover,\nusers could more reliably predict the model outcome based on an explanation\ndialogue rather than one-off explanations.\n","authors":["Nils Feldhus","Qianli Wang","Tatiana Anikina","Sahil Chopra","Cennet Oguz","Sebastian Mller"],"pdf_url":"https://arxiv.org/pdf/2310.05592v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.05589v1","updated":"2023-10-09T10:21:42Z","published":"2023-10-09T10:21:42Z","title":"DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) is a task that aims to link ambiguous\nmentions within multimodal contexts to referential entities in a multimodal\nknowledge base. Recent methods for MEL adopt a common framework: they first\ninteract and fuse the text and image to obtain representations of the mention\nand entity respectively, and then compute the similarity between them to\npredict the correct entity. However, these methods still suffer from two\nlimitations: first, as they fuse the features of text and image before\nmatching, they cannot fully exploit the fine-grained alignment relations\nbetween the mention and entity. Second, their alignment is static, leading to\nlow performance when dealing with complex and diverse data. To address these\nissues, we propose a novel framework called Dynamic Relation Interactive\nNetwork (DRIN) for MEL tasks. DRIN explicitly models four different types of\nalignment between a mention and entity and builds a dynamic Graph Convolutional\nNetwork (GCN) to dynamically select the corresponding alignment relations for\ndifferent input samples. Experiments on two datasets show that DRIN outperforms\nstate-of-the-art methods by a large margin, demonstrating the effectiveness of\nour approach.\n","authors":["Shangyu Xing","Fei Zhao","Zhen Wu","Chunhui Li","Jianbing Zhang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2310.05589v1.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.05553v1","updated":"2023-10-09T09:22:40Z","published":"2023-10-09T09:22:40Z","title":"Regulation and NLP (RegNLP): Taming Large Language Models","summary":"  The scientific innovation in Natural Language Processing (NLP) and more\nbroadly in artificial intelligence (AI) is at its fastest pace to date. As\nlarge language models (LLMs) unleash a new era of automation, important debates\nemerge regarding the benefits and risks of their development, deployment and\nuse. Currently, these debates have been dominated by often polarized narratives\nmainly led by the AI Safety and AI Ethics movements. This polarization, often\namplified by social media, is swaying political agendas on AI regulation and\ngovernance and posing issues of regulatory capture. Capture occurs when the\nregulator advances the interests of the industry it is supposed to regulate, or\nof special interest groups rather than pursuing the general public interest.\nMeanwhile in NLP research, attention has been increasingly paid to the\ndiscussion of regulating risks and harms. This often happens without systematic\nmethodologies or sufficient rooting in the disciplines that inspire an extended\nscope of NLP research, jeopardizing the scientific integrity of these\nendeavors. Regulation studies are a rich source of knowledge on how to\nsystematically deal with risk and uncertainty, as well as with scientific\nevidence, to evaluate and compare regulatory options. This resource has largely\nremained untapped so far. In this paper, we argue how NLP research on these\ntopics can benefit from proximity to regulatory studies and adjacent fields. We\ndo so by discussing basic tenets of regulation, and risk and uncertainty, and\nby highlighting the shortcomings of current NLP discussions dealing with risk\nassessment. Finally, we advocate for the development of a new multidisciplinary\nresearch space on regulation and NLP (RegNLP), focused on connecting scientific\nknowledge to regulatory processes based on systematic methodologies.\n","authors":["Catalina Goanta","Nikolaos Aletras","Ilias Chalkidis","Sofia Ranchordas","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2310.05553v1.pdf","comment":"9 pages, long paper at EMNLP 2023 proceedings"},{"id":"http://arxiv.org/abs/2310.05513v1","updated":"2023-10-09T08:30:01Z","published":"2023-10-09T08:30:01Z","title":"Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation\n  over More Languages and Beyond","summary":"  The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB)\nChallenge expands upon the acclaimed SUPERB framework, emphasizing\nself-supervised models in multilingual speech recognition and language\nidentification. The challenge comprises a research track focused on applying\nML-SUPERB to specific multilingual subjects, a Challenge Track for model\nsubmissions, and a New Language Track where language resource researchers can\ncontribute and evaluate their low-resource language data in the context of the\nlatest progress in multilingual speech recognition. The challenge garnered 12\nmodel submissions and 54 language corpora, resulting in a comprehensive\nbenchmark encompassing 154 languages. The findings indicate that merely scaling\nmodels is not the definitive solution for multilingual speech tasks, and a\nvariety of speech/voice types present significant challenges in multilingual\nspeech processing.\n","authors":["Jiatong Shi","William Chen","Dan Berrebbi","Hsiu-Hsuan Wang","Wei-Ping Huang","En-Pei Hu","Ho-Lam Chuang","Xuankai Chang","Yuxun Tang","Shang-Wen Li","Abdelrahman Mohamed","Hung-yi Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2310.05513v1.pdf","comment":"Accepted by ASRU"},{"id":"http://arxiv.org/abs/2310.05506v1","updated":"2023-10-09T08:18:58Z","published":"2023-10-09T08:18:58Z","title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning\n  Generalization","summary":"  In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,\nby complicating and diversifying the queries from GSM8K and sampling multiple\nreasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning\non subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art\non GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the\nscale of 13B). A log-linear relationship is presented between MuggleMath's\nperformance and the amount of augmented data. We also find that MuggleMath is\nweak in out-of-domain math reasoning generalization to MATH. This is attributed\nto the differences in query distribution between AugGSM8K and MATH which\nsuggest that augmentation on a single benchmark could not help with overall\nmath reasoning performance. Codes and AugGSM8K will be uploaded to\nhttps://github.com/OFA-Sys/gsm8k-ScRel.\n","authors":["Chengpeng Li","Zheng Yuan","Guanting Dong","Keming Lu","Jiancan Wu","Chuanqi Tan","Xiang Wang","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05506v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2309.04106v2","updated":"2023-10-09T08:09:17Z","published":"2023-09-08T03:58:05Z","title":"Meta predictive learning model of languages in neural circuits","summary":"  Large language models based on self-attention mechanisms have achieved\nastonishing performances not only in natural language itself, but also in a\nvariety of tasks of different nature. However, regarding processing language,\nour human brain may not operate using the same principle. Then, a debate is\nestablished on the connection between brain computation and artificial\nself-supervision adopted in large language models. One of most influential\nhypothesis in brain computation is the predictive coding framework, which\nproposes to minimize the prediction error by local learning. However, the role\nof predictive coding and the associated credit assignment in language\nprocessing remains unknown. Here, we propose a mean-field learning model within\nthe predictive coding framework, assuming that the synaptic weight of each\nconnection follows a spike and slab distribution, and only the distribution,\nrather than specific weights, is trained. This meta predictive learning is\nsuccessfully validated on classifying handwritten digits where pixels are input\nto the network in sequence, and moreover on the toy and real language corpus.\nOur model reveals that most of the connections become deterministic after\nlearning, while the output connections have a higher level of variability. The\nperformance of the resulting network ensemble changes continuously with data\nload, further improving with more training data, in analogy with the emergent\nbehavior of large language models. Therefore, our model provides a starting\npoint to investigate the connection among brain computation, next-token\nprediction and general intelligence.\n","authors":["Chan Li","Junbin Qiu","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2309.04106v2.pdf","comment":"31 pages, 6 figures, codes are available in the main text with the\n  link"},{"id":"http://arxiv.org/abs/2310.05502v1","updated":"2023-10-09T08:07:04Z","published":"2023-10-09T08:07:04Z","title":"XAL: EXplainable Active Learning Makes Classifiers Better Low-resource\n  Learners","summary":"  Active learning aims to construct an effective training set by iteratively\ncurating the most informative unlabeled data for annotation, which is practical\nin low-resource tasks. Most active learning techniques in classification rely\non the model's uncertainty or disagreement to choose unlabeled data. However,\nprevious work indicates that existing models are poor at quantifying predictive\nuncertainty, which can lead to over-confidence in superficial patterns and a\nlack of exploration. Inspired by the cognitive processes in which humans deduce\nand predict through causal information, we propose a novel Explainable Active\nLearning framework (XAL) for low-resource text classification, which aims to\nencourage classifiers to justify their inferences and delve into unlabeled data\nfor which they cannot provide reasonable explanations. Specifically, besides\nusing a pre-trained bi-directional encoder for classification, we employ a\npre-trained uni-directional decoder to generate and score the explanation. A\nranking loss is proposed to enhance the decoder's capability in scoring\nexplanations. During the selection of unlabeled data, we combine the predictive\nuncertainty of the encoder and the explanation score of the decoder to acquire\ninformative data for annotation.\n  As XAL is a general framework for text classification, we test our methods on\nsix different classification tasks. Extensive experiments show that XAL\nachieves substantial improvement on all six tasks over previous AL methods.\nAblation studies demonstrate the effectiveness of each component, and human\nevaluation shows that the model trained in XAL performs surprisingly well in\nexplaining its prediction.\n","authors":["Yun Luo","Zhen Yang","Fandong Meng","Yingjie Li","Fang Guo","Qinglin Qi","Jie Zhou","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05492v1","updated":"2023-10-09T07:56:16Z","published":"2023-10-09T07:56:16Z","title":"How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition","summary":"  Large language models (LLMs) with enormous pre-training tokens and parameter\namounts emerge abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each\nability, while proprietary LLMs are versatile for all abilities. It is\nimportant to investigate how to unlock them with multiple abilities via SFT. In\nthis study, we specifically focus on the data composition between mathematical\nreasoning, code generation, and general human-aligning abilities during SFT.\nFrom a scaling perspective, we investigate the relationship between model\nabilities and various factors including data amounts, data composition ratio,\nmodel parameters, and SFT strategies. Our experiments reveal that different\nabilities exhibit different scaling patterns, and larger models generally show\nsuperior performance with the same amount of data. Mathematical reasoning and\ncode generation improve as data amounts increase consistently, while the\ngeneral ability is enhanced with about a thousand samples and improves slowly.\nWe find data composition results in various abilities improvements with low\ndata amounts, while conflicts of abilities with high data amounts. Our\nexperiments further show that composition data amount impacts performance,\nwhile the influence of composition ratio is insignificant. Regarding the SFT\nstrategies, we evaluate sequential learning multiple abilities are prone to\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy learns specialized abilities first and then learns general abilities\nwith a small amount of specialized data to prevent forgetting, offering a\npromising solution to learn multiple abilities with different scaling patterns.\n","authors":["Guanting Dong","Hongyi Yuan","Keming Lu","Chengpeng Li","Mingfeng Xue","Dayiheng Liu","Wei Wang","Zheng Yuan","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05492v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.13225v2","updated":"2023-10-09T07:55:26Z","published":"2023-05-22T16:56:44Z","title":"Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A\n  Preliminary Study on Writing Assistance","summary":"  Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered\nsignificant attention due to their exceptional capabilities in handling a\ndiverse range of tasks. Recent studies demonstrate that open-sourced smaller\nfoundational models, such as 7B-size LLaMA, can also display remarkable\nproficiency in tackling diverse tasks when fine-tuned using instruction-driven\ndata. In this work, we investigate a practical problem setting where the\nprimary focus is on one or a few particular tasks rather than general-purpose\ninstruction following, and explore whether LLMs can be beneficial and further\nimproved for such targeted scenarios. We choose the writing-assistant scenario\nas the testbed, which includes seven writing tasks. We collect training data\nfor these tasks, reframe them in an instruction-following format, and\nsubsequently refine the LLM, specifically LLaMA, via instruction tuning.\nExperimental results show that fine-tuning LLaMA on writing instruction data\nsignificantly improves its ability on writing tasks. We also conduct more\nexperiments and analyses to offer insights for future work on effectively\nfine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion\nregarding the necessity of employing LLMs for only one targeted task, taking\ninto account the efforts required for tuning and the resources consumed during\ndeployment.\n","authors":["Yue Zhang","Leyang Cui","Deng Cai","Xinting Huang","Tao Fang","Wei Bi"],"pdf_url":"https://arxiv.org/pdf/2305.13225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05484v1","updated":"2023-10-09T07:43:57Z","published":"2023-10-09T07:43:57Z","title":"IDTraffickers: An Authorship Attribution Dataset to link and connect\n  Potential Human-Trafficking Operations on Text Escort Advertisements","summary":"  Human trafficking (HT) is a pervasive global issue affecting vulnerable\nindividuals, violating their fundamental human rights. Investigations reveal\nthat a significant number of HT cases are associated with online advertisements\n(ads), particularly in escort markets. Consequently, identifying and connecting\nHT vendors has become increasingly challenging for Law Enforcement Agencies\n(LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset\nconsisting of 87,595 text ads and 5,244 vendor labels to enable the\nverification and identification of potential HT vendors on online escort\nmarkets. To establish a benchmark for authorship identification, we train a\nDeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set\nclassification environment. Next, we leverage the style representations\nextracted from the trained classifier to conduct authorship verification,\nresulting in a mean r-precision score of 0.8852 in an open-set ranking\nenvironment. Finally, to encourage further research and ensure responsible data\nsharing, we plan to release IDTraffickers for the authorship attribution task\nto researchers under specific conditions, considering the sensitive nature of\nthe data. We believe that the availability of our dataset and benchmarks will\nempower future researchers to utilize our findings, thereby facilitating the\neffective linkage of escort ads and the development of more robust approaches\nfor identifying HT indicators.\n","authors":["Vageesh Saxena","Benjamin Bashpole","Gijs Van Dijck","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2310.05484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05481v1","updated":"2023-10-09T07:41:19Z","published":"2023-10-09T07:41:19Z","title":"Cabbage Sweeter than Cake? Analysing the Potential of Large Language\n  Models for Learning Conceptual Spaces","summary":"  The theory of Conceptual Spaces is an influential cognitive-linguistic\nframework for representing the meaning of concepts. Conceptual spaces are\nconstructed from a set of quality dimensions, which essentially correspond to\nprimitive perceptual features (e.g. hue or size). These quality dimensions are\nusually learned from human judgements, which means that applications of\nconceptual spaces tend to be limited to narrow domains (e.g. modelling colour\nor taste). Encouraged by recent findings about the ability of Large Language\nModels (LLMs) to learn perceptually grounded representations, we explore the\npotential of such models for learning conceptual spaces. Our experiments show\nthat LLMs can indeed be used for learning meaningful representations to some\nextent. However, we also find that fine-tuned models of the BERT family are\nable to match or even outperform the largest GPT-3 model, despite being 2 to 3\norders of magnitude smaller.\n","authors":["Usashi Chatterjee","Amit Gajbhiye","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2310.05481v1.pdf","comment":"Accepted for EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.14717v2","updated":"2023-10-09T07:39:04Z","published":"2023-09-26T07:22:23Z","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","summary":"  Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.\n","authors":["Yuhui Xu","Lingxi Xie","Xiaotao Gu","Xin Chen","Heng Chang","Hengheng Zhang","Zhengsu Chen","Xiaopeng Zhang","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14717v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2305.14282v2","updated":"2023-10-09T07:29:54Z","published":"2023-05-23T17:27:22Z","title":"INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained\n  Feedback","summary":"  Automatically evaluating the quality of language generation is critical.\nAlthough recent learned metrics show high correlation with human judgement,\nthese metrics can not explain their verdict or associate the scores with\ndefects in generated text. To address this limitation, we present\nInstructScore, an explainable evaluation metric for text generation. By\nharnessing both explicit human instruction and the implicit knowledge of GPT-4,\nwe fine-tune a text evaluation metric based on LLaMA, producing both a score\nfor generated text and a human readable diagnostic report. We evaluate\nInstructScore on a variety of generation tasks, including translation,\ncaptioning, data-to-text and commonsense generation. Experiments show that our\n7B model surpasses all other unsupervised metrics, including those based on\n175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct\nsupervision from human-rated data, achieves performance levels on par with\nstate-of-the-art metrics like COMET22, which were fine-tuned on human ratings.\n","authors":["Wenda Xu","Danqing Wang","Liangming Pan","Zhenqiao Song","Markus Freitag","William Yang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2305.14282v2.pdf","comment":"Accepted to EMNLP2023 Main Conference"},{"id":"http://arxiv.org/abs/2310.05470v1","updated":"2023-10-09T07:27:15Z","published":"2023-10-09T07:27:15Z","title":"Generative Judge for Evaluating Alignment","summary":"  The rapid development of Large Language Models (LLMs) has substantially\nexpanded the range of tasks they can address. In the field of Natural Language\nProcessing (NLP), researchers have shifted their focus from conventional NLP\ntasks (e.g., sequence tagging and parsing) towards tasks that revolve around\naligning with human needs (e.g., brainstorming and email writing). This shift\nin task distribution imposes new requirements on evaluating these aligned\nmodels regarding generality (i.e., assessing performance across diverse\nscenarios), flexibility (i.e., examining under different protocols), and\ninterpretability (i.e., scrutinizing models with explanations). In this paper,\nwe propose a generative judge with 13B parameters, Auto-J, designed to address\nthese challenges. Our model is trained on user queries and LLM-generated\nresponses under massive real-world scenarios and accommodates diverse\nevaluation protocols (e.g., pairwise response comparison and single-response\nevaluation) with well-structured natural language critiques. To demonstrate the\nefficacy of our approach, we construct a new testbed covering 58 different\nscenarios. Experimentally, Auto-J outperforms a series of strong competitors,\nincluding both open-source and closed-source models, by a large margin. We also\nprovide detailed analysis and case studies to further reveal the potential of\nour method and make a variety of resources public at\nhttps://github.com/GAIR-NLP/auto-j.\n","authors":["Junlong Li","Shichao Sun","Weizhe Yuan","Run-Ze Fan","Hai Zhao","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14835v2","updated":"2023-10-09T07:26:40Z","published":"2023-05-24T07:40:06Z","title":"SummIt: Iterative Text Summarization via ChatGPT","summary":"  Text summarization systems have made significant progress in recent years,\nbut typically generate summaries in one single step. However, the one-shot\nsummarization setting is sometimes inadequate, as the generated summary may\ncontain hallucinations or overlook essential details related to the reader's\ninterests. This paper addresses this limitation by proposing SummIt, an\niterative text summarization framework based on large language models like\nChatGPT. Our framework enables the model to refine the generated summary\niteratively through self-evaluation and feedback, resembling humans' iterative\nprocess when drafting and revising summaries. Furthermore, we explore the\npotential benefits of integrating knowledge and topic extractors into the\nframework to enhance summary faithfulness and controllability. We automatically\nevaluate the performance of our framework on three benchmark summarization\ndatasets. We also conduct a human evaluation to validate the effectiveness of\nthe iterative refinements and identify a potential issue of over-correction.\n","authors":["Haopeng Zhang","Xiao Liu","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.14835v2.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2307.10864v2","updated":"2023-10-09T07:20:00Z","published":"2023-07-20T13:33:28Z","title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing","summary":"  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., ``a cat and a dog''. However, its efficacy declines when dealing with\nmore complex prompts, and it does not explicitly address the problem of\nimproper attribute binding. To address the challenges posed by complex prompts\nor scenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n","authors":["Yumeng Li","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2307.10864v2.pdf","comment":"Accepted at BMVC 2023 as Oral. Code:\n  https://github.com/boschresearch/Divide-and-Bind and project page:\n  https://sites.google.com/view/divide-and-bind"},{"id":"http://arxiv.org/abs/2310.01432v2","updated":"2023-10-09T07:19:10Z","published":"2023-09-29T14:38:58Z","title":"Split and Merge: Aligning Position Biases in Large Language Model based\n  Evaluators","summary":"  Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.\n","authors":["Zongjie Li","Chaozheng Wang","Pingchuan Ma","Daoyuan Wu","Shuai Wang","Cuiyun Gao","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.01432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15413v3","updated":"2023-10-09T07:02:43Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation","summary":"  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v3.pdf","comment":"Accepted to NeurIPS 2023. Project Page:\n  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/"},{"id":"http://arxiv.org/abs/2310.05452v1","updated":"2023-10-09T06:57:45Z","published":"2023-10-09T06:57:45Z","title":"Explaining the Complex Task Reasoning of Large Language Models with\n  Template-Content Structure","summary":"  The continuous evolution of pre-trained large language models with\never-growing parameters and corpus sizes has augmented their capacity to solve\ncomplex tasks. This ability, which obviates the necessity for task-specific\ntraining or fine-tuning, relies on providing the model with a language\ndescription or some task exemplars -- referred to the prompt -- that guide the\ndesired autoregressive generation. Despite the remarkable success, the\nunderlying mechanisms that facilitate such exceptional generalization abilities\nremain an open question. In this paper, we present a novel framework that\nformally conceptualizes answer generation for complex natural language tasks as\na hierarchical ``template-content'' structure. According to our modeling, there\nexist pre-trained models that can automatically decompose tasks into\nconstituent steps during autoregressive generation, through language modeling\non a sufficiently large corpus, thereby solving them. Our framework offers an\nexplanatory tool for the complex reasoning abilities of large language models\nfrom the perspective of modeling autoregressive generation tasks. Our\nexperiments show that practical models exhibit different behaviors for\n``template'' and ``content'' providing support for our modeling.\n","authors":["Haotong Yang","Fanxu Meng","Zhouchen Lin","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05450v1","updated":"2023-10-09T06:54:02Z","published":"2023-10-09T06:54:02Z","title":"Empower Nested Boolean Logic via Self-Supervised Curriculum Learning","summary":"  Beyond the great cognitive powers showcased by language models, it is crucial\nto scrutinize whether their reasoning capabilities stem from strong\ngeneralization or merely exposure to relevant data. As opposed to constructing\nincreasingly complex logic, this paper probes into the boolean logic, the root\ncapability of a logical reasoner. We find that any pre-trained language models\neven including large language models only behave like a random selector in the\nface of multi-nested boolean logic, a task that humans can handle with ease. To\nempower language models with this fundamental capability, this paper proposes a\nnew self-supervised learning method \\textit{Curriculum Logical Reasoning}\n(\\textsc{Clr}), where we augment the training data with nested boolean logic\nchain step-by-step, and program the training from simpler logical patterns\ngradually to harder ones. This new training paradigm allows language models to\neffectively generalize to much harder and longer-hop logic, which can hardly be\nlearned through naive training. Furthermore, we show that boolean logic is a\ngreat foundation for improving the subsequent general logical tasks.\n","authors":["Hongqiu Wu","Linfeng Liu","Hai Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05450v1.pdf","comment":"Accepted by EMNLP2023"},{"id":"http://arxiv.org/abs/2310.05442v1","updated":"2023-10-09T06:32:10Z","published":"2023-10-09T06:32:10Z","title":"Establishing Trustworthiness: Rethinking Tasks and Model Evaluation","summary":"  Language understanding is a multi-faceted cognitive capability, which the\nNatural Language Processing (NLP) community has striven to model\ncomputationally for decades. Traditionally, facets of linguistic intelligence\nhave been compartmentalized into tasks with specialized model architectures and\ncorresponding evaluation protocols. With the advent of large language models\n(LLMs) the community has witnessed a dramatic shift towards general purpose,\ntask-agnostic approaches powered by generative models. As a consequence, the\ntraditional compartmentalized notion of language tasks is breaking down,\nfollowed by an increasing challenge for evaluation and analysis. At the same\ntime, LLMs are being deployed in more real-world scenarios, including\npreviously unforeseen zero-shot setups, increasing the need for trustworthy and\nreliable systems. Therefore, we argue that it is time to rethink what\nconstitutes tasks and model evaluation in NLP, and pursue a more holistic view\non language, placing trustworthiness at the center. Towards this goal, we\nreview existing compartmentalized approaches for understanding the origins of a\nmodel's functional capacity, and provide recommendations for more multi-faceted\nevaluation protocols.\n","authors":["Robert Litschko","Max Mller-Eberstein","Rob van der Goot","Leon Weber","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2310.05442v1.pdf","comment":"Accepted at EMNLP 2023 (Main Conference)"},{"id":"http://arxiv.org/abs/2310.05424v1","updated":"2023-10-09T05:53:05Z","published":"2023-10-09T05:53:05Z","title":"Fast and Robust Early-Exiting Framework for Autoregressive Language\n  Models with Synchronized Parallel Decoding","summary":"  To tackle the high inference latency exhibited by autoregressive language\nmodels, previous studies have proposed an early-exiting framework that\nallocates adaptive computation paths for each token based on the complexity of\ngenerating the subsequent token. However, we observed several shortcomings,\nincluding performance degradation caused by a state copying mechanism or\nnumerous exit paths, and sensitivity to exit confidence thresholds.\nConsequently, we propose a Fast and Robust Early-Exiting (FREE) framework,\nwhich incorporates a shallow-deep module and a synchronized parallel decoding.\nOur framework enables faster inference by synchronizing the decoding process of\nthe current token with previously stacked early-exited tokens. Furthermore, as\nparallel decoding allows us to observe predictions from both shallow and deep\nmodels, we present a novel adaptive threshold estimator that exploits a Beta\nmixture model to determine suitable confidence thresholds. We empirically\ndemonstrated the superiority of our proposed framework on extensive generation\ntasks.\n","authors":["Sangmin Bae","Jongwoo Ko","Hwanjun Song","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2310.05424v1.pdf","comment":"EMNLP 2023 (Long)"},{"id":"http://arxiv.org/abs/2310.05421v1","updated":"2023-10-09T05:35:10Z","published":"2023-10-09T05:35:10Z","title":"Automating Customer Service using LangChain: Building custom open-source\n  GPT Chatbot for organizations","summary":"  In the digital age, the dynamics of customer service are evolving, driven by\ntechnological advancements and the integration of Large Language Models (LLMs).\nThis research paper introduces a groundbreaking approach to automating customer\nservice using LangChain, a custom LLM tailored for organizations. The paper\nexplores the obsolescence of traditional customer support techniques,\nparticularly Frequently Asked Questions (FAQs), and proposes a paradigm shift\ntowards responsive, context-aware, and personalized customer interactions. The\nheart of this innovation lies in the fusion of open-source methodologies, web\nscraping, fine-tuning, and the seamless integration of LangChain into customer\nservice platforms. This open-source state-of-the-art framework, presented as\n\"Sahaay,\" demonstrates the ability to scale across industries and\norganizations, offering real-time support and query resolution. Key elements of\nthis research encompass data collection via web scraping, the role of\nembeddings, the utilization of Google's Flan T5 XXL, Base and Small language\nmodels for knowledge retrieval, and the integration of the chatbot into\ncustomer service platforms. The results section provides insights into their\nperformance and use cases, here particularly within an educational institution.\nThis research heralds a new era in customer service, where technology is\nharnessed to create efficient, personalized, and responsive interactions.\nSahaay, powered by LangChain, redefines the customer-company relationship,\nelevating customer retention, value extraction, and brand image. As\norganizations embrace LLMs, customer service becomes a dynamic and\ncustomer-centric ecosystem.\n","authors":["Keivalya Pandya","Mehfuza Holia"],"pdf_url":"https://arxiv.org/pdf/2310.05421v1.pdf","comment":"4 pages, 2 figures, Submitted to appear in the Proceedings of the 3rd\n  International Conference on Women in Science & Technology Creating\n  Sustainable Career (ICWSTCSC 2023)"},{"id":"http://arxiv.org/abs/2310.05418v1","updated":"2023-10-09T05:30:42Z","published":"2023-10-09T05:30:42Z","title":"Humanoid Agents: Platform for Simulating Human-like Generative Agents","summary":"  Just as computational simulations of atoms, molecules and cells have shaped\nthe way we study the sciences, true-to-life simulations of human-like agents\ncan be valuable tools for studying human behavior. We propose Humanoid Agents,\na system that guides Generative Agents to behave more like humans by\nintroducing three elements of System 1 processing: Basic needs (e.g. hunger,\nhealth and energy), Emotion and Closeness in Relationships. Humanoid Agents are\nable to use these dynamic elements to adapt their daily activities and\nconversations with other agents, as supported with empirical experiments. Our\nsystem is designed to be extensible to various settings, three of which we\ndemonstrate, as well as to other elements influencing human behavior (e.g.\nempathy, moral values and cultural background). Our platform also includes a\nUnity WebGL game interface for visualization and an interactive analytics\ndashboard to show agent statuses over time. Our platform is available on\nhttps://www.humanoidagents.com/ and code is on\nhttps://github.com/HumanoidAgents/HumanoidAgents\n","authors":["Zhilin Wang","Yu Ying Chiu","Yu Cheung Chiu"],"pdf_url":"https://arxiv.org/pdf/2310.05418v1.pdf","comment":"Accepted at EMNLP System Demonstrations 2023"},{"id":"http://arxiv.org/abs/2211.06869v4","updated":"2023-10-09T05:08:23Z","published":"2022-11-13T10:16:39Z","title":"Large Language Models Meet Harry Potter: A Bilingual Dataset for\n  Aligning Dialogue Agents with Characters","summary":"  In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.\n","authors":["Nuo Chen","Yan Wang","Haiyun Jiang","Deng Cai","Yuhan Li","Ziyang Chen","Longyue Wang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2211.06869v4.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2302.08817v2","updated":"2023-10-09T04:57:56Z","published":"2023-02-17T11:31:05Z","title":"Natural Response Generation for Chinese Reading Comprehension","summary":"  Machine reading comprehension (MRC) is an important area of conversation\nagents and draws a lot of attention. However, there is a notable limitation to\ncurrent MRC benchmarks: The labeled answers are mostly either spans extracted\nfrom the target corpus or the choices of the given candidates, ignoring the\nnatural aspect of high-quality responses. As a result, MRC models trained on\nthese datasets can not generate human-like responses in real QA scenarios. To\nthis end, we construct a new dataset called Penguin to promote the research of\nMRC, providing a training and test bed for natural response generation to real\nscenarios. Concretely, Penguin consists of 200k training data with high-quality\nfluent, and well-informed responses. Penguin is the first benchmark towards\nnatural response generation in Chinese MRC on a relatively large scale. To\naddress the challenges in Penguin, we develop two strong baselines: end-to-end\nand two-stage frameworks. Following that, we further design Prompt-BART:\nfine-tuning the pre-trained generative language models with a mixture of prefix\nprompts in Penguin. Extensive experiments validated the effectiveness of this\ndesign.\n","authors":["Nuo Chen","Hongguang Li","Yinan Bao","Baoyuan Wang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2302.08817v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2310.05404v1","updated":"2023-10-09T04:48:14Z","published":"2023-10-09T04:48:14Z","title":"mBBC: Exploring the Multilingual Maze","summary":"  Multilingual language models have gained significant attention in recent\nyears, enabling the development of applications that cater to diverse\nlinguistic contexts. In this paper, we present a comprehensive evaluation of\nthree prominent multilingual language models: mBERT, XLM-R, and GPT-3. Using\nthe self-supervised task of next token prediction, we assess their performance\nacross a diverse set of languages, with a focus on understanding the impact of\nresource availability, word order, language family, and script type on model\naccuracy. Our findings reveal that resource availability plays a crucial role\nin model performance, with higher resource levels leading to improved accuracy.\nWe also identify the complex relationship between resource availability,\nlanguage families, and script types, highlighting the need for further\ninvestigation into language-specific characteristics and structural variations.\nAdditionally, our statistical inference analysis identifies significant\nfeatures contributing to model performance, providing insights for model\nselection and deployment. Our study contributes to a deeper understanding of\nmultilingual language models and informs future research and development to\nenhance their performance and generalizability across languages and linguistic\ncontexts.\n","authors":["Sina Bagheri Nezhad","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2310.05404v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2310.05388v1","updated":"2023-10-09T03:55:55Z","published":"2023-10-09T03:55:55Z","title":"GROVE: A Retrieval-augmented Complex Story Generation Framework with A\n  Forest of Evidence","summary":"  Conditional story generation is significant in human-machine interaction,\nparticularly in producing stories with complex plots. While Large language\nmodels (LLMs) perform well on multiple NLP tasks, including story generation,\nit is challenging to generate stories with both complex and creative plots.\nExisting methods often rely on detailed prompts to guide LLMs to meet target\nconditions, which inadvertently restrict the creative potential of the\ngenerated stories. We argue that leveraging information from exemplary\nhuman-written stories facilitates generating more diverse plotlines. Delving\ndeeper into story details helps build complex and credible plots. In this\npaper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation\nframework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to\nenhance stories' complexity. We build a retrieval repository for target\nconditions to produce few-shot examples to prompt LLMs. Additionally, we design\nan ``asking-why'' prompting scheme that extracts a forest of evidence,\nproviding compensation for the ambiguities that may occur in the generated\nstory. This iterative process uncovers underlying story backgrounds. Finally,\nwe select the most fitting chains of evidence from the evidence forest and\nintegrate them into the generated story, thereby enhancing the narrative's\ncomplexity and credibility. Experimental results and numerous examples verify\nthe effectiveness of our method.\n","authors":["Zhihua Wen","Zhiliang Tian","Wei Wu","Yuxin Yang","Yanqi Shi","Zhen Huang","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2310.05388v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2105.02605v3","updated":"2023-10-09T03:36:29Z","published":"2021-05-06T12:20:41Z","title":"GraphFormers: GNN-nested Transformers for Representation Learning on\n  Textual Graph","summary":"  The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.\n","authors":["Junhan Yang","Zheng Liu","Shitao Xiao","Chaozhuo Li","Defu Lian","Sanjay Agrawal","Amit Singh","Guangzhong Sun","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2105.02605v3.pdf","comment":"Accepted to NeurIPS 2021"},{"id":"http://arxiv.org/abs/2310.05381v1","updated":"2023-10-09T03:34:15Z","published":"2023-10-09T03:34:15Z","title":"CCAE: A Corpus of Chinese-based Asian Englishes","summary":"  Language models have been foundations in various scenarios of NLP\napplications, but it has not been well applied in language variety studies,\neven for the most popular language like English. This paper represents one of\nthe few initial efforts to utilize the NLP technology in the paradigm of World\nEnglishes, specifically in creating a multi-variety corpus for studying Asian\nEnglishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian\nEnglish, a suite of corpora comprising six Chinese-based Asian English\nvarieties. It is based on 340 million tokens in 448 thousand web documents from\nsix regions. The ontology of data would make the corpus a helpful resource with\nenormous research potential for Asian Englishes (especially for Chinese\nEnglishes for which there has not been a publicly accessible corpus yet so far)\nand an ideal source for variety-specific language modeling and downstream\ntasks, thus setting the stage for NLP-based World Englishes studies. And\npreliminary experiments on this corpus reveal the practical value of CCAE.\nFinally, we make CCAE available at\n\\href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.\n","authors":["Yang Liu","Melissa Xiaohui Qin","Long Wang","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05381v1.pdf","comment":"NLPCC'2023 (12 pages, 3 figures, 4 charts)"},{"id":"http://arxiv.org/abs/2309.07124v2","updated":"2023-10-09T03:34:01Z","published":"2023-09-13T17:59:09Z","title":"RAIN: Your Language Models Can Align Themselves without Finetuning","summary":"  Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n","authors":["Yuhui Li","Fangyun Wei","Jinjing Zhao","Chao Zhang","Hongyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05378v1","updated":"2023-10-09T03:27:05Z","published":"2023-10-09T03:27:05Z","title":"Transcending the Attention Paradigm: Implicit Learning from Geospatial\n  Social Media Data","summary":"  While transformers have pioneered attention-driven architectures as a\ncornerstone of research, their dependence on explicitly contextual information\nunderscores limitations in their abilities to tacitly learn overarching textual\nthemes. This study investigates social media data as a source of distributed\npatterns, challenging the heuristic paradigm of performance benchmarking. In\nstark contrast to networks that rely on capturing complex long-term\ndependencies, models of online data inherently lack structure and are forced to\nlearn underlying patterns in the aggregate. To properly represent these\nabstract relationships, this research dissects empirical social media corpora\ninto their elemental components and analyzes over two billion tweets across\npopulation-dense locations. Exploring the relationship between location and\nvernacular in Twitter data, we employ Bag-of-Words models specific to each city\nand evaluate their respective representation. This demonstrates that hidden\ninsights can be uncovered without the crutch of advanced algorithms and\ndemonstrates that even amidst noisy data, geographic location has a\nconsiderable influence on online communication. This evidence presents tangible\ninsights regarding geospatial communication patterns and their implications in\nsocial science. It also challenges the notion that intricate models are\nprerequisites for pattern recognition in natural language, aligning with the\nevolving landscape that questions the embrace of absolute interpretability over\nabstract understanding. This study bridges the divide between sophisticated\nframeworks and intangible relationships, paving the way for systems that blend\nstructured models with conjectural reasoning.\n","authors":["Nick DiSanto","Anthony Corso","Benjamin Sanders","Gavin Harding"],"pdf_url":"https://arxiv.org/pdf/2310.05378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05374v1","updated":"2023-10-09T03:10:49Z","published":"2023-10-09T03:10:49Z","title":"Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis","summary":"  Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data. The source code will be\navailable to the community.\n","authors":["Jianqiao Lu","Wenyong Huang","Nianzu Zheng","Xingshan Zeng","Yu Ting Yeung","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05374v1.pdf","comment":"15 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2310.05364v1","updated":"2023-10-09T02:50:54Z","published":"2023-10-09T02:50:54Z","title":"Universal Multi-modal Entity Alignment via Iteratively Fusing Modality\n  Similarity Paths","summary":"  The objective of Entity Alignment (EA) is to identify equivalent entity pairs\nfrom multiple Knowledge Graphs (KGs) and create a more comprehensive and\nunified KG. The majority of EA methods have primarily focused on the structural\nmodality of KGs, lacking exploration of multi-modal information. A few\nmulti-modal EA methods have made good attempts in this field. Still, they have\ntwo shortcomings: (1) inconsistent and inefficient modality modeling that\ndesigns complex and distinct models for each modality; (2) ineffective modality\nfusion due to the heterogeneous nature of modalities in EA. To tackle these\nchallenges, we propose PathFusion, consisting of two main components: (1) MSP,\na unified modeling approach that simplifies the alignment process by\nconstructing paths connecting entities and modality nodes to represent multiple\nmodalities; (2) IRF, an iterative fusion method that effectively combines\ninformation from different modalities using the path as an information carrier.\nExperimental results on real-world datasets demonstrate the superiority of\nPathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement\non Hits@1, and 0.194-0.245 absolute improvement on MRR.\n","authors":["Bolin Zhu","Xiaoze Liu","Xin Mao","Zhuo Chen","Lingbing Guo","Tao Gui","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11713v3","updated":"2023-10-09T02:44:47Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v3.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2305.05181v2","updated":"2023-10-09T02:44:12Z","published":"2023-05-09T05:25:05Z","title":"MoT: Memory-of-Thought Enables ChatGPT to Self-Improve","summary":"  Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.\n","authors":["Xiaonan Li","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2305.05181v2.pdf","comment":"Accepted to appear at EMNLP 2023"},{"id":"http://arxiv.org/abs/2302.13539v3","updated":"2023-10-09T02:39:04Z","published":"2023-02-27T06:32:45Z","title":"Finding Support Examples for In-Context Learning","summary":"  Additionally, the strong dependency among in-context examples makes it an\nNP-hard combinatorial optimization problem and enumerating all permutations is\ninfeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this\nchallenge in two stages: First we filter the dataset to obtain informative\nin-context examples individually. Specifically, we propose a novel metric,\nInfoScore, to evaluate the example's in-context informativeness based on the\nlanguage model's feedback, and further propose a progressive filtering process\nto filter out uninformative examples. Then we propose diversity-guided example\nsearch which iteratively refines and evaluates the selected example\npermutations, to find examples that fully depict the task. The experimental\nresults show that LENS significantly outperforms a wide range of baselines.\n","authors":["Xiaonan Li","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2302.13539v3.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05352v1","updated":"2023-10-09T02:28:19Z","published":"2023-10-09T02:28:19Z","title":"A Glance is Enough: Extract Target Sentence By Looking at A keyword","summary":"  This paper investigates the possibility of extracting a target sentence from\nmulti-talker speech using only a keyword as input. For example, in social\nsecurity applications, the keyword might be \"help\", and the goal is to identify\nwhat the person who called for help is articulating while ignoring other\nspeakers. To address this problem, we propose using the Transformer\narchitecture to embed both the keyword and the speech utterance and then rely\non the cross-attention mechanism to select the correct content from the\nconcatenated or overlapping speech. Experimental results on Librispeech\ndemonstrate that our proposed method can effectively extract target sentences\nfrom very noisy and mixed speech (SNR=-3dB), achieving a phone error rate (PER)\nof 26\\%, compared to the baseline system's PER of 96%.\n","authors":["Ying Shi","Dong Wang","Lantian Li","Jiqing Han"],"pdf_url":"https://arxiv.org/pdf/2310.05352v1.pdf","comment":"submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05344v1","updated":"2023-10-09T02:11:21Z","published":"2023-10-09T02:11:21Z","title":"SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to\n  RLHF","summary":"  Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B\n","authors":["Yi Dong","Zhilin Wang","Makesh Narsimhan Sreedhar","Xianchao Wu","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2310.05344v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05338v1","updated":"2023-10-09T01:52:27Z","published":"2023-10-09T01:52:27Z","title":"Negative Object Presence Evaluation (NOPE) to Measure Object\n  Hallucination in Vision-Language Models","summary":"  Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.\n","authors":["Holy Lovenia","Wenliang Dai","Samuel Cahyawijaya","Ziwei Ji","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2310.05338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05318v1","updated":"2023-10-09T00:45:20Z","published":"2023-10-09T00:45:20Z","title":"Resolving the Imbalance Issue in Hierarchical Disciplinary Topic\n  Inference via LLM-based Data Augmentation","summary":"  In addressing the imbalanced issue of data within the realm of Natural\nLanguage Processing, text data augmentation methods have emerged as pivotal\nsolutions. This data imbalance is prevalent in the research proposals submitted\nduring the funding application process. Such imbalances, resulting from the\nvarying popularity of disciplines or the emergence of interdisciplinary\nstudies, significantly impede the precision of downstream topic models that\ndeduce the affiliated disciplines of these proposals. At the data level,\nproposals penned by experts and scientists are inherently complex technological\ntexts, replete with intricate terminologies, which augmenting such specialized\ntext data poses unique challenges. At the system level, this, in turn,\ncompromises the fairness of AI-assisted reviewer assignment systems, which\nraises a spotlight on solving this issue. This study leverages large language\nmodels (Llama V1) as data generators to augment research proposals categorized\nwithin intricate disciplinary hierarchies, aiming to rectify data imbalances\nand enhance the equity of expert assignments. We first sample within the\nhierarchical structure to find the under-represented class. Then we designed a\nprompt for keyword-based research proposal generation. Our experiments attests\nto the efficacy of the generated data, demonstrating that research proposals\nproduced using the prompts can effectively address the aforementioned issues\nand generate high quality scientific text data, thus help the model overcome\nthe imbalanced issue.\n","authors":["Xunxin Cai","Meng Xiao","Zhiyuan Ning","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05318v1.pdf","comment":"6 pages, accepted by ICDM 2023"},{"id":"http://arxiv.org/abs/2310.03965v2","updated":"2023-10-09T00:35:22Z","published":"2023-10-06T01:40:09Z","title":"Thought Propagation: An Analogical Approach to Complex Reasoning with\n  Large Language Models","summary":"  Large Language Models (LLMs) have achieved remarkable success in reasoning\ntasks with the development of prompting methods. However, existing prompting\napproaches cannot reuse insights of solving similar problems and suffer from\naccumulated errors in multi-step reasoning, since they prompt LLMs to reason\n\\textit{from scratch}. To address these issues, we propose\n\\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous\nproblems and leverages their solutions to enhance the complex reasoning ability\nof LLMs. These analogous problems are related to the input one, with reusable\nsolutions and problem-solving strategies. Thus, it is promising to propagate\ninsights of solving previous analogous problems to inspire new problem-solving.\nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous\nproblems that are related to the input one. Then, TP reuses the results of\nanalogous problems to directly yield a new solution or derive a\nknowledge-intensive plan for execution to amend the initial solution obtained\nfrom scratch. TP is compatible with existing prompting approaches, allowing\nplug-and-play generalization and enhancement in a wide range of tasks without\nmuch labor in task-specific prompt engineering. Experiments across three\nchallenging tasks demonstrate TP enjoys a substantial improvement over the\nbaselines by an average of 12\\% absolute increase in finding the optimal\nsolutions in Shortest-path Reasoning, 13\\% improvement of human preference in\nCreative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent\nPlanning.\n","authors":["Junchi Yu","Ran He","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2310.03965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05317v1","updated":"2023-10-09T00:20:59Z","published":"2023-10-09T00:20:59Z","title":"Enhancing Long-form Text Generation in Mental Health\\\\ with\n  Task-adaptive Tokenization","summary":"  We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.\n","authors":["Siyang Liu","Naihao Deng","Sahand Sabour","Yilin Jia","Minlie Huang","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2310.05317v1.pdf","comment":"Accepted at main conference of The 2023 Conference on Empirical\n  Methods in Natural Language Processing; 8 pages"},{"id":"http://arxiv.org/abs/2304.04193v2","updated":"2023-10-09T23:40:26Z","published":"2023-04-09T08:26:04Z","title":"Extractive Summarization via ChatGPT for Faithful Summary Generation","summary":"  Extractive summarization is a crucial task in natural language processing\nthat aims to condense long documents into shorter versions by directly\nextracting sentences. The recent introduction of large language models has\nattracted significant interest in the NLP community due to its remarkable\nperformance on a wide range of downstream tasks. This paper first presents a\nthorough evaluation of ChatGPT's performance on extractive summarization and\ncompares it with traditional fine-tuning methods on various benchmark datasets.\nOur experimental analysis reveals that ChatGPT exhibits inferior extractive\nsummarization performance in terms of ROUGE scores compared to existing\nsupervised systems, while achieving higher performance based on LLM-based\nevaluation metrics. In addition, we explore the effectiveness of in-context\nlearning and chain-of-thought reasoning for enhancing its performance.\nFurthermore, we find that applying an extract-then-generate pipeline with\nChatGPT yields significant performance improvements over abstractive baselines\nin terms of summary faithfulness. These observations highlight potential\ndirections for enhancing ChatGPT's capabilities in faithful summarization using\ntwo-stage approaches.\n","authors":["Haopeng Zhang","Xiao Liu","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.04193v2.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2306.00398v2","updated":"2023-10-09T23:31:44Z","published":"2023-06-01T07:00:07Z","title":"Preference-grounded Token-level Guidance for Language Model Fine-tuning","summary":"  Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the *sequence level* while LM training and generation both occur at\nthe *token level*. There is, therefore, a *granularity mismatch* between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\n*minimalist* learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization.\n","authors":["Shentao Yang","Shujian Zhang","Congying Xia","Yihao Feng","Caiming Xiong","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2306.00398v2.pdf","comment":"37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)"},{"id":"http://arxiv.org/abs/2310.06204v1","updated":"2023-10-09T23:07:05Z","published":"2023-10-09T23:07:05Z","title":"Estimating Numbers without Regression","summary":"  Despite recent successes in language models, their ability to represent\nnumbers is insufficient. Humans conceptualize numbers based on their\nmagnitudes, effectively projecting them on a number line; whereas subword\ntokenization fails to explicitly capture magnitude by splitting numbers into\narbitrary chunks. To alleviate this shortcoming, alternative approaches have\nbeen proposed that modify numbers at various stages of the language modeling\npipeline. These methods change either the (1) notation in which numbers are\nwritten (\\eg scientific vs decimal), the (2) vocabulary used to represent\nnumbers or the entire (3) architecture of the underlying language model, to\ndirectly regress to a desired number.\n  Previous work suggests that architectural change helps achieve\nstate-of-the-art on number estimation but we find an insightful ablation:\nchanging the model's vocabulary instead (\\eg introduce a new token for numbers\nin range 10-100) is a far better trade-off. In the context of masked number\nprediction, a carefully designed tokenization scheme is both the simplest to\nimplement and sufficient, \\ie with similar performance to the state-of-the-art\napproach that requires making significant architectural changes. Finally, we\nreport similar trends on the downstream task of numerical fact estimation (for\nFermi Problems) and discuss reasons behind our findings.\n","authors":["Avijit Thawani","Jay Pujara","Ashwin Kalyan"],"pdf_url":"https://arxiv.org/pdf/2310.06204v1.pdf","comment":"Workshop on Insights from Negative Results in NLP at EACL 2023"},{"id":"http://arxiv.org/abs/2310.06202v1","updated":"2023-10-09T23:06:05Z","published":"2023-10-09T23:06:05Z","title":"GPT-who: An Information Density-based Machine-Generated Text Detector","summary":"  The Uniform Information Density principle posits that humans prefer to spread\ninformation evenly during language production. In this work, we examine if the\nUID principle can help capture differences between Large Language Models (LLMs)\nand human-generated text. We propose GPT-who, the first\npsycholinguistically-aware multi-class domain-agnostic statistical-based\ndetector. This detector employs UID-based features to model the unique\nstatistical signature of each LLM and human author for accurate authorship\nattribution. We evaluate our method using 4 large-scale benchmark datasets and\nfind that GPT-who outperforms state-of-the-art detectors (both statistical- &\nnon-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by\nover $20$% across domains. In addition to superior performance, it is\ncomputationally inexpensive and utilizes an interpretable representation of\ntext articles. We present the largest analysis of the UID-based representations\nof human and machine-generated texts (over 400k articles) to demonstrate how\nauthors distribute information differently, and in ways that enable their\ndetection using an off-the-shelf LM without any fine-tuning. We find that\nGPT-who can distinguish texts generated by very sophisticated LLMs, even when\nthe overlying text is indiscernible.\n","authors":["Saranya Venkatraman","Adaku Uchendu","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.06202v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.06201v1","updated":"2023-10-09T23:03:24Z","published":"2023-10-09T23:03:24Z","title":"Compressing Context to Enhance Inference Efficiency of Large Language\n  Models","summary":"  Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.\n","authors":["Yucheng Li","Bo Dong","Chenghua Lin","Frank Guerin"],"pdf_url":"https://arxiv.org/pdf/2310.06201v1.pdf","comment":"EMNLP 2023. arXiv admin note: substantial text overlap with\n  arXiv:2304.12102; text overlap with arXiv:2303.11076 by other authors"},{"id":"http://arxiv.org/abs/2310.06200v1","updated":"2023-10-09T23:02:07Z","published":"2023-10-09T23:02:07Z","title":"The Importance of Prompt Tuning for Automated Neuron Explanations","summary":"  Recent advances have greatly increased the capabilities of large language\nmodels (LLMs), but our understanding of the models and their safety has not\nprogressed as fast. In this paper we aim to understand LLMs deeper by studying\ntheir individual neurons. We build upon previous work showing large language\nmodels such as GPT-4 can be useful in explaining what each neuron in a language\nmodel does. Specifically, we analyze the effect of the prompt used to generate\nexplanations and show that reformatting the explanation prompt in a more\nnatural way can significantly improve neuron explanation quality and greatly\nreduce computational cost. We demonstrate the effects of our new prompts in\nthree different ways, incorporating both automated and human evaluations.\n","authors":["Justin Lee","Tuomas Oikarinen","Arjun Chantha","Keng-Chi Chang","Yilan Chen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2310.06200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14030v3","updated":"2023-10-09T22:19:02Z","published":"2023-02-27T18:41:48Z","title":"Multimodal Speech Recognition for Language-Guided Embodied Agents","summary":"  Benchmarks for language-guided embodied agents typically assume text-based\ninstructions, but deployed agents will encounter spoken instructions. While\nAutomatic Speech Recognition (ASR) models can bridge the input gap, erroneous\nASR transcripts can hurt the agents' ability to complete tasks. In this work,\nwe propose training a multimodal ASR model to reduce errors in transcribing\nspoken instructions by considering the accompanying visual context. We train\nour model on a dataset of spoken instructions, synthesized from the ALFRED task\ncompletion dataset, where we simulate acoustic noise by systematically masking\nspoken words. We find that utilizing visual observations facilitates masked\nword recovery, with multimodal ASR models recovering up to 30% more masked\nwords than unimodal baselines. We also find that a text-trained embodied agent\nsuccessfully completes tasks more often by following transcribed instructions\nfrom multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr\n","authors":["Allen Chang","Xiaoyuan Zhu","Aarav Monga","Seoho Ahn","Tejas Srinivasan","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2302.14030v3.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.01002v3","updated":"2023-10-09T21:57:47Z","published":"2023-04-03T14:06:47Z","title":"Does Human Collaboration Enhance the Accuracy of Identifying\n  LLM-Generated Deepfake Texts?","summary":"  Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git\n","authors":["Adaku Uchendu","Jooyoung Lee","Hua Shen","Thai Le","Ting-Hao 'Kenneth' Huang","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2304.01002v3.pdf","comment":"Accepted at The 11th AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2023)"},{"id":"http://arxiv.org/abs/2310.06165v1","updated":"2023-10-09T21:32:49Z","published":"2023-10-09T21:32:49Z","title":"CAW-coref: Conjunction-Aware Word-level Coreference Resolution","summary":"  State-of-the-art coreference resolutions systems depend on multiple LLM calls\nper document and are thus prohibitively expensive for many use cases (e.g.,\ninformation extraction with large corpora). The leading word-level coreference\nsystem (WL-coref) attains 96.6% of these SOTA systems' performance while being\nmuch more efficient. In this work, we identify a routine yet important failure\ncase of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We\noffer a simple yet effective solution that improves the performance on the\nOntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level\ncoreference resolution and expensive SOTA approaches by 34.6%. Our\nConjunction-Aware Word-level coreference model (CAW-coref) and code is\navailable at https://github.com/KarelDO/wl-coref.\n","authors":["Karel D'Oosterlinck","Semere Kiros Bitew","Brandon Papineau","Christopher Potts","Thomas Demeester","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2310.06165v1.pdf","comment":"Accepted at CRAC 2023"},{"id":"http://arxiv.org/abs/2302.11520v4","updated":"2023-10-09T21:01:22Z","published":"2023-02-22T17:44:15Z","title":"Guiding Large Language Models via Directional Stimulus Prompting","summary":"  We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.\n","authors":["Zekun Li","Baolin Peng","Pengcheng He","Michel Galley","Jianfeng Gao","Xifeng Yan"],"pdf_url":"https://arxiv.org/pdf/2302.11520v4.pdf","comment":"Accepted by NeurIPS2023. The code and data are available at\n  https://github.com/Leezekun/Directional-Stimulus-Prompting"},{"id":"http://arxiv.org/abs/2304.03347v3","updated":"2023-10-09T20:17:16Z","published":"2023-04-06T19:53:59Z","title":"Towards Interpretable Mental Health Analysis with Large Language Models","summary":"  The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.\n","authors":["Kailai Yang","Shaoxiong Ji","Tianlin Zhang","Qianqian Xie","Ziyan Kuang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2304.03347v3.pdf","comment":"Accepted by EMNLP 2023 main conference as a long paper"},{"id":"http://arxiv.org/abs/2305.14342v2","updated":"2023-10-09T19:54:09Z","published":"2023-05-23T17:59:21Z","title":"Sophia: A Scalable Stochastic Second-order Optimizer for Language Model\n  Pre-training","summary":"  Given the massive cost of language model pre-training, a non-trivial\nimprovement of the optimization algorithm would lead to a material reduction on\nthe time and cost of training. Adam and its variants have been state-of-the-art\nfor years, and more sophisticated second-order (Hessian-based) optimizers often\nincur too much per-step overhead. In this paper, we propose Sophia,\nSecond-order Clipped Stochastic Optimization, a simple scalable second-order\noptimizer that uses a light-weight estimate of the diagonal Hessian as the\npre-conditioner. The update is the moving average of the gradients divided by\nthe moving average of the estimated Hessian, followed by element-wise clipping.\nThe clipping controls the worst-case update size and tames the negative impact\nof non-convexity and rapid change of Hessian along the trajectory. Sophia only\nestimates the diagonal Hessian every handful of iterations, which has\nnegligible average per-step time and memory overhead. On language modeling with\nGPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up\ncompared to Adam in the number of steps, total compute, and wall-clock time,\nachieving the same perplexity with 50% fewer steps, less total compute, and\nreduced wall-clock time. Theoretically, we show that Sophia, in a much\nsimplified setting, adapts to the heterogeneous curvatures in different\nparameter dimensions, and thus has a run-time bound that does not depend on the\ncondition number of the loss.\n","authors":["Hong Liu","Zhiyuan Li","David Hall","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2305.14342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06117v1","updated":"2023-10-09T19:48:55Z","published":"2023-10-09T19:48:55Z","title":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models","summary":"  We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide the reasoning steps, LLMs significantly improve their abilities in\nfollowing a correct reasoning path towards the solution. We conduct experiments\nof Step-Back Prompting with PaLM-2L models and observe substantial performance\ngains on a wide range of challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting\nimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,\nTimeQA by 27%, and MuSiQue by 7%.\n","authors":["Huaixiu Steven Zheng","Swaroop Mishra","Xinyun Chen","Heng-Tze Cheng","Ed H. Chi","Quoc V Le","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06111v1","updated":"2023-10-09T19:37:38Z","published":"2023-10-09T19:37:38Z","title":"BYOC: Personalized Few-Shot Classification with Co-Authored Class\n  Descriptions","summary":"  Text classification is a well-studied and versatile building block for many\nNLP applications. Yet, existing approaches require either large annotated\ncorpora to train a model with or, when using large language models as a base,\nrequire carefully crafting the prompt as well as using a long context that can\nfit many examples. As a result, it is not possible for end-users to build\nclassifiers for themselves. To address this issue, we propose a novel approach\nto few-shot text classification using an LLM. Rather than few-shot examples,\nthe LLM is prompted with descriptions of the salient features of each class.\nThese descriptions are coauthored by the user and the LLM interactively: while\nthe user annotates each few-shot example, the LLM asks relevant questions that\nthe user answers. Examples, questions, and answers are summarized to form the\nclassification prompt. Our experiments show that our approach yields high\naccuracy classifiers, within 82% of the performance of models trained with\nsignificantly larger datasets while using only 1% of their training sets.\nAdditionally, in a study with 30 participants, we show that end-users are able\nto build classifiers to suit their specific needs. The personalized classifiers\nshow an average accuracy of 90%, which is 15% higher than the state-of-the-art\napproach.\n","authors":["Arth Bohra","Govert Verkes","Artem Harutyunyan","Pascal Weinberger","Giovanni Campagna"],"pdf_url":"https://arxiv.org/pdf/2310.06111v1.pdf","comment":"Accepted at EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2310.06103v1","updated":"2023-10-09T19:22:51Z","published":"2023-10-09T19:22:51Z","title":"Leveraging Multilingual Self-Supervised Pretrained Models for\n  Sequence-to-Sequence End-to-End Spoken Language Understanding","summary":"  A number of methods have been proposed for End-to-End Spoken Language\nUnderstanding (E2E-SLU) using pretrained models, however their evaluation often\nlacks multilingual setup and tasks that require prediction of lexical fillers,\nsuch as slot filling. In this work, we propose a unified method that integrates\nmultilingual pretrained speech and text models and performs E2E-SLU on six\ndatasets in four languages in a generative manner, including the prediction of\nlexical fillers. We investigate how the proposed method can be improved by\npretraining on widely available speech recognition data using several training\nobjectives. Pretraining on 7000 hours of multilingual data allows us to\noutperform the state-of-the-art ultimately on two SLU datasets and partly on\ntwo more SLU datasets. Finally, we examine the cross-lingual capabilities of\nthe proposed model and improve on the best known result on the\nPortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate\nof 23.65%.\n","authors":["Pavel Denisov","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2310.06103v1.pdf","comment":"IEEE Workshop on Automatic Speech Recognition and Understanding\n  (ASRU) 2023"},{"id":"http://arxiv.org/abs/2305.12696v2","updated":"2023-10-09T19:20:32Z","published":"2023-05-22T04:07:54Z","title":"Learning Interpretable Style Embeddings via Prompting LLMs","summary":"  Style representation learning builds content-independent representations of\nauthor style in text. Stylometry, the analysis of style in text, is often\nperformed by expert forensic linguists and no large dataset of stylometric\nannotations exists for training. Current style representation learning uses\nneural methods to disentangle style from content to create style vectors,\nhowever, these approaches result in uninterpretable representations,\ncomplicating their usage in downstream applications like authorship attribution\nwhere auditing and explainability is critical. In this work, we use prompting\nto perform stylometry on a large number of texts to create a synthetic dataset\nand train human-interpretable style representations we call LISA embeddings. We\nrelease our synthetic stylometry dataset and our interpretable style models as\nresources.\n","authors":["Ajay Patel","Delip Rao","Ansh Kothary","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2305.12696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14956v2","updated":"2023-10-09T19:00:44Z","published":"2023-05-24T09:50:54Z","title":"Editing Common Sense in Transformers","summary":"  Editing model parameters directly in Transformers makes updating black-box\nmodels possible without re-training (Meng et al., 2023). However, these editing\nmethods have only been evaluated on statements about encyclopedic knowledge\nwith a single correct answer. Commonsense knowledge with multiple correct\nanswers, e.g., an apple can be green or red but not transparent, has not been\nstudied but is as essential for enhancing transformers' reliability and\nusefulness. In this paper, we investigate whether commonsense judgments are\ncausally associated with localized, editable parameters in Transformers, and we\nprovide an affirmative answer. We find that directly applying the MEMIT editing\nalgorithm results in sub-par performance and improve it for the commonsense\ndomain by varying edit tokens and improving the layer selection strategy, i.e.,\n$MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform\nbest-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q\ndatasets. In addition, we propose a novel evaluation dataset, PROBE SET, that\ncontains unaffected and affected neighborhoods, affected paraphrases, and\naffected reasoning challenges. $MEMIT_{CSK}$ performs well across the metrics\nwhile fine-tuning baselines show significant trade-offs between unaffected and\naffected metrics. These results suggest a compelling future direction for\nincorporating feedback about common sense into Transformers through direct\nmodel editing.\n","authors":["Anshita Gupta","Debanjan Mondal","Akshay Krishna Sheshadri","Wenlong Zhao","Xiang Lorraine Li","Sarah Wiegreffe","Niket Tandon"],"pdf_url":"https://arxiv.org/pdf/2305.14956v2.pdf","comment":"Accepted to EMNLP 2023. Anshita, Debanjan, Akshay are co-first\n  authors"},{"id":"http://arxiv.org/abs/2309.10818v2","updated":"2023-10-09T18:30:48Z","published":"2023-09-19T17:59:54Z","title":"SlimPajama-DC: Understanding Data Combinations for LLM Training","summary":"  This paper aims to understand the impacts of various data combinations (e.g.,\nweb text, wikipedia, github, books) on the training of large language models\nusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-source\ndataset, which has been refined and further deduplicated to 627B tokens from\nthe extensive 1.2T tokens RedPajama dataset contributed by Together. We've\ntermed our research as SlimPajama-DC, an empirical analysis designed to uncover\nfundamental characteristics and best practices associated with employing\nSlimPajama in the training of large language models. During our research with\nSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.\nlocal deduplication. We analyze and discuss how global (across different\nsources of datasets) and local (within the single source of dataset)\ndeduplications affect the performance of trained models. (2) Proportions of\nhigh-quality/highly-deduplicated multi-source datasets in the combination. To\nstudy this, we construct six configurations of SlimPajama dataset and train\nindividual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best\nconfiguration outperforms the 1.3B model trained on RedPajama using the same\nnumber of training tokens by a significant margin. All our 1.3B models are\ntrained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16\nmixed precision. We further extend our discoveries (such as increasing data\ndiversity is crucial after global deduplication) on a 7B model with large\nbatch-size training. Our models and the separate SlimPajama-DC datasets are\navailable at: https://huggingface.co/MBZUAI-LLM and\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.\n","authors":["Zhiqiang Shen","Tianhua Tao","Liqun Ma","Willie Neiswanger","Zhengzhong Liu","Hongyi Wang","Bowen Tan","Joel Hestness","Natalia Vassilieva","Daria Soboleva","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2309.10818v2.pdf","comment":"Technical report. Huggingface: https://huggingface.co/MBZUAI-LLM and\n  https://huggingface.co/datasets/cerebras/SlimPajama-627B"},{"id":"http://arxiv.org/abs/2310.03951v2","updated":"2023-10-09T18:15:21Z","published":"2023-10-06T00:10:46Z","title":"Chain of Natural Language Inference for Reducing Large Language Model\n  Ungrounded Hallucinations","summary":"  Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.\n","authors":["Deren Lei","Yaxi Li","Mengya Hu","Mingyu Wang","Vincent Yun","Emily Ching","Eslam Kamal"],"pdf_url":"https://arxiv.org/pdf/2310.03951v2.pdf","comment":"The source code is available at\n  https://github.com/microsoft/CoNLI_hallucination"},{"id":"http://arxiv.org/abs/2310.06061v1","updated":"2023-10-09T18:13:07Z","published":"2023-10-09T18:13:07Z","title":"Auditing Gender Analyzers on Text Data","summary":"  AI models have become extremely popular and accessible to the general public.\nHowever, they are continuously under the scanner due to their demonstrable\nbiases toward various sections of the society like people of color and\nnon-binary people. In this study, we audit three existing gender analyzers --\nuClassify, Readable and HackerFactor, for biases against non-binary\nindividuals. These tools are designed to predict only the cisgender binary\nlabels, which leads to discrimination against non-binary members of the\nsociety. We curate two datasets -- Reddit comments (660k) and, Tumblr posts\n(2.05M) and our experimental evaluation shows that the tools are highly\ninaccurate with the overall accuracy being ~50% on all platforms. Predictions\nfor non-binary comments on all platforms are mostly female, thus propagating\nthe societal bias that non-binary individuals are effeminate. To address this,\nwe fine-tune a BERT multi-label classifier on the two datasets in multiple\ncombinations, observe an overall performance of ~77% on the most realistically\ndeployable setting and a surprisingly higher performance of 90% for the\nnon-binary class. We also audit ChatGPT using zero-shot prompts on a small\ndataset (due to high pricing) and observe an average accuracy of 58% for Reddit\nand Tumblr combined (with overall better results for Reddit).\n  Thus, we show that existing systems, including highly advanced ones like\nChatGPT are biased, and need better audits and moderation and, that such\nsocietal biases can be addressed and alleviated through simple off-the-shelf\nmodels like BERT trained on more gender inclusive datasets.\n","authors":["Siddharth D Jaiswal","Ankit Kumar Verma","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2310.06061v1.pdf","comment":"This work has been accepted at IEEE/ACM ASONAM 2023. Please cite the\n  version appearing in the ASONAM proceedings"},{"id":"http://arxiv.org/abs/2310.06046v1","updated":"2023-10-09T18:02:38Z","published":"2023-10-09T18:02:38Z","title":"LLM for SoC Security: A Paradigm Shift","summary":"  As the ubiquity and complexity of system-on-chip (SoC) designs increase\nacross electronic devices, the task of incorporating security into an SoC\ndesign flow poses significant challenges. Existing security solutions are\ninadequate to provide effective verification of modern SoC designs due to their\nlimitations in scalability, comprehensiveness, and adaptability. On the other\nhand, Large Language Models (LLMs) are celebrated for their remarkable success\nin natural language understanding, advanced reasoning, and program synthesis\ntasks. Recognizing an opportunity, our research delves into leveraging the\nemergent capabilities of Generative Pre-trained Transformers (GPTs) to address\nthe existing gaps in SoC security, aiming for a more efficient, scalable, and\nadaptable methodology. By integrating LLMs into the SoC security verification\nparadigm, we open a new frontier of possibilities and challenges to ensure the\nsecurity of increasingly complex SoCs. This paper offers an in-depth analysis\nof existing works, showcases practical case studies, demonstrates comprehensive\nexperiments, and provides useful promoting guidelines. We also present the\nachievements, prospects, and challenges of employing LLM in different SoC\nsecurity verification tasks.\n","authors":["Dipayan Saha","Shams Tarek","Katayoon Yahyaei","Sujan Kumar Saha","Jingbo Zhou","Mark Tehranipoor","Farimah Farahmandi"],"pdf_url":"https://arxiv.org/pdf/2310.06046v1.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2308.14120v3","updated":"2023-10-09T18:01:12Z","published":"2023-08-27T14:28:38Z","title":"Large Language Models Streamline Automated Machine Learning for Clinical\n  Studies","summary":"  A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P>0.474). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.\n","authors":["Soroosh Tayebi Arasteh","Tianyu Han","Mahshad Lotfinia","Christiane Kuhl","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.14120v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.05922v1","updated":"2023-10-09T17:59:53Z","published":"2023-10-09T17:59:53Z","title":"FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video\n  editing","summary":"  Text-to-video editing aims to edit the visual appearance of a source video\nconditional on textual prompts. A major challenge in this task is to ensure\nthat all frames in the edited video are visually consistent. Most recent works\napply advanced text-to-image diffusion models to this task by inflating 2D\nspatial attention in the U-Net into spatio-temporal attention. Although\ntemporal context can be added through spatio-temporal attention, it may\nintroduce some irrelevant information for each patch and therefore cause\ninconsistency in the edited video. In this paper, for the first time, we\nintroduce optical flow into the attention module in the diffusion model's U-Net\nto address the inconsistency issue for text-to-video editing. Our method,\nFLATTEN, enforces the patches on the same flow path across different frames to\nattend to each other in the attention module, thus improving the visual\nconsistency in the edited videos. Additionally, our method is training-free and\ncan be seamlessly integrated into any diffusion-based text-to-video editing\nmethods and improve their visual consistency. Experiment results on existing\ntext-to-video editing benchmarks show that our proposed method achieves the new\nstate-of-the-art performance. In particular, our method excels in maintaining\nthe visual consistency in the edited videos.\n","authors":["Yuren Cong","Mengmeng Xu","Christian Simon","Shoufa Chen","Jiawei Ren","Yanping Xie","Juan-Manuel Perez-Rua","Bodo Rosenhahn","Tao Xiang","Sen He"],"pdf_url":"https://arxiv.org/pdf/2310.05922v1.pdf","comment":"Project page: https://flatten-video-editing.github.io/"},{"id":"http://arxiv.org/abs/2310.05920v1","updated":"2023-10-09T17:59:26Z","published":"2023-10-09T17:59:26Z","title":"SimPLR: A Simple and Plain Transformer for Object Detection and\n  Segmentation","summary":"  The ability to detect objects in images at varying scales has played a\npivotal role in the design of modern object detectors. Despite considerable\nprogress in removing handcrafted components using transformers, multi-scale\nfeature maps remain a key factor for their empirical success, even with a plain\nbackbone like the Vision Transformer (ViT). In this paper, we show that this\nreliance on feature pyramids is unnecessary and a transformer-based detector\nwith scale-aware attention enables the plain detector `SimPLR' whose backbone\nand detection head both operate on single-scale features. The plain\narchitecture allows SimPLR to effectively take advantages of self-supervised\nlearning and scaling approaches with ViTs, yielding strong performance compared\nto multi-scale counterparts. We demonstrate through our experiments that when\nscaling to larger backbones, SimPLR indicates better performance than\nend-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while\nconsistently being faster. The code will be released.\n","authors":["Duy-Kien Nguyen","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2310.05920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05917v1","updated":"2023-10-09T17:59:12Z","published":"2023-10-09T17:59:12Z","title":"Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input","summary":"  Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.\n","authors":["Donglai Xiang","Fabian Prada","Zhe Cao","Kaiwen Guo","Chenglei Wu","Jessica Hodgins","Timur Bagautdinov"],"pdf_url":"https://arxiv.org/pdf/2310.05917v1.pdf","comment":"SIGGRAPH Asia 2023 Conference Paper. Project website:\n  https://xiangdonglai.github.io/www-sa23-drivable-clothing/"},{"id":"http://arxiv.org/abs/2310.05916v1","updated":"2023-10-09T17:59:04Z","published":"2023-10-09T17:59:04Z","title":"Interpreting CLIP's Image Representation via Text-Based Decomposition","summary":"  We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2310.05916v1.pdf","comment":"Project page and code:\n  https://yossigandelsman.github.io/clip_decomposition/"},{"id":"http://arxiv.org/abs/2301.08245v2","updated":"2023-10-09T17:58:14Z","published":"2023-01-19T18:59:28Z","title":"Booster: a Benchmark for Depth from Images of Specular and Transparent\n  Surfaces","summary":"  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n","authors":["Pierluigi Zama Ramirez","Alex Costanzino","Fabio Tosi","Matteo Poggi","Samuele Salti","Stefano Mattoccia","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2301.08245v2.pdf","comment":"Extension of the paper \"Open Challenges in Deep Stereo: the Booster\n  Dataset\" presented at CVPR 2022. Accepted at TPAMI"},{"id":"http://arxiv.org/abs/2306.00977v2","updated":"2023-10-09T17:51:12Z","published":"2023-06-01T17:59:10Z","title":"AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation","summary":"  During interactive segmentation, a model and a user work together to\ndelineate objects of interest in a 3D point cloud. In an iterative process, the\nmodel assigns each data point to an object (or the background), while the user\ncorrects errors in the resulting segmentation and feeds them back into the\nmodel. The current best practice formulates the problem as binary\nclassification and segments objects one at a time. The model expects the user\nto provide positive clicks to indicate regions wrongly assigned to the\nbackground and negative clicks on regions wrongly assigned to the object.\nSequentially visiting objects is wasteful since it disregards synergies between\nobjects: a positive click for a given object can, by definition, serve as a\nnegative click for nearby objects. Moreover, a direct competition between\nadjacent objects can speed up the identification of their common boundary. We\nintroduce AGILE3D, an efficient, attention-based model that (1) supports\nsimultaneous segmentation of multiple 3D objects, (2) yields more accurate\nsegmentation masks with fewer user clicks, and (3) offers faster inference. Our\ncore idea is to encode user clicks as spatial-temporal queries and enable\nexplicit interactions between click queries as well as between them and the 3D\nscene through a click attention module. Every time new clicks are added, we\nonly need to run a lightweight decoder that produces updated segmentation\nmasks. In experiments with four different 3D point cloud datasets, AGILE3D sets\na new state-of-the-art. Moreover, we also verify its practicality in real-world\nsetups with real user studies.\n","authors":["Yuanwen Yue","Sabarinath Mahadevan","Jonas Schult","Francis Engelmann","Bastian Leibe","Konrad Schindler","Theodora Kontogianni"],"pdf_url":"https://arxiv.org/pdf/2306.00977v2.pdf","comment":"Project page: https://ywyue.github.io/AGILE3D"},{"id":"http://arxiv.org/abs/2310.05886v1","updated":"2023-10-09T17:28:35Z","published":"2023-10-09T17:28:35Z","title":"Streaming Anchor Loss: Augmenting Supervision with Temporal Significance","summary":"  Streaming neural network models for fast frame-wise responses to various\nspeech and sensory signals are widely adopted on resource-constrained\nplatforms. Hence, increasing the learning capacity of such streaming models\n(i.e., by adding more parameters) to improve the predictive power may not be\nviable for real-world tasks. In this work, we propose a new loss, Streaming\nAnchor Loss (SAL), to better utilize the given learning capacity by encouraging\nthe model to learn more from essential frames. More specifically, our SAL and\nits focal variations dynamically modulate the frame-wise cross entropy loss\nbased on the importance of the corresponding frames so that a higher loss\npenalty is assigned for frames within the temporal proximity of semantically\ncritical events. Therefore, our loss ensures that the model training focuses on\npredicting the relatively rare but task-relevant frames. Experimental results\nwith standard lightweight convolutional and recurrent streaming networks on\nthree different speech based detection tasks demonstrate that SAL enables the\nmodel to learn the overall task more effectively with improved accuracy and\nlatency, without any additional data, model parameters, or architectural\nchanges.\n","authors":[" Utkarsh"," Sarawgi","John Berkowitz","Vineet Garg","Arnav Kundu","Minsik Cho","Sai Srujana Buddi","Saurabh Adya","Ahmed Tewfik"],"pdf_url":"https://arxiv.org/pdf/2310.05886v1.pdf","comment":"Under review for ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05881v1","updated":"2023-10-09T17:22:58Z","published":"2023-10-09T17:22:58Z","title":"Controllable Chest X-Ray Report Generation from Longitudinal\n  Representations","summary":"  Radiology reports are detailed text descriptions of the content of medical\nscans. Each report describes the presence/absence and location of relevant\nclinical findings, commonly including comparison with prior exams of the same\npatient to describe how they evolved. Radiology reporting is a time-consuming\nprocess, and scan results are often subject to delays. One strategy to speed up\nreporting is to integrate automated reporting systems, however clinical\ndeployment requires high accuracy and interpretability. Previous approaches to\nautomated radiology reporting generally do not provide the prior study as\ninput, precluding comparison which is required for clinical accuracy in some\ntypes of scans, and offer only unreliable methods of interpretability.\nTherefore, leveraging an existing visual input format of anatomical tokens, we\nintroduce two novel aspects: (1) longitudinal representation learning -- we\ninput the prior scan as an additional input, proposing a method to align,\nconcatenate and fuse the current and prior visual information into a joint\nlongitudinal representation which can be provided to the multimodal report\ngeneration model; (2) sentence-anatomy dropout -- a training strategy for\ncontrollability in which the report generator model is trained to predict only\nsentences from the original report which correspond to the subset of anatomical\nregions given as input. We show through in-depth experiments on the MIMIC-CXR\ndataset how the proposed approach achieves state-of-the-art results while\nenabling anatomy-wise controllable report generation.\n","authors":["Francesco Dalla Serra","Chaoyang Wang","Fani Deligianni","Jeffrey Dalton","Alison Q O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.05881v1.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05873v1","updated":"2023-10-09T17:13:10Z","published":"2023-10-09T17:13:10Z","title":"Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion\n  Models","summary":"  Fine-tuning diffusion models through personalized datasets is an acknowledged\nmethod for improving generation quality across downstream tasks, which,\nhowever, often inadvertently generates unintended concepts such as watermarks\nand QR codes, attributed to the limitations in image sources and collecting\nmethods within specific downstream tasks. Existing solutions suffer from\neliminating these unintentionally learned implicit concepts, primarily due to\nthe dependency on the model's ability to recognize concepts that it actually\ncannot discern. In this work, we introduce \\methodname, a novel approach that\nsuccessfully removes the implicit concepts with either an additional accessible\nclassifier or detector model to encode geometric information of these concepts\ninto text domain. Moreover, we propose \\textit{Implicit Concept}, a novel\nimage-text dataset imbued with three implicit concepts (\\ie, watermarks, QR\ncodes, and text) for training and evaluation. Experimental results demonstrate\nthat \\methodname not only identifies but also proficiently eradicates implicit\nconcepts, revealing a significant improvement over the existing methods. The\nintegration of geometric information marks a substantial progression in the\nprecise removal of implicit concepts in diffusion models.\n","authors":["Zhili Liu","Kai Chen","Yifan Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.05873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05872v1","updated":"2023-10-09T17:10:35Z","published":"2023-10-09T17:10:35Z","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models","summary":"  In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.\n","authors":["Kaiwen Zhou","Kwonjoon Lee","Teruhisa Misu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05867v1","updated":"2023-10-09T17:03:39Z","published":"2023-10-09T17:03:39Z","title":"Domain-wise Invariant Learning for Panoptic Scene Graph Generation","summary":"  Panoptic Scene Graph Generation (PSG) involves the detection of objects and\nthe prediction of their corresponding relationships (predicates). However, the\npresence of biased predicate annotations poses a significant challenge for PSG\nmodels, as it hinders their ability to establish a clear decision boundary\namong different predicates. This issue substantially impedes the practical\nutility and real-world applicability of PSG models. To address the intrinsic\nbias above, we propose a novel framework to infer potentially biased\nannotations by measuring the predicate prediction risks within each\nsubject-object pair (domain), and adaptively transfer the biased annotations to\nconsistent ones by learning invariant predicate representation embeddings.\nExperiments show that our method significantly improves the performance of\nbenchmark models, achieving a new state-of-the-art performance, and shows great\ngeneralization and effectiveness on PSG dataset.\n","authors":["Li Li","You Qin","Wei Ji","Yuxiao Zhou","Roger Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2310.05867v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.15567"},{"id":"http://arxiv.org/abs/2208.10765v2","updated":"2023-10-09T17:03:26Z","published":"2022-08-23T06:55:53Z","title":"A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots","summary":"  Duckiebots are low-cost mobile robots that are widely used in the fields of\nresearch and education. Although there are existing self-driving algorithms for\nthe Duckietown platform, they are either too complex or perform too poorly to\nnavigate a multi-lane track. Moreover, it is essential to give memory and\ncomputational resources to a Duckiebot so it can perform additional tasks such\nas out-of-distribution input detection. In order to satisfy these constraints,\nwe built a low-cost autonomous driving algorithm capable of driving on a\ntwo-lane track. The algorithm uses traditional computer vision techniques to\nidentify the central lane on the track and obtain the relevant steering angle.\nThe steering is then controlled by a PID controller that smoothens the movement\nof the Duckiebot. The performance of the algorithm was compared to that of the\nNeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but\none finalists. The two main contributions of our algorithm are its low\ncomputational requirements and very quick set-up, with ongoing efforts to make\nit more reliable.\n","authors":["Archit Gupta","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2208.10765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05863v1","updated":"2023-10-09T17:00:20Z","published":"2023-10-09T17:00:20Z","title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models","summary":"  Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/the-anonymous-bs/FAVOR.git, and the training code and model\ncheckpoints will be released upon acceptance.\n","authors":["Guangzhi Sun","Wenyi Yu","Changli Tang","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05861v1","updated":"2023-10-09T16:57:57Z","published":"2023-10-09T16:57:57Z","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models","summary":"  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to a LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on two visual question answering tasks, we show that RepARe can result\nin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%\npoint increase on A-OKVQA. Additionally, we find that using gold answers for\noracle question candidate selection achieves a substantial gain in VQA accuracy\nby up to 14.41%. Through extensive analysis, we demonstrate that outputs from\nRepARe increase syntactic complexity, and effectively utilize vision-language\ninteraction and the frozen language model in LVLMs.\n","authors":["Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.05861v1.pdf","comment":"22 pages, 4 figures, Code: https://github.com/archiki/RepARe"},{"id":"http://arxiv.org/abs/2303.14465v2","updated":"2023-10-09T16:55:08Z","published":"2023-03-25T13:22:56Z","title":"Equivariant Similarity for Vision-Language Foundation Models","summary":"  This study explores the concept of equivariance in vision-language foundation\nmodels (VLMs), focusing specifically on the multimodal similarity function that\nis not only the major training objective but also the core delivery to support\ndownstream tasks. Unlike the existing image-text similarity objective which\nonly categorizes matched pairs as similar and unmatched pairs as dissimilar,\nequivariance also requires similarity to vary faithfully according to the\nsemantic changes. This allows VLMs to generalize better to nuanced and unseen\nmultimodal compositions. However, modeling equivariance is challenging as the\nground truth of semantic change is difficult to collect. For example, given an\nimage-text pair about a dog, it is unclear to what extent the similarity\nchanges when the pixel is changed from dog to cat? To this end, we propose\nEqSim, a regularization loss that can be efficiently calculated from any two\nmatched training pairs and easily pluggable into existing image-text retrieval\nfine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we\npresent a new challenging benchmark EqBen. Compared to the existing evaluation\nsets, EqBen is the first to focus on \"visual-minimal change\". Extensive\nexperiments show the lack of equivariance in current VLMs and validate the\neffectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen.\n","authors":["Tan Wang","Kevin Lin","Linjie Li","Chung-Ching Lin","Zhengyuan Yang","Hanwang Zhang","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14465v2.pdf","comment":"Accepted by ICCV'23 (Oral); Add evaluation on MLLM"},{"id":"http://arxiv.org/abs/2310.05837v1","updated":"2023-10-09T16:26:34Z","published":"2023-10-09T16:26:34Z","title":"A Real-time Method for Inserting Virtual Objects into Neural Radiance\n  Fields","summary":"  We present the first real-time method for inserting a rigid virtual object\ninto a neural radiance field, which produces realistic lighting and shadowing\neffects, as well as allows interactive manipulation of the object. By\nexploiting the rich information about lighting and geometry in a NeRF, our\nmethod overcomes several challenges of object insertion in augmented reality.\nFor lighting estimation, we produce accurate, robust and 3D spatially-varying\nincident lighting that combines the near-field lighting from NeRF and an\nenvironment lighting to account for sources not covered by the NeRF. For\nocclusion, we blend the rendered virtual object with the background scene using\nan opacity map integrated from the NeRF. For shadows, with a precomputed field\nof spherical signed distance field, we query the visibility term for any point\naround the virtual object, and cast soft, detailed shadows onto 3D surfaces.\nCompared with state-of-the-art techniques, our approach can insert virtual\nobject into scenes with superior fidelity, and has a great potential to be\nfurther applied to augmented reality systems.\n","authors":["Keyang Ye","Hongzhi Wu","Xin Tong","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04819v2","updated":"2023-10-09T16:24:26Z","published":"2023-04-10T19:00:29Z","title":"Recent Advancements in Machine Learning For Cybercrime Prediction","summary":"  Cybercrime is a growing threat to organizations and individuals worldwide,\nwith criminals using sophisticated techniques to breach security systems and\nsteal sensitive data. This paper aims to comprehensively survey the latest\nadvancements in cybercrime prediction, highlighting the relevant research. For\nthis purpose, we reviewed more than 150 research articles and discussed 50 most\nrecent and appropriate ones. We start the review with some standard methods\ncybercriminals use and then focus on the latest machine and deep learning\ntechniques, which detect anomalous behavior and identify potential threats. We\nalso discuss transfer learning, which allows models trained on one dataset to\nbe adapted for use on another dataset. We then focus on active and\nreinforcement learning as part of early-stage algorithmic research in\ncybercrime prediction. Finally, we discuss critical innovations, research gaps,\nand future research opportunities in Cybercrime prediction. This paper presents\na holistic view of cutting-edge developments and publicly available datasets.\n","authors":["Lavanya Elluri","Varun Mandalapu","Piyush Vyas","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2304.04819v2.pdf","comment":"Accepted in Journal of Computer Information Systems, 2023"},{"id":"http://arxiv.org/abs/2304.06385v3","updated":"2023-10-09T16:22:55Z","published":"2023-04-13T10:37:41Z","title":"TransHP: Image Classification with Hierarchical Prompting","summary":"  This paper explores a hierarchical prompting mechanism for the hierarchical\nimage classification (HIC) task. Different from prior HIC methods, our\nhierarchical prompting is the first to explicitly inject ancestor-class\ninformation as a tokenized hint that benefits the descendant-class\ndiscrimination. We think it well imitates human visual recognition, i.e.,\nhumans may use the ancestor class as a prompt to draw focus on the subtle\ndifferences among descendant classes. We model this prompting mechanism into a\nTransformer with Hierarchical Prompting (TransHP). TransHP consists of three\nsteps: 1) learning a set of prompt tokens to represent the coarse (ancestor)\nclasses, 2) on-the-fly predicting the coarse class of the input image at an\nintermediate block, and 3) injecting the prompt token of the predicted coarse\nclass into the intermediate feature. Though the parameters of TransHP maintain\nthe same for all input images, the injected coarse-class prompt conditions\n(modifies) the subsequent feature extraction and encourages a dynamic focus on\nrelatively subtle differences among the descendant classes. Extensive\nexperiments show that TransHP improves image classification on accuracy (e.g.,\nimproving ViT-B/16 by +2.83% ImageNet classification accuracy), training data\nefficiency (e.g., +12.69% improvement under 10% ImageNet training data), and\nmodel explainability. Moreover, TransHP also performs favorably against prior\nHIC methods, showing that TransHP well exploits the hierarchical information.\n","authors":["Wenhao Wang","Yifan Sun","Wei Li","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.06385v3.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05829v1","updated":"2023-10-09T16:17:42Z","published":"2023-10-09T16:17:42Z","title":"Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning\n  under A Unified View","summary":"  Spatio-temporal predictive learning plays a crucial role in self-supervised\nlearning, with wide-ranging applications across a diverse range of fields.\nPrevious approaches for temporal modeling fall into two categories:\nrecurrent-based and recurrent-free methods. The former, while meticulously\nprocessing frames one by one, neglect short-term spatio-temporal information\nredundancies, leading to inefficiencies. The latter naively stack frames\nsequentially, overlooking the inherent temporal dependencies. In this paper, we\nre-examine the two dominant temporal modeling approaches within the realm of\nspatio-temporal predictive learning, offering a unified perspective. Building\nupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive\nlearning), an innovative framework that reconciles the recurrent-based and\nrecurrent-free methods by integrating both micro-temporal and macro-temporal\nscales. Extensive experiments on a wide range of spatio-temporal predictive\nlearning demonstrate that USTEP achieves significant improvements over existing\ntemporal modeling approaches, thereby establishing it as a robust solution for\na wide range of spatio-temporal applications.\n","authors":["Cheng Tan","Jue Wang","Zhangyang Gao","Siyuan Li","Lirong Wu","Jun Xia","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2310.05829v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2208.08288v2","updated":"2023-10-09T16:01:45Z","published":"2022-08-17T13:31:38Z","title":"Deep learning based projection domain metal segmentation for metal\n  artifact reduction in cone beam computed tomography","summary":"  Metal artifact correction is a challenging problem in cone beam computed\ntomography (CBCT) scanning. Metal implants inserted into the anatomy cause\nsevere artifacts in reconstructed images. Widely used inpainting-based metal\nartifact reduction (MAR) methods require segmentation of metal traces in the\nprojections as a first step, which is a challenging task. One approach is to\nuse a deep learning method to segment metals in the projections. However, the\nsuccess of deep learning methods is limited by the availability of realistic\ntraining data. It is laborious and time consuming to get reliable ground truth\nannotations due to unclear implant boundaries and large numbers of projections.\nWe propose to use X-ray simulations to generate synthetic metal segmentation\ntraining dataset from clinical CBCT scans. We compare the effect of simulations\nwith different numbers of photons and also compare several training strategies\nto augment the available data. We compare our model's performance on real\nclinical scans with conventional region growing threshold-based MAR, moving\nmetal artifact reduction method, and a recent deep learning method. We show\nthat simulations with relatively small number of photons are suitable for the\nmetal segmentation task and that training the deep learning model with full\nsize and cropped projections together improves the robustness of the model. We\nshow substantial improvement in the image quality affected by severe motion,\nvoxel size under-sampling, and out-of-FOV metals. Our method can be easily\nintegrated into the existing projection-based MAR pipeline to get improved\nimage quality. This method can provide a novel paradigm to accurately segment\nmetals in CBCT projections.\n","authors":["Harshit Agrawal","Ari Hietanen","Simo Srkk"],"pdf_url":"https://arxiv.org/pdf/2208.08288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01830v2","updated":"2023-10-09T16:01:32Z","published":"2023-10-03T06:55:19Z","title":"AI-Generated Images as Data Source: The Dawn of Synthetic Era","summary":"  The advancement of visual intelligence is intrinsically tethered to the\navailability of data. In parallel, generative Artificial Intelligence (AI) has\nunlocked the potential to create synthetic images that closely resemble\nreal-world photographs, which prompts a compelling inquiry: how visual\nintelligence benefit from the advance of generative AI? This paper explores the\ninnovative concept of harnessing these AI-generated images as a new data\nsource, reshaping traditional model paradigms in visual intelligence. In\ncontrast to real data, AI-generated data sources exhibit remarkable advantages,\nincluding unmatched abundance and scalability, the rapid generation of vast\ndatasets, and the effortless simulation of edge cases. Built on the success of\ngenerative AI models, we examines the potential of their generated data in a\nrange of applications, from training machine learning models to simulating\nscenarios for computational modeling, testing, and validation. We probe the\ntechnological foundations that support this groundbreaking use of generative\nAI, engaging in an in-depth discussion on the ethical, legal, and practical\nconsiderations that accompany this transformative paradigm shift. Through an\nexhaustive survey of current technologies and applications, this paper presents\na comprehensive view of the synthetic era in visual intelligence. A project\nassociated with this paper can be found at https://github.com/mwxely/AIGS .\n","authors":["Zuhao Yang","Fangneng Zhan","Kunhao Liu","Muyu Xu","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2310.01830v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.05812v1","updated":"2023-10-09T15:52:59Z","published":"2023-10-09T15:52:59Z","title":"Provably Convergent Data-Driven Convex-Nonconvex Regularization","summary":"  An emerging new paradigm for solving inverse problems is via the use of deep\nlearning to learn a regularizer from data. This leads to high-quality results,\nbut often at the cost of provable guarantees. In this work, we show how\nwell-posedness and convergent regularization arises within the convex-nonconvex\n(CNC) framework for inverse problems. We introduce a novel input weakly convex\nneural network (IWCNN) construction to adapt the method of learned adversarial\nregularization to the CNC framework. Empirically we show that our method\novercomes numerical issues of previous adversarial methods.\n","authors":["Zakhar Shumaylov","Jeremy Budd","Subhadip Mukherjee","Carola-Bibiane Schnlieb"],"pdf_url":"https://arxiv.org/pdf/2310.05812v1.pdf","comment":"4 pages + 3 pages appendices; preprint"},{"id":"http://arxiv.org/abs/2303.05118v4","updated":"2023-10-09T15:50:00Z","published":"2023-03-09T08:57:01Z","title":"SLCA: Slow Learner with Classifier Alignment for Continual Learning on a\n  Pre-trained Model","summary":"  The goal of continual learning is to improve the performance of recognition\nmodels in learning sequentially arrived data. Although most existing works are\nestablished on the premise of learning from scratch, growing efforts have been\ndevoted to incorporating the benefits of pre-training. However, how to\nadaptively exploit the pre-trained knowledge for each incremental task while\nmaintaining its generalizability remains an open question. In this work, we\npresent an extensive analysis for continual learning on a pre-trained model\n(CLPM), and attribute the key challenge to a progressive overfitting problem.\nObserving that selectively reducing the learning rate can almost resolve this\nissue in the representation layer, we propose a simple but extremely effective\napproach named Slow Learner with Classifier Alignment (SLCA), which further\nimproves the classification layer by modeling the class-wise distributions and\naligning the classification layers in a post-hoc fashion. Across a variety of\nscenarios, our proposal provides substantial improvements for CLPM (e.g., up to\n49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split\nCUB-200 and Split Cars-196, respectively), and thus outperforms\nstate-of-the-art approaches by a large margin. Based on such a strong baseline,\ncritical factors and promising directions are analyzed in-depth to facilitate\nsubsequent research. Code has been made available at:\nhttps://github.com/GengDavid/SLCA.\n","authors":["Gengwei Zhang","Liyuan Wang","Guoliang Kang","Ling Chen","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2303.05118v4.pdf","comment":"ICCV 2023, code released"},{"id":"http://arxiv.org/abs/2310.05804v1","updated":"2023-10-09T15:43:07Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11723v6","updated":"2023-10-09T15:18:32Z","published":"2022-06-23T14:16:30Z","title":"Self-Supervised Training with Autoencoders for Visual Anomaly Detection","summary":"  Deep autoencoders provide an effective tool for learning non-linear\ndimensionality reduction in an unsupervised way. Recently, they have been used\nfor the task of anomaly detection in the visual domain. By optimizing for the\nreconstruction error using anomaly-free examples, the common belief is that a\ncorresponding network should fail to accurately reconstruct anomalous regions\nin the application phase. This goal is typically addressed by controlling the\ncapacity of the network, either by reducing the size of the bottleneck layer or\nby enforcing sparsity constraints on the activations. However, neither of these\ntechniques does explicitly penalize reconstruction of anomalous signals often\nresulting in poor detection. We tackle this problem by adapting a\nself-supervised learning regime that allows the use of discriminative\ninformation during training but focuses on the data manifold of normal\nexamples. We emphasize that inference with our approach is very efficient\nduring training and prediction requiring a single forward pass for each input\nimage. Our experiments on the MVTec AD dataset demonstrate high detection and\nlocalization performance. On the texture-subset, in particular, our approach\nconsistently outperforms recent anomaly detection methods by a significant\nmargin.\n","authors":["Alexander Bauer","Shinichi Nakajima","Klaus-Robert Mller"],"pdf_url":"https://arxiv.org/pdf/2206.11723v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09268v2","updated":"2023-10-09T15:17:25Z","published":"2023-03-16T12:44:44Z","title":"StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized\n  Tokenizer of a Large-Scale Generative Model","summary":"  Despite the progress made in the style transfer task, most previous work\nfocus on transferring only relatively simple features like color or texture,\nwhile missing more abstract concepts such as overall art expression or\npainter-specific traits. However, these abstract semantics can be captured by\nmodels like DALL-E or CLIP, which have been trained using huge datasets of\nimages and textual documents. In this paper, we propose StylerDALLE, a style\ntransfer method that exploits both of these models and uses natural language to\ndescribe abstract art styles. Specifically, we formulate the language-guided\nstyle transfer task as a non-autoregressive token sequence translation, i.e.,\nfrom input content image to output stylized image, in the discrete latent space\nof a large-scale pretrained vector-quantized tokenizer, e.g., the discrete\nvariational auto-encoder (dVAE) of DALL-E. To incorporate style information, we\npropose a Reinforcement Learning strategy with CLIP-based language supervision\nthat ensures stylization and content preservation simultaneously. Experimental\nresults demonstrate the superiority of our method, which can effectively\ntransfer art styles using language instructions at different granularities.\nCode is available at https://github.com/zipengxuc/StylerDALLE.\n","authors":["Zipeng Xu","Enver Sangineto","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.09268v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05785v1","updated":"2023-10-09T15:16:35Z","published":"2023-10-09T15:16:35Z","title":"Joint object detection and re-identification for 3D obstacle\n  multi-camera systems","summary":"  In recent years, the field of autonomous driving has witnessed remarkable\nadvancements, driven by the integration of a multitude of sensors, including\ncameras and LiDAR systems, in different prototypes. However, with the\nproliferation of sensor data comes the pressing need for more sophisticated\ninformation processing techniques. This research paper introduces a novel\nmodification to an object detection network that uses camera and lidar\ninformation, incorporating an additional branch designed for the task of\nre-identifying objects across adjacent cameras within the same vehicle while\nelevating the quality of the baseline 3D object detection outcomes. The\nproposed methodology employs a two-step detection pipeline: initially, an\nobject detection network is employed, followed by a 3D box estimator that\noperates on the filtered point cloud generated from the network's detections.\nExtensive experimental evaluations encompassing both 2D and 3D domains validate\nthe effectiveness of the proposed approach and the results underscore the\nsuperiority of this method over traditional Non-Maximum Suppression (NMS)\ntechniques, with an improvement of more than 5\\% in the car category in the\noverlapping areas.\n","authors":["Irene Corts","Jorge Beltrn","Arturo de la Escalera","Fernando Garca"],"pdf_url":"https://arxiv.org/pdf/2310.05785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05773v1","updated":"2023-10-09T14:57:41Z","published":"2023-10-09T14:57:41Z","title":"Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory\n  Matching","summary":"  The ultimate goal of Dataset Distillation is to synthesize a small synthetic\ndataset such that a model trained on this synthetic set will perform equally\nwell as a model trained on the full, real dataset. Until now, no method of\nDataset Distillation has reached this completely lossless goal, in part due to\nthe fact that previous methods only remain effective when the total number of\nsynthetic samples is extremely small. Since only so much information can be\ncontained in such a small number of samples, it seems that to achieve truly\nloss dataset distillation, we must develop a distillation method that remains\neffective as the size of the synthetic dataset grows. In this work, we present\nsuch an algorithm and elucidate why existing methods fail to generate larger,\nhigh-quality synthetic sets. Current state-of-the-art methods rely on\ntrajectory-matching, or optimizing the synthetic data to induce similar\nlong-term training dynamics as the real data. We empirically find that the\ntraining stage of the trajectories we choose to match (i.e., early or late)\ngreatly affects the effectiveness of the distilled dataset. Specifically, early\ntrajectories (where the teacher network learns easy patterns) work well for a\nlow-cardinality synthetic set since there are fewer examples wherein to\ndistribute the necessary information. Conversely, late trajectories (where the\nteacher network learns hard patterns) provide better signals for larger\nsynthetic sets since there are now enough samples to represent the necessary\ncomplex patterns. Based on our findings, we propose to align the difficulty of\nthe generated patterns with the size of the synthetic dataset. In doing so, we\nsuccessfully scale trajectory matching-based methods to larger synthetic\ndatasets, achieving lossless dataset distillation for the very first time. Code\nand distilled datasets are available at https://gzyaftermath.github.io/DATM.\n","authors":["Ziyao Guo","Kai Wang","George Cazenavette","Hui Li","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2310.05773v1.pdf","comment":"First lossless dataset distillation method"},{"id":"http://arxiv.org/abs/2310.05768v1","updated":"2023-10-09T14:54:37Z","published":"2023-10-09T14:54:37Z","title":"DANet: Enhancing Small Object Detection through an Efficient Deformable\n  Attention Network","summary":"  Efficient and accurate detection of small objects in manufacturing settings,\nsuch as defects and cracks, is crucial for ensuring product quality and safety.\nTo address this issue, we proposed a comprehensive strategy by synergizing\nFaster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature\nPyramid Network, we enable the model to efficiently handle multi-scale features\nintrinsic to manufacturing environments. Additionally, Deformable Net is used\nthat contorts and conforms to the geometric variations of defects, bringing\nprecision in detecting even the minuscule and complex features. Then, we\nincorporated an attention mechanism called Convolutional Block Attention Module\nin each block of our base ResNet50 network to selectively emphasize informative\nfeatures and suppress less useful ones. After that we incorporated RoI Align,\nreplacing RoI Pooling for finer region-of-interest alignment and finally the\nintegration of Focal Loss effectively handles class imbalance, crucial for rare\ndefect occurrences. The rigorous evaluation of our model on both the NEU-DET\nand Pascal VOC datasets underscores its robust performance and generalization\ncapabilities. On the NEU-DET dataset, our model exhibited a profound\nunderstanding of steel defects, achieving state-of-the-art accuracy in\nidentifying various defects. Simultaneously, when evaluated on the Pascal VOC\ndataset, our model showcases its ability to detect objects across a wide\nspectrum of categories within complex and small scenes.\n","authors":["Md Sohag Mia","Abdullah Al Bary Voban","Abu Bakor Hayat Arnob","Abdu Naim","Md Kawsar Ahmed","Md Shariful Islam"],"pdf_url":"https://arxiv.org/pdf/2310.05768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05762v1","updated":"2023-10-09T14:44:01Z","published":"2023-10-09T14:44:01Z","title":"3D tomatoes' localisation with monocular cameras using histogram filters","summary":"  Performing tasks in agriculture, such as fruit monitoring or harvesting,\nrequires perceiving the objects' spatial position. RGB-D cameras are limited\nunder open-field environments due to lightning interferences. Therefore, in\nthis study, we approach the use of Histogram Filters (Bayesian Discrete\nFilters) to estimate the position of tomatoes in the tomato plant. Two kernel\nfilters were studied: the square kernel and the Gaussian kernel. The\nimplemented algorithm was essayed in simulation, with and without Gaussian\nnoise and random noise, and in a testbed at laboratory conditions. The\nalgorithm reported a mean absolute error lower than 10 mm in simulation and 20\nmm in the testbed at laboratory conditions with an assessing distance of about\n0.5 m. So, the results are viable for real environments and should be improved\nat closer distances.\n","authors":["Sandro Costa Magalhes","Filipe Neves dos Santos","Antnio Paulo Moreira","Jorge Dias"],"pdf_url":"https://arxiv.org/pdf/2310.05762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15112v4","updated":"2023-10-09T14:41:11Z","published":"2023-09-26T17:58:20Z","title":"InternLM-XComposer: A Vision-Language Large Model for Advanced\n  Text-image Comprehension and Composition","summary":"  We propose InternLM-XComposer, a vision-language large model that enables\nadvanced image-text comprehension and composition. The innovative nature of our\nmodel is highlighted by three appealing properties: 1) Interleaved Text-Image\nComposition: InternLM-XComposer can effortlessly generate coherent and\ncontextual articles that seamlessly integrate images, providing a more engaging\nand immersive reading experience. Simply provide a title, and our system will\ngenerate the corresponding manuscript. It can intelligently identify the areas\nin the text where images would enhance the content and automatically insert the\nmost appropriate visual candidates. 2) Comprehension with Rich Multilingual\nKnowledge: The text-image comprehension is empowered by training on extensive\nmulti-modal multilingual concepts with carefully crafted strategies, resulting\nin a deep understanding of visual content. 3) State-of-the-art Performance: Our\nmodel consistently achieves state-of-the-art results across various mainstream\nbenchmarks for vision-language foundational models, including MME Benchmark,\nMMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).\nCollectively, InternLM-XComposer seamlessly blends advanced text-image\ncomprehension and composition, revolutionizing vision-language interaction and\noffering new insights and opportunities. The InternLM-XComposer model series\nwith 7B parameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.\n","authors":["Pan Zhang","Xiaoyi Dong","Bin Wang","Yuhang Cao","Chao Xu","Linke Ouyang","Zhiyuan Zhao","Shuangrui Ding","Songyang Zhang","Haodong Duan","Wenwei Zhang","Hang Yan","Xinyue Zhang","Wei Li","Jingwen Li","Kai Chen","Conghui He","Xingcheng Zhang","Yu Qiao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2309.15112v4.pdf","comment":"Code and models are available at\n  https://github.com/InternLM/InternLM-XComposer"},{"id":"http://arxiv.org/abs/2307.09052v2","updated":"2023-10-09T14:32:21Z","published":"2023-07-18T08:06:14Z","title":"Connections between Operator-splitting Methods and Deep Neural Networks\n  with Applications in Image Segmentation","summary":"  Deep neural network is a powerful tool for many tasks. Understanding why it\nis so successful and providing a mathematical explanation is an important\nproblem and has been one popular research direction in past years. In the\nliterature of mathematical analysis of deep neural networks, a lot of works is\ndedicated to establishing representation theories. How to make connections\nbetween deep neural networks and mathematical algorithms is still under\ndevelopment. In this paper, we give an algorithmic explanation for deep neural\nnetworks, especially in their connections with operator splitting. We show that\nwith certain splitting strategies, operator-splitting methods have the same\nstructure as networks. Utilizing this connection and the Potts model for image\nsegmentation, two networks inspired by operator-splitting methods are proposed.\nThe two networks are essentially two operator-splitting algorithms solving the\nPotts model. Numerical experiments are presented to demonstrate the\neffectiveness of the proposed networks.\n","authors":["Hao Liu","Xue-Cheng Tai","Raymond Chan"],"pdf_url":"https://arxiv.org/pdf/2307.09052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05754v1","updated":"2023-10-09T14:30:10Z","published":"2023-10-09T14:30:10Z","title":"Unleashing the power of Neural Collapse for Transferability Estimation","summary":"  Transferability estimation aims to provide heuristics for quantifying how\nsuitable a pre-trained model is for a specific downstream task, without\nfine-tuning them all. Prior studies have revealed that well-trained models\nexhibit the phenomenon of Neural Collapse. Based on a widely used neural\ncollapse metric in existing literature, we observe a strong correlation between\nthe neural collapse of pre-trained models and their corresponding fine-tuned\nmodels. Inspired by this observation, we propose a novel method termed Fair\nCollapse (FaCe) for transferability estimation by comprehensively measuring the\ndegree of neural collapse in the pre-trained model. Typically, FaCe comprises\ntwo different terms: the variance collapse term, which assesses the class\nseparation and within-class compactness, and the class fairness term, which\nquantifies the fairness of the pre-trained model towards each class. We\ninvestigate FaCe on a variety of pre-trained classification models across\ndifferent network architectures, source datasets, and training loss functions.\nResults show that FaCe yields state-of-the-art performance on different tasks\nincluding image classification, semantic segmentation, and text classification,\nwhich demonstrate the effectiveness and generalization of our method.\n","authors":["Yuhe Ding","Bo Jiang","Lijun Sheng","Aihua Zheng","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2310.05754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05500v2","updated":"2023-10-09T14:17:15Z","published":"2023-01-13T12:03:58Z","title":"RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised\n  Medical Image Segmentation","summary":"  Medical image segmentation methods are generally designed as fully-supervised\nto guarantee model performance, which require a significant amount of expert\nannotated samples that are high-cost and laborious. Semi-supervised image\nsegmentation can alleviate the problem by utilizing a large number of unlabeled\nimages along with limited labeled images. However, learning a robust\nrepresentation from numerous unlabeled images remains challenging due to\npotential noise in pseudo labels and insufficient class separability in feature\nspace, which undermines the performance of current semi-supervised segmentation\napproaches. To address the issues above, we propose a novel semi-supervised\nsegmentation method named as Rectified Contrastive Pseudo Supervision (RCPS),\nwhich combines a rectified pseudo supervision and voxel-level contrastive\nlearning to improve the effectiveness of semi-supervised segmentation.\nParticularly, we design a novel rectification strategy for the pseudo\nsupervision method based on uncertainty estimation and consistency\nregularization to reduce the noise influence in pseudo labels. Furthermore, we\nintroduce a bidirectional voxel contrastive loss to the network to ensure\nintra-class consistency and inter-class contrast in feature space, which\nincreases class separability in the segmentation. The proposed RCPS\nsegmentation method has been validated on two public datasets and an in-house\nclinical dataset. Experimental results reveal that the proposed method yields\nbetter segmentation performance compared with the state-of-the-art methods in\nsemi-supervised medical image segmentation. The source code is available at\nhttps://github.com/hsiangyuzhao/RCPS.\n","authors":["Xiangyu Zhao","Zengxin Qi","Sheng Wang","Qian Wang","Xuehai Wu","Ying Mao","Lichi Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.05500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05737v1","updated":"2023-10-09T14:10:29Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","Jos Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08010v3","updated":"2023-10-09T13:56:25Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v3.pdf","comment":"Accepted to ICCV 2023 (camera-ready version, 9 pages)"},{"id":"http://arxiv.org/abs/2305.08685v3","updated":"2023-10-09T13:45:46Z","published":"2023-05-15T14:42:02Z","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding","summary":"  Visual Grounding (VG) is a crucial topic in the field of vision and language,\nwhich involves locating a specific region described by expressions within an\nimage. To reduce the reliance on manually labeled data, unsupervised methods\nhave been developed to locate regions using pseudo-labels. However, the\nperformance of existing unsupervised methods is highly dependent on the quality\nof pseudo-labels and these methods always encounter issues with limited\ndiversity. In order to utilize vision and language pre-trained models to\naddress the grounding problem, and reasonably take advantage of pseudo-labels,\nwe propose CLIP-VG, a novel method that can conduct self-paced curriculum\nadapting of CLIP with pseudo-language labels. We propose a simple yet efficient\nend-to-end network architecture to realize the transfer of CLIP to the visual\ngrounding. Based on the CLIP-based architecture, we further propose\nsingle-source and multi-source curriculum adapting algorithms, which can\nprogressively find more reliable pseudo-labels to learn an optimal model,\nthereby achieving a balance between reliability and diversity for the\npseudo-language labels. Our method outperforms the current state-of-the-art\nunsupervised method by a significant margin on RefCOCO/+/g datasets in both\nsingle-source and multi-source scenarios, with improvements ranging from 6.78%\nto 10.67% and 11.39% to 14.87%, respectively. Furthermore, our approach even\noutperforms existing weakly supervised methods. The code and models are\navailable at https://github.com/linhuixiao/CLIP-VG.\n","authors":["Linhui Xiao","Xiaoshan Yang","Fang Peng","Ming Yan","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2305.08685v3.pdf","comment":"Accepted by IEEE Transaction on Multimedia (2023), Paper page:\n  https://ieeexplore.ieee.org/abstract/document/10269126. Code will be released\n  at https://github.com/linhuixiao/CLIP-VG"},{"id":"http://arxiv.org/abs/2310.05720v1","updated":"2023-10-09T13:45:21Z","published":"2023-10-09T13:45:21Z","title":"HyperLips: Hyper Control Lips with High Resolution Decoder for Talking\n  Face Generation","summary":"  Talking face generation has a wide range of potential applications in the\nfield of virtual digital humans. However, rendering high-fidelity facial video\nwhile ensuring lip synchronization is still a challenge for existing\naudio-driven talking face generation approaches. To address this issue, we\npropose HyperLips, a two-stage framework consisting of a hypernetwork for\ncontrolling lips and a high-resolution decoder for rendering high-fidelity\nfaces.In the first stage, we construct a base face generation network that uses\nthe hypernetwork to control the encoding latent code of the visual face\ninformation over audio. First, FaceEncoder is used to obtain latent code by\nextracting features from the visual face information taken from the video\nsource containing the face frame.Then, HyperConv, which weighting parameters\nare updated by HyperNet with the audio features as input, will modify the\nlatent code to synchronize the lip movement with the audio. Finally,\nFaceDecoder will decode the modified and synchronized latent code into visual\nface content. In the second stage, we obtain higher quality face videos through\na high-resolution decoder. To further improve the quality of face generation,\nwe trained a high-resolution decoder, HRDecoder, using face images and detected\nsketches generated from the first stage as input.Extensive quantitative and\nqualitative experiments show that our method outperforms state-of-the-art work\nwith more realistic, high-fidelity, and lip synchronization. Project page:\nhttps://semchan.github.io/HyperLips/\n","authors":["Yaosen Chen","Yu Yao","Zhiqiang Li","Wei Wang","Yanru Zhang","Han Yang","Xuming Wen"],"pdf_url":"https://arxiv.org/pdf/2310.05720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05718v1","updated":"2023-10-09T13:39:26Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models.\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05717v1","updated":"2023-10-09T13:39:06Z","published":"2023-10-09T13:39:06Z","title":"STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects\n  on Production Lines","summary":"  In this work, we present STOPNet, a framework for 6-DoF object suction\ndetection on production lines, with a focus on but not limited to transparent\nobjects, which is an important and challenging problem in robotic systems and\nmodern industry. Current methods requiring depth input fail on transparent\nobjects due to depth cameras' deficiency in sensing their geometry, while we\nproposed a novel framework to reconstruct the scene on the production line\ndepending only on RGB input, based on multiview stereo. Compared to existing\nworks, our method not only reconstructs the whole 3D scene in order to obtain\nhigh-quality 6-DoF suction poses in real time but also generalizes to novel\nenvironments, novel arrangements and novel objects, including challenging\ntransparent objects, both in simulation and the real world. Extensive\nexperiments in simulation and the real world show that our method significantly\nsurpasses the baselines and has better generalizability, which caters to\npractical industrial needs.\n","authors":["Yuxuan Kuang","Qin Han","Danshi Li","Qiyu Dai","Lian Ding","Dong Sun","Hanlin Zhao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05717v1.pdf","comment":"Under Review. ICRA 2024 submission"},{"id":"http://arxiv.org/abs/2306.11891v2","updated":"2023-10-09T13:24:54Z","published":"2023-06-02T17:47:29Z","title":"Vital Videos: A dataset of face videos with PPG and blood pressure\n  ground truths","summary":"  We collected a large dataset consisting of nearly 900 unique participants.\nFor every participant we recorded two 30 second uncompressed videos,\nsynchronized PPG waveforms and a single blood pressure measurement. Gender, age\nand skin color were also registered for every participant. The dataset includes\nroughly equal numbers of males and females, as well as participants of all\nages. While the skin color distribution could have been more balanced, the\ndataset contains individuals from every skin color. The data was collected in a\ndiverse set of locations to ensure a wide variety of backgrounds and lighting\nconditions. In an effort to assist in the research and development of remote\nvital sign measurement we are now opening up access to this dataset.\n","authors":["Pieter-Jan Toye"],"pdf_url":"https://arxiv.org/pdf/2306.11891v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2310.05699v1","updated":"2023-10-09T13:20:20Z","published":"2023-10-09T13:20:20Z","title":"Uni3DETR: Unified 3D Detection Transformer","summary":"  Existing point cloud based 3D detectors are designed for the particular\nscene, either indoor or outdoor ones. Because of the substantial differences in\nobject distribution and point density within point clouds collected from\nvarious environments, coupled with the intricate nature of 3D metrics, there is\nstill a lack of a unified network architecture that can accommodate diverse\nscenes. In this paper, we propose Uni3DETR, a unified 3D detector that\naddresses indoor and outdoor 3D detection within the same framework.\nSpecifically, we employ the detection transformer with point-voxel interaction\nfor object prediction, which leverages voxel features and points for\ncross-attention and behaves resistant to the discrepancies from data. We then\npropose the mixture of query points, which sufficiently exploits global\ninformation for dense small-range indoor scenes and local information for\nlarge-range sparse outdoor ones. Furthermore, our proposed decoupled IoU\nprovides an easy-to-optimize training target for localization by disentangling\nthe xy and z space. Extensive experiments validate that Uni3DETR exhibits\nexcellent performance consistently on both indoor and outdoor 3D detection. In\ncontrast to previous specialized detectors, which may perform well on some\nparticular datasets but suffer a substantial degradation on different scenes,\nUni3DETR demonstrates the strong generalization ability under heterogeneous\nconditions (Fig. 1).\n  Codes are available at\n\\href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}.\n","authors":["Zhenyu Wang","Yali Li","Xi Chen","Hengshuang Zhao","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05699v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05697v1","updated":"2023-10-09T13:16:20Z","published":"2023-10-09T13:16:20Z","title":"Combining recurrent and residual learning for deforestation monitoring\n  using multitemporal SAR images","summary":"  With its vast expanse, exceeding that of Western Europe by twice, the Amazon\nrainforest stands as the largest forest of the Earth, holding immense\nimportance in global climate regulation. Yet, deforestation detection from\nremote sensing data in this region poses a critical challenge, often hindered\nby the persistent cloud cover that obscures optical satellite data for much of\nthe year. Addressing this need, this paper proposes three deep-learning models\ntailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar)\nmultitemporal data moved by its independence on atmospheric conditions.\nSpecifically, the study proposes three novel recurrent fully convolutional\nnetwork architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance\nthe accuracy of deforestation detection. Additionally, this research explores\nreplacing a bitemporal with multitemporal SAR sequences, motivated by the\nhypothesis that deforestation signs quickly fade in SAR images over time. A\ncomprehensive assessment of the proposed approaches was conducted using a\nSentinel-1 multitemporal sequence from a sample site in the Brazilian\nrainforest. The experimental analysis confirmed that analyzing a sequence of\nSAR images over an observation period can reveal deforestation spots\nundetectable in a pair of images. Notably, experimental results underscored the\nsuperiority of the multitemporal approach, yielding approximately a five\npercent enhancement in F1-Score across all tested network architectures.\nParticularly the RRCNN-1 achieved the highest accuracy and also boasted half\nthe processing time of its closest counterpart.\n","authors":["Carla Nascimento Neves","Raul Queiroz Feitosa","Mabel X. Ortega Adarme","Gilson Antonio Giraldi"],"pdf_url":"https://arxiv.org/pdf/2310.05697v1.pdf","comment":"21 pages, 19 Figures"},{"id":"http://arxiv.org/abs/2205.11521v4","updated":"2023-10-09T13:09:25Z","published":"2022-05-23T17:59:58Z","title":"From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging\n  via Differentiable Microscopy","summary":"  With applications ranging from metabolomics to histopathology, quantitative\nphase microscopy (QPM) is a powerful label-free imaging modality. Despite\nsignificant advances in fast multiplexed imaging sensors and\ndeep-learning-based inverse solvers, the throughput of QPM is currently limited\nby the speed of electronic hardware. Complementarily, to improve throughput\nfurther, here we propose to acquire images in a compressed form such that more\ninformation can be transferred beyond the existing electronic hardware\nbottleneck. To this end, we present a learnable optical\ncompression-decompression framework that learns content-specific features. The\nproposed differentiable quantitative phase microscopy ($\\partial \\mu$) first\nuses learnable optical feature extractors as image compressors. The intensity\nrepresentation produced by these networks is then captured by the imaging\nsensor. Finally, a reconstruction network running on electronic hardware\ndecompresses the QPM images. In numerical experiments, the proposed system\nachieves compression of $\\times$ 64 while maintaining the SSIM of $\\sim 0.90$\nand PSNR of $\\sim 30$ dB on cells. The results demonstrated by our experiments\nopen up a new pathway for achieving end-to-end optimized (i.e., optics and\nelectronic) compact QPM systems that may provide unprecedented throughput\nimprovements.\n","authors":["Udith Haputhanthri","Kithmini Herath","Ramith Hettiarachchi","Hasindu Kariyawasam","Azeem Ahmad","Balpreet S. Ahluwalia","Chamira U. S. Edussooriya","Dushan N. Wadduwage"],"pdf_url":"https://arxiv.org/pdf/2205.11521v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05691v1","updated":"2023-10-09T13:07:23Z","published":"2023-10-09T13:07:23Z","title":"Climate-sensitive Urban Planning through Optimization of Tree Placements","summary":"  Climate change is increasing the intensity and frequency of many extreme\nweather events, including heatwaves, which results in increased thermal\ndiscomfort and mortality rates. While global mitigation action is undoubtedly\nnecessary, so is climate adaptation, e.g., through climate-sensitive urban\nplanning. Among the most promising strategies is harnessing the benefits of\nurban trees in shading and cooling pedestrian-level environments. Our work\ninvestigates the challenge of optimal placement of such trees. Physical\nsimulations can estimate the radiative and thermal impact of trees on human\nthermal comfort but induce high computational costs. This rules out\noptimization of tree placements over large areas and considering effects over\nlonger time scales. Hence, we employ neural networks to simulate the point-wise\nmean radiant temperatures--a driving factor of outdoor human thermal\ncomfort--across various time scales, spanning from daily variations to extended\ntime scales of heatwave events and even decades. To optimize tree placements,\nwe harness the innate local effect of trees within the iterated local search\nframework with tailored adaptations. We show the efficacy of our approach\nacross a wide spectrum of study areas and time scales. We believe that our\napproach is a step towards empowering decision-makers, urban designers and\nplanners to proactively and effectively assess the potential of urban trees to\nmitigate heat stress.\n","authors":["Simon Schrodi","Ferdinand Briegel","Max Argus","Andreas Christen","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.05691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01587v2","updated":"2023-10-09T12:56:17Z","published":"2023-08-03T07:45:53Z","title":"Consistency Regularization for Generalizable Source-free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) aims to adapt a well-trained source\nmodel to an unlabelled target domain without accessing the source dataset,\nmaking it applicable in a variety of real-world scenarios. Existing SFDA\nmethods ONLY assess their adapted models on the target training set, neglecting\nthe data from unseen but identically distributed testing sets. This oversight\nleads to overfitting issues and constrains the model's generalization ability.\nIn this paper, we propose a consistency regularization framework to develop a\nmore generalizable SFDA method, which simultaneously boosts model performance\non both target training and testing datasets. Our method leverages soft\npseudo-labels generated from weakly augmented images to supervise strongly\naugmented images, facilitating the model training process and enhancing the\ngeneralization ability of the adapted model. To leverage more potentially\nuseful supervision, we present a sampling-based pseudo-label selection\nstrategy, taking samples with severer domain shift into consideration.\nMoreover, global-oriented calibration methods are introduced to exploit global\nclass distribution and feature cluster information, further improving the\nadaptation process. Extensive experiments demonstrate our method achieves\nstate-of-the-art performance on several SFDA benchmarks, and exhibits\nrobustness on unseen testing datasets.\n","authors":["Longxiang Tang","Kai Li","Chunming He","Yulun Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2308.01587v2.pdf","comment":"Accepted by ICCV 2023 workshop"},{"id":"http://arxiv.org/abs/2310.05682v1","updated":"2023-10-09T12:51:46Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05666v1","updated":"2023-10-09T12:35:05Z","published":"2023-10-09T12:35:05Z","title":"Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for\n  Accurate Object Detection","summary":"  Anchor-based detectors have been continuously developed for object detection.\nHowever, the individual anchor box makes it difficult to predict the boundary's\noffset accurately. Instead of taking each bounding box as a closed individual,\nwe consider using multiple boxes together to get prediction boxes. To this end,\nthis paper proposes the \\textbf{Box Decouple-Couple(BDC) strategy} in the\ninference, which no longer discards the overlapping boxes, but decouples the\ncorner points of these boxes. Then, according to each corner's score, we couple\nthe corner points to select the most accurate corner pairs. To meet the BDC\nstrategy, a simple but novel model is designed named the\n\\textbf{Anchor-Intermediate Detector(AID)}, which contains two head networks,\ni.e., an anchor-based head and an anchor-free \\textbf{Corner-aware head}. The\ncorner-aware head is able to score the corners of each bounding box to\nfacilitate the coupling between corner points. Extensive experiments on MS COCO\nshow that the proposed anchor-intermediate detector respectively outperforms\ntheir baseline RetinaNet and GFL method by $\\sim$2.4 and $\\sim$1.2 AP on the MS\nCOCO test-dev dataset without any bells and whistles. Code is available at:\nhttps://github.com/YilongLv/AID.\n","authors":["Yilong Lv","Min Li","Yujie He","Shaopeng Li","Zhuzhen He","Aitao Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05666v1.pdf","comment":"Submitted 29 September, 2023; originally announced October 2023.\n  Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2206.09900v7","updated":"2023-10-09T12:34:02Z","published":"2022-06-20T17:15:50Z","title":"Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point\n  Clouds with Masked Occupancy Autoencoders","summary":"  Current perception models in autonomous driving heavily rely on large-scale\nlabelled 3D data, which is both costly and time-consuming to annotate. This\nwork proposes a solution to reduce the dependence on labelled 3D training data\nby leveraging pre-training on large-scale unlabeled outdoor LiDAR point clouds\nusing masked autoencoders (MAE). While existing masked point autoencoding\nmethods mainly focus on small-scale indoor point clouds or pillar-based\nlarge-scale outdoor LiDAR data, our approach introduces a new self-supervised\nmasked occupancy pre-training method called Occupancy-MAE, specifically\ndesigned for voxel-based large-scale outdoor LiDAR point clouds. Occupancy-MAE\ntakes advantage of the gradually sparse voxel occupancy structure of outdoor\nLiDAR point clouds and incorporates a range-aware random masking strategy and a\npretext task of occupancy prediction. By randomly masking voxels based on their\ndistance to the LiDAR and predicting the masked occupancy structure of the\nentire 3D surrounding scene, Occupancy-MAE encourages the extraction of\nhigh-level semantic information to reconstruct the masked voxel using only a\nsmall number of visible voxels. Extensive experiments demonstrate the\neffectiveness of Occupancy-MAE across several downstream tasks. For 3D object\ndetection, Occupancy-MAE reduces the labelled data required for car detection\non the KITTI dataset by half and improves small object detection by\napproximately 2% in AP on the Waymo dataset. For 3D semantic segmentation,\nOccupancy-MAE outperforms training from scratch by around 2% in mIoU. For\nmulti-object tracking, Occupancy-MAE enhances training from scratch by\napproximately 1% in terms of AMOTA and AMOTP. Codes are publicly available at\nhttps://github.com/chaytonmin/Occupancy-MAE.\n","authors":["Chen Min","Xinli Xu","Dawei Zhao","Liang Xiao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2206.09900v7.pdf","comment":"Accepted by TIV"},{"id":"http://arxiv.org/abs/2310.05664v1","updated":"2023-10-09T12:31:30Z","published":"2023-10-09T12:31:30Z","title":"ViTs are Everywhere: A Comprehensive Study Showcasing Vision\n  Transformers in Different Domain","summary":"  Transformer design is the de facto standard for natural language processing\ntasks. The success of the transformer design in natural language processing has\nlately piqued the interest of researchers in the domain of computer vision.\nWhen compared to Convolutional Neural Networks (CNNs), Vision Transformers\n(ViTs) are becoming more popular and dominant solutions for many vision\nproblems. Transformer-based models outperform other types of networks, such as\nconvolutional and recurrent neural networks, in a range of visual benchmarks.\nWe evaluate various vision transformer models in this work by dividing them\ninto distinct jobs and examining their benefits and drawbacks. ViTs can\novercome several possible difficulties with convolutional neural networks\n(CNNs). The goal of this survey is to show the first use of ViTs in CV. In the\nfirst phase, we categorize various CV applications where ViTs are appropriate.\nImage classification, object identification, image segmentation, video\ntransformer, image denoising, and NAS are all CV applications. Our next step\nwill be to analyze the state-of-the-art in each area and identify the models\nthat are currently available. In addition, we outline numerous open research\ndifficulties as well as prospective research possibilities.\n","authors":["Md Sohag Mia","Abu Bakor Hayat Arnob","Abdu Naim+","Abdullah Al Bary Voban","Md Shariful Islam"],"pdf_url":"https://arxiv.org/pdf/2310.05664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05654v1","updated":"2023-10-09T12:10:41Z","published":"2023-10-09T12:10:41Z","title":"No Token Left Behind: Efficient Vision Transformer via Dynamic Token\n  Idling","summary":"  Vision Transformers (ViTs) have demonstrated outstanding performance in\ncomputer vision tasks, yet their high computational complexity prevents their\ndeployment in computing resource-constrained environments. Various token\npruning techniques have been introduced to alleviate the high computational\nburden of ViTs by dynamically dropping image tokens. However, some undesirable\npruning at early stages may result in permanent loss of image information in\nsubsequent layers, consequently hindering model performance. To address this\nproblem, we propose IdleViT, a dynamic token-idle-based method that achieves an\nexcellent trade-off between performance and efficiency. Specifically, in each\nlayer, IdleViT selects a subset of the image tokens to participate in\ncomputations while keeping the rest of the tokens idle and directly passing\nthem to this layer's output. By allowing the idle tokens to be re-selected in\nthe following layers, IdleViT mitigates the negative impact of improper pruning\nin the early stages. Furthermore, inspired by the normalized graph cut, we\ndevise a token cut loss on the attention map as regularization to improve\nIdleViT's token selection ability. Our method is simple yet effective and can\nbe extended to pyramid ViTs since no token is completely dropped. Extensive\nexperimental results on various ViT architectures have shown that IdleViT can\ndiminish the complexity of pretrained ViTs by up to 33\\% with no more than\n0.2\\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.\nNotably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art\nEViT on DeiT-S by 0.5\\% higher accuracy and even faster inference speed. The\nsource code is available in the supplementary material.\n","authors":["Xuwei Xu","Changlin Li","Yudong Chen","Xiaojun Chang","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05654v1.pdf","comment":"Accepted to AJCAI2023"},{"id":"http://arxiv.org/abs/2305.18829v3","updated":"2023-10-09T11:59:31Z","published":"2023-05-30T08:23:06Z","title":"UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction","summary":"  Multi-camera 3D perception has emerged as a prominent research field in\nautonomous driving, offering a viable and cost-effective alternative to\nLiDAR-based solutions. The existing multi-camera algorithms primarily rely on\nmonocular 2D pre-training. However, the monocular 2D pre-training overlooks the\nspatial and temporal correlations among the multi-camera system. To address\nthis limitation, we propose the first multi-camera unified pre-training\nframework, called UniScene, which involves initially reconstructing the 3D\nscene as the foundational stage and subsequently fine-tuning the model on\ndownstream tasks. Specifically, we employ Occupancy as the general\nrepresentation for the 3D scene, enabling the model to grasp geometric priors\nof the surrounding world through pre-training. A significant benefit of\nUniScene is its capability to utilize a considerable volume of unlabeled\nimage-LiDAR pairs for pre-training purposes. The proposed multi-camera unified\npre-training framework demonstrates promising results in key tasks such as\nmulti-camera 3D object detection and surrounding semantic scene completion.\nWhen compared to monocular pre-training methods on the nuScenes dataset,\nUniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS\nfor multi-camera 3D object detection, as well as a 3% increase in mIoU for\nsurrounding semantic scene completion. By adopting our unified pre-training\nmethod, a 25% reduction in 3D training annotation costs can be achieved,\noffering significant practical value for the implementation of real-world\nautonomous driving. Codes are publicly available at\nhttps://github.com/chaytonmin/UniScene.\n","authors":["Chen Min","Liang Xiao","Dawei Zhao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2305.18829v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05647v1","updated":"2023-10-09T11:59:11Z","published":"2023-10-09T11:59:11Z","title":"Exploiting Manifold Structured Data Priors for Improved MR\n  Fingerprinting Reconstruction","summary":"  Estimating tissue parameter maps with high accuracy and precision from highly\nundersampled measurements presents one of the major challenges in MR\nfingerprinting (MRF). Many existing works project the recovered voxel\nfingerprints onto the Bloch manifold to improve reconstruction performance.\nHowever, little research focuses on exploiting the latent manifold structure\npriors among fingerprints. To fill this gap, we propose a novel MRF\nreconstruction framework based on manifold structured data priors. Since it is\ndifficult to directly estimate the fingerprint manifold structure, we model the\ntissue parameters as points on a low-dimensional parameter manifold. We reveal\nthat the fingerprint manifold shares the same intrinsic topology as the\nparameter manifold, although being embedded in different Euclidean spaces. To\nexploit the non-linear and non-local redundancies in MRF data, we divide the\nMRF data into spatial patches, and the similarity measurement among data\npatches can be accurately obtained using the Euclidean distance between the\ncorresponding patches in the parameter manifold. The measured similarity is\nthen used to construct the graph Laplacian operator, which represents the\nfingerprint manifold structure. Thus, the fingerprint manifold structure is\nintroduced in the reconstruction framework by using the low-dimensional\nparameter manifold. Additionally, we incorporate the locally low-rank prior in\nthe reconstruction framework to further utilize the local correlations within\neach patch for improved reconstruction performance. We also adopt a\nGPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian\nsampling scenarios. Experimental results demonstrate that our method can\nachieve significantly improved reconstruction performance with reduced\ncomputational time over the state-of-the-art methods.\n","authors":["Peng Li","Yuping Ji","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2310.05647v1.pdf","comment":"10 pages, 10 figures, will submit to IEEE Transactions on Medical\n  Imaging"},{"id":"http://arxiv.org/abs/2310.05644v1","updated":"2023-10-09T11:57:46Z","published":"2023-10-09T11:57:46Z","title":"Diagnosing Catastrophe: Large parts of accuracy loss in continual\n  learning can be accounted for by readout misalignment","summary":"  Unlike primates, training artificial neural networks on changing data\ndistributions leads to a rapid decrease in performance on old tasks. This\nphenomenon is commonly referred to as catastrophic forgetting. In this paper,\nwe investigate the representational changes that underlie this performance\ndecrease and identify three distinct processes that together account for the\nphenomenon. The largest component is a misalignment between hidden\nrepresentations and readout layers. Misalignment occurs due to learning on\nadditional tasks and causes internal representations to shift. Representational\ngeometry is partially conserved under this misalignment and only a small part\nof the information is irrecoverably lost. All types of representational changes\nscale with the dimensionality of hidden representations. These insights have\nimplications for deep learning applications that need to be continuously\nupdated, but may also aid aligning ANN models to the rather robust biological\nvision.\n","authors":["Daniel Anthes","Sushrut Thorat","Peter Knig","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2310.05644v1.pdf","comment":"3 pages, 1 figure; published at the 2023 Conference on Cognitive\n  Computational Neuroscience"},{"id":"http://arxiv.org/abs/2310.05642v1","updated":"2023-10-09T11:56:35Z","published":"2023-10-09T11:56:35Z","title":"Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision\n  Transformers","summary":"  Vision Transformers (ViTs) have demonstrated remarkable performance in\nvarious computer vision tasks. However, the high computational complexity\nhinders ViTs' applicability on devices with limited memory and computing\nresources. Although certain investigations have delved into the fusion of\nconvolutional layers with self-attention mechanisms to enhance the efficiency\nof ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs\nsolely based on the self-attention mechanism. Furthermore, the straightforward\nstrategy of reducing the feature channels in a large but outperforming ViT\noften results in significant performance degradation despite improved\nefficiency. To address these challenges, we propose a novel channel shuffle\nmodule to improve tiny-size ViTs, showing the potential of pure self-attention\nmodels in environments with constrained computing resources. Inspired by the\nchannel shuffle design in ShuffleNetV2 \\cite{ma2018shufflenet}, our module\nexpands the feature channels of a tiny ViT and partitions the channels into two\ngroups: the \\textit{Attended} and \\textit{Idle} groups. Self-attention\ncomputations are exclusively employed on the designated \\textit{Attended}\ngroup, followed by a channel shuffle operation that facilitates information\nexchange between the two groups. By incorporating our module into a tiny ViT,\nwe can achieve superior performance while maintaining a comparable\ncomputational complexity to the vanilla model. Specifically, our proposed\nchannel shuffle module consistently improves the top-1 accuracy on the\nImageNet-1K dataset for various tiny ViT models by up to 2.8\\%, with the\nchanges in model complexity being less than 0.03 GMACs.\n","authors":["Xuwei Xu","Sen Wang","Yudong Chen","Jiajun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02722v3","updated":"2023-10-09T11:52:48Z","published":"2023-05-04T10:43:11Z","title":"Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with\n  Uncertainty","summary":"  Knowledge distillation is an effective paradigm for boosting the performance\nof pocket-size model, especially when multiple teacher models are available,\nthe student would break the upper limit again. However, it is not economical to\ntrain diverse teacher models for the disposable distillation. In this paper, we\nintroduce a new concept dubbed Avatars for distillation, which are the\ninference ensemble models derived from the teacher. Concretely, (1) For each\niteration of distillation training, various Avatars are generated by a\nperturbation transformation. We validate that Avatars own higher upper limit of\nworking capacity and teaching ability, aiding the student model in learning\ndiverse and receptive knowledge perspectives from the teacher model. (2) During\nthe distillation, we propose an uncertainty-aware factor from the variance of\nstatistical differences between the vanilla teacher and Avatars, to adjust\nAvatars' contribution on knowledge transfer adaptively. Avatar Knowledge\nDistillation AKD is fundamentally different from existing methods and refines\nwith the innovative view of unequal training. Comprehensive experiments\ndemonstrate the effectiveness of our Avatars mechanism, which polishes up the\nstate-of-the-art distillation methods for dense prediction without more extra\ncomputational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object\nDetection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,\nrespectively.\n","authors":["Yuan Zhang","Weihua Chen","Yichen Lu","Tao Huang","Xiuyu Sun","Jian Cao"],"pdf_url":"https://arxiv.org/pdf/2305.02722v3.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.05638v1","updated":"2023-10-09T11:50:52Z","published":"2023-10-09T11:50:52Z","title":"High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway\n  Segmentation","summary":"  We propose a novel Deep Active Learning (DeepAL) model-3D Wasserstein\nDiscriminative UNet (WD-UNet) for reducing the annotation effort of medical 3D\nComputed Tomography (CT) segmentation. The proposed WD-UNet learns in a\nsemi-supervised way and accelerates learning convergence to meet or exceed the\nprediction metrics of supervised learning models. Our method can be embedded\nwith different Active Learning (AL) strategies and different network\nstructures. The model is evaluated on 3D lung airway CT scans for medical\nsegmentation and show that the use of uncertainty metric, which is parametrized\nas an input of query strategy, leads to more accurate prediction results than\nsome state-of-the-art Deep Learning (DL) supervised models, e.g.,3DUNet and 3D\nCEUNet. Compared to the above supervised DL methods, our WD-UNet not only saves\nthe cost of annotation for radiologists but also saves computational resources.\nWD-UNet uses a limited amount of annotated data (35% of the total) to achieve\nbetter predictive metrics with a more efficient deep learning model algorithm.\n","authors":["Shiyi Wang","Yang Nan","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05624v1","updated":"2023-10-09T11:26:58Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation}","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.05618v1","updated":"2023-10-09T11:18:22Z","published":"2023-10-09T11:18:22Z","title":"ASM: Adaptive Sample Mining for In-The-Wild Facial Expression\n  Recognition","summary":"  Given the similarity between facial expression categories, the presence of\ncompound facial expressions, and the subjectivity of annotators, facial\nexpression recognition (FER) datasets often suffer from ambiguity and noisy\nlabels. Ambiguous expressions are challenging to differentiate from expressions\nwith noisy labels, which hurt the robustness of FER models. Furthermore, the\ndifficulty of recognition varies across different expression categories,\nrendering a uniform approach unfair for all expressions. In this paper, we\nintroduce a novel approach called Adaptive Sample Mining (ASM) to dynamically\naddress ambiguity and noise within each expression category. First, the\nAdaptive Threshold Learning module generates two thresholds, namely the clean\nand noisy thresholds, for each category. These thresholds are based on the mean\nclass probabilities at each training epoch. Next, the Sample Mining module\npartitions the dataset into three subsets: clean, ambiguity, and noise, by\ncomparing the sample confidence with the clean and noisy thresholds. Finally,\nthe Tri-Regularization module employs a mutual learning strategy for the\nambiguity subset to enhance discrimination ability, and an unsupervised\nlearning strategy for the noise subset to mitigate the impact of noisy labels.\nExtensive experiments prove that our method can effectively mine both ambiguity\nand noise, and outperform SOTA methods on both synthetic noisy and original\ndatasets. The supplement material is available at\nhttps://github.com/zzzzzzyang/ASM.\n","authors":["Ziyang Zhang","Xiao Sun","Liuwei An","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05615v1","updated":"2023-10-09T11:08:34Z","published":"2023-10-09T11:08:34Z","title":"Adaptive Multi-head Contrastive Learning","summary":"  In contrastive learning, two views of an original image generated by\ndifferent augmentations are considered as a positive pair whose similarity is\nrequired to be high. Moreover, two views of two different images are considered\nas a negative pair, and their similarity is encouraged to be low. Normally, a\nsingle similarity measure given by a single projection head is used to evaluate\npositive and negative sample pairs, respectively. However, due to the various\naugmentation strategies and varying intra-sample similarity, augmented views\nfrom the same image are often not similar. Moreover, due to inter-sample\nsimilarity, augmented views of two different images may be more similar than\naugmented views from the same image. As such, enforcing a high similarity for\npositive pairs and a low similarity for negative pairs may not always be\nachievable, and in the case of some pairs, forcing so may be detrimental to the\nperformance. To address this issue, we propose to use multiple projection\nheads, each producing a separate set of features. Our loss function for\npre-training emerges from a solution to the maximum likelihood estimation over\nhead-wise posterior distributions of positive samples given observations. The\nloss contains the similarity measure over positive and negative pairs, each\nre-weighted by an individual adaptive temperature that is regularized to\nprevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can\nbe applied to and experimentally improves several popular contrastive learning\nmethods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent\nunder various backbones and linear probing epoches and is more significant when\nmultiple augmentation methods are used.\n","authors":["Lei Wang","Piotr Koniusz","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05615v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2310.05600v1","updated":"2023-10-09T10:35:37Z","published":"2023-10-09T10:35:37Z","title":"Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care\n  Environments","summary":"  As labor shortage increases in the health sector, the demand for assistive\nrobotics grows. However, the needed test data to develop those robots is\nscarce, especially for the application of active 3D object detection, where no\nreal data exists at all. This short paper counters this by introducing such an\nannotated dataset of real environments. The captured environments represent\nareas which are already in use in the field of robotic health care research. We\nfurther provide ground truth data within one room, for assessing SLAM\nalgorithms running directly on a health care robot.\n","authors":["Michael G. Adam","Sebastian Eger","Martin Piccolrovazzi","Maged Iskandar","Joern Vogel","Alexander Dietrich","Seongjien Bien","Jon Skerlj","Abdeldjallil Naceri","Eckehard Steinbach","Alin Albu-Schaeffer","Sami Haddadin","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2310.05600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10764v4","updated":"2023-10-09T10:35:01Z","published":"2023-02-14T13:41:57Z","title":"On The Coherence of Quantitative Evaluation of Visual Explanations","summary":"  Recent years have shown an increased development of methods for justifying\nthe predictions of neural networks through visual explanations. These\nexplanations usually take the form of heatmaps which assign a saliency (or\nrelevance) value to each pixel of the input image that expresses how relevant\nthe pixel is for the prediction of a label.\n  Complementing this development, evaluation methods have been proposed to\nassess the \"goodness\" of such explanations. On the one hand, some of these\nmethods rely on synthetic datasets. However, this introduces the weakness of\nhaving limited guarantees regarding their applicability on more realistic\nsettings. On the other hand, some methods rely on metrics for objective\nevaluation. However the level to which some of these evaluation methods perform\nwith respect to each other is uncertain.\n  Taking this into account, we conduct a comprehensive study on a subset of the\nImageNet-1k validation set where we evaluate a number of different\ncommonly-used explanation methods following a set of evaluation methods. We\ncomplement our study with sanity checks on the studied evaluation methods as a\nmeans to investigate their reliability and the impact of characteristics of the\nexplanations on the evaluation methods.\n  Results of our study suggest that there is a lack of coherency on the grading\nprovided by some of the considered evaluation methods. Moreover, we have\nidentified some characteristics of the explanations, e.g. sparsity, which can\nhave a significant effect on the performance.\n","authors":["Benjamin Vandersmissen","Jose Oramas"],"pdf_url":"https://arxiv.org/pdf/2302.10764v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05590v1","updated":"2023-10-09T10:22:08Z","published":"2023-10-09T10:22:08Z","title":"Perceptual Artifacts Localization for Image Synthesis Tasks","summary":"  Recent advancements in deep generative models have facilitated the creation\nof photo-realistic images across various tasks. However, these generated images\noften exhibit perceptual artifacts in specific regions, necessitating manual\ncorrection. In this study, we present a comprehensive empirical examination of\nPerceptual Artifacts Localization (PAL) spanning diverse image synthesis\nendeavors. We introduce a novel dataset comprising 10,168 generated images,\neach annotated with per-pixel perceptual artifact labels across ten synthesis\ntasks. A segmentation model, trained on our proposed dataset, effectively\nlocalizes artifacts across a range of tasks. Additionally, we illustrate its\nproficiency in adapting to previously unseen models using minimal training\nsamples. We further propose an innovative zoom-in inpainting pipeline that\nseamlessly rectifies perceptual artifacts in the generated images. Through our\nexperimental analyses, we elucidate several practical downstream applications,\nsuch as automated artifact rectification, non-referential image quality\nevaluation, and abnormal region detection in images. The dataset and code are\nreleased.\n","authors":["Lingzhi Zhang","Zhengjie Xu","Connelly Barnes","Yuqian Zhou","Qing Liu","He Zhang","Sohrab Amirghodsi","Zhe Lin","Eli Shechtman","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2310.05590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07688v2","updated":"2023-10-09T10:08:00Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05572v1","updated":"2023-10-09T09:51:44Z","published":"2023-10-09T09:51:44Z","title":"A Simple and Robust Framework for Cross-Modality Medical Image\n  Segmentation applied to Vision Transformers","summary":"  When it comes to clinical images, automatic segmentation has a wide variety\nof applications and a considerable diversity of input domains, such as\ndifferent types of Magnetic Resonance Images (MRIs) and Computerized Tomography\n(CT) scans. This heterogeneity is a challenge for cross-modality algorithms\nthat should equally perform independently of the input image type fed to them.\nOften, segmentation models are trained using a single modality, preventing\ngeneralization to other types of input data without resorting to transfer\nlearning techniques. Furthermore, the multi-modal or cross-modality\narchitectures proposed in the literature frequently require registered images,\nwhich are not easy to collect in clinical environments, or need additional\nprocessing steps, such as synthetic image generation. In this work, we propose\na simple framework to achieve fair image segmentation of multiple modalities\nusing a single conditional model that adapts its normalization layers based on\nthe input type, trained with non-registered interleaved mixed data. We show\nthat our framework outperforms other cross-modality segmentation methods, when\napplied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart\nSegmentation Challenge. Furthermore, we define the Conditional Vision\nTransformer (C-ViT) encoder, based on the proposed cross-modality framework,\nand we show that it brings significant improvements to the resulting\nsegmentation, up to 6.87\\% of Dice accuracy, with respect to its baseline\nreference. The code to reproduce our experiments and the trained model weights\nare available at https://github.com/matteo-bastico/MI-Seg.\n","authors":["Matteo Bastico","David Ryckelynck","Laurent Cort","Yannick Tillier","Etienne Decencire"],"pdf_url":"https://arxiv.org/pdf/2310.05572v1.pdf","comment":"This paper has been accepted in International Conference on Computer\n  Vision Workshops (ICCVW) 2023"},{"id":"http://arxiv.org/abs/2310.05556v1","updated":"2023-10-09T09:26:27Z","published":"2023-10-09T09:26:27Z","title":"WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth\n  Estimation under Adverse Weather Conditions","summary":"  Depth estimation models have shown promising performance on clear scenes but\nfail to generalize to adverse weather conditions due to illumination\nvariations, weather particles, etc. In this paper, we propose WeatherDepth, a\nself-supervised robust depth estimation model with curriculum contrastive\nlearning, to tackle performance degradation in complex weather conditions.\nConcretely, we first present a progressive curriculum learning scheme with\nthree simple-to-complex curricula to gradually adapt the model from clear to\nrelative adverse, and then to adverse weather scenes. It encourages the model\nto gradually grasp beneficial depth cues against the weather effect, yielding\nsmoother and better domain adaption. Meanwhile, to prevent the model from\nforgetting previous curricula, we integrate contrastive learning into different\ncurricula. Drawn the reference knowledge from the previous course, our strategy\nestablishes a depth consistency constraint between different courses towards\nrobust depth estimation in diverse weather. Besides, to reduce manual\nintervention and better adapt to different models, we designed an adaptive\ncurriculum scheduler to automatically search for the best timing for course\nswitching. In the experiment, the proposed solution is proven to be easily\nincorporated into various architectures and demonstrates state-of-the-art\n(SoTA) performance on both synthetic and real weather datasets.\n","authors":["Jiyuan Wang","Chunyu Lin","Lang Nie","Shujun Huang","Yao Zhao","Xing Pan","Rui Ai"],"pdf_url":"https://arxiv.org/pdf/2310.05556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08653v2","updated":"2023-10-09T09:03:12Z","published":"2022-12-16T18:59:12Z","title":"Attentive Mask CLIP","summary":"  Image token removal is an efficient augmentation strategy for reducing the\ncost of computing image features. However, this efficient augmentation strategy\nhas been found to adversely affect the accuracy of CLIP-based training. We\nhypothesize that removing a large portion of image tokens may improperly\ndiscard the semantic content associated with a given text description, thus\nconstituting an incorrect pairing target in CLIP training. To address this\nissue, we propose an attentive token removal approach for CLIP training, which\nretains tokens with a high semantic correlation to the text description. The\ncorrelation scores are computed in an online fashion using the EMA version of\nthe visual encoder. Our experiments show that the proposed attentive masking\napproach performs better than the previous method of random token removal for\nCLIP training. The approach also makes it efficient to apply multiple\naugmentation views to the image, as well as introducing instance contrastive\nlearning tasks between these views into the CLIP framework. Compared to other\nCLIP improvements that combine different pre-training targets such as SLIP and\nMaskCLIP, our method is not only more effective, but also much more efficient.\nSpecifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\\%$\ntop-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$\nand $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are\n$+1.1\\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being\n$2.30\\times$ faster. An efficient version of our approach running $1.16\\times$\nfaster than the plain CLIP model achieves significant gains of $+5.3\\%$,\n$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.\n","authors":["Yifan Yang","Weiquan Huang","Yixuan Wei","Houwen Peng","Xinyang Jiang","Huiqiang Jiang","Fangyun Wei","Yin Wang","Han Hu","Lili Qiu","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05538v1","updated":"2023-10-09T09:01:53Z","published":"2023-10-09T09:01:53Z","title":"M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion\n  for Polyp Localization in Colonoscopy Images","summary":"  Polyp segmentation is crucial for preventing colorectal cancer a common type\nof cancer. Deep learning has been used to segment polyps automatically, which\nreduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images\nis challenging because of its complex characteristics, such as color,\nocclusion, and various shapes of polyps. To address this challenge, a novel\nfrequency-based fully convolutional neural network, Multi-Frequency Feature\nFusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose\nthe input image into low/high/full-frequency components to use the\ncharacteristics of each component. We used three independent multi-frequency\nencoders to map multiple input images into a high-dimensional feature space. In\nthe Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied\nbetween each frequency component to preserve scale information. Subsequently,\nscalable attention was applied to emphasize polyp regions in a high-dimensional\nfeature space. Finally, we designed three multi-task learning (i.e., region,\nedge, and distance) in four decoder blocks to learn the structural\ncharacteristics of the region. The proposed model outperformed various\nsegmentation models with performance gains of 6.92% and 7.52% on average for\nall metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyeong Park","Nur Suriza Syazwany","Yerim Jung","Yu-Han Im","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05538v1.pdf","comment":"5pages. 2023 IEEE International Conference on Image Processing\n  (ICIP). IEEE, 2023"},{"id":"http://arxiv.org/abs/2305.14951v2","updated":"2023-10-09T08:57:38Z","published":"2023-05-24T09:42:08Z","title":"DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer","summary":"  To solve the problem of pose distortion in the forward propagation of pose\nfeatures in existing methods, this pa-per proposes a Dual-Side Feature Fusion\nNetwork for pose transfer (DSFFNet). Firstly, a fixed-length pose code is\nextracted from the source mesh by a pose encoder and combined with the target\nvertices to form a mixed feature; Then, a Feature Fusion Adaptive Instance\nNormalization module (FFAdaIN) is designed, which can process both pose and\nidentity features simultaneously, so that the pose features can be compensated\nin layer-by-layer for-ward propagation, thus solving the pose distortion\nproblem; Finally, using the mesh decoder composed of this module, the pose are\ngradually transferred to the target mesh. Experimental results on SMPL, SMAL,\nFAUST and MultiGarment datasets show that DSFFNet successfully solves the pose\ndistortion problem while maintaining a smaller network structure with stronger\npose transfer capability and faster convergence speed, and can adapt to meshes\nwith different numbers of vertices. Code is available at\nhttps://github.com/YikiDragon/DSFFNet\n","authors":["Jue Liu"],"pdf_url":"https://arxiv.org/pdf/2305.14951v2.pdf","comment":"in Chinese language"},{"id":"http://arxiv.org/abs/2310.05524v1","updated":"2023-10-09T08:42:40Z","published":"2023-10-09T08:42:40Z","title":"Bi-directional Deformation for Parameterization of Neural Implicit\n  Surfaces","summary":"  The growing capabilities of neural rendering have increased the demand for\nnew techniques that enable the intuitive editing of 3D objects, particularly\nwhen they are represented as neural implicit surfaces. In this paper, we\npresent a novel neural algorithm to parameterize neural implicit surfaces to\nsimple parametric domains, such as spheres, cubes or polycubes, where 3D\nradiance field can be represented as a 2D field, thereby facilitating\nvisualization and various editing tasks. Technically, our method computes a\nbi-directional deformation between 3D objects and their chosen parametric\ndomains, eliminating the need for any prior information. We adopt a forward\nmapping of points on the zero level set of the 3D object to a parametric\ndomain, followed by a backward mapping through inverse deformation. To ensure\nthe map is bijective, we employ a cycle loss while optimizing the smoothness of\nboth deformations. Additionally, we leverage a Laplacian regularizer to\neffectively control angle distortion and offer the flexibility to choose from a\nrange of parametric domains for managing area distortion. Designed for\ncompatibility, our framework integrates seamlessly with existing neural\nrendering pipelines, taking multi-view images as input to reconstruct 3D\ngeometry and compute the corresponding texture map. We also introduce a simple\nyet effective technique for intrinsic radiance decomposition, facilitating both\nview-independent material editing and view-dependent shading editing. Our\nmethod allows for the immediate rendering of edited textures through volume\nrendering, without the need for network re-training. Moreover, our approach\nsupports the co-parameterization of multiple objects and enables texture\ntransfer between them. We demonstrate the effectiveness of our method on images\nof human heads and man-made objects. We will make the source code publicly\navailable.\n","authors":["Baixin Xu","Jiangbei Hu","Fei Hou","Kwan-Yee Lin","Wayne Wu","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2310.05524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05512v1","updated":"2023-10-09T08:27:35Z","published":"2023-10-09T08:27:35Z","title":"UAVs and Neural Networks for search and rescue missions","summary":"  In this paper, we present a method for detecting objects of interest,\nincluding cars, humans, and fire, in aerial images captured by unmanned aerial\nvehicles (UAVs) usually during vegetation fires. To achieve this, we use\nartificial neural networks and create a dataset for supervised learning. We\naccomplish the assisted labeling of the dataset through the implementation of\nan object detection pipeline that combines classic image processing techniques\nwith pretrained neural networks. In addition, we develop a data augmentation\npipeline to augment the dataset with automatically labeled images. Finally, we\nevaluate the performance of different neural networks.\n","authors":["Hartmut Surmann","Artur Leinweber","Gerhard Senkowski","Julien Meine","Dominik Slomma"],"pdf_url":"https://arxiv.org/pdf/2310.05512v1.pdf","comment":"8 pages, 56th International Symposium on Robotics (ISR Europe) |\n  September 26-27, 2023"},{"id":"http://arxiv.org/abs/2310.05511v1","updated":"2023-10-09T08:27:05Z","published":"2023-10-09T08:27:05Z","title":"Proposal-based Temporal Action Localization with Point-level Supervision","summary":"  Point-level supervised temporal action localization (PTAL) aims at\nrecognizing and localizing actions in untrimmed videos where only a single\npoint (frame) within every action instance is annotated in training data.\nWithout temporal annotations, most previous works adopt the multiple instance\nlearning (MIL) framework, where the input video is segmented into\nnon-overlapped short snippets, and action classification is performed\nindependently on every short snippet. We argue that the MIL framework is\nsuboptimal for PTAL because it operates on separated short snippets that\ncontain limited temporal information. Therefore, the classifier only focuses on\nseveral easy-to-distinguish snippets instead of discovering the whole action\ninstance without missing any relevant snippets. To alleviate this problem, we\npropose a novel method that localizes actions by generating and evaluating\naction proposals of flexible duration that involve more comprehensive temporal\ninformation. Moreover, we introduce an efficient clustering algorithm to\nefficiently generate dense pseudo labels that provide stronger supervision, and\na fine-grained contrastive loss to further refine the quality of pseudo labels.\nExperiments show that our proposed method achieves competitive or superior\nperformance to the state-of-the-art methods and some fully-supervised methods\non four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.\n","authors":["Yuan Yin","Yifei Huang","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2310.05511v1.pdf","comment":"BMVC 2023"},{"id":"http://arxiv.org/abs/2310.05504v1","updated":"2023-10-09T08:09:15Z","published":"2023-10-09T08:09:15Z","title":"Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud\n  Registration","summary":"  State-of-the-art techniques for monocular camera reconstruction predominantly\nrely on the Structure from Motion (SfM) pipeline. However, such methods often\nyield reconstruction outcomes that lack crucial scale information, and over\ntime, accumulation of images leads to inevitable drift issues. In contrast,\nmapping methods based on LiDAR scans are popular in large-scale urban scene\nreconstruction due to their precise distance measurements, a capability\nfundamentally absent in visual-based approaches. Researchers have made attempts\nto utilize concurrent LiDAR and camera measurements in pursuit of precise\nscaling and color details within mapping outcomes. However, the outcomes are\nsubject to extrinsic calibration and time synchronization precision. In this\npaper, we propose a novel cost-effective reconstruction pipeline that utilizes\na pre-established LiDAR map as a fixed constraint to effectively address the\ninherent scale challenges present in monocular camera reconstruction. To our\nknowledge, our method is the first to register images onto the point cloud map\nwithout requiring synchronous capture of camera and LiDAR data, granting us the\nflexibility to manage reconstruction detail levels across various areas of\ninterest. To facilitate further research in this domain, we have released\nColmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that\nenables precise fine-scale registration of images to the point cloud map.\n","authors":["Chunge Bai","Ruijie Fu","Xiang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.05504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05498v1","updated":"2023-10-09T07:59:31Z","published":"2023-10-09T07:59:31Z","title":"Semi-Supervised Object Detection with Uncurated Unlabeled Data for\n  Remote Sensing Images","summary":"  Annotating remote sensing images (RSIs) presents a notable challenge due to\nits labor-intensive nature. Semi-supervised object detection (SSOD) methods\ntackle this issue by generating pseudo-labels for the unlabeled data, assuming\nthat all classes found in the unlabeled dataset are also represented in the\nlabeled data. However, real-world situations introduce the possibility of\nout-of-distribution (OOD) samples being mixed with in-distribution (ID) samples\nwithin the unlabeled dataset. In this paper, we delve into techniques for\nconducting SSOD directly on uncurated unlabeled data, which is termed Open-Set\nSemi-Supervised Object Detection (OSSOD). Our approach commences by employing\nlabeled in-distribution data to dynamically construct a class-wise feature bank\n(CFB) that captures features specific to each class. Subsequently, we compare\nthe features of predicted object bounding boxes with the corresponding entries\nin the CFB to calculate OOD scores. We design an adaptive threshold based on\nthe statistical properties of the CFB, allowing us to filter out OOD samples\neffectively. The effectiveness of our proposed method is substantiated through\nextensive experiments on two widely used remote sensing object detection\ndatasets: DIOR and DOTA. These experiments showcase the superior performance\nand efficacy of our approach for OSSOD on RSIs.\n","authors":["Nanqing Liu","Xun Xu","Yingjie Gao","Heng-Chao Li"],"pdf_url":"https://arxiv.org/pdf/2310.05498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16177v4","updated":"2023-10-09T07:58:13Z","published":"2023-07-30T09:15:38Z","title":"Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof\n  Classification in the Caribbean","summary":"  Accurate and up-to-date information on building characteristics is essential\nfor vulnerability assessment; however, the high costs and long timeframes\nassociated with conducting traditional field surveys can be an obstacle to\nobtaining critical exposure datasets needed for disaster risk management. In\nthis work, we leverage deep learning techniques for the automated\nclassification of roof characteristics from very high-resolution orthophotos\nand airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.\nWe demonstrate that the fusion of multimodal earth observation data performs\nbetter than using any single data source alone. Using our proposed methods, we\nachieve F1 scores of 0.93 and 0.92 for roof type and roof material\nclassification, respectively. This work is intended to help governments produce\nmore timely building information to improve resilience and disaster response in\nthe Caribbean.\n","authors":["Isabelle Tingzon","Nuala Margaret Cowan","Pierre Chrzanowski"],"pdf_url":"https://arxiv.org/pdf/2307.16177v4.pdf","comment":"ICCV 2023 Workshop on Artificial Intelligence for Humanitarian\n  Assistance and Disaster Response"},{"id":"http://arxiv.org/abs/2309.13038v2","updated":"2023-10-09T07:56:19Z","published":"2023-09-22T17:58:04Z","title":"Privacy Assessment on Reconstructed Images: Are Existing Evaluation\n  Metrics Faithful to Human Perception?","summary":"  Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used\nto evaluate model privacy risk under reconstruction attacks. Under these\nmetrics, reconstructed images that are determined to resemble the original one\ngenerally indicate more privacy leakage. Images determined as overall\ndissimilar, on the other hand, indicate higher robustness against attack.\nHowever, there is no guarantee that these metrics well reflect human opinions,\nwhich, as a judgement for model privacy leakage, are more trustworthy. In this\npaper, we comprehensively study the faithfulness of these hand-crafted metrics\nto human perception of privacy information from the reconstructed images. On 5\ndatasets ranging from natural images, faces, to fine-grained classes, we use 4\nexisting attack methods to reconstruct images from many different\nclassification models and, for each reconstructed image, we ask multiple human\nannotators to assess whether this image is recognizable. Our studies reveal\nthat the hand-crafted metrics only have a weak correlation with the human\nevaluation of privacy leakage and that even these metrics themselves often\ncontradict each other. These observations suggest risks of current metrics in\nthe community. To address this potential risk, we propose a learning-based\nmeasure called SemSim to evaluate the Semantic Similarity between the original\nand reconstructed images. SemSim is trained with a standard triplet loss, using\nan original image as an anchor, one of its recognizable reconstructed images as\na positive sample, and an unrecognizable one as a negative. By training on\nhuman annotations, SemSim exhibits a greater reflection of privacy leakage on\nthe semantic level. We show that SemSim has a significantly higher correlation\nwith human judgment compared with existing metrics. Moreover, this strong\ncorrelation generalizes to unseen datasets, models and attack methods.\n","authors":["Xiaoxiao Sun","Nidham Gazagnadou","Vivek Sharma","Lingjuan Lyu","Hongdong Li","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.13038v2.pdf","comment":"15 pages, 9 figures and 3 tables"},{"id":"http://arxiv.org/abs/2310.04414v2","updated":"2023-10-09T07:54:54Z","published":"2023-10-06T17:58:20Z","title":"CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis","summary":"  Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.\n","authors":["Xiaoxiao Sun","Xingjian Leng","Zijian Wang","Yang Yang","Zi Huang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04414v2.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.05483v1","updated":"2023-10-09T07:42:33Z","published":"2023-10-09T07:42:33Z","title":"Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with\n  Sparse Views","summary":"  In this paper, we propose a novel method for 3D scene and object\nreconstruction from sparse multi-view images. Different from previous methods\nthat leverage extra information such as depth or generalizable features across\nscenes, our approach leverages the scene properties embedded in the multi-view\ninputs to create precise pseudo-labels for optimization without any prior\ntraining. Specifically, we introduce a geometry-guided approach that improves\nsurface reconstruction accuracy from sparse views by leveraging spherical\nharmonics to predict the novel radiance while holistically considering all\ncolor observations for a point in the scene. Also, our pipeline exploits proxy\ngeometry and correctly handles the occlusion in generating the pseudo-labels of\nradiance, which previous image-warping methods fail to avoid. Our method,\ndubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender\ndatasets without requiring prior training, demonstrating its effectiveness in\naddressing the problem of sparse view reconstruction. Our pipeline is flexible\nand can be integrated into other implicit neural reconstruction methods for\nsparse views.\n","authors":["Jiawei Yao","Chen Wang","Tong Wu","Chuming Li"],"pdf_url":"https://arxiv.org/pdf/2310.05483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05473v1","updated":"2023-10-09T07:31:44Z","published":"2023-10-09T07:31:44Z","title":"Sentence-level Prompts Benefit Composed Image Retrieval","summary":"  Composed image retrieval (CIR) is the task of retrieving specific images by\nusing a query that involves both a reference image and a relative caption. Most\nexisting CIR models adopt the late-fusion strategy to combine visual and\nlanguage features. Besides, several approaches have also been suggested to\ngenerate a pseudo-word token from the reference image, which is further\nintegrated into the relative caption for CIR. However, these pseudo-word-based\nprompting methods have limitations when target image encompasses complex\nchanges on reference image, e.g., object removal and attribute modification. In\nthis work, we demonstrate that learning an appropriate sentence-level prompt\nfor the relative caption (SPRC) is sufficient for achieving effective composed\nimage retrieval. Instead of relying on pseudo-word-based prompts, we propose to\nleverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level\nprompts. By concatenating the learned sentence-level prompt with the relative\ncaption, one can readily use existing text-based image retrieval models to\nenhance CIR performance. Furthermore, we introduce both image-text contrastive\nloss and text prompt alignment loss to enforce the learning of suitable\nsentence-level prompts. Experiments show that our proposed method performs\nfavorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR\ndatasets. The source code and pretrained model are publicly available at\nhttps://github.com/chunmeifeng/SPRC\n","authors":["Yang Bai","Xinxing Xu","Yong Liu","Salman Khan","Fahad Khan","Wangmeng Zuo","Rick Siow Mong Goh","Chun-Mei Feng"],"pdf_url":"https://arxiv.org/pdf/2310.05473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15521v2","updated":"2023-10-09T07:24:45Z","published":"2023-06-27T14:47:43Z","title":"What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation","summary":"  While semantic segmentation has seen tremendous improvements in the past,\nthere are still significant labeling efforts necessary and the problem of\nlimited generalization to classes that have not been present during training.\nTo address this problem, zero-shot semantic segmentation makes use of large\nself-supervised vision-language models, allowing zero-shot transfer to unseen\nclasses. In this work, we build a benchmark for Multi-domain Evaluation of\nSemantic Segmentation (MESS), which allows a holistic analysis of performance\nacross a wide range of domain-specific datasets such as medicine, engineering,\nearth monitoring, biology, and agriculture. To do this, we reviewed 120\ndatasets, developed a taxonomy, and classified the datasets according to the\ndeveloped taxonomy. We select a representative subset consisting of 22 datasets\nand propose it as the MESS benchmark. We evaluate eight recently published\nmodels on the proposed MESS benchmark and analyze characteristics for the\nperformance of zero-shot transfer models. The toolkit is available at\nhttps://github.com/blumenstiel/MESS.\n","authors":["Benedikt Blumenstiel","Johannes Jakubik","Hilde Khne","Michael Vssing"],"pdf_url":"https://arxiv.org/pdf/2306.15521v2.pdf","comment":"Accepted at NeurIPS 2023 Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2307.10864v2","updated":"2023-10-09T07:20:00Z","published":"2023-07-20T13:33:28Z","title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing","summary":"  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., ``a cat and a dog''. However, its efficacy declines when dealing with\nmore complex prompts, and it does not explicitly address the problem of\nimproper attribute binding. To address the challenges posed by complex prompts\nor scenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n","authors":["Yumeng Li","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2307.10864v2.pdf","comment":"Accepted at BMVC 2023 as Oral. Code:\n  https://github.com/boschresearch/Divide-and-Bind and project page:\n  https://sites.google.com/view/divide-and-bind"},{"id":"http://arxiv.org/abs/2309.07616v2","updated":"2023-10-09T07:18:01Z","published":"2023-09-14T11:25:19Z","title":"Road Disease Detection based on Latent Domain Background Feature\n  Separation and Suppression","summary":"  Road disease detection is challenging due to the the small proportion of road\ndamage in target region and the diverse background,which introduce lots of\ndomain information.Besides, disease categories have high similarity,makes the\ndetection more difficult. In this paper, we propose a new LDBFSS(Latent Domain\nBackground Feature Separation and Suppression) network which could perform\nbackground information separation and suppression without domain supervision\nand contrastive enhancement of object features.We combine our LDBFSS network\nwith YOLOv5 model to enhance disease features for better road disease\ndetection. As the components of LDBFSS network, we first design a latent domain\ndiscovery module and a domain adversarial learning module to obtain pseudo\ndomain labels through unsupervised method, guiding domain discriminator and\nmodel to train adversarially to suppress background information. In addition,\nwe introduce a contrastive learning module and design k-instance contrastive\nloss, optimize the disease feature representation by increasing the inter-class\ndistance and reducing the intra-class distance for object features. We\nconducted experiments on two road disease detection datasets, GRDDC and CNRDD,\nand compared with other models,which show an increase of nearly 4% on GRDDC\ndataset compared with optimal model, and an increase of 4.6% on CNRDD dataset.\nExperimental results prove the effectiveness and superiority of our model.\n","authors":["Juwu Zheng","Jiangtao Ren"],"pdf_url":"https://arxiv.org/pdf/2309.07616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05462v1","updated":"2023-10-09T07:10:30Z","published":"2023-10-09T07:10:30Z","title":"AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential\n  Cross Attention","summary":"  Multi-modal medical image fusion is essential for the precise clinical\ndiagnosis and surgical navigation since it can merge the complementary\ninformation in multi-modalities into a single image. The quality of the fused\nimage depends on the extracted single modality features as well as the fusion\nrules for multi-modal information. Existing deep learning-based fusion methods\ncan fully exploit the semantic features of each modality, they cannot\ndistinguish the effective low and high frequency information of each modality\nand fuse them adaptively. To address this issue, we propose AdaFuse, in which\nmultimodal image information is fused adaptively through frequency-guided\nattention mechanism based on Fourier transform. Specifically, we propose the\ncross-attention fusion (CAF) block, which adaptively fuses features of two\nmodalities in the spatial and frequency domains by exchanging key and query\nvalues, and then calculates the cross-attention scores between the spatial and\nfrequency features to further guide the spatial-frequential information fusion.\nThe CAF block enhances the high-frequency features of the different modalities\nso that the details in the fused images can be retained. Moreover, we design a\nnovel loss function composed of structure loss and content loss to preserve\nboth low and high frequency information. Extensive comparison experiments on\nseveral datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in terms of both visual quality and quantitative\nmetrics. The ablation experiments also validate the effectiveness of the\nproposed loss and fusion strategy. Our code is publicly available at\nhttps://github.com/xianming-gu/AdaFuse.\n","authors":["Xianming Gu","Lihui Wang","Zeyu Deng","Ying Cao","Xingyu Huang","Yue-min Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.05462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.09565v3","updated":"2023-10-09T07:10:20Z","published":"2020-06-16T23:41:42Z","title":"Mining Label Distribution Drift in Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation targets to transfer task-related knowledge\nfrom labeled source domain to unlabeled target domain. Although tremendous\nefforts have been made to minimize domain divergence, most existing methods\nonly partially manage by aligning feature representations from diverse domains.\nBeyond the discrepancy in data distribution, the gap between source and target\nlabel distribution, recognized as label distribution drift, is another crucial\nfactor raising domain divergence, and has been under insufficient exploration.\nFrom this perspective, we first reveal how label distribution drift brings\nnegative influence. Next, we propose Label distribution Matching Domain\nAdversarial Network (LMDAN) to handle data distribution shift and label\ndistribution drift jointly. In LMDAN, label distribution drift is addressed by\na source sample weighting strategy, which selects samples that contribute to\npositive adaptation and avoid adverse effects brought by the mismatched\nsamples. Experiments show that LMDAN delivers superior performance under\nconsiderable label distribution drift.\n","authors":["Peizhao Li","Zhengming Ding","Hongfu Liu"],"pdf_url":"https://arxiv.org/pdf/2006.09565v3.pdf","comment":"Accepted to AJCAI'23"},{"id":"http://arxiv.org/abs/2303.15413v3","updated":"2023-10-09T07:02:43Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation","summary":"  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v3.pdf","comment":"Accepted to NeurIPS 2023. Project Page:\n  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/"},{"id":"http://arxiv.org/abs/2310.05453v1","updated":"2023-10-09T06:57:55Z","published":"2023-10-09T06:57:55Z","title":"Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation","summary":"  Universal domain adaptation aims to align the classes and reduce the feature\ngap between the same category of the source and target domains. The target\nprivate category is set as the unknown class during the adaptation process, as\nit is not included in the source domain. However, most existing methods\noverlook the intra-class structure within a category, especially in cases where\nthere exists significant concept shift between the samples belonging to the\nsame category. When samples with large concept shift are forced to be pushed\ntogether, it may negatively affect the adaptation performance. Moreover, from\nthe interpretability aspect, it is unreasonable to align visual features with\nsignificant differences, such as fighter jets and civil aircraft, into the same\ncategory. Unfortunately, due to such semantic ambiguity and annotation cost,\ncategories are not always classified in detail, making it difficult for the\nmodel to perform precise adaptation. To address these issues, we propose a\nnovel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the\ndifferences between samples belonging to the same category and mine sub-classes\nwhen there exists significant concept shift between them. By doing so, our\nmodel learns a more reasonable feature space that enhances the transferability\nand reflects the inherent differences among samples annotated as the same\ncategory. We evaluate the effectiveness of our MemSPM method over multiple\nscenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art\nperformance on four benchmarks in most cases.\n","authors":["Yuxiang Lai","Xinghong Liu","Tao Zhou","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05447v1","updated":"2023-10-09T06:43:48Z","published":"2023-10-09T06:43:48Z","title":"Towards Fair and Comprehensive Comparisons for Image-Based 3D Object\n  Detection","summary":"  In this work, we build a modular-designed codebase, formulate strong training\nrecipes, design an error diagnosis toolbox, and discuss current methods for\nimage-based 3D object detection. In particular, different from other highly\nmature tasks, e.g., 2D object detection, the community of image-based 3D object\ndetection is still evolving, where methods often adopt different training\nrecipes and tricks resulting in unfair evaluations and comparisons. What is\nworse, these tricks may overwhelm their proposed designs in performance, even\nleading to wrong conclusions. To address this issue, we build a module-designed\ncodebase and formulate unified training standards for the community.\nFurthermore, we also design an error diagnosis toolbox to measure the detailed\ncharacterization of detection models. Using these tools, we analyze current\nmethods in-depth under varying settings and provide discussions for some open\nquestions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes\ndatasets, which have led to different dominant methods for these datasets. We\nhope that this work will facilitate future research in image-based 3D object\ndetection. Our codes will be released at\n\\url{https://github.com/OpenGVLab/3dodi}\n","authors":["Xinzhu Ma","Yongtao Wan","Yinmin Zhang","Zhiyi Xia","Yuan Meng","Zhihui Wang","Haojie Li","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.05447v1.pdf","comment":"ICCV23, code will be released soon"},{"id":"http://arxiv.org/abs/2310.05446v1","updated":"2023-10-09T06:43:38Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05445v1","updated":"2023-10-09T06:42:43Z","published":"2023-10-09T06:42:43Z","title":"AngioMoCo: Learning-based Motion Correction in Cerebral Digital\n  Subtraction Angiography","summary":"  Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging\ntechnique for visualizing blood flow and guiding endovascular treatments. The\nquality of DSA is often negatively impacted by body motion during acquisition,\nleading to decreased diagnostic value. Time-consuming iterative methods address\nmotion correction based on non-rigid registration, and employ sparse key points\nand non-rigidity penalties to limit vessel distortion. Recent methods alleviate\nsubtraction artifacts by predicting the subtracted frame from the corresponding\nunsubtracted frame, but do not explicitly compensate for motion-induced\nmisalignment between frames. This hinders the serial evaluation of blood flow,\nand often causes undesired vasculature and contrast flow alterations, leading\nto impeded usability in clinical practice. To address these limitations, we\npresent AngioMoCo, a learning-based framework that generates motion-compensated\nDSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction\nand motion correction, enabling differentiation between patient motion and\nintensity changes caused by contrast flow. This strategy improves registration\nquality while being substantially faster than iterative elastix-based methods.\nWe demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN\nRegistry) of clinically acquired angiographic images through comprehensive\nqualitative and quantitative analyses. AngioMoCo produces high-quality\nmotion-compensated DSA, removing motion artifacts while preserving contrast\nflow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.\n","authors":["Ruisheng Su","Matthijs van der Sluijs","Sandra Cornelissen","Wim van Zwam","Aad van der Lugt","Wiro Niessen","Danny Ruijters","Theo van Walsum","Adrian Dalca"],"pdf_url":"https://arxiv.org/pdf/2310.05445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05428v1","updated":"2023-10-09T05:57:01Z","published":"2023-10-09T05:57:01Z","title":"Semantic-aware Temporal Channel-wise Attention for Cardiac Function\n  Assessment","summary":"  Cardiac function assessment aims at predicting left ventricular ejection\nfraction (LVEF) given an echocardiogram video, which requests models to focus\non the changes in the left ventricle during the cardiac cycle. How to assess\ncardiac function accurately and automatically from an echocardiogram video is a\nvaluable topic in intelligent assisted healthcare. Existing video-based methods\ndo not pay much attention to the left ventricular region, nor the left\nventricular changes caused by motion. In this work, we propose a\nsemi-supervised auxiliary learning paradigm with a left ventricular\nsegmentation task, which contributes to the representation learning for the\nleft ventricular region. To better model the importance of motion information,\nwe introduce a temporal channel-wise attention (TCA) module to excite those\nchannels used to describe motion. Furthermore, we reform the TCA module with\nsemantic perception by taking the segmentation map of the left ventricle as\ninput to focus on the motion patterns of the left ventricle. Finally, to reduce\nthe difficulty of direct LVEF regression, we utilize an anchor-based\nclassification and regression method to predict LVEF. Our approach achieves\nstate-of-the-art performance on the Stanford dataset with an improvement of\n0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.\n","authors":["Guanqi Chen","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.05428v1.pdf","comment":"Accepted by ISBI 2022 (oral)"},{"id":"http://arxiv.org/abs/2310.05425v1","updated":"2023-10-09T05:55:17Z","published":"2023-10-09T05:55:17Z","title":"Divide and Ensemble: Progressively Learning for the Unknown","summary":"  In the wheat nutrient deficiencies classification challenge, we present the\nDividE and EnseMble (DEEM) method for progressive test data predictions. We\nfind that (1) test images are provided in the challenge; (2) samples are\nequipped with their collection dates; (3) the samples of different dates show\nnotable discrepancies. Based on the findings, we partition the dataset into\ndiscrete groups by the dates and train models on each divided group. We then\nadopt the pseudo-labeling approach to label the test data and incorporate those\nwith high confidence into the training set. In pseudo-labeling, we leverage\nmodels ensemble with different architectures to enhance the reliability of\npredictions. The pseudo-labeling and ensembled model training are iteratively\nconducted until all test samples are labeled. Finally, the separated models for\neach group are unified to obtain the model for the whole dataset. Our method\nachieves an average of 93.6\\% Top-1 test accuracy~(94.0\\% on WW2020 and 93.2\\%\non WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency\nChallenge~\\footnote{https://cvppa2023.github.io/challenges/}.\n","authors":["Hu Zhang","Xin Shen","Heming Du","Huiqiang Chen","Chen Liu","Hongwei Sheng","Qingzheng Xu","MD Wahiduzzaman Khan","Qingtao Yu","Tianqing Zhu","Scott Chapman","Zi Huang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12270v2","updated":"2023-10-09T05:48:11Z","published":"2023-07-23T09:04:13Z","title":"Context Perception Parallel Decoder for Scene Text Recognition","summary":"  Scene text recognition (STR) methods have struggled to attain high accuracy\nand fast inference speed. Autoregressive (AR)-based models implement the\nrecognition in a character-by-character manner, showing superiority in accuracy\nbut with slow inference speed. Alternatively, parallel decoding (PD)-based\nmodels infer all characters in a single decoding pass, offering faster\ninference speed but generally worse accuracy. We first present an empirical\nstudy of AR decoding in STR, and discover that the AR decoder not only models\nlinguistic context, but also provides guidance on visual context perception.\nConsequently, we propose Context Perception Parallel Decoder (CPPD) to predict\nthe character sequence in a PD pass. CPPD devises a character counting module\nto infer the occurrence count of each character, and a character ordering\nmodule to deduce the content-free reading order and placeholders. Meanwhile,\nthe character prediction task associates the placeholders with characters. They\ntogether build a comprehensive recognition context. We construct a series of\nCPPD models and also plug the proposed modules into existing STR decoders.\nExperiments on both English and Chinese benchmarks demonstrate that the CPPD\nmodels achieve highly competitive accuracy while running approximately 8x\nfaster than their AR-based counterparts. Moreover, the plugged models achieve\nsignificant accuracy improvements. Code is at\n\\href{https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md}{this\nhttps URL}.\n","authors":["Yongkun Du","Zhineng Chen","Caiyan Jia","Xiaoting Yin","Chenxia Li","Yuning Du","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2307.12270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05406v1","updated":"2023-10-09T04:54:30Z","published":"2023-10-09T04:54:30Z","title":"GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB\n  Video","summary":"  This paper proposes GradientSurf, a novel algorithm for real time surface\nreconstruction from monocular RGB video. Inspired by Poisson Surface\nReconstruction, the proposed method builds on the tight coupling between\nsurface, volume, and oriented point cloud and solves the reconstruction problem\nin gradient-domain. Unlike Poisson Surface Reconstruction which finds an\noffline solution to the Poisson equation by solving a linear system after the\nscanning process is finished, our method finds online solutions from partial\nscans with a neural network incrementally where the Poisson layer is designed\nto supervise both local and global reconstruction. The main challenge that\nexisting methods suffer from when reconstructing from RGB signal is a lack of\ndetails in the reconstructed surface. We hypothesize this is due to the\nspectral bias of neural networks towards learning low frequency geometric\nfeatures. To address this issue, the reconstruction problem is cast onto\ngradient domain, where zeroth-order and first-order energies are minimized. The\nzeroth-order term penalizes location of the surface. The first-order term\npenalizes the difference between the gradient of reconstructed implicit\nfunction and the vector field formulated from oriented point clouds sampled at\nadaptive local densities. For the task of indoor scene reconstruction, visual\nand quantitative experimental results show that the proposed method\nreconstructs surfaces with more details in curved regions and higher fidelity\nfor small objects than previous methods.\n","authors":["Crane He Chen","Joerg Liebelt"],"pdf_url":"https://arxiv.org/pdf/2310.05406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02127v2","updated":"2023-10-09T04:46:38Z","published":"2022-09-05T20:21:37Z","title":"Design of the topology for contrastive visual-textual alignment","summary":"  Cosine similarity is the common choice for measuring the distance between the\nfeature representations in contrastive visual-textual alignment learning.\nHowever, empirically a learnable softmax temperature parameter is required when\nlearning on large-scale noisy training data. In this work, we first discuss the\nrole of softmax temperature from the embedding space's topological properties.\nWe argue that the softmax temperature is the key mechanism for contrastive\nlearning on noisy training data. It acts as a scaling factor of the distance\nrange (e.g. [-1, 1] for the cosine similarity), and its learned value indicates\nthe level of noise in the training data. Then, we propose an alternative design\nof the topology for the embedding alignment. We make use of multiple class\ntokens in the transformer architecture; then map the feature representations\nonto an oblique manifold endowed with the negative inner product as the\ndistance function. With this configuration, we largely improve the zero-shot\nclassification performance of baseline CLIP models pre-trained on large-scale\ndatasets by an average of 6.1\\%.\n","authors":["Zhun Sun"],"pdf_url":"https://arxiv.org/pdf/2209.02127v2.pdf","comment":"https://github.com/minogame/clip-mtob"},{"id":"http://arxiv.org/abs/2310.05400v1","updated":"2023-10-09T04:38:52Z","published":"2023-10-09T04:38:52Z","title":"Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient\n  Vision Transformers","summary":"  Vector-quantized image modeling has shown great potential in synthesizing\nhigh-quality images. However, generating high-resolution images remains a\nchallenging task due to the quadratic computational overhead of the\nself-attention process. In this study, we seek to explore a more efficient\ntwo-stage framework for high-resolution image generation with improvements in\nthe following three aspects. (1) Based on the observation that the first\nquantization stage has solid local property, we employ a local attention-based\nquantization model instead of the global attention mechanism used in previous\nmethods, leading to better efficiency and reconstruction quality. (2) We\nemphasize the importance of multi-grained feature interaction during image\ngeneration and introduce an efficient attention mechanism that combines global\nattention (long-range semantic consistency within the whole image) and local\nattention (fined-grained details). This approach results in faster generation\nspeed, higher generation fidelity, and improved resolution. (3) We propose a\nnew generation pipeline incorporating autoencoding training and autoregressive\ngeneration strategy, demonstrating a better paradigm for image synthesis.\nExtensive experiments demonstrate the superiority of our approach in\nhigh-quality and high-resolution image reconstruction and generation.\n","authors":["Shiyue Cao","Yueqin Yin","Lianghua Huang","Yu Liu","Xin Zhao","Deli Zhao","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05400v1.pdf","comment":"This paper is accepted to ICCV2023"},{"id":"http://arxiv.org/abs/2307.12493v3","updated":"2023-10-09T04:37:52Z","published":"2023-07-24T02:50:44Z","title":"TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition","summary":"  Text-driven diffusion models have exhibited impressive generative\ncapabilities, enabling various image editing tasks. In this paper, we propose\nTF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the\npower of text-driven diffusion models for cross-domain image-guided\ncomposition. This task aims to seamlessly integrate user-provided objects into\na specific visual context. Current diffusion-based methods often involve costly\ninstance-based optimization or finetuning of pretrained models on customized\ndatasets, which can potentially undermine their rich prior. In contrast,\nTF-ICON can leverage off-the-shelf diffusion models to perform cross-domain\nimage-guided composition without requiring additional training, finetuning, or\noptimization. Moreover, we introduce the exceptional prompt, which contains no\ninformation, to facilitate text-driven diffusion models in accurately inverting\nreal images into latent representations, forming the basis for compositing. Our\nexperiments show that equipping Stable Diffusion with the exceptional prompt\noutperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,\nCOCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile\nvisual domains. Code is available at https://github.com/Shilin-LU/TF-ICON\n","authors":["Shilin Lu","Yanzhu Liu","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2307.12493v3.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05394v1","updated":"2023-10-09T04:17:21Z","published":"2023-10-09T04:17:21Z","title":"CAMEL2: Enhancing weakly supervised learning for histopathology images\n  by incorporating the significance ratio","summary":"  Histopathology image analysis plays a crucial role in cancer diagnosis.\nHowever, training a clinically applicable segmentation algorithm requires\npathologists to engage in labour-intensive labelling. In contrast, weakly\nsupervised learning methods, which only require coarse-grained labels at the\nimage level, can significantly reduce the labeling efforts. Unfortunately,\nwhile these methods perform reasonably well in slide-level prediction, their\nability to locate cancerous regions, which is essential for many clinical\napplications, remains unsatisfactory. Previously, we proposed CAMEL, which\nachieves comparable results to those of fully supervised baselines in\npixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level\nbinary annotations for positive WSIs. Here, we present CAMEL2, by introducing a\nthreshold of the cancerous ratio for positive bags, it allows us to better\nutilize the information, consequently enabling us to scale up the image-level\nsetting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our\nresults with various datasets, demonstrate that CAMEL2, with the help of\n5,120x5,120 image-level binary annotations, which are easy to annotate,\nachieves comparable performance to that of a fully supervised baseline in both\ninstance- and slide-level classifications.\n","authors":["Gang Xu","Shuhao Wang","Lingyu Zhao","Xiao Chen","Tongwei Wang","Lang Wang","Zhenwei Luo","Dahan Wang","Zewen Zhang","Aijun Liu","Wei Ba","Zhigang Song","Huaiyin Shi","Dingrong Zhong","Jianpeng Ma"],"pdf_url":"https://arxiv.org/pdf/2310.05394v1.pdf","comment":"41 pages, 13 figures"},{"id":"http://arxiv.org/abs/2310.05393v1","updated":"2023-10-09T04:16:35Z","published":"2023-10-09T04:16:35Z","title":"Hierarchical Side-Tuning for Vision Transformers","summary":"  Fine-tuning pre-trained Vision Transformers (ViT) has consistently\ndemonstrated promising performance in the realm of visual recognition. However,\nadapting large pre-trained models to various tasks poses a significant\nchallenge. This challenge arises from the need for each model to undergo an\nindependent and comprehensive fine-tuning process, leading to substantial\ncomputational and memory demands. While recent advancements in\nParameter-efficient Transfer Learning (PETL) have demonstrated their ability to\nachieve superior performance compared to full fine-tuning with a smaller subset\nof parameter updates, they tend to overlook dense prediction tasks such as\nobject detection and segmentation. In this paper, we introduce Hierarchical\nSide-Tuning (HST), a novel PETL approach that enables ViT transfer to various\ndownstream tasks effectively. Diverging from existing methods that exclusively\nfine-tune parameters within input spaces or certain modules connected to the\nbackbone, we tune a lightweight and hierarchical side network (HSN) that\nleverages intermediate activations extracted from the backbone and generates\nmulti-scale features to make predictions. To validate HST, we conducted\nextensive experiments encompassing diverse visual tasks, including\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, our method achieves state-of-the-art average Top-1\naccuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.\nWhen applied to object detection tasks on COCO testdev benchmark, HST even\nsurpasses full fine-tuning and obtains better performance with 49.7 box AP and\n43.2 mask AP using Cascade Mask R-CNN.\n","authors":["Weifeng Lin","Ziheng Wu","Jiayu Chen","Wentao Yang","Mingxin Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2310.05393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05392v1","updated":"2023-10-09T04:07:35Z","published":"2023-10-09T04:07:35Z","title":"Lightweight Full-Convolutional Siamese Tracker","summary":"  Although single object trackers have achieved advanced performance, their\nlarge-scale network models make it difficult to apply them on the platforms\nwith limited resources. Moreover, existing lightweight trackers only achieve\nbalance between 2-3 points in terms of parameters, performance, Flops and FPS.\nTo achieve the balance among all 4 points, this paper propose a lightweight\nfull-convolutional Siamese tracker called lightFC. LightFC employs a noval\nefficient cross-correlation module (ECM) and a noval efficient rep-center head\n(ERH) to enhance the nonlinear expressiveness of the convoluational tracking\npipeline. The ECM adopts an architecture of attention-like module and fuses\nlocal spatial and channel features from the pixel-wise correlation fusion\nfeatures and enhance model nonlinearity with an inversion activation block.\nAdditionally, skip-connections and the reuse of search area features are\nintroduced by the ECM to improve its performance. The ERH reasonably introduces\nreparameterization technology and channel attention to enhance the nonlinear\nexpressiveness of the center head. Comprehensive experiments show that LightFC\nachieves a good balance between performance, parameters, Flops and FPS. The\nprecision score of LightFC outperforms MixFormerV2-S by 3.7 \\% and 6.5 \\% on\nLaSOT and TNL2K, respectively, while using 5x fewer parameters and 4.6x fewer\nFlops. Besides, LightFC runs 2x faster than MixFormerV2-S on CPUs. Our code and\nraw results can be found at https://github.com/LiYunfengLYF/LightFC\n","authors":["Li Yunfeng","Wang Bo","Li Ye","Liu Zhuoyan","Wu Xueyi"],"pdf_url":"https://arxiv.org/pdf/2310.05392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05391v1","updated":"2023-10-09T04:07:00Z","published":"2023-10-09T04:07:00Z","title":"Neural Impostor: Editing Neural Radiance Fields with Explicit Shape\n  Manipulation","summary":"  Neural Radiance Fields (NeRF) have significantly advanced the generation of\nhighly realistic and expressive 3D scenes. However, the task of editing NeRF,\nparticularly in terms of geometry modification, poses a significant challenge.\nThis issue has obstructed NeRF's wider adoption across various applications. To\ntackle the problem of efficiently editing neural implicit fields, we introduce\nNeural Impostor, a hybrid representation incorporating an explicit tetrahedral\nmesh alongside a multigrid implicit field designated for each tetrahedron\nwithin the explicit mesh. Our framework bridges the explicit shape manipulation\nand the geometric editing of implicit fields by utilizing multigrid barycentric\ncoordinate encoding, thus offering a pragmatic solution to deform, composite,\nand generate neural implicit fields while maintaining a complex volumetric\nappearance. Furthermore, we propose a comprehensive pipeline for editing neural\nimplicit fields based on a set of explicit geometric editing operations. We\nshow the robustness and adaptability of our system through diverse examples and\nexperiments, including the editing of both synthetic objects and real captured\ndata. Finally, we demonstrate the authoring process of a hybrid\nsynthetic-captured object utilizing a variety of editing operations,\nunderlining the transformative potential of Neural Impostor in the field of 3D\ncontent creation and manipulation.\n","authors":["Ruiyang Liu","Jinxu Xiang","Bowen Zhao","Ran Zhang","Jingyi Yu","Changxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05391v1.pdf","comment":"Accepted at Pacific Graphics 2023 and Computer Graphics Forum"},{"id":"http://arxiv.org/abs/2305.12653v2","updated":"2023-10-09T03:56:34Z","published":"2023-05-22T02:52:29Z","title":"Estimating Discrete Total Curvature with Per Triangle Normal Variation","summary":"  We introduce a novel approach for measuring the total curvature at every\ntriangle of a discrete surface. This method takes advantage of the relationship\nbetween per triangle total curvature and the Dirichlet energy of the Gauss map.\nThis new tool can be used on both triangle meshes and point clouds and has\nnumerous applications. In this study, we demonstrate the effectiveness of our\ntechnique by using it for feature-aware mesh decimation, and show that it\noutperforms existing curvature-estimation methods from popular libraries such\nas Meshlab, Trimesh2, and Libigl. When estimating curvature on point clouds,\nour method outperforms popular libraries PCL and CGAL.\n","authors":["Crane He Chen"],"pdf_url":"https://arxiv.org/pdf/2305.12653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16940v2","updated":"2023-10-09T03:46:41Z","published":"2023-09-29T02:45:56Z","title":"Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow","summary":"  Collaborative perception can substantially boost each agent's perception\nability by facilitating communication among multiple agents. However, temporal\nasynchrony among agents is inevitable in the real world due to communication\ndelays, interruptions, and clock misalignments. This issue causes information\nmismatch during multi-agent fusion, seriously shaking the foundation of\ncollaboration. To address this issue, we propose CoBEVFlow, an\nasynchrony-robust collaborative perception system based on bird's eye view\n(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align\nasynchronous collaboration messages sent by multiple agents. To model the\nmotion in a scene, we propose BEV flow, which is a collection of the motion\nvector corresponding to each spatial location. Based on BEV flow, asynchronous\nperceptual features can be reassigned to appropriate positions, mitigating the\nimpact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle\nasynchronous collaboration messages sent at irregular, continuous time stamps\nwithout discretization; and (ii) with BEV flow, CoBEVFlow only transports the\noriginal perceptual features, instead of generating new perceptual features,\navoiding additional noises. To validate CoBEVFlow's efficacy, we create\nIRregular V2V(IRV2V), the first synthetic collaborative perception dataset with\nvarious temporal asynchronies that simulate different real-world scenarios.\nExtensive experiments conducted on both IRV2V and the real-world dataset\nDAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is\nrobust in extremely asynchronous settings. The code is available at\nhttps://github.com/MediaBrain-SJTU/CoBEVFlow.\n","authors":["Sizhe Wei","Yuxi Wei","Yue Hu","Yifan Lu","Yiqi Zhong","Siheng Chen","Ya Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.16940v2.pdf","comment":"16 pages, 9 figures. Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05383v1","updated":"2023-10-09T03:37:30Z","published":"2023-10-09T03:37:30Z","title":"Three-Stage Cascade Framework for Blurry Video Frame Interpolation","summary":"  Blurry video frame interpolation (BVFI) aims to generate high-frame-rate\nclear videos from low-frame-rate blurry videos, is a challenging but important\ntopic in the computer vision community. Blurry videos not only provide spatial\nand temporal information like clear videos, but also contain additional motion\ninformation hidden in each blurry frame. However, existing BVFI methods usually\nfail to fully leverage all valuable information, which ultimately hinders their\nperformance. In this paper, we propose a simple end-to-end three-stage\nframework to fully explore useful information from blurry videos. The frame\ninterpolation stage designs a temporal deformable network to directly sample\nuseful information from blurry inputs and synthesize an intermediate frame at\nan arbitrary time interval. The temporal feature fusion stage explores the\nlong-term temporal information for each target frame through a bi-directional\nrecurrent deformable alignment network. And the deblurring stage applies a\ntransformer-empowered Taylor approximation network to recursively recover the\nhigh-frequency details. The proposed three-stage framework has clear task\nassignment for each module and offers good expandability, the effectiveness of\nwhich are demonstrated by various experimental results. We evaluate our model\non four benchmarks, including the Adobe240 dataset, GoPro dataset, YouTube240\ndataset and Sony dataset. Quantitative and qualitative results indicate that\nour model outperforms existing SOTA methods. Besides, experiments on real-world\nblurry videos also indicate the good generalization ability of our model.\n","authors":["Pengcheng Lei","Zaoming Yan","Tingting Wang","Faming Fang","Guixu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03602v2","updated":"2023-10-09T03:12:46Z","published":"2023-10-05T15:29:52Z","title":"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout\n  Constraints","summary":"  Text-driven 3D indoor scene generation could be useful for gaming, film\nindustry, and AR/VR applications. However, existing methods cannot faithfully\ncapture the room layout, nor do they allow flexible editing of individual\nobjects in the room. To address these problems, we present Ctrl-Room, which is\nable to generate convincing 3D rooms with designer-style layouts and\nhigh-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables\nversatile interactive editing operations such as resizing or moving individual\nfurniture items. Our key insight is to separate the modeling of layouts and\nappearance. %how to model the room that takes into account both scene texture\nand geometry at the same time. To this end, Our proposed method consists of two\nstages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The\n`Layout Generation Stage' trains a text-conditional diffusion model to learn\nthe layout distribution with our holistic scene code parameterization. Next,\nthe `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a\nvivid panoramic image of the room guided by the 3D scene layout and text\nprompt. In this way, we achieve a high-quality 3D room with convincing layouts\nand lively textures. Benefiting from the scene code parameterization, we can\neasily edit the generated room model through our mask-guided editing module,\nwithout expensive editing-specific training. Extensive experiments on the\nStructured3D dataset demonstrate that our method outperforms existing methods\nin producing more reasonable, view-consistent, and editable 3D rooms from\nnatural language prompts.\n","authors":["Chuan Fang","Xiaotao Hu","Kunming Luo","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2310.03602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05375v1","updated":"2023-10-09T03:11:08Z","published":"2023-10-09T03:11:08Z","title":"IPDreamer: Appearance-Controllable 3D Object Generation with Image\n  Prompts","summary":"  Recent advances in text-to-3D generation have been remarkable, with methods\nsuch as DreamFusion leveraging large-scale text-to-image diffusion-based models\nto supervise 3D generation. These methods, including the variational score\ndistillation proposed by ProlificDreamer, enable the synthesis of detailed and\nphotorealistic textured meshes. However, the appearance of 3D objects generated\nby these methods is often random and uncontrollable, posing a challenge in\nachieving appearance-controllable 3D objects. To address this challenge, we\nintroduce IPDreamer, a novel approach that incorporates image prompts to\nprovide specific and comprehensive appearance information for 3D object\ngeneration. Our results demonstrate that IPDreamer effectively generates\nhigh-quality 3D objects that are consistent with both the provided text and\nimage prompts, demonstrating its promising capability in\nappearance-controllable 3D object generation.\n","authors":["Bohan Zeng","Shanglin Li","Yutang Feng","Hong Li","Sicheng Gao","Jiaming Liu","Huaxia Li","Xu Tang","Jianzhuang Liu","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05375v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05371v1","updated":"2023-10-09T03:00:15Z","published":"2023-10-09T03:00:15Z","title":"Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using\n  mpMRI Segmentation and Classification","summary":"  Prostate cancer (PCa) is a severe disease among men globally. It is important\nto identify PCa early and make a precise diagnosis for effective treatment. For\nPCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as\nan invaluable imaging modality that offers a precise anatomical view of the\nprostate gland and its tissue structure. Deep learning (DL) models can enhance\nexisting clinical systems and improve patient care by locating regions of\ninterest for physicians. Recently, DL techniques have been employed to develop\na pipeline for segmenting and classifying different cancer types. These studies\nshow that DL can be used to increase diagnostic precision and give objective\nresults without variability. This work uses well-known DL models for the\nclassification and segmentation of mpMRI images to detect PCa. Our\nimplementation involves four pipelines; Semantic DeepSegNet with ResNet50,\nDeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with\na long short-term memory (LSTM). Each segmentation model is paired with a\ndifferent classifier to evaluate the performance using different metrics. The\nresults of our experiments show that the pipeline that uses the combination of\nU-Net and the LSTM model outperforms all other combinations, excelling in both\nsegmentation and classification tasks\n","authors":["Anil B. Gavade","Neel Kanwal","Priyanka A. Gavade","Rajendra Nerli"],"pdf_url":"https://arxiv.org/pdf/2310.05371v1.pdf","comment":"Accepted at CISCON-2023"},{"id":"http://arxiv.org/abs/2310.05370v1","updated":"2023-10-09T02:59:21Z","published":"2023-10-09T02:59:21Z","title":"SocialCircle: Learning the Angle-based Social Interaction Representation\n  for Pedestrian Trajectory Prediction","summary":"  Analyzing and forecasting trajectories of agents like pedestrians and cars in\ncomplex scenes has become more and more significant in many intelligent systems\nand applications. The diversity and uncertainty in socially interactive\nbehaviors among a rich variety of agents make this task more challenging than\nother deterministic computer vision tasks. Researchers have made a lot of\nefforts to quantify the effects of these interactions on future trajectories\nthrough different mathematical models and network structures, but this problem\nhas not been well solved. Inspired by marine animals that localize the\npositions of their companions underwater through echoes, we build a new\nanglebased trainable social representation, named SocialCircle, for\ncontinuously reflecting the context of social interactions at different angular\norientations relative to the target agent. We validate the effect of the\nproposed SocialCircle by training it along with several newly released\ntrajectory prediction models, and experiments show that the SocialCircle not\nonly quantitatively improves the prediction performance, but also qualitatively\nhelps better consider social interactions when forecasting pedestrian\ntrajectories in a way that is consistent with human intuitions.\n","authors":["Conghao Wong","Beihao Xia","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2310.05370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05366v1","updated":"2023-10-09T02:52:22Z","published":"2023-10-09T02:52:22Z","title":"Rotation Matters: Generalized Monocular 3D Object Detection for Various\n  Camera Systems","summary":"  Research on monocular 3D object detection is being actively studied, and as a\nresult, performance has been steadily improving. However, 3D object detection\nperformance is significantly reduced when applied to a camera system different\nfrom the system used to capture the training datasets. For example, a 3D\ndetector trained on datasets from a passenger car mostly fails to regress\naccurate 3D bounding boxes for a camera mounted on a bus. In this paper, we\nconduct extensive experiments to analyze the factors that cause performance\ndegradation. We find that changing the camera pose, especially camera\norientation, relative to the road plane caused performance degradation. In\naddition, we propose a generalized 3D object detection method that can be\nuniversally applied to various camera systems. We newly design a compensation\nmodule that corrects the estimated 3D bounding box location and heading\ndirection. The proposed module can be applied to most of the recent 3D object\ndetection networks. It increases AP3D score (KITTI moderate, IoU $> 70\\%$)\nabout 6-to-10-times above the baselines without additional training. Both\nquantitative and qualitative results show the effectiveness of the proposed\nmethod.\n","authors":["SungHo Moon","JinWoo Bae","SungHoon Im"],"pdf_url":"https://arxiv.org/pdf/2310.05366v1.pdf","comment":"Accepted to CVPRw 2023"},{"id":"http://arxiv.org/abs/2302.11713v3","updated":"2023-10-09T02:44:47Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v3.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2310.05355v1","updated":"2023-10-09T02:31:36Z","published":"2023-10-09T02:31:36Z","title":"C^2M-DoT: Cross-modal consistent multi-view medical report generation\n  with domain transfer network","summary":"  In clinical scenarios, multiple medical images with different views are\nusually generated simultaneously, and these images have high semantic\nconsistency. However, most existing medical report generation methods only\nconsider single-view data. The rich multi-view mutual information of medical\nimages can help generate more accurate reports, however, the dependence of\nmulti-view models on multi-view data in the inference stage severely limits\ntheir application in clinical practice. In addition, word-level optimization\nbased on numbers ignores the semantics of reports and medical images, and the\ngenerated reports often cannot achieve good performance. Therefore, we propose\na cross-modal consistent multi-view medical report generation with a domain\ntransfer network (C^2M-DoT). Specifically, (i) a semantic-based multi-view\ncontrastive learning medical report generation framework is adopted to utilize\ncross-view information to learn the semantic representation of lesions; (ii) a\ndomain transfer network is further proposed to ensure that the multi-view\nreport generation model can still achieve good inference performance under\nsingle-view input; (iii) meanwhile, optimization using a cross-modal\nconsistency loss facilitates the generation of textual reports that are\nsemantically consistent with medical images. Extensive experimental studies on\ntwo public benchmark datasets demonstrate that C^2M-DoT substantially\noutperforms state-of-the-art baselines in all metrics. Ablation studies also\nconfirmed the validity and necessity of each component in C^2M-DoT.\n","authors":["Ruizhi Wang","Xiangtao Wang","Jie Zhou","Thomas Lukasiewicz","Zhenghua Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05351v1","updated":"2023-10-09T02:27:04Z","published":"2023-10-09T02:27:04Z","title":"Generalized Neural Collapse for a Large Number of Classes","summary":"  Neural collapse provides an elegant mathematical characterization of learned\nlast layer representations (a.k.a. features) and classifier weights in deep\nclassification models. Such results not only provide insights but also motivate\nnew techniques for improving practical deep models. However, most of the\nexisting empirical and theoretical studies in neural collapse focus on the case\nthat the number of classes is small relative to the dimension of the feature\nspace. This paper extends neural collapse to cases where the number of classes\nare much larger than the dimension of feature space, which broadly occur for\nlanguage models, retrieval systems, and face recognition applications. We show\nthat the features and classifier exhibit a generalized neural collapse\nphenomenon, where the minimum one-vs-rest margins is maximized.We provide\nempirical study to verify the occurrence of generalized neural collapse in\npractical deep neural networks. Moreover, we provide theoretical study to show\nthat the generalized neural collapse provably occurs under unconstrained\nfeature model with spherical constraint, under certain technical conditions on\nfeature dimension and number of classes.\n","authors":["Jiachen Jiang","Jinxin Zhou","Peng Wang","Qing Qu","Dustin Mixon","Chong You","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.05351v1.pdf","comment":"32 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.05347v1","updated":"2023-10-09T02:17:31Z","published":"2023-10-09T02:17:31Z","title":"Infrared Small Target Detection Using Double-Weighted Multi-Granularity\n  Patch Tensor Model With Tensor-Train Decomposition","summary":"  Infrared small target detection plays an important role in the remote sensing\nfields. Therefore, many detection algorithms have been proposed, in which the\ninfrared patch-tensor (IPT) model has become a mainstream tool due to its\nexcellent performance. However, most IPT-based methods face great challenges,\nsuch as inaccurate measure of the tensor low-rankness and poor robustness to\ncomplex scenes, which will leadto poor detection performance. In order to solve\nthese problems, this paper proposes a novel double-weighted multi-granularity\ninfrared patch tensor (DWMGIPT) model. First, to capture different granularity\ninformation of tensor from multiple modes, a multi-granularity infrared patch\ntensor (MGIPT) model is constructed by collecting nonoverlapping patches and\ntensor augmentation based on the tensor train (TT) decomposition. Second, to\nexplore the latent structure of tensor more efficiently, we utilize the\nauto-weighted mechanism to balance the importance of information at different\ngranularity. Then, the steering kernel (SK) is employed to extract local\nstructure prior, which suppresses background interference such as strong edges\nand noise. Finally, an efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) is presented to solve the\nmodel. Extensive experiments in various challenging scenes show that the\nproposed algorithm is robust to noise and different scenes. Compared with the\nother eight state-of-the-art methods, different evaluation metrics demonstrate\nthat our method achieves better detection performance in various complex\nscenes.\n","authors":["Guiyu Zhang","Qunbo Lv","Zui Tao","Baoyu Zhu","Zheng Tan","Yuan Ma"],"pdf_url":"https://arxiv.org/pdf/2310.05347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05346v1","updated":"2023-10-09T02:15:45Z","published":"2023-10-09T02:15:45Z","title":"Anyview: Generalizable Indoor 3D Object Detection with Variable Frames","summary":"  In this paper, we propose a novel network framework for indoor 3D object\ndetection to handle variable input frame numbers in practical scenarios.\nExisting methods only consider fixed frames of input data for a single\ndetector, such as monocular RGB-D images or point clouds reconstructed from\ndense multi-view RGB-D images. While in practical application scenes such as\nrobot navigation and manipulation, the raw input to the 3D detectors is the\nRGB-D images with variable frame numbers instead of the reconstructed scene\npoint cloud. However, the previous approaches can only handle fixed frame input\ndata and have poor performance with variable frame input. In order to\nfacilitate 3D object detection methods suitable for practical tasks, we present\na novel 3D detection framework named AnyView for our practical applications,\nwhich generalizes well across different numbers of input frames with a single\nmodel. To be specific, we propose a geometric learner to mine the local\ngeometric features of each input RGB-D image frame and implement local-global\nfeature interaction through a designed spatial mixture module. Meanwhile, we\nfurther utilize a dynamic token strategy to adaptively adjust the number of\nextracted features for each frame, which ensures consistent global feature\ndensity and further enhances the generalization after fusion. Extensive\nexperiments on the ScanNet dataset show our method achieves both great\ngeneralizability and high detection accuracy with a simple and clean\narchitecture containing a similar amount of parameters with the baselines.\n","authors":["Zhenyu Wu","Xiuwei Xu","Ziwei Wang","Chong Xia","Linqing Zhao","Jiwen Lu","Haibin Yan"],"pdf_url":"https://arxiv.org/pdf/2310.05346v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.15128v3","updated":"2023-10-09T02:15:22Z","published":"2023-06-27T00:40:12Z","title":"MIMIC: Masked Image Modeling with Image Correspondences","summary":"  Dense pixel-specific representation learning at scale has been bottlenecked\ndue to the unavailability of large-scale multi-view datasets. Current methods\nfor building effective pretraining datasets heavily rely on annotated 3D\nmeshes, point clouds, and camera parameters from simulated environments,\npreventing them from building datasets from real-world data sources where such\nmetadata is lacking. We propose a pretraining dataset-curation approach that\ndoes not require any additional annotations. Our method allows us to generate\nmulti-view datasets from both real-world videos and simulated environments at\nscale. Specifically, we experiment with two scales: MIMIC-1M with 1.3M and\nMIMIC-3M with 3.1M multi-view image pairs. We train multiple models with\ndifferent masked image modeling objectives to showcase the following findings:\nRepresentations trained on our automatically generated MIMIC-3M outperform\nthose learned from expensive crowdsourced datasets (ImageNet-1K) and those\nlearned from synthetic environments (MULTIVIEW-HABITAT) on two dense geometric\ntasks: depth estimation on NYUv2 (1.7%), and surface normals estimation on\nTaskonomy (2.05%). For dense tasks which also require object understanding, we\noutperform MULTIVIEW-HABITAT, on semantic segmentation on ADE20K (3.89%), pose\nestimation on MSCOCO (9.4%), and reduce the gap with models pre-trained on the\nobject-centric expensive ImageNet-1K. We outperform even when the\nrepresentations are frozen, and when downstream training data is limited to\nfew-shot. Larger dataset (MIMIC-3M) significantly improves performance, which\nis promising since our curation method can arbitrarily scale to produce even\nlarger datasets. MIMIC code, dataset, and pretrained models are open-sourced at\nhttps://github.com/RAIVNLab/MIMIC.\n","authors":["Kalyani Marathe","Mahtab Bigverdi","Nishat Khan","Tuhin Kundu","Aniruddha Kembhavi","Linda G. Shapiro","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2306.15128v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15490v2","updated":"2023-10-09T02:14:02Z","published":"2023-09-27T08:39:03Z","title":"Survey on Deep Face Restoration: From Non-blind to Blind and Beyond","summary":"  Face restoration (FR) is a specialized field within image restoration that\naims to recover low-quality (LQ) face images into high-quality (HQ) face\nimages. Recent advances in deep learning technology have led to significant\nprogress in FR methods. In this paper, we begin by examining the prevalent\nfactors responsible for real-world LQ images and introduce degradation\ntechniques used to synthesize LQ images. We also discuss notable benchmarks\ncommonly utilized in the field. Next, we categorize FR methods based on\ndifferent tasks and explain their evolution over time. Furthermore, we explore\nthe various facial priors commonly utilized in the restoration process and\ndiscuss strategies to enhance their effectiveness. In the experimental section,\nwe thoroughly evaluate the performance of state-of-the-art FR methods across\nvarious tasks using a unified benchmark. We analyze their performance from\ndifferent perspectives. Finally, we discuss the challenges faced in the field\nof FR and propose potential directions for future advancements. The open-source\nrepository corresponding to this work can be found at https:// github.com/\n24wenjie-li/ Awesome-Face-Restoration.\n","authors":["Wenjie Li","Mei Wang","Kai Zhang","Juncheng Li","Xiaoming Li","Yuhang Zhang","Guangwei Gao","Weihong Deng","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2309.15490v2.pdf","comment":"Face restoration, Survey, Deep learning, Non-blind/Blind, Joint\n  restoration tasks, Facial priors"},{"id":"http://arxiv.org/abs/2309.05257v3","updated":"2023-10-09T02:09:11Z","published":"2023-09-11T06:27:25Z","title":"FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal\n  Consistent Transformer for 3D Object Detection","summary":"  Multi-sensor modal fusion has demonstrated strong advantages in 3D object\ndetection tasks. However, existing methods that fuse multi-modal features\nrequire transforming features into the bird's eye view space and may lose\ncertain information on Z-axis, thus leading to inferior performance. To this\nend, we propose a novel end-to-end multi-modal fusion transformer-based\nframework, dubbed FusionFormer, that incorporates deformable attention and\nresidual structures within the fusion encoding module. Specifically, by\ndeveloping a uniform sampling strategy, our method can easily sample from 2D\nimage and 3D voxel features spontaneously, thus exploiting flexible\nadaptability and avoiding explicit transformation to the bird's eye view space\nduring the feature concatenation process. We further implement a residual\nstructure in our feature encoder to ensure the model's robustness in case of\nmissing an input modality. Through extensive experiments on a popular\nautonomous driving benchmark dataset, nuScenes, our method achieves\nstate-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D\nobject detection task without test time augmentation.\n","authors":["Chunyong Hu","Hang Zheng","Kun Li","Jianyun Xu","Weibo Mao","Maochun Luo","Lingxuan Wang","Mingxia Chen","Qihao Peng","Kaixuan Liu","Yiru Zhao","Peihan Hao","Minzhe Liu","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2309.05257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v1","updated":"2023-10-09T01:59:49Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05338v1","updated":"2023-10-09T01:52:27Z","published":"2023-10-09T01:52:27Z","title":"Negative Object Presence Evaluation (NOPE) to Measure Object\n  Hallucination in Vision-Language Models","summary":"  Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.\n","authors":["Holy Lovenia","Wenliang Dai","Samuel Cahyawijaya","Ziwei Ji","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2310.05338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05337v1","updated":"2023-10-09T01:52:07Z","published":"2023-10-09T01:52:07Z","title":"What do larger image classifiers memorise?","summary":"  The success of modern neural networks has prompted study of the connection\nbetween memorisation and generalisation: overparameterised models generalise\nwell, despite being able to perfectly fit (memorise) completely random labels.\nTo carefully study this issue, Feldman proposed a metric to quantify the degree\nof memorisation of individual training examples, and empirically computed the\ncorresponding memorisation profile of a ResNet on image classification\nbench-marks. While an exciting first glimpse into what real-world models\nmemorise, this leaves open a fundamental question: do larger neural models\nmemorise more? We present a comprehensive empirical analysis of this question\non image classification benchmarks. We find that training examples exhibit an\nunexpectedly diverse set of memorisation trajectories across model sizes: most\nsamples experience decreased memorisation under larger models, while the rest\nexhibit cap-shaped or increasing memorisation. We show that various proxies for\nthe Feldman memorization score fail to capture these fundamental trends.\nLastly, we find that knowledge distillation, an effective and popular model\ncompression technique, tends to inhibit memorisation, while also improving\ngeneralisation. Specifically, memorisation is mostly inhibited on examples with\nincreasing memorisation trajectories, thus pointing at how distillation\nimproves generalisation.\n","authors":["Michal Lukasik","Vaishnavh Nagarajan","Ankit Singh Rawat","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2310.05337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05336v1","updated":"2023-10-09T01:44:06Z","published":"2023-10-09T01:44:06Z","title":"GReAT: A Graph Regularized Adversarial Training Method","summary":"  This paper proposes a regularization method called GReAT, Graph Regularized\nAdversarial Training, to improve deep learning models' classification\nperformance. Adversarial examples are a well-known challenge in machine\nlearning, where small, purposeful perturbations to input data can mislead\nmodels. Adversarial training, a powerful and one of the most effective defense\nstrategies, involves training models with both regular and adversarial\nexamples. However, it often neglects the underlying structure of the data. In\nresponse, we propose GReAT, a method that leverages data graph structure to\nenhance model robustness. GReAT deploys the graph structure of the data into\nthe adversarial training process, resulting in more robust models that better\ngeneralize its testing performance and defend against adversarial attacks.\nThrough extensive evaluation on benchmark datasets, we demonstrate GReAT's\neffectiveness compared to state-of-the-art classification methods, highlighting\nits potential in improving deep learning models' classification performance.\n","authors":["Samet Bayram","Kenneth Barner"],"pdf_url":"https://arxiv.org/pdf/2310.05336v1.pdf","comment":"25 pages including references. 7 figures and 4 tables"},{"id":"http://arxiv.org/abs/2310.05330v1","updated":"2023-10-09T01:23:08Z","published":"2023-10-09T01:23:08Z","title":"A Lightweight Video Anomaly Detection Model with Weak Supervision and\n  Adaptive Instance Selection","summary":"  Video anomaly detection is to determine whether there are any abnormal\nevents, behaviors or objects in a given video, which enables effective and\nintelligent public safety management. As video anomaly labeling is both\ntime-consuming and expensive, most existing works employ unsupervised or weakly\nsupervised learning methods. This paper focuses on weakly supervised video\nanomaly detection, in which the training videos are labeled whether or not they\ncontain any anomalies, but there is no information about which frames the\nanomalies are located. However, the uncertainty of weakly labeled data and the\nlarge model size prevent existing methods from wide deployment in real\nscenarios, especially the resource-limit situations such as edge-computing. In\nthis paper, we develop a lightweight video anomaly detection model. On the one\nhand, we propose an adaptive instance selection strategy, which is based on the\nmodel's current status to select confident instances, thereby mitigating the\nuncertainty of weakly labeled data and subsequently promoting the model's\nperformance. On the other hand, we design a lightweight multi-level temporal\ncorrelation attention module and an hourglass-shaped fully connected layer to\nconstruct the model, which can reduce the model parameters to only 0.56\\% of\nthe existing methods (e.g. RTFM). Our extensive experiments on two public\ndatasets UCF-Crime and ShanghaiTech show that our model can achieve comparable\nor even superior AUC score compared to the state-of-the-art methods, with a\nsignificantly reduced number of model parameters.\n","authors":["Yang Wang","Jiaogen Zhou","Jihong Guan"],"pdf_url":"https://arxiv.org/pdf/2310.05330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08851v3","updated":"2023-10-09T01:15:32Z","published":"2023-05-15T17:59:15Z","title":"MV-Map: Offboard HD-Map Generation with Multi-view Consistency","summary":"  While bird's-eye-view (BEV) perception models can be useful for building\nhigh-definition maps (HD-Maps) with less human labor, their results are often\nunreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps\nfrom different viewpoints. This is because BEV perception is typically set up\nin an 'onboard' manner, which restricts the computation and consequently\nprevents algorithms from reasoning multiple views simultaneously. This paper\novercomes these limitations and advocates a more practical 'offboard' HD-Map\ngeneration setup that removes the computation constraints, based on the fact\nthat HD-Maps are commonly reusable infrastructures built offline in data\ncenters. To this end, we propose a novel offboard pipeline called MV-Map that\ncapitalizes multi-view consistency and can handle an arbitrary number of frames\nwith the key design of a 'region-centric' framework. In MV-Map, the target\nHD-Maps are created by aggregating all the frames of onboard predictions,\nweighted by the confidence scores assigned by an 'uncertainty network'. To\nfurther enhance multi-view consistency, we augment the uncertainty network with\nthe global 3D structure optimized by a voxelized neural radiance field\n(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map\nsignificantly improves the quality of HD-Maps, further highlighting the\nimportance of offboard methods for HD-Map generation.\n","authors":["Ziyang Xie","Ziqi Pang","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2305.08851v3.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05321v1","updated":"2023-10-09T00:55:41Z","published":"2023-10-09T00:55:41Z","title":"Edge Computing-Enabled Road Condition Monitoring: System Development and\n  Evaluation","summary":"  Real-time pavement condition monitoring provides highway agencies with timely\nand accurate information that could form the basis of pavement maintenance and\nrehabilitation policies. Existing technologies rely heavily on manual data\nprocessing, are expensive and therefore, difficult to scale for frequent,\nnetworklevel pavement condition monitoring. Additionally, these systems require\nsending large packets of data to the cloud which requires large storage space,\nare computationally expensive to process, and results in high latency. The\ncurrent study proposes a solution that capitalizes on the widespread\navailability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge\ncomputing and internet connection capabilities of microcontrollers, and\ndeployable machine learning (ML) models to (a) design an Internet of Things\n(IoT)-enabled device that can be mounted on axles of vehicles to stream live\npavement condition data (b) reduce latency through on-device processing and\nanalytics of pavement condition sensor data before sending to the cloud\nservers. In this study, three ML models including Random Forest, LightGBM and\nXGBoost were trained to predict International Roughness Index (IRI) at every\n0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of\n16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI\nof pavement segments based on ride quality according to MAP-21 criteria, our\nproposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on\nSouth Providence. Overall, our proposed device demonstrates significant\npotential in providing real-time pavement condition data to State Highway\nAgencies (SHA) and Department of Transportation (DOTs) with a satisfactory\nlevel of accuracy.\n","authors":["Abdulateef Daud","Mark Amo-Boateng","Neema Jakisa Owor","Armstrong Aboah","Yaw Adu-Gyamfi"],"pdf_url":"https://arxiv.org/pdf/2310.05321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05316v1","updated":"2023-10-09T00:17:20Z","published":"2023-10-09T00:17:20Z","title":"Understanding the Feature Norm for Out-of-Distribution Detection","summary":"  A neural network trained on a classification dataset often exhibits a higher\nvector norm of hidden layer features for in-distribution (ID) samples, while\nproducing relatively lower norm values on unseen instances from\nout-of-distribution (OOD). Despite this intriguing phenomenon being utilized in\nmany applications, the underlying cause has not been thoroughly investigated.\nIn this study, we demystify this very phenomenon by scrutinizing the\ndiscriminative structures concealed in the intermediate layers of a neural\nnetwork. Our analysis leads to the following discoveries: (1) The feature norm\nis a confidence value of a classifier hidden in the network layer, specifically\nits maximum logit. Hence, the feature norm distinguishes OOD from ID in the\nsame manner that a classifier confidence does. (2) The feature norm is\nclass-agnostic, thus it can detect OOD samples across diverse discriminative\nmodels. (3) The conventional feature norm fails to capture the deactivation\ntendency of hidden layer neurons, which may lead to misidentification of ID\nsamples as OOD instances. To resolve this drawback, we propose a novel\nnegative-aware norm (NAN) that can capture both the activation and deactivation\ntendencies of hidden layer neurons. We conduct extensive experiments on NAN,\ndemonstrating its efficacy and compatibility with existing OOD detectors, as\nwell as its capability in label-free environments.\n","authors":["Jaewoo Park","Jacky Chen Long Chai","Jaeho Yoon","Andrew Beng Jin Teoh"],"pdf_url":"https://arxiv.org/pdf/2310.05316v1.pdf","comment":"Accepted to ICCV2023"},{"id":"http://arxiv.org/abs/2310.06196v1","updated":"2023-10-09T22:52:43Z","published":"2023-10-09T22:52:43Z","title":"DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised\n  Transformers for Weakly Supervised Object Localization","summary":"  Self-supervised vision transformers (SSTs) have shown great potential to\nyield rich localization maps that highlight different objects in an image.\nHowever, these maps remain class-agnostic since the model is unsupervised. They\noften tend to decompose the image into multiple maps containing different\nobjects while being unable to distinguish the object of interest from\nbackground noise objects. In this paper, Discriminative Pseudo-label Sampling\n(DiPS) is introduced to leverage these class-agnostic maps for\nweakly-supervised object localization (WSOL), where only image-class labels are\navailable. Given multiple attention maps, DiPS relies on a pre-trained\nclassifier to identify the most discriminative regions of each attention map.\nThis ensures that the selected ROIs cover the correct image object while\ndiscarding the background ones, and, as such, provides a rich pool of diverse\nand discriminative proposals to cover different parts of the object.\nSubsequently, these proposals are used as pseudo-labels to train our new\ntransformer-based WSOL model designed to perform classification and\nlocalization tasks. Unlike standard WSOL methods, DiPS optimizes performance in\nboth tasks by using a transformer encoder and a dedicated output head for each\ntask, each trained using dedicated loss functions. To avoid overfitting a\nsingle proposal and promote better object coverage, a single proposal is\nrandomly selected among the top ones for a training image at each training\nstep. Experimental results on the challenging CUB, ILSVRC, OpenImages, and\nTelDrone datasets indicate that our architecture, in combination with our\ntransformer-based proposals, can yield better localization performance than\nstate-of-the-art methods.\n","authors":["Shakeeb Murtaza","Soufiane Belharbi","Marco Pedersoli","Aydin Sarraf","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2310.06196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12361v2","updated":"2023-10-09T21:03:24Z","published":"2022-02-24T20:56:29Z","title":"RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset\n  for Natural Disaster Damage Assessment","summary":"  Recent advancements in computer vision and deep learning techniques have\nfacilitated notable progress in scene understanding, thereby assisting rescue\nteams in achieving precise damage assessment. In this paper, we present\nRescueNet, a meticulously curated high-resolution post-disaster dataset that\nincludes detailed classification and semantic segmentation annotations. This\ndataset aims to facilitate comprehensive scene understanding in the aftermath\nof natural disasters. RescueNet comprises post-disaster images collected after\nHurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple\nimpacted regions. The uniqueness of RescueNet lies in its provision of\nhigh-resolution post-disaster imagery, accompanied by comprehensive annotations\nfor each image. Unlike existing datasets that offer annotations limited to\nspecific scene elements such as buildings, RescueNet provides pixel-level\nannotations for all classes, including buildings, roads, pools, trees, and\nmore. Furthermore, we evaluate the utility of the dataset by implementing\nstate-of-the-art segmentation models on RescueNet, demonstrating its value in\nenhancing existing methodologies for natural disaster damage assessment.\n","authors":["Maryam Rahnemoonfar","Tashnim Chowdhury","Robin Murphy"],"pdf_url":"https://arxiv.org/pdf/2202.12361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06143v1","updated":"2023-10-09T20:45:29Z","published":"2023-10-09T20:45:29Z","title":"HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease\n  Classification from Chest X-ray Images","summary":"  Chest X-ray is an essential diagnostic tool in the identification of chest\ndiseases given its high sensitivity to pathological abnormalities in the lungs.\nHowever, image-driven diagnosis is still challenging due to heterogeneity in\nsize and location of pathology, as well as visual similarities and\nco-occurrence of separate pathology. Since disease-related regions often occupy\na relatively small portion of diagnostic images, classification models based on\ntraditional convolutional neural networks (CNNs) are adversely affected given\ntheir locality bias. While CNNs were previously augmented with attention maps\nor spatial masks to guide focus on potentially critical regions, learning\nlocalization guidance under heterogeneity in the spatial distribution of\npathology is challenging. To improve multi-label classification performance,\nhere we propose a novel method, HydraViT, that synergistically combines a\ntransformer backbone with a multi-branch output module with learned weighting.\nThe transformer backbone enhances sensitivity to long-range context in X-ray\nimages, while using the self-attention mechanism to adaptively focus on\ntask-critical regions. The multi-branch output module dedicates an independent\nbranch to each disease label to attain robust learning across separate disease\nclasses, along with an aggregated branch across labels to maintain sensitivity\nto co-occurrence relationships among pathology. Experiments demonstrate that,\non average, HydraViT outperforms competing attention-guided methods by 1.2%,\nregion-guided methods by 1.4%, and semantic-guided methods by 1.0% in\nmulti-label classification performance.\n","authors":["aban ztrk","M. Yiit Tural","Tolga ukur"],"pdf_url":"https://arxiv.org/pdf/2310.06143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06138v1","updated":"2023-10-09T20:32:49Z","published":"2023-10-09T20:32:49Z","title":"Layout Sequence Prediction From Noisy Mobile Modality","summary":"  Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.\n","authors":["Haichao Zhang","Yi Xu","Hongsheng Lu","Takayuki Shimizu","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2310.06138v1.pdf","comment":"In Proceedings of the 31st ACM International Conference on Multimedia\n  2023 (MM 23)"},{"id":"http://arxiv.org/abs/2307.16368v2","updated":"2023-10-09T20:12:08Z","published":"2023-07-31T02:14:19Z","title":"AntGPT: Can Large Language Models Help Long-term Action Anticipation\n  from Videos?","summary":"  Can we better anticipate an actor's future actions (e.g. mix eggs) by knowing\nwhat commonly happens after his/her current action (e.g. crack eggs)? What if\nwe also know the longer-term goal of the actor (e.g. making egg fried rice)?\nThe long-term action anticipation (LTA) task aims to predict an actor's future\nbehavior from video observations in the form of verb and noun sequences, and it\nis crucial for human-machine interaction. We propose to formulate the LTA task\nfrom two perspectives: a bottom-up approach that predicts the next actions\nautoregressively by modeling temporal dynamics; and a top-down approach that\ninfers the goal of the actor and plans the needed procedure to accomplish the\ngoal. We hypothesize that large language models (LLMs), which have been\npretrained on procedure text data (e.g. recipes, how-tos), have the potential\nto help LTA from both perspectives. It can help provide the prior knowledge on\nthe possible next actions, and infer the goal given the observed part of a\nprocedure, respectively. To leverage the LLMs, we propose a two-stage\nframework, AntGPT. It first recognizes the actions already performed in the\nobserved videos and then asks an LLM to predict the future actions via\nconditioned generation, or to infer the goal and plan the whole procedure by\nchain-of-thought prompting. Empirical results on the Ego4D LTA v1 and v2\nbenchmarks, EPIC-Kitchens-55, as well as EGTEA GAZE+ demonstrate the\neffectiveness of our proposed approach. AntGPT achieves state-of-the-art\nperformance on all above benchmarks, and can successfully infer the goal and\nthus perform goal-conditioned \"counterfactual\" prediction via qualitative\nanalysis. Code and model will be released at\nhttps://brown-palm.github.io/AntGPT\n","authors":["Qi Zhao","Shijie Wang","Ce Zhang","Changcheng Fu","Minh Quan Do","Nakul Agarwal","Kwonjoon Lee","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2307.16368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06124v1","updated":"2023-10-09T19:59:59Z","published":"2023-10-09T19:59:59Z","title":"Factorized Tensor Networks for Multi-Task and Multi-Domain Learning","summary":"  Multi-task and multi-domain learning methods seek to learn multiple\ntasks/domains, jointly or one after another, using a single unified network.\nThe key challenge and opportunity is to exploit shared information across tasks\nand domains to improve the efficiency of the unified network. The efficiency\ncan be in terms of accuracy, storage cost, computation, or sample complexity.\nIn this paper, we propose a factorized tensor network (FTN) that can achieve\naccuracy comparable to independent single-task/domain networks with a small\nnumber of additional parameters. FTN uses a frozen backbone network from a\nsource model and incrementally adds task/domain-specific low-rank tensor\nfactors to the shared frozen network. This approach can adapt to a large number\nof target domains and tasks without catastrophic forgetting. Furthermore, FTN\nrequires a significantly smaller number of task-specific parameters compared to\nexisting methods. We performed experiments on widely used multi-domain and\nmulti-task datasets. We show the experiments on convolutional-based\narchitecture with different backbones and on transformer-based architecture. We\nobserved that FTN achieves similar accuracy as single-task/domain methods while\nusing only a fraction of additional parameters per task.\n","authors":["Yash Garg","Nebiyou Yismaw","Rakib Hyder","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2310.06124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06123v1","updated":"2023-10-09T19:57:24Z","published":"2023-10-09T19:57:24Z","title":"Text-driven Prompt Generation for Vision-Language Models in Federated\n  Learning","summary":"  Prompt learning for vision-language models, e.g., CoOp, has shown great\nsuccess in adapting CLIP to different downstream tasks, making it a promising\nsolution for federated learning due to computational reasons. Existing prompt\nlearning techniques replace hand-crafted text prompts with learned vectors that\noffer improvements on seen classes, but struggle to generalize to unseen\nclasses. Our work addresses this challenge by proposing Federated Text-driven\nPrompt Generation (FedTPG), which learns a unified prompt generation network\nacross multiple remote clients in a scalable manner. The prompt generation\nnetwork is conditioned on task-related text input, thus is context-aware,\nmaking it suitable to generalize for both seen and unseen classes. Our\ncomprehensive empirical evaluations on nine diverse image classification\ndatasets show that our method is superior to existing federated prompt learning\nmethods, that achieve overall better generalization on both seen and unseen\nclasses and is also generalizable to unseen datasets.\n","authors":["Chen Qiu","Xingyu Li","Chaithanya Kumar Mummadi","Madan Ravi Ganesh","Zhenzhen Li","Lu Peng","Wan-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.06123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06109v1","updated":"2023-10-09T19:33:06Z","published":"2023-10-09T19:33:06Z","title":"QR-Tag: Angular Measurement and Tracking with a QR-Design Marker","summary":"  Directional information measurement has many applications in domains such as\nrobotics, virtual and augmented reality, and industrial computer vision.\nConventional methods either require pre-calibration or necessitate controlled\nenvironments. The state-of-the-art MoireTag approach exploits the Moire effect\nand QR-design to continuously track the angular shift precisely. However, it is\nstill not a fully QR code design. To overcome the above challenges, we propose\na novel snapshot method for discrete angular measurement and tracking with\nscannable QR-design patterns that are generated by binary structures printed on\nboth sides of a glass plate. The QR codes, resulting from the parallax effect\ndue to the geometry alignment between two layers, can be readily measured as\nangular information using a phone camera. The simulation results show that the\nproposed non-contact object tracking framework is computationally efficient\nwith high accuracy.\n","authors":["Simeng Qiu","Hadi Amata","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2310.06109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06107v1","updated":"2023-10-09T19:27:02Z","published":"2023-10-09T19:27:02Z","title":"Developing and Refining a Multifunctional Facial Recognition System for\n  Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality\n  of Life","summary":"  In an era where the global population is aging significantly, cognitive\nimpairments among the elderly have become a major health concern. The need for\neffective assistive technologies is clear, and facial recognition systems are\nemerging as promising tools to address this issue. This document discusses the\ndevelopment and evaluation of a new Multifunctional Facial Recognition System\n(MFRS), designed specifically to assist older adults with cognitive\nimpairments. The MFRS leverages face_recognition [1], a powerful open-source\nlibrary capable of extracting, identifying, and manipulating facial features.\nOur system integrates the face recognition and retrieval capabilities of\nface_recognition, along with additional functionalities to capture images and\nrecord voice memos. This combination of features notably enhances the system's\nusability and versatility, making it a more user-friendly and universally\napplicable tool for end-users. The source code for this project can be accessed\nat https://github.com/Li-8023/Multi-function-face-recognition.git.\n","authors":["Li He"],"pdf_url":"https://arxiv.org/pdf/2310.06107v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2305.00087v2","updated":"2023-10-09T19:21:35Z","published":"2023-04-28T20:38:58Z","title":"Inverse Consistency by Construction for Multistep Deep Registration","summary":"  Inverse consistency is a desirable property for image registration. We\npropose a simple technique to make a neural registration network inverse\nconsistent by construction, as a consequence of its structure, as long as it\nparameterizes its output transform by a Lie group. We extend this technique to\nmulti-step neural registration by composing many such networks in a way that\npreserves inverse consistency. This multi-step approach also allows for\ninverse-consistent coarse to fine registration. We evaluate our technique on\nsynthetic 2-D data and four 3-D medical image registration tasks and obtain\nexcellent registration accuracy while assuring inverse consistency.\n","authors":["Hastings Greer","Lin Tian","Francois-Xavier Vialard","Roland Kwitt","Sylvain Bouix","Raul San Jose Estepar","Richard Rushmore","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2305.00087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04297v2","updated":"2023-10-09T19:09:12Z","published":"2022-04-08T21:12:13Z","title":"Learning to Modulate Random Weights: Neuromodulation-inspired Neural\n  Networks For Efficient Continual Learning","summary":"  Existing Continual Learning (CL) approaches have focused on addressing\ncatastrophic forgetting by leveraging regularization methods, replay buffers,\nand task-specific components. However, realistic CL solutions must be shaped\nnot only by metrics of catastrophic forgetting but also by computational\nefficiency and running time. Here, we introduce a novel neural network\narchitecture inspired by neuromodulation in biological nervous systems to\neconomically and efficiently address catastrophic forgetting and provide new\navenues for interpreting learned representations. Neuromodulation is a\nbiological mechanism that has received limited attention in machine learning;\nit dynamically controls and fine tunes synaptic dynamics in real time to track\nthe demands of different behavioral contexts. Inspired by this, our proposed\narchitecture learns a relatively small set of parameters per task context that\n\\emph{neuromodulates} the activity of unchanging, randomized weights that\ntransform the input. We show that this approach has strong learning performance\nper task despite the very small number of learnable parameters. Furthermore,\nbecause context vectors are so compact, multiple networks can be stored\nconcurrently with no interference and little spatial footprint, thus completely\neliminating catastrophic forgetting and accelerating the training process.\n","authors":["Jinyung Hong","Theodore P. Pavlic"],"pdf_url":"https://arxiv.org/pdf/2204.04297v2.pdf","comment":"13 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2309.08021v2","updated":"2023-10-09T19:00:13Z","published":"2023-09-14T20:34:30Z","title":"Vision-based Analysis of Driver Activity and Driving Performance Under\n  the Influence of Alcohol","summary":"  About 30% of all traffic crash fatalities in the United States involve drunk\ndrivers, making the prevention of drunk driving paramount to vehicle safety in\nthe US and other locations which have a high prevalence of driving while under\nthe influence of alcohol. Driving impairment can be monitored through active\nuse of sensors (when drivers are asked to engage in providing breath samples to\na vehicle instrument or when pulled over by a police officer), but a more\npassive and robust mechanism of sensing may allow for wider adoption and\nbenefit of intelligent systems that reduce drunk driving accidents. This could\nassist in identifying impaired drivers before they drive, or early in the\ndriving process (before a crash or detection by law enforcement). In this\nresearch, we introduce a study which adopts a multi-modal ensemble of visual,\nthermal, audio, and chemical sensors to (1) examine the impact of acute alcohol\nadministration on driving performance in a driving simulator, and (2) identify\ndata-driven methods for detecting driving under the influence of alcohol. We\ndescribe computer vision and machine learning models for analyzing the driver's\nface in thermal imagery, and introduce a pipeline for training models on data\ncollected from drivers with a range of breath-alcohol content levels, including\ndiscussion of relevant machine learning phenomena which can help in future\nexperiment design for related studies.\n","authors":["Ross Greer","Akshay Gopalkrishnan","Sumega Mandadi","Pujitha Gunaratne","Mohan M. Trivedi","Thomas D. Marcotte"],"pdf_url":"https://arxiv.org/pdf/2309.08021v2.pdf","comment":"Withdrawn at the request of industry research collaborators, per\n  contract agreement"},{"id":"http://arxiv.org/abs/2306.04642v2","updated":"2023-10-09T18:58:24Z","published":"2023-05-25T11:59:28Z","title":"DiffusionShield: A Watermark for Copyright Protection against Generative\n  Diffusion Models","summary":"  Recently, Generative Diffusion Models (GDMs) have showcased their remarkable\ncapabilities in learning and generating images. A large community of GDMs has\nnaturally emerged, further promoting the diversified applications of GDMs in\nvarious fields. However, this unrestricted proliferation has raised serious\nconcerns about copyright protection. For example, artists including painters\nand photographers are becoming increasingly concerned that GDMs could\neffortlessly replicate their unique creative works without authorization. In\nresponse to these challenges, we introduce a novel watermarking scheme,\nDiffusionShield, tailored for GDMs. DiffusionShield protects images from\ncopyright infringement by GDMs through encoding the ownership information into\nan imperceptible watermark and injecting it into the images. Its watermark can\nbe easily learned by GDMs and will be reproduced in their generated images. By\ndetecting the watermark from generated images, copyright infringement can be\nexposed with evidence. Benefiting from the uniformity of the watermarks and the\njoint optimization method, DiffusionShield ensures low distortion of the\noriginal image, high watermark detection performance, and the ability to embed\nlengthy messages. We conduct rigorous and comprehensive experiments to show the\neffectiveness of DiffusionShield in defending against infringement by GDMs and\nits superiority over traditional watermarking methods.\n","authors":["Yingqian Cui","Jie Ren","Han Xu","Pengfei He","Hui Liu","Lichao Sun","Yue Xing","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2306.04642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06080v1","updated":"2023-10-09T18:38:49Z","published":"2023-10-09T18:38:49Z","title":"Advancing Diagnostic Precision: Leveraging Machine Learning Techniques\n  for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest\n  X-Ray Images","summary":"  Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to\nbe serious global health concerns that affect millions of people worldwide. In\nmedical practice, chest X-ray examinations have emerged as the norm for\ndiagnosing diseases, particularly chest infections such as COVID-19. Paramedics\nand scientists are working intensively to create a reliable and precise\napproach for early-stage COVID-19 diagnosis in order to save lives. But with a\nvariety of symptoms, medical diagnosis of these disorders poses special\ndifficulties. It is essential to address their identification and timely\ndiagnosis in order to successfully treat and prevent these illnesses. In this\nresearch, a multiclass classification approach using state-of-the-art methods\nfor deep learning and image processing is proposed. This method takes into\naccount the robustness and efficiency of the system in order to increase\ndiagnostic precision of chest diseases. A comparison between a brand-new\nconvolution neural network (CNN) and several transfer learning pre-trained\nmodels including VGG19, ResNet, DenseNet, EfficientNet, and InceptionNet is\nrecommended. Publicly available and widely used research datasets like Shenzen,\nMontogomery, the multiclass Kaggle dataset and the NIH dataset were used to\nrigorously test the model. Recall, precision, F1-score, and Area Under Curve\n(AUC) score are used to evaluate and compare the performance of the proposed\nmodel. An AUC value of 0.95 for COVID-19, 0.99 for TB, and 0.98 for pneumonia\nis obtained using the proposed network. Recall and precision ratings of 0.95,\n0.98, and 0.97, respectively, likewise met high standards.\n","authors":["Aditya Kulkarni","Guruprasad Parasnis","Harish Balasubramanian","Vansh Jain","Anmol Chokshi","Reena Sonkusare"],"pdf_url":"https://arxiv.org/pdf/2310.06080v1.pdf","comment":"11 pages, 18 figures, Under review in Discover Artificial\n  Intelligence Journal by Springer Nature"},{"id":"http://arxiv.org/abs/2310.06068v1","updated":"2023-10-09T18:19:51Z","published":"2023-10-09T18:19:51Z","title":"Augmenting Vision-Based Human Pose Estimation with Rotation Matrix","summary":"  Fitness applications are commonly used to monitor activities within the gym,\nbut they often fail to automatically track indoor activities inside the gym.\nThis study proposes a model that utilizes pose estimation combined with a novel\ndata augmentation method, i.e., rotation matrix. We aim to enhance the\nclassification accuracy of activity recognition based on pose estimation data.\nThrough our experiments, we experiment with different classification algorithms\nalong with image augmentation approaches. Our findings demonstrate that the SVM\nwith SGD optimization, using data augmentation with the Rotation Matrix, yields\nthe most accurate results, achieving a 96% accuracy rate in classifying five\nphysical activities. Conversely, without implementing the data augmentation\ntechniques, the baseline accuracy remains at a modest 64%.\n","authors":["Milad Vazan","Fatemeh Sadat Masoumi","Ruizhi Ou","Reza Rawassizadeh"],"pdf_url":"https://arxiv.org/pdf/2310.06068v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2310.06020v1","updated":"2023-10-09T18:00:01Z","published":"2023-10-09T18:00:01Z","title":"DyST: Towards Dynamic Neural Scene Representations on Real-World Videos","summary":"  Visual understanding of the world goes beyond the semantics and flat\nstructure of individual images. In this work, we aim to capture both the 3D\nstructure and dynamics of real-world scenes from monocular real-world videos.\nOur Dynamic Scene Transformer (DyST) model leverages recent work in neural\nscene representation to learn a latent decomposition of monocular real-world\nvideos into scene content, per-view scene dynamics, and camera pose. This\nseparation is achieved through a novel co-training scheme on monocular videos\nand our new synthetic dataset DySO. DyST learns tangible latent representations\nfor dynamic scenes that enable view generation with separate control over the\ncamera and the content of the scene.\n","authors":["Maximilian Seitzer","Sjoerd van Steenkiste","Thomas Kipf","Klaus Greff","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2310.06020v1.pdf","comment":"Project website: https://dyst-paper.github.io/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.03481v2","updated":"2023-10-09T17:46:14Z","published":"2023-10-05T11:46:39Z","title":"Personalized Transformer-based Ranking for e-Commerce at Yandex","summary":"  Personalizing user experience with high-quality recommendations based on user\nactivity is vital for e-commerce platforms. This is particularly important in\nscenarios where the user's intent is not explicit, such as on the homepage.\nRecently, personalized embedding-based systems have significantly improved the\nquality of recommendations and search in the e-commerce domain. However, most\nof these works focus on enhancing the retrieval stage. In this paper, we\ndemonstrate that features produced by retrieval-focused deep learning models\nare sub-optimal for ranking stage in e-commerce recommendations. To address\nthis issue, we propose a two-stage training process that fine-tunes two-tower\nmodels to achieve optimal ranking performance. We provide a detailed\ndescription of our transformer-based two-tower model architecture, which is\nspecifically designed for personalization in e-commerce. Additionally, we\nintroduce a novel technique for debiasing context in offline models and report\nsignificant improvements in ranking performance when using web-search queries\nfor e-commerce recommendations. Our model has been successfully deployed at\nYandex, serves millions of users daily, and has delivered strong performance in\nonline A/B testing.\n","authors":["Kirill Khrylchenko","Alexander Fritzler"],"pdf_url":"https://arxiv.org/pdf/2310.03481v2.pdf","comment":"6 pages, 1 figure, pre-print"},{"id":"http://arxiv.org/abs/2305.08732v2","updated":"2023-10-09T16:55:39Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2306.03516v2","updated":"2023-10-09T15:49:03Z","published":"2023-06-06T09:08:40Z","title":"COPR: Consistency-Oriented Pre-Ranking for Online Advertising","summary":"  Cascading architecture has been widely adopted in large-scale advertising\nsystems to balance efficiency and effectiveness. In this architecture, the\npre-ranking model is expected to be a lightweight approximation of the ranking\nmodel, which handles more candidates with strict latency requirements. Due to\nthe gap in model capacity, the pre-ranking and ranking models usually generate\ninconsistent ranked results, thus hurting the overall system effectiveness. The\nparadigm of score alignment is proposed to regularize their raw scores to be\nconsistent. However, it suffers from inevitable alignment errors and error\namplification by bids when applied in online advertising. To this end, we\nintroduce a consistency-oriented pre-ranking framework for online advertising,\nwhich employs a chunk-based sampling module and a plug-and-play rank alignment\nmodule to explicitly optimize consistency of ECPM-ranked results. A $\\Delta\nNDCG$-based weighting mechanism is adopted to better distinguish the importance\nof inter-chunk samples in optimization. Both online and offline experiments\nhave validated the superiority of our framework. When deployed in Taobao\ndisplay advertising system, it achieves an improvement of up to +12.3\\% CTR and\n+5.6\\% RPM.\n","authors":["Zhishan Zhao","Jingyue Gao","Yu Zhang","Shuguang Han","Siyuan Lou","Xiang-Rong Sheng","Zhe Wang","Han Zhu","Yuning Jiang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2306.03516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04268v2","updated":"2023-10-09T12:10:04Z","published":"2023-10-06T14:11:58Z","title":"Workload-aware and Learned Z-Indexes","summary":"  In this paper, we present a learned and workload-aware variant of a Z-index,\nwhich jointly optimizes storage layout and search structures. Specifically, we\nfirst formulate a cost function to measure the performance of a Z-index on a\ndataset for a range-query workload. Then, we optimize the Z-index structure by\nminimizing the cost function through adaptive partitioning and ordering for\nindex construction. Moreover, we design a novel page-skipping mechanism to\nimprove its query performance by reducing access to irrelevant data pages. Our\nextensive experiments show that our index improves range query time by 40% on\naverage over the baselines, while always performing better or comparably to\nstate-of-the-art spatial indexes. Additionally, our index maintains good point\nquery performance while providing favourable construction time and index size\ntradeoffs.\n","authors":["Sachith Pai","Michael Mathioudakis","Yanhao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04268v2.pdf","comment":"Fixed grammatical error in abstract"},{"id":"http://arxiv.org/abs/2309.05238v2","updated":"2023-10-09T06:18:11Z","published":"2023-09-11T05:12:14Z","title":"Generating Natural Language Queries for More Effective Systematic Review\n  Screening Prioritisation","summary":"  Screening prioritisation in medical systematic reviews aims to rank the set\nof documents retrieved by complex Boolean queries. Prioritising the most\nimportant documents ensures that subsequent review steps can be carried out\nmore efficiently and effectively. The current state of the art uses the final\ntitle of the review as a query to rank the documents using BERT-based neural\nrankers. However, the final title is only formulated at the end of the review\nprocess, which makes this approach impractical as it relies on ex post facto\ninformation. At the time of screening, only a rough working title is available,\nwith which the BERT-based ranker performs significantly worse than with the\nfinal title. In this paper, we explore alternative sources of queries for\nprioritising screening, such as the Boolean query used to retrieve the\ndocuments to be screened and queries generated by instruction-based generative\nlarge-scale language models such as ChatGPT and Alpaca. Our best approach is\nnot only viable based on the information available at the time of screening,\nbut also has similar effectiveness to the final title.\n","authors":["Shuai Wang","Harrisen Scells","Martin Potthast","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2309.05238v2.pdf","comment":"Preprints for Accepted paper in SIGIR-AP-2023"},{"id":"http://arxiv.org/abs/2310.05423v1","updated":"2023-10-09T05:42:47Z","published":"2023-10-09T05:42:47Z","title":"Sequential Tag Recommendation","summary":"  With the development of Internet technology and the expansion of social\nnetworks, online platforms have become an important way for people to obtain\ninformation. The introduction of tags facilitates information categorization\nand retrieval. Meanwhile, the development of tag recommendation systems not\nonly enables users to input tags more efficiently, but also improves the\nquality of tags. However, current tag recommendation methods only consider the\ncontent of the current post and do not take into account the influence of user\npreferences. Since the main body of tag recommendation is the user, it is very\nnecessary to obtain the user's tagging habits. Therefore, this paper proposes a\ntag recommendation algorithm (MLP4STR) based on the dynamic preference of\nuser's behavioral sequence, which models the user's historical post information\nand historical tag information to obtain the user's dynamic interest changes. A\npure MLP structure across feature dimensions is used in sequence modeling to\nmodel the interaction between tag content and post content to fully extract the\nuser's interests. Finally tag recommendation is performed.\n","authors":["Bing Liu","Pengyu Xu","Sijin Lu","Shijing Wang","Hongjian Sun","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2310.05423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.02605v3","updated":"2023-10-09T03:36:29Z","published":"2021-05-06T12:20:41Z","title":"GraphFormers: GNN-nested Transformers for Representation Learning on\n  Textual Graph","summary":"  The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.\n","authors":["Junhan Yang","Zheng Liu","Shitao Xiao","Chaozhuo Li","Defu Lian","Sanjay Agrawal","Amit Singh","Guangzhong Sun","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2105.02605v3.pdf","comment":"Accepted to NeurIPS 2021"},{"id":"http://arxiv.org/abs/2310.05380v1","updated":"2023-10-09T03:29:35Z","published":"2023-10-09T03:29:35Z","title":"Augmented Embeddings for Custom Retrievals","summary":"  Information retrieval involves selecting artifacts from a corpus that are\nmost relevant to a given search query. The flavor of retrieval typically used\nin classical applications can be termed as homogeneous and relaxed, where\nqueries and corpus elements are both natural language (NL) utterances\n(homogeneous) and the goal is to pick most relevant elements from the corpus in\nthe Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).\nRecently, retrieval is being used extensively in preparing prompts for large\nlanguage models (LLMs) to enable LLMs to perform targeted tasks. These new\napplications of retrieval are often heterogeneous and strict -- the queries and\nthe corpus contain different kinds of entities, such as NL and code, and there\nis a need for improving retrieval at Top-K for small values of K, such as K=1\nor 3 or 5. Current dense retrieval techniques based on pretrained embeddings\nprovide a general-purpose and powerful approach for retrieval, but they are\noblivious to task-specific notions of similarity of heterogeneous artifacts. We\nintroduce Adapted Dense Retrieval, a mechanism to transform embeddings to\nenable improved task-specific, heterogeneous and strict retrieval. Adapted\nDense Retrieval works by learning a low-rank residual adaptation of the\npretrained black-box embedding. We empirically validate our approach by showing\nimprovements over the state-of-the-art general-purpose embeddings-based\nbaseline.\n","authors":["Anirudh Khatry","Yasharth Bajpai","Priyanshu Gupta","Sumit Gulwani","Ashish Tiwari"],"pdf_url":"https://arxiv.org/pdf/2310.05380v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2310.03813v2","updated":"2023-10-09T00:20:48Z","published":"2023-10-05T18:02:03Z","title":"Accurate Cold-start Bundle Recommendation via Popularity-based\n  Coalescence and Curriculum Heating","summary":"  How can we accurately recommend cold-start bundles to users? The cold-start\nproblem in bundle recommendation is critical in practical scenarios since new\nbundles are continuously created for various marketing purposes. Despite its\nimportance, no previous studies have addressed cold-start bundle\nrecommendation. Moreover, existing methods for cold-start item recommendation\noverly rely on historical information, even for unpopular bundles, failing to\ntackle the primary challenge of the highly skewed distribution of bundle\ninteractions. In this work, we propose CoHeat (Popularity-based Coalescence and\nCurriculum Heating), an accurate approach for the cold-start bundle\nrecommendation. CoHeat tackles the highly skewed distribution of bundle\ninteractions by incorporating both historical and affiliation information based\non the bundle's popularity when estimating the user-bundle relationship.\nFurthermore, CoHeat effectively learns latent representations by exploiting\ncurriculum learning and contrastive learning. CoHeat demonstrates superior\nperformance in cold-start bundle recommendation, achieving up to 193% higher\nnDCG@20 compared to the best competitor.\n","authors":["Hyunsik Jeon","Jong-eun Lee","Jeongin Yun","U Kang"],"pdf_url":"https://arxiv.org/pdf/2310.03813v2.pdf","comment":"8 pages, 4 figures, 4 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2310.05921v1","updated":"2023-10-09T17:59:30Z","published":"2023-10-09T17:59:30Z","title":"Conformal Decision Theory: Safe Autonomous Decisions from Imperfect\n  Predictions","summary":"  We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin\n","authors":["Jordan Lekeufack","Anastasios A. Angelopoulos","Andrea Bajcsy","Michael I. Jordan","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2310.05921v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05918v1","updated":"2023-10-09T17:59:18Z","published":"2023-10-09T17:59:18Z","title":"Grokking as Compression: A Nonlinear Complexity Perspective","summary":"  We attribute grokking, the phenomenon where generalization is much delayed\nafter memorization, to compression. To do so, we define linear mapping number\n(LMN) to measure network complexity, which is a generalized version of linear\nregion number for ReLU networks. LMN can nicely characterize neural network\ncompression before generalization. Although the $L_2$ norm has been a popular\nchoice for characterizing model complexity, we argue in favor of LMN for a\nnumber of reasons: (1) LMN can be naturally interpreted as\ninformation/computation, while $L_2$ cannot. (2) In the compression phase, LMN\nhas linear relations with test losses, while $L_2$ is correlated with test\nlosses in a complicated nonlinear way. (3) LMN also reveals an intriguing\nphenomenon of the XOR network switching between two generalization solutions,\nwhile $L_2$ does not. Besides explaining grokking, we argue that LMN is a\npromising candidate as the neural network version of the Kolmogorov complexity\nsince it explicitly considers local or conditioned linear computations aligned\nwith the nature of modern artificial neural networks.\n","authors":["Ziming Liu","Ziqian Zhong","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2310.05918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05915v1","updated":"2023-10-09T17:58:38Z","published":"2023-10-09T17:58:38Z","title":"FireAct: Toward Language Agent Fine-tuning","summary":"  Recent efforts have augmented language models (LMs) with external tools or\nenvironments, leading to the development of language agents that can reason and\nact. However, most of these agents rely on few-shot prompting techniques with\noff-the-shelf LMs. In this paper, we investigate and argue for the overlooked\ndirection of fine-tuning LMs to obtain language agents. Using a setup of\nquestion answering (QA) with a Google search API, we explore a variety of base\nLMs, prompting methods, fine-tuning data, and QA tasks, and find language\nagents are consistently improved after fine-tuning their backbone LMs. For\nexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4\nleads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,\na novel approach to fine-tuning LMs with trajectories from multiple tasks and\nprompting methods, and show having more diverse fine-tuning data can further\nimprove agents. Along with other findings regarding scaling effects,\nrobustness, generalization, efficiency and cost, our work establishes\ncomprehensive benefits of fine-tuning LMs for agents, and provides an initial\nset of experimental designs, insights, as well as open questions toward\nlanguage agent fine-tuning.\n","authors":["Baian Chen","Chang Shu","Ehsan Shareghi","Nigel Collier","Karthik Narasimhan","Shunyu Yao"],"pdf_url":"https://arxiv.org/pdf/2310.05915v1.pdf","comment":"Code, data, and models are available at\n  https://fireact-agent.github.io"},{"id":"http://arxiv.org/abs/2310.05914v1","updated":"2023-10-09T17:58:34Z","published":"2023-10-09T17:58:34Z","title":"NEFTune: Noisy Embeddings Improve Instruction Finetuning","summary":"  We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.\n","authors":["Neel Jain","Ping-yeh Chiang","Yuxin Wen","John Kirchenbauer","Hong-Min Chu","Gowthami Somepalli","Brian R. Bartoldson","Bhavya Kailkhura","Avi Schwarzschild","Aniruddha Saha","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2310.05914v1.pdf","comment":"25 pages, Code is available on Github:\n  https://github.com/neelsjain/NEFTune"},{"id":"http://arxiv.org/abs/2310.05910v1","updated":"2023-10-09T17:56:53Z","published":"2023-10-09T17:56:53Z","title":"SALMON: Self-Alignment with Principle-Following Reward Models","summary":"  Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON (Self-ALignMent with\nprinciple-fOllowiNg reward models), to align base language models with minimal\nhuman supervision, using only a small set of human-defined principles, yet\nachieving superior performance. Central to our approach is a\nprinciple-following reward model. Trained on synthetic preference data, this\nmodel can generate reward scores based on arbitrary human-defined principles.\nBy merely adjusting these principles during the RL training phase, we gain full\ncontrol over the preferences with the reward model, subsequently influencing\nthe behavior of the RL-trained policies, and eliminating the reliance on the\ncollection of online human preferences. Applying our method to the LLaMA-2-70b\nbase language model, we developed an AI assistant named Dromedary-2. With only\n6 exemplars for in-context learning and 31 human-defined principles,\nDromedary-2 significantly surpasses the performance of several state-of-the-art\nAI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have\nopen-sourced the code and model weights to encourage further research into\naligning LLM-based AI agents with enhanced supervision efficiency, improved\ncontrollability, and scalable oversight.\n","authors":["Zhiqing Sun","Yikang Shen","Hongxin Zhang","Qinhong Zhou","Zhenfang Chen","David Cox","Yiming Yang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2310.05910v1.pdf","comment":"Project page: https://github.com/IBM/SALMON"},{"id":"http://arxiv.org/abs/2310.05905v1","updated":"2023-10-09T17:49:50Z","published":"2023-10-09T17:49:50Z","title":"TAIL: Task-specific Adapters for Imitation Learning with Large\n  Pretrained Models","summary":"  The full potential of large pretrained models remains largely untapped in\ncontrol domains like robotics. This is mainly because of the scarcity of data\nand the computational challenges associated with training or fine-tuning these\nlarge models for such applications. Prior work mainly emphasizes effective\npretraining of large models for decision-making, with little exploration into\nhow to perform data-efficient continual adaptation of these models for new\ntasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters\nfor Imitation Learning), a framework for efficient adaptation to new control\ntasks. Inspired by recent advancements in parameter-efficient fine-tuning in\nlanguage domains, we explore efficient fine-tuning techniques -- e.g.,\nBottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to\nadapt large pretrained models for new tasks with limited demonstration data.\nOur extensive experiments in large-scale language-conditioned manipulation\ntasks comparing prevalent parameter-efficient fine-tuning techniques and\nadaptation baselines suggest that TAIL with LoRA can achieve the best\npost-adaptation performance with only 1\\% of the trainable parameters of full\nfine-tuning, while avoiding catastrophic forgetting and preserving adaptation\nplasticity in continual learning settings.\n","authors":["Zuxin Liu","Jesse Zhang","Kavosh Asadi","Yao Liu","Ding Zhao","Shoham Sabach","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2310.05905v1.pdf","comment":"21 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2310.04343v2","updated":"2023-10-09T17:49:12Z","published":"2023-10-06T16:08:41Z","title":"Functional Geometry Guided Protein Sequence and Backbone Structure\n  Co-Design","summary":"  Proteins are macromolecules responsible for essential functions in almost all\nliving organisms. Designing reasonable proteins with desired functions is\ncrucial. A protein's sequence and structure are strongly correlated and they\ntogether determine its function. In this paper, we propose NAEPro, a model to\njointly design Protein sequence and structure based on automatically detected\nfunctional sites. NAEPro is powered by an interleaving network of attention and\nequivariant layers, which can capture global correlation in a whole sequence\nand local influence from nearest amino acids in three dimensional (3D) space.\nSuch an architecture facilitates effective yet economic message passing at two\nlevels. We evaluate our model and several strong baselines on two protein\ndatasets, $\\beta$-lactamase and myoglobin. Experimental results show that our\nmodel consistently achieves the highest amino acid recovery rate, TM-score, and\nthe lowest RMSD among all competitors. These findings prove the capability of\nour model to design protein sequences and structures that closely resemble\ntheir natural counterparts. Furthermore, in-depth analysis further confirms our\nmodel's ability to generate highly effective proteins capable of binding to\ntheir target metallocofactors. We provide code, data and models in Github.\n","authors":["Zhenqiao Song","Yunlong Zhao","Wenxian Shi","Yang Yang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2310.04343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.11593v2","updated":"2023-10-09T17:48:32Z","published":"2022-02-23T16:14:35Z","title":"Finding Safe Zones of policies Markov Decision Processes","summary":"  Given a policy of a Markov Decision Process, we define a SafeZone as a subset\nof states, such that most of the policy's trajectories are confined to this\nsubset. The quality of a SafeZone is parameterized by the number of states and\nthe escape probability, i.e., the probability that a random trajectory will\nleave the subset. SafeZones are especially interesting when they have a small\nnumber of states and low escape probability. We study the complexity of finding\noptimal SafeZones, and show that in general, the problem is computationally\nhard. Our main result is a bi-criteria approximation learning algorithm with a\nfactor of almost $2$ approximation for both the escape probability and SafeZone\nsize, using a polynomial size sample complexity.\n","authors":["Lee Cohen","Yishay Mansour","Michal Moshkovitz"],"pdf_url":"https://arxiv.org/pdf/2202.11593v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05900v1","updated":"2023-10-09T17:41:37Z","published":"2023-10-09T17:41:37Z","title":"Learning to Decode the Surface Code with a Recurrent, Transformer-Based\n  Neural Network","summary":"  Quantum error-correction is a prerequisite for reliable quantum computation.\nTowards this goal, we present a recurrent, transformer-based neural network\nwhich learns to decode the surface code, the leading quantum error-correction\ncode. Our decoder outperforms state-of-the-art algorithmic decoders on\nreal-world data from Google's Sycamore quantum processor for distance 3 and 5\nsurface codes. On distances up to 11, the decoder maintains its advantage on\nsimulated data with realistic noise including cross-talk, leakage, and analog\nreadout signals, and sustains its accuracy far beyond the 25 cycles it was\ntrained on. Our work illustrates the ability of machine learning to go beyond\nhuman-designed algorithms by learning from data directly, highlighting machine\nlearning as a strong contender for decoding in quantum computers.\n","authors":["Johannes Bausch","Andrew W Senior","Francisco J H Heras","Thomas Edlich","Alex Davies","Michael Newman","Cody Jones","Kevin Satzinger","Murphy Yuezhen Niu","Sam Blackwell","George Holland","Dvir Kafri","Juan Atalaya","Craig Gidney","Demis Hassabis","Sergio Boixo","Hartmut Neven","Pushmeet Kohli"],"pdf_url":"https://arxiv.org/pdf/2310.05900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05898v1","updated":"2023-10-09T17:41:29Z","published":"2023-10-09T17:41:29Z","title":"Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts","summary":"  Lion (Evolved Sign Momentum), a new optimizer discovered through program\nsearch, has shown promising results in training large AI models. It performs\ncomparably or favorably to AdamW but with greater memory efficiency. As we can\nexpect from the results of a random search program, Lion incorporates elements\nfrom several existing algorithms, including signed momentum, decoupled weight\ndecay, Polak, and Nesterov momentum, but does not fit into any existing\ncategory of theoretically grounded optimizers. Thus, even though Lion appears\nto perform well as a general-purpose optimizer for a wide range of tasks, its\ntheoretical basis remains uncertain. This lack of theoretical clarity limits\nopportunities to further enhance and expand Lion's efficacy.\n  This work aims to demystify Lion. Based on both continuous-time and\ndiscrete-time analysis, we demonstrate that Lion is a theoretically novel and\nprincipled approach for minimizing a general loss function $f(x)$ while\nenforcing a bound constraint $\\|x\\|_\\infty \\leq 1/\\lambda$. Lion achieves this\nthrough the incorporation of decoupled weight decay, where $\\lambda$ represents\nthe weight decay coefficient. Our analysis is made possible by the development\nof a new Lyapunov function for the Lion updates. It applies to a broader family\nof Lion-$\\kappa$ algorithms, where the $\\text{sign}(\\cdot)$ operator in Lion is\nreplaced by the subgradient of a convex function $\\kappa$, leading to the\nsolution of a general composite optimization problem of $\\min_x f(x) +\n\\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion\nand pave the way for further improvements and extensions of Lion-related\nalgorithms.\n","authors":["Lizhang Chen","Bo Liu","Kaizhao Liang","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05898v1.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.05892v1","updated":"2023-10-09T17:33:37Z","published":"2023-10-09T17:33:37Z","title":"A Generalization Bound of Deep Neural Networks for Dependent Data","summary":"  Existing generalization bounds for deep neural networks require data to be\nindependent and identically distributed (iid). This assumption may not hold in\nreal-life applications such as evolutionary biology, infectious disease\nepidemiology, and stock price prediction. This work establishes a\ngeneralization bound of feed-forward neural networks for non-stationary\n$\\phi$-mixing data.\n","authors":["Quan Huu Do","Binh T. Nguyen","Lam Si Tung Ho"],"pdf_url":"https://arxiv.org/pdf/2310.05892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05886v1","updated":"2023-10-09T17:28:35Z","published":"2023-10-09T17:28:35Z","title":"Streaming Anchor Loss: Augmenting Supervision with Temporal Significance","summary":"  Streaming neural network models for fast frame-wise responses to various\nspeech and sensory signals are widely adopted on resource-constrained\nplatforms. Hence, increasing the learning capacity of such streaming models\n(i.e., by adding more parameters) to improve the predictive power may not be\nviable for real-world tasks. In this work, we propose a new loss, Streaming\nAnchor Loss (SAL), to better utilize the given learning capacity by encouraging\nthe model to learn more from essential frames. More specifically, our SAL and\nits focal variations dynamically modulate the frame-wise cross entropy loss\nbased on the importance of the corresponding frames so that a higher loss\npenalty is assigned for frames within the temporal proximity of semantically\ncritical events. Therefore, our loss ensures that the model training focuses on\npredicting the relatively rare but task-relevant frames. Experimental results\nwith standard lightweight convolutional and recurrent streaming networks on\nthree different speech based detection tasks demonstrate that SAL enables the\nmodel to learn the overall task more effectively with improved accuracy and\nlatency, without any additional data, model parameters, or architectural\nchanges.\n","authors":[" Utkarsh"," Sarawgi","John Berkowitz","Vineet Garg","Arnav Kundu","Minsik Cho","Sai Srujana Buddi","Saurabh Adya","Ahmed Tewfik"],"pdf_url":"https://arxiv.org/pdf/2310.05886v1.pdf","comment":"Under review for ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05884v1","updated":"2023-10-09T17:27:36Z","published":"2023-10-09T17:27:36Z","title":"A Meta-Learning Perspective on Transformers for Causal Language Modeling","summary":"  The Transformer architecture has become prominent in developing large causal\nlanguage models. However, mechanisms to explain its capabilities are not well\nunderstood. Focused on the training process, here we establish a meta-learning\nview of the Transformer architecture when trained for the causal language\nmodeling task, by explicating an inner optimization process that may happen\nwithin the Transformer. Further, from within the inner optimization, we\ndiscover and theoretically analyze a special characteristic of the norms of\nlearned token representations within Transformer-based causal language models.\nOur analysis is supported by experiments conducted on pre-trained large\nlanguage models and real-world data.\n","authors":["Xinbo Wu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.05884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14970v2","updated":"2023-10-09T17:25:52Z","published":"2023-09-26T14:42:28Z","title":"Recurrent Hypernetworks are Surprisingly Strong in Meta-RL","summary":"  Deep reinforcement learning (RL) is notoriously impractical to deploy due to\nsample inefficiency. Meta-RL directly addresses this sample inefficiency by\nlearning to perform few-shot learning when a distribution of related tasks is\navailable for meta-training. While many specialized meta-RL methods have been\nproposed, recent work suggests that end-to-end learning in conjunction with an\noff-the-shelf sequential model, such as a recurrent network, is a surprisingly\nstrong baseline. However, such claims have been controversial due to limited\nsupporting evidence, particularly in the face of prior work establishing\nprecisely the opposite. In this paper, we conduct an empirical investigation.\nWhile we likewise find that a recurrent network can achieve strong performance,\nwe demonstrate that the use of hypernetworks is crucial to maximizing their\npotential. Surprisingly, when combined with hypernetworks, the recurrent\nbaselines that are far simpler than existing specialized methods actually\nachieve the strongest performance of all methods evaluated.\n","authors":["Jacob Beck","Risto Vuorio","Zheng Xiong","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2309.14970v2.pdf","comment":"Published at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05878v1","updated":"2023-10-09T17:19:49Z","published":"2023-10-09T17:19:49Z","title":"A Machine Learning Approach to Predicting Single Event Upsets","summary":"  A single event upset (SEU) is a critical soft error that occurs in\nsemiconductor devices on exposure to ionising particles from space\nenvironments. SEUs cause bit flips in the memory component of semiconductors.\nThis creates a multitude of safety hazards as stored information becomes less\nreliable. Currently, SEUs are only detected several hours after their\noccurrence. CREMER, the model presented in this paper, predicts SEUs in advance\nusing machine learning. CREMER uses only positional data to predict SEU\noccurrence, making it robust, inexpensive and scalable. Upon implementation,\nthe improved reliability of memory devices will create a digitally safer\nenvironment onboard space vehicles.\n","authors":["Archit Gupta","Chong Yock Eng","Deon Lim Meng Wee","Rashna Analia Ahmed","See Min Sim"],"pdf_url":"https://arxiv.org/pdf/2310.05878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05871v1","updated":"2023-10-09T17:07:26Z","published":"2023-10-09T17:07:26Z","title":"Dynamic value alignment through preference aggregation of multiple\n  objectives","summary":"  The development of ethical AI systems is currently geared toward setting\nobjective functions that align with human objectives. However, finding such\nfunctions remains a research challenge, while in RL, setting rewards by hand is\na fairly standard approach. We present a methodology for dynamic value\nalignment, where the values that are to be aligned with are dynamically\nchanging, using a multiple-objective approach. We apply this approach to extend\nDeep $Q$-Learning to accommodate multiple objectives and evaluate this method\non a simplified two-leg intersection controlled by a switching agent.Our\napproach dynamically accommodates the preferences of drivers on the system and\nachieves better overall performance across three metrics (speeds, stops, and\nwaits) while integrating objectives that have competing or conflicting actions.\n","authors":["Marcin Korecki","Damian Dailisan","Cesare Carissimo"],"pdf_url":"https://arxiv.org/pdf/2310.05871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04367v2","updated":"2023-10-09T17:06:05Z","published":"2023-10-06T16:41:51Z","title":"A Marketplace Price Anomaly Detection System at Scale","summary":"  Online marketplaces execute large volume of price updates that are initiated\nby individual marketplace sellers each day on the platform. This price\ndemocratization comes with increasing challenges with data quality. Lack of\ncentralized guardrails that are available for a traditional online retailer\ncauses a higher likelihood for inaccurate prices to get published on the\nwebsite, leading to poor customer experience and potential for revenue loss. We\npresent MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling\nand Unsupervised Statistical-features), a scalable price anomaly detection\nframework for a growing marketplace platform. The goal is to leverage proximity\nand historical price trends from unsupervised statistical features to generate\nan upper price bound. We build an ensemble of models to detect irregularities\nin price-based features, exclude irregular features and use optimized weighting\nscheme to build a reliable price bound in real-time pricing pipeline. We\nobserved that our approach improves precise anchor coverage by up to 46.6% in\nhigh-vulnerability item subsets\n","authors":["Akshit Sarpal","Qiwen Kang","Fangping Huang","Yang Song","Lijie Wan"],"pdf_url":"https://arxiv.org/pdf/2310.04367v2.pdf","comment":"10 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2310.05869v1","updated":"2023-10-09T17:05:25Z","published":"2023-10-09T17:05:25Z","title":"HyperAttention: Long-context Attention in Near-Linear Time","summary":"  We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.\n","authors":["Insu Han","Rajesh Jarayam","Amin Karbasi","Vahab Mirrokni","David P. Woodruff","Amir Zandieh"],"pdf_url":"https://arxiv.org/pdf/2310.05869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05868v1","updated":"2023-10-09T17:05:23Z","published":"2023-10-09T17:05:23Z","title":"Bio-inspired computational memory model of the Hippocampus: an approach\n  to a neuromorphic spike-based Content-Addressable Memory","summary":"  The brain has computational capabilities that surpass those of modern\nsystems, being able to solve complex problems efficiently in a simple way.\nNeuromorphic engineering aims to mimic biology in order to develop new systems\ncapable of incorporating such capabilities. Bio-inspired learning systems\ncontinue to be a challenge that must be solved, and much work needs to be done\nin this regard. Among all brain regions, the hippocampus stands out as an\nautoassociative short-term memory with the capacity to learn and recall\nmemories from any fragment of them. These characteristics make the hippocampus\nan ideal candidate for developing bio-inspired learning systems that, in\naddition, resemble content-addressable memories. Therefore, in this work we\npropose a bio-inspired spiking content-addressable memory model based on the\nCA3 region of the hippocampus with the ability to learn, forget and recall\nmemories, both orthogonal and non-orthogonal, from any fragment of them. The\nmodel was implemented on the SpiNNaker hardware platform using Spiking Neural\nNetworks. A set of experiments based on functional, stress and applicability\ntests were performed to demonstrate its correct functioning. This work presents\nthe first hardware implementation of a fully-functional bio-inspired spiking\nhippocampal content-addressable memory model, paving the way for the\ndevelopment of future more complex neuromorphic systems.\n","authors":["Daniel Casanueva-Morato","Alvaro Ayuso-Martinez","Juan P. Dominguez-Morales","Angel Jimenez-Fernandez","Gabriel Jimenez-Moreno"],"pdf_url":"https://arxiv.org/pdf/2310.05868v1.pdf","comment":"15 pages, 5 figures, journal, Spiking Neural Network"},{"id":"http://arxiv.org/abs/2310.05866v1","updated":"2023-10-09T17:03:08Z","published":"2023-10-09T17:03:08Z","title":"Generative quantum machine learning via denoising diffusion\n  probabilistic models","summary":"  Deep generative models are key-enabling technology to computer vision, text\ngeneration and large language models. Denoising diffusion probabilistic models\n(DDPMs) have recently gained much attention due to their ability to generate\ndiverse and high-quality samples in many computer vision tasks, as well as to\nincorporate flexible model architectures and relatively simple training scheme.\nQuantum generative models, empowered by entanglement and superposition, have\nbrought new insight to learning classical and quantum data. Inspired by the\nclassical counterpart, we propose the quantum denoising diffusion probabilistic\nmodels (QuDDPM) to enable efficiently trainable generative learning of quantum\ndata. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,\nwhile introduces multiple intermediate training tasks as interpolation between\nthe target distribution and noise to avoid barren plateau and guarantee\nefficient training. We demonstrate QuDDPM's capability in learning correlated\nquantum noise model and learning topological structure of nontrivial\ndistribution of quantum data.\n","authors":["Bingzhi Zhang","Peng Xu","Xiaohui Chen","Quntao Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.05866v1.pdf","comment":"7+6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2308.08653v2","updated":"2023-10-09T17:01:58Z","published":"2023-08-16T19:59:25Z","title":"Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral\n  Abundance Prediction","summary":"  Hyperspectral measurements from long range sensors can give a detailed\npicture of the items, materials, and chemicals in a scene but analysis can be\ndifficult, slow, and expensive due to high spatial and spectral resolutions of\nstate-of-the-art sensors. As such, sparsity is important to enable the future\nof spectral compression and analytics. It has been observed that environmental\nand atmospheric effects, including scattering, can produce nonlinear effects\nposing challenges for existing source separation and compression methods. We\npresent a novel transformation into Hilbert spaces for pruning and constructing\nsparse representations via non-negative least squares minimization. Then we\nintroduce max likelihood compression vectors to decrease information loss. Our\napproach is benchmarked against standard pruning and least squares as well as\ndeep learning methods. Our methods are evaluated in terms of overall spectral\nreconstruction error and compression rate using real and synthetic data. We\nfind that pruning least squares methods converge quickly unlike matching\npursuit methods. We find that Hilbert space pruning can reduce error by as much\nas 40% of the error of standard pruning and also outperform neural network\nautoencoders.\n","authors":["Michael G. Rawson","Timothy Doster"],"pdf_url":"https://arxiv.org/pdf/2308.08653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05861v1","updated":"2023-10-09T16:57:57Z","published":"2023-10-09T16:57:57Z","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models","summary":"  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to a LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on two visual question answering tasks, we show that RepARe can result\nin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%\npoint increase on A-OKVQA. Additionally, we find that using gold answers for\noracle question candidate selection achieves a substantial gain in VQA accuracy\nby up to 14.41%. Through extensive analysis, we demonstrate that outputs from\nRepARe increase syntactic complexity, and effectively utilize vision-language\ninteraction and the frozen language model in LVLMs.\n","authors":["Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.05861v1.pdf","comment":"22 pages, 4 figures, Code: https://github.com/archiki/RepARe"},{"id":"http://arxiv.org/abs/2305.08732v2","updated":"2023-10-09T16:55:39Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05858v1","updated":"2023-10-09T16:52:48Z","published":"2023-10-09T16:52:48Z","title":"DSAC-T: Distributional Soft Actor-Critic with Three Refinements","summary":"  Reinforcement learning (RL) has proven to be highly effective in tackling\ncomplex decision-making and control tasks. However, prevalent model-free RL\nmethods often face severe performance degradation due to the well-known\noverestimation issue. In response to this problem, we recently introduced an\noff-policy RL algorithm, called distributional soft actor-critic (DSAC or\nDSAC-v1), which can effectively improve the value estimation accuracy by\nlearning a continuous Gaussian value distribution. Nonetheless, standard DSAC\nhas its own shortcomings, including occasionally unstable learning processes\nand needs for task-specific reward scaling, which may hinder its overall\nperformance and adaptability in some special tasks. This paper further\nintroduces three important refinements to standard DSAC in order to address\nthese shortcomings. These refinements consist of critic gradient adjusting,\ntwin value distribution learning, and variance-based target return clipping.\nThe modified RL algorithm is named as DSAC with three refinements (DSAC-T or\nDSAC-v2), and its performances are systematically evaluated on a diverse set of\nbenchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T\nsurpasses a lot of mainstream model-free RL algorithms, including SAC, TD3,\nDDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlike\nits standard version, ensures a highly stable learning process and delivers\nsimilar performance across varying reward scales.\n","authors":["Jingliang Duan","Wenxuan Wang","Liming Xiao","Jiaxin Gao","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2310.05858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05857v1","updated":"2023-10-09T16:52:07Z","published":"2023-10-09T16:52:07Z","title":"Improving Summarization with Human Edits","summary":"  Recent work has shown the promise of learning with human feedback paradigms\nto produce human-determined high-quality text. Existing works use human\nfeedback to train large language models (LLMs) in general domain abstractive\nsummarization and have obtained summary quality exceeding traditional\nlikelihood training. In this paper, we focus on a less explored form of human\nfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training\n(SALT), a novel technique to use both the human-edited and model-generated data\ntogether in the training loop. In addition, we demonstrate simulating Human\nEdits with ground truth summaries coming from existing training data --\nImitation edits, along with the model-generated summaries obtained after the\ntraining, to reduce the need for expensive human-edit data. In our experiments,\nwe extend human feedback exploration from general domain summarization to\nmedical domain summarization. Our results demonstrate the effectiveness of SALT\nto improve the summary quality with Human and Imitation Edits.\n","authors":["Zonghai Yao","Benjamin J Schloss","Sai P. Selvaraj"],"pdf_url":"https://arxiv.org/pdf/2310.05857v1.pdf","comment":"To appear in proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2023"},{"id":"http://arxiv.org/abs/2208.03835v2","updated":"2023-10-09T16:48:57Z","published":"2022-08-07T23:00:40Z","title":"On Transfer of Adversarial Robustness from Pretraining to Downstream\n  Tasks","summary":"  As large-scale training regimes have gained popularity, the use of pretrained\nmodels for downstream tasks has become common practice in machine learning.\nWhile pretraining has been shown to enhance the performance of models in\npractice, the transfer of robustness properties from pretraining to downstream\ntasks remains poorly understood. In this study, we demonstrate that the\nrobustness of a linear predictor on downstream tasks can be constrained by the\nrobustness of its underlying representation, regardless of the protocol used\nfor pretraining. We prove (i) a bound on the loss that holds independent of any\ndownstream task, as well as (ii) a criterion for robust classification in\nparticular. We validate our theoretical results in practical applications, show\nhow our results can be used for calibrating expectations of downstream\nrobustness, and when our results are useful for optimal transfer learning.\nTaken together, our results offer an initial step towards characterizing the\nrequirements of the representation function for reliable post-adaptation\nperformance.\n","authors":["Laura Fee Nern","Harsh Raj","Maurice Georgi","Yash Sharma"],"pdf_url":"https://arxiv.org/pdf/2208.03835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03758v2","updated":"2023-10-09T16:48:44Z","published":"2023-09-25T17:54:19Z","title":"A Unified Framework for Uniform Signal Recovery in Nonlinear Generative\n  Compressed Sensing","summary":"  In generative compressed sensing (GCS), we want to recover a signal\n$\\mathbf{x}^* \\in \\mathbb{R}^n$ from $m$ measurements ($m\\ll n$) using a\ngenerative prior $\\mathbf{x}^*\\in G(\\mathbb{B}_2^k(r))$, where $G$ is typically\nan $L$-Lipschitz continuous generative model and $\\mathbb{B}_2^k(r)$ represents\nthe radius-$r$ $\\ell_2$-ball in $\\mathbb{R}^k$. Under nonlinear measurements,\nmost prior results are non-uniform, i.e., they hold with high probability for a\nfixed $\\mathbf{x}^*$ rather than for all $\\mathbf{x}^*$ simultaneously. In this\npaper, we build a unified framework to derive uniform recovery guarantees for\nnonlinear GCS where the observation model is nonlinear and possibly\ndiscontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly\nquantized observations and single index models as canonical examples.\nSpecifically, using a single realization of the sensing ensemble and\ngeneralized Lasso, {\\em all} $\\mathbf{x}^*\\in G(\\mathbb{B}_2^k(r))$ can be\nrecovered up to an $\\ell_2$-error at most $\\epsilon$ using roughly\n$\\tilde{O}({k}/{\\epsilon^2})$ samples, with omitted logarithmic factors\ntypically being dominated by $\\log L$. Notably, this almost coincides with\nexisting non-uniform guarantees up to logarithmic factors, hence the uniformity\ncosts very little. As part of our technical contributions, we introduce the\nLipschitz approximation to handle discontinuous observation models. We also\ndevelop a concentration inequality that produces tighter bounds for product\nprocesses whose index sets have low metric entropy. Experimental results are\npresented to corroborate our theory.\n","authors":["Junren Chen","Jonathan Scarlett","Michael K. Ng","Zhaoqiang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.03758v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05842v1","updated":"2023-10-09T16:37:19Z","published":"2023-10-09T16:37:19Z","title":"Robust Angular Synchronization via Directed Graph Neural Networks","summary":"  The angular synchronization problem aims to accurately estimate (up to a\nconstant additive phase) a set of unknown angles $\\theta_1, \\dots,\n\\theta_n\\in[0, 2\\pi)$ from $m$ noisy measurements of their offsets\n$\\theta_i-\\theta_j \\;\\mbox{mod} \\; 2\\pi.$ Applications include, for example,\nsensor network localization, phase retrieval, and distributed clock\nsynchronization. An extension of the problem to the heterogeneous setting\n(dubbed $k$-synchronization) is to estimate $k$ groups of angles\nsimultaneously, given noisy observations (with unknown group assignment) from\neach group. Existing methods for angular synchronization usually perform poorly\nin high-noise regimes, which are common in applications. In this paper, we\nleverage neural networks for the angular synchronization problem, and its\nheterogeneous extension, by proposing GNNSync, a theoretically-grounded\nend-to-end trainable framework using directed graph neural networks. In\naddition, new loss functions are devised to encode synchronization objectives.\nExperimental results on extensive data sets demonstrate that GNNSync attains\ncompetitive, and often superior, performance against a comprehensive set of\nbaselines for the angular synchronization problem and its extension, validating\nthe robustness of GNNSync even at high noise levels.\n","authors":["Yixuan He","Gesine Reinert","David Wipf","Mihai Cucuringu"],"pdf_url":"https://arxiv.org/pdf/2310.05842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05840v1","updated":"2023-10-09T16:33:44Z","published":"2023-10-09T16:33:44Z","title":"Predicting Accident Severity: An Analysis Of Factors Affecting Accident\n  Severity Using Random Forest Model","summary":"  Road accidents have significant economic and societal costs, with a small\nnumber of severe accidents accounting for a large portion of these costs.\nPredicting accident severity can help in the proactive approach to road safety\nby identifying potential unsafe road conditions and taking well-informed\nactions to reduce the number of severe accidents. This study investigates the\neffectiveness of the Random Forest machine learning algorithm for predicting\nthe severity of an accident. The model is trained on a dataset of accident\nrecords from a large metropolitan area and evaluated using various metrics.\nHyperparameters and feature selection are optimized to improve the model's\nperformance. The results show that the Random Forest model is an effective tool\nfor predicting accident severity with an accuracy of over 80%. The study also\nidentifies the top six most important variables in the model, which include\nwind speed, pressure, humidity, visibility, clear conditions, and cloud cover.\nThe fitted model has an Area Under the Curve of 80%, a recall of 79.2%, a\nprecision of 97.1%, and an F1 score of 87.3%. These results suggest that the\nproposed model has higher performance in explaining the target variable, which\nis the accident severity class. Overall, the study provides evidence that the\nRandom Forest model is a viable and reliable tool for predicting accident\nseverity and can be used to help reduce the number of fatalities and injuries\ndue to road accidents in the United States\n","authors":["Adekunle Adefabi","Somtobe Olisah","Callistus Obunadike","Oluwatosin Oyetubo","Esther Taiwo","Edward Tella"],"pdf_url":"https://arxiv.org/pdf/2310.05840v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2302.04810v2","updated":"2023-10-09T16:31:46Z","published":"2023-02-09T17:57:02Z","title":"Real-world Machine Learning Systems: A survey from a Data-Oriented\n  Architecture Perspective","summary":"  Machine Learning models are being deployed as parts of real-world systems\nwith the upsurge of interest in artificial intelligence. The design,\nimplementation, and maintenance of such systems are challenged by real-world\nenvironments that produce larger amounts of heterogeneous data and users\nrequiring increasingly faster responses with efficient resource consumption.\nThese requirements push prevalent software architectures to the limit when\ndeploying ML-based systems. Data-oriented Architecture (DOA) is an emerging\nconcept that equips systems better for integrating ML models. DOA extends\ncurrent architectures to create data-driven, loosely coupled, decentralised,\nopen systems. Even though papers on deployed ML-based systems do not mention\nDOA, their authors made design decisions that implicitly follow DOA. The\nreasons why, how, and the extent to which DOA is adopted in these systems are\nunclear. Implicit design decisions limit the practitioners' knowledge of DOA to\ndesign ML-based systems in the real world. This paper answers these questions\nby surveying real-world deployments of ML-based systems. The survey shows the\ndesign decisions of the systems and the requirements these satisfy. Based on\nthe survey findings, we also formulate practical advice to facilitate the\ndeployment of ML-based systems. Finally, we outline open challenges to\ndeploying DOA-based systems that integrate ML models.\n","authors":["Christian Cabrera","Andrei Paleyes","Pierre Thodoroff","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2302.04810v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2305.19187v2","updated":"2023-10-09T16:30:08Z","published":"2023-05-30T16:31:26Z","title":"Generating with Confidence: Uncertainty Quantification for Black-box\n  Large Language Models","summary":"  Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for black-box LLMs. We first differentiate\nuncertainty vs confidence: the former refers to the \"dispersion\" of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty metrics, applying them to selective NLG where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple metric for the\nsemantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.\n","authors":["Zhen Lin","Shubhendu Trivedi","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2305.19187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04819v2","updated":"2023-10-09T16:24:26Z","published":"2023-04-10T19:00:29Z","title":"Recent Advancements in Machine Learning For Cybercrime Prediction","summary":"  Cybercrime is a growing threat to organizations and individuals worldwide,\nwith criminals using sophisticated techniques to breach security systems and\nsteal sensitive data. This paper aims to comprehensively survey the latest\nadvancements in cybercrime prediction, highlighting the relevant research. For\nthis purpose, we reviewed more than 150 research articles and discussed 50 most\nrecent and appropriate ones. We start the review with some standard methods\ncybercriminals use and then focus on the latest machine and deep learning\ntechniques, which detect anomalous behavior and identify potential threats. We\nalso discuss transfer learning, which allows models trained on one dataset to\nbe adapted for use on another dataset. We then focus on active and\nreinforcement learning as part of early-stage algorithmic research in\ncybercrime prediction. Finally, we discuss critical innovations, research gaps,\nand future research opportunities in Cybercrime prediction. This paper presents\na holistic view of cutting-edge developments and publicly available datasets.\n","authors":["Lavanya Elluri","Varun Mandalapu","Piyush Vyas","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2304.04819v2.pdf","comment":"Accepted in Journal of Computer Information Systems, 2023"},{"id":"http://arxiv.org/abs/2310.05833v1","updated":"2023-10-09T16:22:11Z","published":"2023-10-09T16:22:11Z","title":"A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative\n  Models","summary":"  Generative models, like large language models, are becoming increasingly\nrelevant in our daily lives, yet a theoretical framework to assess their\ngeneralization behavior and uncertainty does not exist. Particularly, the\nproblem of uncertainty estimation is commonly solved in an ad-hoc manner and\ntask dependent. For example, natural language approaches cannot be transferred\nto image generation. In this paper we introduce the first\nbias-variance-covariance decomposition for kernel scores and their associated\nentropy. We propose unbiased and consistent estimators for each quantity which\nonly require generated samples but not the underlying model itself. As an\napplication, we offer a generalization evaluation of diffusion models and\ndiscover how mode collapse of minority groups is a contrary phenomenon to\noverfitting. Further, we demonstrate that variance and predictive kernel\nentropy are viable measures of uncertainty for image, audio, and language\ngeneration. Specifically, our approach for uncertainty estimation is more\npredictive of performance on CoQA and TriviaQA question answering datasets than\nexisting baselines and can also be applied to closed-source models.\n","authors":["Sebastian G. Gruber","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2310.05833v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.04015v2","updated":"2023-10-09T16:20:49Z","published":"2023-10-06T04:52:46Z","title":"Anonymous Learning via Look-Alike Clustering: A Precise Analysis of\n  Model Generalization","summary":"  While personalized recommendations systems have become increasingly popular,\nensuring user data protection remains a top concern in the development of these\nlearning systems. A common approach to enhancing privacy involves training\nmodels using anonymous data rather than individual data. In this paper, we\nexplore a natural technique called \\emph{look-alike clustering}, which involves\nreplacing sensitive features of individuals with the cluster's average values.\nWe provide a precise analysis of how training models using anonymous cluster\ncenters affects their generalization capabilities. We focus on an asymptotic\nregime where the size of the training set grows in proportion to the features\ndimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT)\nand allows us to theoretically understand the role of different model\ncomponents on the generalization error. In addition, we demonstrate that in\ncertain high-dimensional regimes, training over anonymous cluster centers acts\nas a regularization and improves generalization error of the trained models.\nFinally, we corroborate our asymptotic theory with finite-sample numerical\nexperiments where we observe a perfect match when the sample size is only of\norder of a few hundreds.\n","authors":["Adel Javanmard","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2310.04015v2.pdf","comment":"accepted at the Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2309.03249v2","updated":"2023-10-09T16:20:33Z","published":"2023-09-06T15:47:18Z","title":"Graph Theory Applications in Advanced Geospatial Research","summary":"  Geospatial sciences include a wide range of applications, from environmental\nmonitoring transportation to infrastructure planning, as well as location-based\nanalysis and services. Graph theory algorithms in mathematics have emerged as\nindispensable tools in these domains due to their capability to model and\nanalyse spatial relationships efficiently. This article explores the\napplications of graph theory algorithms in geospatial sciences, highlighting\ntheir role in network analysis, spatial connectivity, geographic information\nsystems, and various other spatial problem-solving scenarios like digital twin.\nThe article provides a comprehensive idea about graph theory's key concepts and\nalgorithms that assist the geospatial modelling processes and insights into\nreal-world geospatial challenges and opportunities. It lists the extensive\nresearch, innovative technologies and methodologies implemented in this domain.\n","authors":["Surajit Ghosh","Archita Mallick","Anuva Chowdhury","Kounik De Sarkar"],"pdf_url":"https://arxiv.org/pdf/2309.03249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05821v1","updated":"2023-10-09T16:05:43Z","published":"2023-10-09T16:05:43Z","title":"Pre-trained Spatial Priors on Multichannel NMF for Music Source\n  Separation","summary":"  This paper presents a novel approach to sound source separation that\nleverages spatial information obtained during the recording setup. Our method\ntrains a spatial mixing filter using solo passages to capture information about\nthe room impulse response and transducer response at each sensor location. This\npre-trained filter is then integrated into a multichannel non-negative matrix\nfactorization (MNMF) scheme to better capture the variances of different sound\nsources. The recording setup used in our experiments is the typical setup for\norchestra recordings, with a main microphone and a close \"cardioid\" or\n\"supercardioid\" microphone for each section of the orchestra. This makes the\nproposed method applicable to many existing recordings. Experiments on\npolyphonic ensembles demonstrate the effectiveness of the proposed framework in\nseparating individual sound sources, improving performance compared to\nconventional MNMF methods.\n","authors":["Pablo Cabanas-Molero","Antonio J. Munoz-Montoro","Julio Carabias-Orti","Pedro Vera-Candeas"],"pdf_url":"https://arxiv.org/pdf/2310.05821v1.pdf","comment":"Accepted for publication at Forum Acusticum 2023"},{"id":"http://arxiv.org/abs/2310.05812v1","updated":"2023-10-09T15:52:59Z","published":"2023-10-09T15:52:59Z","title":"Provably Convergent Data-Driven Convex-Nonconvex Regularization","summary":"  An emerging new paradigm for solving inverse problems is via the use of deep\nlearning to learn a regularizer from data. This leads to high-quality results,\nbut often at the cost of provable guarantees. In this work, we show how\nwell-posedness and convergent regularization arises within the convex-nonconvex\n(CNC) framework for inverse problems. We introduce a novel input weakly convex\nneural network (IWCNN) construction to adapt the method of learned adversarial\nregularization to the CNC framework. Empirically we show that our method\novercomes numerical issues of previous adversarial methods.\n","authors":["Zakhar Shumaylov","Jeremy Budd","Subhadip Mukherjee","Carola-Bibiane Schnlieb"],"pdf_url":"https://arxiv.org/pdf/2310.05812v1.pdf","comment":"4 pages + 3 pages appendices; preprint"},{"id":"http://arxiv.org/abs/2303.05118v4","updated":"2023-10-09T15:50:00Z","published":"2023-03-09T08:57:01Z","title":"SLCA: Slow Learner with Classifier Alignment for Continual Learning on a\n  Pre-trained Model","summary":"  The goal of continual learning is to improve the performance of recognition\nmodels in learning sequentially arrived data. Although most existing works are\nestablished on the premise of learning from scratch, growing efforts have been\ndevoted to incorporating the benefits of pre-training. However, how to\nadaptively exploit the pre-trained knowledge for each incremental task while\nmaintaining its generalizability remains an open question. In this work, we\npresent an extensive analysis for continual learning on a pre-trained model\n(CLPM), and attribute the key challenge to a progressive overfitting problem.\nObserving that selectively reducing the learning rate can almost resolve this\nissue in the representation layer, we propose a simple but extremely effective\napproach named Slow Learner with Classifier Alignment (SLCA), which further\nimproves the classification layer by modeling the class-wise distributions and\naligning the classification layers in a post-hoc fashion. Across a variety of\nscenarios, our proposal provides substantial improvements for CLPM (e.g., up to\n49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split\nCUB-200 and Split Cars-196, respectively), and thus outperforms\nstate-of-the-art approaches by a large margin. Based on such a strong baseline,\ncritical factors and promising directions are analyzed in-depth to facilitate\nsubsequent research. Code has been made available at:\nhttps://github.com/GengDavid/SLCA.\n","authors":["Gengwei Zhang","Liyuan Wang","Guoliang Kang","Ling Chen","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2303.05118v4.pdf","comment":"ICCV 2023, code released"},{"id":"http://arxiv.org/abs/2306.03516v2","updated":"2023-10-09T15:49:03Z","published":"2023-06-06T09:08:40Z","title":"COPR: Consistency-Oriented Pre-Ranking for Online Advertising","summary":"  Cascading architecture has been widely adopted in large-scale advertising\nsystems to balance efficiency and effectiveness. In this architecture, the\npre-ranking model is expected to be a lightweight approximation of the ranking\nmodel, which handles more candidates with strict latency requirements. Due to\nthe gap in model capacity, the pre-ranking and ranking models usually generate\ninconsistent ranked results, thus hurting the overall system effectiveness. The\nparadigm of score alignment is proposed to regularize their raw scores to be\nconsistent. However, it suffers from inevitable alignment errors and error\namplification by bids when applied in online advertising. To this end, we\nintroduce a consistency-oriented pre-ranking framework for online advertising,\nwhich employs a chunk-based sampling module and a plug-and-play rank alignment\nmodule to explicitly optimize consistency of ECPM-ranked results. A $\\Delta\nNDCG$-based weighting mechanism is adopted to better distinguish the importance\nof inter-chunk samples in optimization. Both online and offline experiments\nhave validated the superiority of our framework. When deployed in Taobao\ndisplay advertising system, it achieves an improvement of up to +12.3\\% CTR and\n+5.6\\% RPM.\n","authors":["Zhishan Zhao","Jingyue Gao","Yu Zhang","Shuguang Han","Siyuan Lou","Xiang-Rong Sheng","Zhe Wang","Han Zhu","Yuning Jiang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2306.03516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05807v1","updated":"2023-10-09T15:44:35Z","published":"2023-10-09T15:44:35Z","title":"Sharing Information Between Machine Tools to Improve Surface Finish\n  Forecasting","summary":"  At present, most surface-quality prediction methods can only perform\nsingle-task prediction which results in under-utilised datasets, repetitive\nwork and increased experimental costs. To counter this, the authors propose a\nBayesian hierarchical model to predict surface-roughness measurements for a\nturning machining process. The hierarchical model is compared to multiple\nindependent Bayesian linear regression models to showcase the benefits of\npartial pooling in a machining setting with respect to prediction accuracy and\nuncertainty quantification.\n","authors":["Daniel R. Clarkson","Lawrence A. Bull","Tina A. Dardeno","Chandula T. Wickramarachchi","Elizabeth J. Cross","Timothy J. Rogers","Keith Worden","Nikolaos Dervilis","Aidan J. Hughes"],"pdf_url":"https://arxiv.org/pdf/2310.05807v1.pdf","comment":"Submitted to International Workshop on Structural Health Monitoring\n  2023, Stanford University, California, USA"},{"id":"http://arxiv.org/abs/2310.05805v1","updated":"2023-10-09T15:43:46Z","published":"2023-10-09T15:43:46Z","title":"Boosted Control Functions","summary":"  Modern machine learning methods and the availability of large-scale data\nopened the door to accurately predict target quantities from large sets of\ncovariates. However, existing prediction methods can perform poorly when the\ntraining and testing data are different, especially in the presence of hidden\nconfounding. While hidden confounding is well studied for causal effect\nestimation (e.g., instrumental variables), this is not the case for prediction\ntasks. This work aims to bridge this gap by addressing predictions under\ndifferent training and testing distributions in the presence of unobserved\nconfounding. In particular, we establish a novel connection between the field\nof distribution generalization from machine learning, and simultaneous equation\nmodels and control function from econometrics. Central to our contribution are\nsimultaneous equation models for distribution generalization (SIMDGs) which\ndescribe the data-generating process under a set of distributional shifts.\nWithin this framework, we propose a strong notion of invariance for a\npredictive model and compare it with existing (weaker) versions. Building on\nthe control function approach from instrumental variable regression, we propose\nthe boosted control function (BCF) as a target of inference and prove its\nability to successfully predict even in intervened versions of the underlying\nSIMDG. We provide necessary and sufficient conditions for identifying the BCF\nand show that it is worst-case optimal. We introduce the ControlTwicing\nalgorithm to estimate the BCF and analyze its predictive performance on\nsimulated and real world data.\n","authors":["Nicola Gnecco","Jonas Peters","Sebastian Engelke","Niklas Pfister"],"pdf_url":"https://arxiv.org/pdf/2310.05805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01769v2","updated":"2023-10-09T15:43:26Z","published":"2023-10-03T03:34:22Z","title":"How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing:\n  The Curses of Symmetry and Initialization","summary":"  This paper rigorously shows how over-parameterization changes the convergence\nbehaviors of gradient descent (GD) for the matrix sensing problem, where the\ngoal is to recover an unknown low-rank ground-truth matrix from near-isotropic\nlinear measurements. First, we consider the symmetric setting with the\nsymmetric parameterization where $M^* \\in \\mathbb{R}^{n \\times n}$ is a\npositive semi-definite unknown matrix of rank $r \\ll n$, and one uses a\nsymmetric parameterization $XX^\\top$ to learn $M^*$. Here $X \\in \\mathbb{R}^{n\n\\times k}$ with $k > r$ is the factor matrix. We give a novel $\\Omega (1/T^2)$\nlower bound of randomly initialized GD for the over-parameterized case ($k >r$)\nwhere $T$ is the number of iterations. This is in stark contrast to the\nexact-parameterization scenario ($k=r$) where the convergence rate is $\\exp\n(-\\Omega (T))$. Next, we study asymmetric setting where $M^* \\in\n\\mathbb{R}^{n_1 \\times n_2}$ is the unknown matrix of rank $r \\ll\n\\min\\{n_1,n_2\\}$, and one uses an asymmetric parameterization $FG^\\top$ to\nlearn $M^*$ where $F \\in \\mathbb{R}^{n_1 \\times k}$ and $G \\in \\mathbb{R}^{n_2\n\\times k}$. Building on prior work, we give a global exact convergence result\nof randomly initialized GD for the exact-parameterization case ($k=r$) with an\n$\\exp (-\\Omega(T))$ rate. Furthermore, we give the first global exact\nconvergence result for the over-parameterization case ($k>r$) with an\n$\\exp(-\\Omega(\\alpha^2 T))$ rate where $\\alpha$ is the initialization scale.\nThis linear convergence result in the over-parameterization case is especially\nsignificant because one can apply the asymmetric parameterization to the\nsymmetric setting to speed up from $\\Omega (1/T^2)$ to linear convergence. On\nthe other hand, we propose a novel method that only modifies one step of GD and\nobtains a convergence rate independent of $\\alpha$, recovering the rate in the\nexact-parameterization case.\n","authors":["Nuoya Xiong","Lijun Ding","Simon S. Du"],"pdf_url":"https://arxiv.org/pdf/2310.01769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05801v1","updated":"2023-10-09T15:37:06Z","published":"2023-10-09T15:37:06Z","title":"An operator preconditioning perspective on training in physics-informed\n  machine learning","summary":"  In this paper, we investigate the behavior of gradient descent algorithms in\nphysics-informed machine learning methods like PINNs, which minimize residuals\nconnected to partial differential equations (PDEs). Our key result is that the\ndifficulty in training these models is closely related to the conditioning of a\nspecific differential operator. This operator, in turn, is associated to the\nHermitian square of the differential operator of the underlying PDE. If this\noperator is ill-conditioned, it results in slow or infeasible training.\nTherefore, preconditioning this operator is crucial. We employ both rigorous\nmathematical analysis and empirical evaluations to investigate various\nstrategies, explaining how they better condition this critical operator, and\nconsequently improve training.\n","authors":["Tim De Ryck","Florent Bonnet","Siddhartha Mishra","Emmanuel de Bzenac"],"pdf_url":"https://arxiv.org/pdf/2310.05801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10792v4","updated":"2023-10-09T15:36:49Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v4.pdf","comment":"A Survey paper, Pre-print"},{"id":"http://arxiv.org/abs/2310.05799v1","updated":"2023-10-09T15:36:15Z","published":"2023-10-09T15:36:15Z","title":"The First Cadenza Signal Processing Challenge: Improving Music for Those\n  With a Hearing Loss","summary":"  The Cadenza project aims to improve the audio quality of music for those who\nhave a hearing loss. This is being done through a series of signal processing\nchallenges, to foster better and more inclusive technologies. In the first\nround, two common listening scenarios are considered: listening to music over\nheadphones, and with a hearing aid in a car. The first scenario is cast as a\ndemixing-remixing problem, where the music is decomposed into vocals, bass,\ndrums and other components. These can then be intelligently remixed in a\npersonalized way, to increase the audio quality for a person who has a hearing\nloss. In the second scenario, music is coming from car loudspeakers, and the\nmusic has to be enhanced to overcome the masking effect of the car noise. This\nis done by taking into account the music, the hearing ability of the listener,\nthe hearing aid and the speed of the car. The audio quality of the submissions\nwill be evaluated using the Hearing Aid Audio Quality Index (HAAQI) for\nobjective assessment and by a panel of people with hearing loss for subjective\nevaluation.\n","authors":["Gerardo Roa Dabike","Scott Bannister","Jennifer Firth","Simone Graetzer","Rebecca Vos","Michael A. Akeroyd","Jon Barker","Trevor J. Cox","Bruno Fazenda","Alinka Greasley","William Whitmer"],"pdf_url":"https://arxiv.org/pdf/2310.05799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05797v1","updated":"2023-10-09T15:31:03Z","published":"2023-10-09T15:31:03Z","title":"Are Large Language Models Post Hoc Explainers?","summary":"  Large Language Models (LLMs) are increasingly used as powerful tools for a\nplethora of natural language processing (NLP) applications. A recent\ninnovation, in-context learning (ICL), enables LLMs to learn new tasks by\nsupplying a few examples in the prompt during inference time, thereby\neliminating the need for model fine-tuning. While LLMs have been utilized in\nseveral applications, their applicability in explaining the behavior of other\nmodels remains relatively unexplored. Despite the growing number of new\nexplanation techniques, many require white-box access to the model and/or are\ncomputationally expensive, highlighting a need for next-generation post hoc\nexplainers. In this work, we present the first framework to study the\neffectiveness of LLMs in explaining other predictive models. More specifically,\nwe propose a novel framework encompassing multiple prompting strategies: i)\nPerturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,\nand iv) Explanation-based ICL, with varying levels of information about the\nunderlying ML model and the local neighborhood of the test sample. We conduct\nextensive experiments with real-world benchmark datasets to demonstrate that\nLLM-generated explanations perform on par with state-of-the-art post hoc\nexplainers using their ability to leverage ICL examples and their internal\nknowledge in generating model explanations. On average, across four datasets\nand two ML models, we observe that LLMs identify the most important feature\nwith 72.19% accuracy, opening up new frontiers in explainable artificial\nintelligence (XAI) to explore LLM-based explanation frameworks.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05793v1","updated":"2023-10-09T15:29:10Z","published":"2023-10-09T15:29:10Z","title":"DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for\n  Accelerated Seq2Seq Diffusion Models","summary":"  Diffusion models have gained prominence in generating high-quality sequences\nof text. Nevertheless, current approaches predominantly represent discrete text\nwithin a continuous diffusion space, which incurs substantial computational\noverhead during training and results in slower sampling speeds. In this paper,\nwe introduce a soft absorbing state that facilitates the diffusion model in\nlearning to reconstruct discrete mutations based on the underlying Gaussian\nspace, thereby enhancing its capacity to recover conditional signals. During\nthe sampling phase, we employ state-of-the-art ODE solvers within the\ncontinuous space to expedite the sampling process. Comprehensive experimental\nevaluations reveal that our proposed method effectively accelerates the\ntraining convergence by 4x and generates samples of similar quality 800x\nfaster, rendering it significantly closer to practical application.\n\\footnote{The code is released at \\url{https://github.com/Shark-NLP/DiffuSeq}\n","authors":["Shansan Gong","Mukai Li","Jiangtao Feng","Zhiyong Wu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2310.05793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06382v2","updated":"2023-10-09T15:24:11Z","published":"2023-09-12T16:48:00Z","title":"Ensemble Mask Networks","summary":"  Can an $\\mathbb{R}^n\\rightarrow \\mathbb{R}^n$ feedforward network learn\nmatrix-vector multiplication? This study introduces two mechanisms - flexible\nmasking to take matrix inputs, and a unique network pruning to respect the\nmask's dependency structure. Networks can approximate fixed operations such as\nmatrix-vector multiplication $\\phi(A,x) \\rightarrow Ax$, motivating the\nmechanisms introduced with applications towards litmus-testing dependencies or\ninteraction order in graph-based models.\n","authors":["Jonny Luntzel"],"pdf_url":"https://arxiv.org/pdf/2309.06382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00239v2","updated":"2023-10-09T15:23:38Z","published":"2023-09-30T03:19:51Z","title":"AdaptNet: Policy Adaptation for Physics-Based Character Control","summary":"  Motivated by humans' ability to adapt skills in the learning of new ones,\nthis paper presents AdaptNet, an approach for modifying the latent space of\nexisting policies to allow new behaviors to be quickly learned from like tasks\nin comparison to learning from scratch. Building on top of a given\nreinforcement learning controller, AdaptNet uses a two-tier hierarchy that\naugments the original state embedding to support modest changes in a behavior\nand further modifies the policy network layers to make more substantive\nchanges. The technique is shown to be effective for adapting existing\nphysics-based controllers to a wide range of new styles for locomotion, new\ntask targets, changes in character morphology and extensive changes in\nenvironment. Furthermore, it exhibits significant increase in learning\nefficiency, as indicated by greatly reduced training times when compared to\ntraining from scratch or using other approaches that modify existing policies.\nCode is available at https://motion-lab.github.io/AdaptNet.\n","authors":["Pei Xu","Kaixiang Xie","Sheldon Andrews","Paul G. Kry","Michael Neff","Morgan McGuire","Ioannis Karamouzas","Victor Zordan"],"pdf_url":"https://arxiv.org/pdf/2310.00239v2.pdf","comment":"SIGGRAPH Asia 2023. Video: https://youtu.be/WxmJSCNFb28. Website:\n  https://motion-lab.github.io/AdaptNet, https://pei-xu.github.io/AdaptNet"},{"id":"http://arxiv.org/abs/2310.05789v1","updated":"2023-10-09T15:22:13Z","published":"2023-10-09T15:22:13Z","title":"Efficient Hybrid Oversampling and Intelligent Undersampling for\n  Imbalanced Big Data Classification","summary":"  Imbalanced classification is a well-known challenge faced by many real-world\napplications. This issue occurs when the distribution of the target variable is\nskewed, leading to a prediction bias toward the majority class. With the\narrival of the Big Data era, there is a pressing need for efficient solutions\nto solve this problem. In this work, we present a novel resampling method\ncalled SMOTENN that combines intelligent undersampling and oversampling using a\nMapReduce framework. Both procedures are performed on the same pass over the\ndata, conferring efficiency to the technique. The SMOTENN method is\ncomplemented with an efficient implementation of the neighborhoods related to\nthe minority samples. Our experimental results show the virtues of this\napproach, outperforming alternative resampling techniques for small- and\nmedium-sized datasets while achieving positive results on large datasets with\nreduced running times.\n","authors":["Carla Vairetti","Jos Luis Assadi","Sebastin Maldonado"],"pdf_url":"https://arxiv.org/pdf/2310.05789v1.pdf","comment":"17 pages, 1 figure, submitted to Expert Systems with Applications\n  (Elsevier)"},{"id":"http://arxiv.org/abs/2206.11723v6","updated":"2023-10-09T15:18:32Z","published":"2022-06-23T14:16:30Z","title":"Self-Supervised Training with Autoencoders for Visual Anomaly Detection","summary":"  Deep autoencoders provide an effective tool for learning non-linear\ndimensionality reduction in an unsupervised way. Recently, they have been used\nfor the task of anomaly detection in the visual domain. By optimizing for the\nreconstruction error using anomaly-free examples, the common belief is that a\ncorresponding network should fail to accurately reconstruct anomalous regions\nin the application phase. This goal is typically addressed by controlling the\ncapacity of the network, either by reducing the size of the bottleneck layer or\nby enforcing sparsity constraints on the activations. However, neither of these\ntechniques does explicitly penalize reconstruction of anomalous signals often\nresulting in poor detection. We tackle this problem by adapting a\nself-supervised learning regime that allows the use of discriminative\ninformation during training but focuses on the data manifold of normal\nexamples. We emphasize that inference with our approach is very efficient\nduring training and prediction requiring a single forward pass for each input\nimage. Our experiments on the MVTec AD dataset demonstrate high detection and\nlocalization performance. On the texture-subset, in particular, our approach\nconsistently outperforms recent anomaly detection methods by a significant\nmargin.\n","authors":["Alexander Bauer","Shinichi Nakajima","Klaus-Robert Mller"],"pdf_url":"https://arxiv.org/pdf/2206.11723v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05779v1","updated":"2023-10-09T15:11:02Z","published":"2023-10-09T15:11:02Z","title":"Why Should This Article Be Deleted? Transparent Stance Detection in\n  Multilingual Wikipedia Editor Discussions","summary":"  The moderation of content on online platforms is usually non-transparent. On\nWikipedia, however, this discussion is carried out publicly and the editors are\nencouraged to use the content moderation policies as explanations for making\nmoderation decisions. Currently, only a few comments explicitly mention those\npolicies -- 20% of the English ones, but as few as 2% of the German and Turkish\ncomments. To aid in this process of understanding how content is moderated, we\nconstruct a novel multilingual dataset of Wikipedia editor discussions along\nwith their reasoning in three languages. The dataset contains the stances of\nthe editors (keep, delete, merge, comment), along with the stated reason, and a\ncontent moderation policy, for each edit decision. We demonstrate that stance\nand corresponding reason (policy) can be predicted jointly with a high degree\nof accuracy, adding transparency to the decision-making process. We release\nboth our joint prediction models and the multilingual content moderation\ndataset for further research on automated transparent content moderation.\n","authors":["Lucie-Aime Kaffee","Arnav Arora","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2310.05779v1.pdf","comment":"This submission has been accepted to Conference on Empirical Methods\n  in Natural Language Processing (EMNLP)"},{"id":"http://arxiv.org/abs/2302.04054v7","updated":"2023-10-09T15:05:36Z","published":"2023-02-08T13:47:00Z","title":"Towards Inferential Reproducibility of Machine Learning Research","summary":"  Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n","authors":["Michael Hagmann","Philipp Meier","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2302.04054v7.pdf","comment":"Published at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.03192v2","updated":"2023-10-09T15:05:01Z","published":"2023-03-06T16:47:59Z","title":"Deep symbolic regression for physics guided by units constraints: toward\n  the automated discovery of physical laws","summary":"  Symbolic Regression is the study of algorithms that automate the search for\nanalytic expressions that fit data. While recent advances in deep learning have\ngenerated renewed interest in such approaches, the development of symbolic\nregression methods has not been focused on physics, where we have important\nadditional constraints due to the units associated with our data. Here we\npresent $\\Phi$-SO, a Physical Symbolic Optimization framework for recovering\nanalytical symbolic expressions from physics data using deep reinforcement\nlearning techniques by learning units constraints. Our system is built, from\nthe ground up, to propose solutions where the physical units are consistent by\nconstruction. This is useful not only in eliminating physically impossible\nsolutions, but because the \"grammatical\" rules of dimensional analysis restrict\nenormously the freedom of the equation generator, thus vastly improving\nperformance. The algorithm can be used to fit noiseless data, which can be\nuseful for instance when attempting to derive an analytical property of a\nphysical model, and it can also be used to obtain analytical approximations to\nnoisy data. We test our machinery on a standard benchmark of equations from the\nFeynman Lectures on Physics and other physics textbooks, achieving\nstate-of-the-art performance in the presence of noise (exceeding 0.1%) and show\nthat it is robust even in the presence of substantial (10%) noise. We showcase\nits abilities on a panel of examples from astrophysics.\n","authors":["Wassim Tenachi","Rodrigo Ibata","Foivos I. Diakogiannis"],"pdf_url":"https://arxiv.org/pdf/2303.03192v2.pdf","comment":"29 pages, 9 figures, 11 tables. Accepted for publication at ApJ"},{"id":"http://arxiv.org/abs/2310.05771v1","updated":"2023-10-09T14:57:05Z","published":"2023-10-09T14:57:05Z","title":"Foundation Models Meet Visualizations: Challenges and Opportunities","summary":"  Recent studies have indicated that foundation models, such as BERT and GPT,\nexcel in adapting to a variety of downstream tasks. This adaptability has\nestablished them as the dominant force in building artificial intelligence (AI)\nsystems. As visualization techniques intersect with these models, a new\nresearch paradigm emerges. This paper divides these intersections into two main\nareas: visualizations for foundation models (VIS4FM) and foundation models for\nvisualizations (FM4VIS). In VIS4FM, we explore the primary role of\nvisualizations in understanding, refining, and evaluating these intricate\nmodels. This addresses the pressing need for transparency, explainability,\nfairness, and robustness. Conversely, within FM4VIS, we highlight how\nfoundation models can be utilized to advance the visualization field itself.\nThe confluence of foundation models and visualizations holds great promise, but\nit also comes with its own set of challenges. By highlighting these challenges\nand the growing opportunities, this paper seeks to provide a starting point for\ncontinued exploration in this promising avenue.\n","authors":["Weikai Yang","Mengchen Liu","Zheng Wang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05771v1.pdf","comment":"Submitted to Computational Visual Media"},{"id":"http://arxiv.org/abs/2310.05764v1","updated":"2023-10-09T14:45:33Z","published":"2023-10-09T14:45:33Z","title":"Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and\n  Binding Site Design","summary":"  A significant amount of protein function requires binding small molecules,\nincluding enzymatic catalysis. As such, designing binding pockets for small\nmolecules has several impactful applications ranging from drug synthesis to\nenergy storage. Towards this goal, we first develop HarmonicFlow, an improved\ngenerative process over 3D protein-ligand binding structures based on our\nself-conditioned flow matching objective. FlowSite extends this flow model to\njointly generate a protein pocket's discrete residue types and the molecule's\nbinding 3D structure. We show that HarmonicFlow improves upon the\nstate-of-the-art generative processes for docking in simplicity, generality,\nand performance. Enabled by this structure modeling, FlowSite designs binding\nsites substantially better than baseline approaches and provides the first\ngeneral solution for binding site design.\n","authors":["Hannes Strk","Bowen Jing","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2310.05764v1.pdf","comment":"Under review. 25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.05757v1","updated":"2023-10-09T14:33:32Z","published":"2023-10-09T14:33:32Z","title":"Nonlinear Correct and Smooth for Semi-Supervised Learning","summary":"  Graph-based semi-supervised learning (GSSL) has been used successfully in\nvarious applications. Existing methods leverage the graph structure and labeled\nsamples for classification. Label Propagation (LP) and Graph Neural Networks\n(GNNs) both iteratively pass messages on graphs, where LP propagates node\nlabels through edges and GNN aggregates node features from the neighborhood.\nRecently, combining LP and GNN has led to improved performance. However,\nutilizing labels and features jointly in higher-order graphs has not been\nexplored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which\nimproves the existing post-processing approach by incorporating non-linearity\nand higher-order representation into the residual propagation to handle\nintricate node relationships effectively. Systematic evaluations show that our\nmethod achieves remarkable average improvements of 13.71% over base prediction\nand 2.16% over the state-of-the-art post-processing method on six commonly used\ndatasets. Comparisons and analyses show our method effectively utilizes labels\nand features jointly in higher-order graphs to resolve challenging graph\nrelationships.\n","authors":["Yuanhang Shao","Xiuwen Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05755v1","updated":"2023-10-09T14:31:03Z","published":"2023-10-09T14:31:03Z","title":"Deep Concept Removal","summary":"  We address the problem of concept removal in deep neural networks, aiming to\nlearn representations that do not encode certain specified concepts (e.g.,\ngender etc.) We propose a novel method based on adversarial linear classifiers\ntrained on a concept dataset, which helps to remove the targeted attribute\nwhile maintaining model performance. Our approach Deep Concept Removal\nincorporates adversarial probing classifiers at various layers of the network,\neffectively addressing concept entanglement and improving out-of-distribution\ngeneralization. We also introduce an implicit gradient-based technique to\ntackle the challenges associated with adversarial training using linear\nclassifiers. We evaluate the ability to remove a concept on a set of popular\ndistributionally robust optimization (DRO) benchmarks with spurious\ncorrelations, as well as out-of-distribution (OOD) generalization tasks.\n","authors":["Yegor Klochkov","Jean-Francois Ton","Ruocheng Guo","Yang Liu","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2310.05755v1.pdf","comment":"21 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.05754v1","updated":"2023-10-09T14:30:10Z","published":"2023-10-09T14:30:10Z","title":"Unleashing the power of Neural Collapse for Transferability Estimation","summary":"  Transferability estimation aims to provide heuristics for quantifying how\nsuitable a pre-trained model is for a specific downstream task, without\nfine-tuning them all. Prior studies have revealed that well-trained models\nexhibit the phenomenon of Neural Collapse. Based on a widely used neural\ncollapse metric in existing literature, we observe a strong correlation between\nthe neural collapse of pre-trained models and their corresponding fine-tuned\nmodels. Inspired by this observation, we propose a novel method termed Fair\nCollapse (FaCe) for transferability estimation by comprehensively measuring the\ndegree of neural collapse in the pre-trained model. Typically, FaCe comprises\ntwo different terms: the variance collapse term, which assesses the class\nseparation and within-class compactness, and the class fairness term, which\nquantifies the fairness of the pre-trained model towards each class. We\ninvestigate FaCe on a variety of pre-trained classification models across\ndifferent network architectures, source datasets, and training loss functions.\nResults show that FaCe yields state-of-the-art performance on different tasks\nincluding image classification, semantic segmentation, and text classification,\nwhich demonstrate the effectiveness and generalization of our method.\n","authors":["Yuhe Ding","Bo Jiang","Lijun Sheng","Aihua Zheng","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2310.05754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.08162v2","updated":"2023-10-09T14:29:34Z","published":"2022-10-15T02:13:49Z","title":"AMD-DBSCAN: An Adaptive Multi-density DBSCAN for datasets of extremely\n  variable density","summary":"  DBSCAN has been widely used in density-based clustering algorithms. However,\nwith the increasing demand for Multi-density clustering, previous traditional\nDSBCAN can not have good clustering results on Multi-density datasets. In order\nto address this problem, an adaptive Multi-density DBSCAN algorithm\n(AMD-DBSCAN) is proposed in this paper. An improved parameter adaptation method\nis proposed in AMD-DBSCAN to search for multiple parameter pairs (i.e., Eps and\nMinPts), which are the key parameters to determine the clustering results and\nperformance, therefore allowing the model to be applied to Multi-density\ndatasets. Moreover, only one hyperparameter is required for AMD-DBSCAN to avoid\nthe complicated repetitive initialization operations. Furthermore, the variance\nof the number of neighbors (VNN) is proposed to measure the difference in\ndensity between each cluster. The experimental results show that our AMD-DBSCAN\nreduces execution time by an average of 75% due to lower algorithm complexity\ncompared with the traditional adaptive algorithm. In addition, AMD-DBSCAN\nimproves accuracy by 24.7% on average over the state-of-the-art design on\nMulti-density datasets of extremely variable density, while having no\nperformance loss in Single-density scenarios. Our code and datasets are\navailable at https://github.com/AlexandreWANG915/AMD-DBSCAN.\n","authors":["Ziqing Wang","Zhirong Ye","Yuyang Du","Yi Mao","Yanying Liu","Ziling Wu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2210.08162v2.pdf","comment":"Accepted at DSAA2022"},{"id":"http://arxiv.org/abs/2307.01379v2","updated":"2023-10-09T14:26:59Z","published":"2023-07-03T22:17:16Z","title":"Shifting Attention to Relevance: Towards the Uncertainty Estimation of\n  Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated remarkable potential in\nnatural language generation and instruction following, a persistent challenge\nlies in their susceptibility to \"hallucinations\", which erodes trust in their\noutputs. Although Uncertainty Quantification (UQ) presents a promising\nsolution, its accurate implementation within the context of LLMs remains a\nsignificant hurdle. To address this critical roadblock, our research originates\nfrom a fundamental heuristic insight: tokens within auto-regressive\nLLM-generated text do not equally reflect the underlying meaning. Some tokens\ncarry greater relevance and representativeness than others, owing to the\nphenomenon of \"linguistic redundancy\", wherein a select few keywords suffice to\nconvey the essence of lengthy sentences. Regrettably, existing methodologies\ntreat all tokens with equal importance when estimating uncertainty,\ndisregarding these inherent generative inequalities. Our analysis reveals a\nsignificant issue with state-of-the-art: numerous tokens (and sentences) of\nlimited semantic significance receive equal or even excessive weighting during\nuncertainty estimation. To rectify this bias, we propose to jointly Shifting\nAttention to more Relevant (SAR) components, at both the token- and the\nsentence-levels for accurate uncertainty estimation. We conduct extensive\nexperiments involving a range of popular \"off-the-shelf\" LLMs, including\ninstruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as\npretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B\nparameters. We carry out evaluation across various free-form question-answering\ntasks, encompassing domains such as reading comprehension, science Q&A, and\nmedical Q&A. Our experimental results demonstrate the superior performance of\nSAR in addressing the challenges of uncertainty estimation within the realm of\nLLMs.\n","authors":["Jinhao Duan","Hao Cheng","Shiqi Wang","Alex Zavalny","Chenan Wang","Renjing Xu","Bhavya Kailkhura","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2307.01379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05742v1","updated":"2023-10-09T14:16:34Z","published":"2023-10-09T14:16:34Z","title":"Estimating Shape Distances on Neural Representations with Limited\n  Samples","summary":"  Measuring geometric similarity between high-dimensional network\nrepresentations is a topic of longstanding interest to neuroscience and deep\nlearning. Although many methods have been proposed, only a few works have\nrigorously analyzed their statistical efficiency or quantified estimator\nuncertainty in data-limited regimes. Here, we derive upper and lower bounds on\nthe worst-case convergence of standard estimators of shape\ndistance$\\unicode{x2014}$a measure of representational dissimilarity proposed\nby Williams et al. (2021). These bounds reveal the challenging nature of the\nproblem in high-dimensional feature spaces. To overcome these challenges, we\nintroduce a new method-of-moments estimator with a tunable bias-variance\ntradeoff. We show that this estimator achieves superior performance to standard\nestimators in simulation and on neural data, particularly in high-dimensional\nsettings. Thus, we lay the foundation for a rigorous statistical theory for\nhigh-dimensional shape analysis, and we contribute a new estimation method that\nis well-suited to practical scientific settings.\n","authors":["Dean A. Pospisil","Brett W. Larsen","Sarah E. Harvey","Alex H. Williams"],"pdf_url":"https://arxiv.org/pdf/2310.05742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05736v1","updated":"2023-10-09T14:10:21Z","published":"2023-10-09T14:10:21Z","title":"LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language Models","summary":"  Large language models (LLMs) have been applied in various applications due to\ntheir astonishing capabilities. With advancements in technologies such as\nchain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed\nto LLMs are becoming increasingly lengthy, even exceeding tens of thousands of\ntokens. To accelerate model inference and reduce cost, this paper presents\nLLMLingua, a coarse-to-fine prompt compression method that involves a budget\ncontroller to maintain semantic integrity under high compression ratios, a\ntoken-level iterative compression algorithm to better model the interdependence\nbetween compressed contents, and an instruction tuning based method for\ndistribution alignment between language models. We conduct experiments and\nanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that the proposed approach yields\nstate-of-the-art performance and allows for up to 20x compression with little\nperformance loss. Our code is available at https://aka.ms/LLMLingua.\n","authors":["Huiqiang Jiang","Qianhui Wu","Chin-Yew Lin","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.05736v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.00077v2","updated":"2023-10-09T14:10:12Z","published":"2023-09-29T18:33:10Z","title":"Optimizing with Low Budgets: a Comparison on the Black-box Optimization\n  Benchmarking Suite and OpenAI Gym","summary":"  The growing ubiquity of machine learning (ML) has led it to enter various\nareas of computer science, including black-box optimization (BBO). Recent\nresearch is particularly concerned with Bayesian optimization (BO). BO-based\nalgorithms are popular in the ML community, as they are used for hyperparameter\noptimization and more generally for algorithm configuration. However, their\nefficiency decreases as the dimensionality of the problem and the budget of\nevaluations increase. Meanwhile, derivative-free optimization methods have\nevolved independently in the optimization community. Therefore, we urge to\nunderstand whether cross-fertilization is possible between the two communities,\nML and BBO, i.e., whether algorithms that are heavily used in ML also work well\nin BBO and vice versa. Comparative experiments often involve rather small\nbenchmarks and show visible problems in the experimental setup, such as poor\ninitialization of baselines, overfitting due to problem-specific setting of\nhyperparameters, and low statistical significance.\n  With this paper, we update and extend a comparative study presented by Hutter\net al. in 2013. We compare BBO tools for ML with more classical heuristics,\nfirst on the well-known BBOB benchmark suite from the COCO environment and then\non Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our\nresults confirm that BO-based optimizers perform well on both benchmarks when\nbudgets are limited, albeit with a higher computational cost, while they are\noften outperformed by algorithms from other families when the evaluation budget\nbecomes larger. We also show that some algorithms from the BBO community\nperform surprisingly well on ML tasks.\n","authors":["Elena Raponi","Nathanael Rakotonirina Carraz","Jrmy Rapin","Carola Doerr","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2310.00077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00526v3","updated":"2023-10-09T14:05:16Z","published":"2023-10-01T00:12:31Z","title":"Are Graph Neural Networks Optimal Approximation Algorithms?","summary":"  In this work we design graph neural network architectures that can be used to\nobtain optimal approximation algorithms for a large class of combinatorial\noptimization problems using powerful algorithmic tools from semidefinite\nprogramming (SDP). Concretely, we prove that polynomial-sized message passing\nalgorithms can represent the most powerful polynomial time algorithms for Max\nConstraint Satisfaction Problems assuming the Unique Games Conjecture. We\nleverage this result to construct efficient graph neural network architectures,\nOptGNN, that obtain high-quality approximate solutions on landmark\ncombinatorial optimization problems such as Max Cut and maximum independent\nset. Our approach achieves strong empirical results across a wide range of\nreal-world and synthetic datasets against both neural baselines and classical\nalgorithms. Finally, we take advantage of OptGNN's ability to capture convex\nrelaxations to design an algorithm for producing dual certificates of\noptimality (bounds on the optimal solution) from the learned embeddings of\nOptGNN.\n","authors":["Morris Yau","Eric Lu","Nikolaos Karalias","Jessica Xu","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2310.00526v3.pdf","comment":"Updated figure 1"},{"id":"http://arxiv.org/abs/2307.04754v2","updated":"2023-10-09T14:01:42Z","published":"2023-07-07T09:23:14Z","title":"Action-State Dependent Dynamic Model Selection","summary":"  A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight.\n","authors":["Francesco Cordoni","Alessio Sancetta"],"pdf_url":"https://arxiv.org/pdf/2307.04754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13095v2","updated":"2023-10-09T13:57:56Z","published":"2023-09-22T13:15:02Z","title":"Multiple Independent DE Optimizations to Tackle Uncertainty and\n  Variability in Demand in Inventory Management","summary":"  To determine the effectiveness of metaheuristic Differential Evolution\noptimization strategy for inventory management (IM) in the context of\nstochastic demand, this empirical study undertakes a thorough investigation.\nThe primary objective is to discern the most effective strategy for minimizing\ninventory costs within the context of uncertain demand patterns. Inventory\ncosts refer to the expenses associated with holding and managing inventory\nwithin a business. The approach combines a continuous review of IM policies\nwith a Monte Carlo Simulation (MCS). To find the optimal solution, the study\nfocuses on meta-heuristic approaches and compares multiple algorithms. The\noutcomes reveal that the Differential Evolution (DE) algorithm outperforms its\ncounterparts in optimizing IM. To fine-tune the parameters, the study employs\nthe Latin Hypercube Sampling (LHS) statistical method. To determine the final\nsolution, a method is employed in this study which combines the outcomes of\nmultiple independent DE optimizations, each initiated with different random\ninitial conditions. This approach introduces a novel and promising dimension to\nthe field of inventory management, offering potential enhancements in\nperformance and cost efficiency, especially in the presence of stochastic\ndemand patterns.\n","authors":["Sarit Maitra","Sukanya Kundu","Vivek Mishra"],"pdf_url":"https://arxiv.org/pdf/2309.13095v2.pdf","comment":"6 pages, 2 figures, 6 tables, IEEE (ICITEE 2023)"},{"id":"http://arxiv.org/abs/2303.08010v3","updated":"2023-10-09T13:56:25Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v3.pdf","comment":"Accepted to ICCV 2023 (camera-ready version, 9 pages)"},{"id":"http://arxiv.org/abs/2310.05727v1","updated":"2023-10-09T13:55:45Z","published":"2023-10-09T13:55:45Z","title":"The Program Testing Ability of Large Language Models for Code","summary":"  Recent development of large language models (LLMs) for code like CodeX and\nCodeT5+ demonstrates tremendous promise in achieving code intelligence. Their\nability of synthesizing code that completes a program for performing a\npre-defined task has been intensively tested and verified on benchmark datasets\nincluding HumanEval and MBPP. Yet, evaluation of these LLMs from more\nperspectives (than just program synthesis) is also anticipated, considering\ntheir broad scope of applications in software engineering. In this paper, we\nexplore the ability of LLMs for testing programs/code. By performing thorough\nanalyses of recent LLMs for code in program testing, we show a series of\nintriguing properties of these models and demonstrate how program testing\nability of LLMs can be improved. Following recent work which utilizes generated\ntest cases to enhance program synthesis, we further leverage our findings in\nimproving the quality of the synthesized programs and show +11.77% and +4.22%\nhigher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline\nand the recent state-of-the-art, respectively.\n","authors":["Weimin Xiong","Yiwen Guo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05725v1","updated":"2023-10-09T13:54:08Z","published":"2023-10-09T13:54:08Z","title":"Post-hoc Bias Scoring Is Optimal For Fair Classification","summary":"  We consider a binary classification problem under group fairness constraints,\nwhich can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or\nEqualized Odds (EO). We propose an explicit characterization of Bayes optimal\nclassifier under the fairness constraints, which turns out to be a simple\nmodification rule of the unconstrained classifier. Namely, we introduce a novel\ninstance-level measure of bias, which we call bias score, and the modification\nrule is a simple linear rule on top of the finite amount of bias scores. Based\non this characterization, we develop a post-hoc approach that allows us to\nadapt to fairness constraints while maintaining high accuracy. In the case of\nDP and EOp constraints, the modification rule is thresholding a single bias\nscore, while in the case of EO constraints we are required to fit a linear\nmodification rule with 2 parameters. The method can also be applied for\ncomposite group-fairness criteria, such as ones involving several sensitive\nattributes. We achieve competitive or better performance compared to both\nin-processing and post-processing methods across three datasets: Adult, COMPAS,\nand CelebA. Unlike most post-processing methods, we do not require access to\nsensitive attributes during the inference time.\n","authors":["Wenlong Chen","Yegor Klochkov","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05725v1.pdf","comment":"26 pages, 5 figures, 8 tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2310.05723v1","updated":"2023-10-09T13:47:05Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm that is well matched to a real-world\nRL deployment process: in few real settings would one deploy an offline policy\nwith no test runs and tuning. In this scenario, we aim to find the\nbest-performing policy within a limited budget of online interactions. Previous\nwork in the OtO setting has focused on correcting for bias introduced by the\npolicy-constraint mechanisms of offline RL algorithms. Such constraints keep\nthe learned policy close to the behavior policy that collected the dataset, but\nthis unnecessarily limits policy performance if the behavior policy is far from\noptimal. Instead, we forgo policy constraints and frame OtO RL as an\nexploration problem: we must maximize the benefit of the online\ndata-collection. We study major online RL exploration paradigms, adapting them\nto work well with the OtO setting. These adapted methods contribute several\nstrong baselines. Also, we introduce an algorithm for planning to go out of\ndistribution (PTGOOD), which targets online exploration in relatively\nhigh-reward regions of the state-action space unlikely to be visited by the\nbehavior policy. By leveraging concepts from the Conditional Entropy\nBottleneck, PTGOOD encourages data collected online to provide new information\nrelevant to improving the final deployment policy. In that way the limited\ninteraction budget is used effectively. We show that PTGOOD significantly\nimproves agent returns during online fine-tuning and finds the optimal policy\nin as few as 10k online steps in Walker and in as few as 50k in complex control\ntasks like Humanoid. Also, we find that PTGOOD avoids the suboptimal policy\nconvergence that many of our baselines exhibit in several environments.\n","authors":["Trevor McInroe","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v1.pdf","comment":"9 pages, 12 figures, preprint"},{"id":"http://arxiv.org/abs/2310.05719v1","updated":"2023-10-09T13:40:31Z","published":"2023-10-09T13:40:31Z","title":"Transformer Fusion with Optimal Transport","summary":"  Fusion is a technique for merging multiple independently-trained neural\nnetworks in order to combine their capabilities. Past attempts have been\nrestricted to the case of fully-connected, convolutional, and residual\nnetworks. In this paper, we present a systematic approach for fusing two or\nmore transformer-based networks exploiting Optimal Transport to (soft-)align\nthe various architectural components. We flesh out an abstraction for layer\nalignment, that can generalize to arbitrary architectures -- in principle --\nand we apply this to the key ingredients of Transformers such as multi-head\nself-attention, layer-normalization, and residual connections, and we discuss\nhow to handle them via various ablation studies. Furthermore, our method allows\nthe fusion of models of different sizes (heterogeneous fusion), providing a new\nand efficient way for compression of Transformers. The proposed approach is\nevaluated on both image classification tasks via Vision Transformer and natural\nlanguage modeling tasks using BERT. Our approach consistently outperforms\nvanilla fusion, and, after a surprisingly short finetuning, also outperforms\nthe individual converged parent models. In our analysis, we uncover intriguing\ninsights about the significant role of soft alignment in the case of\nTransformers. Our results showcase the potential of fusing multiple\nTransformers, thus compounding their expertise, in the budding paradigm of\nmodel fusion and recombination.\n","authors":["Moritz Imfeld","Jacopo Graldi","Marco Giordano","Thomas Hofmann","Sotiris Anagnostidis","Sidak Pal Singh"],"pdf_url":"https://arxiv.org/pdf/2310.05719v1.pdf","comment":"M. Imfeld, J. Graldi, and M. Giordano are the first authors and\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2302.13417v4","updated":"2023-10-09T13:39:32Z","published":"2023-02-26T22:10:23Z","title":"Training neural networks with structured noise improves classification\n  and generalization","summary":"  The beneficial role of noise in learning is nowadays a consolidated concept\nin the field of artificial neural networks, suggesting that even biological\nsystems might take advantage of similar mechanisms to maximize their\nperformance. The training-with-noise algorithm proposed by Gardner and\ncollaborators is an emblematic example of a noise injection procedure in\nrecurrent networks, which are usually employed to model real neural systems. We\nshow how adding structure into noisy training data can substantially improve\nthe algorithm performance, allowing to approach perfect classification and\nmaximal basins of attraction. We also prove that the so-called Hebbian\nunlearning rule coincides with the training-with-noise algorithm when noise is\nmaximal and data are fixed points of the network dynamics. A sampling scheme\nfor optimal noisy data is eventually proposed and implemented to outperform\nboth the training-with-noise and the Hebbian unlearning procedures.\n","authors":["Marco Benedetti","Enrico Ventura"],"pdf_url":"https://arxiv.org/pdf/2302.13417v4.pdf","comment":"21 pages, 17 figures, main text and appendices"},{"id":"http://arxiv.org/abs/2310.05718v1","updated":"2023-10-09T13:39:26Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models.\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05712v1","updated":"2023-10-09T13:35:28Z","published":"2023-10-09T13:35:28Z","title":"Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable\n  Environments","summary":"  Imitation learning (IL) enables agents to mimic expert behaviors. Most\nprevious IL techniques focus on precisely imitating one policy through mass\ndemonstrations. However, in many applications, what humans require is the\nability to perform various tasks directly through a few demonstrations of\ncorresponding tasks, where the agent would meet many unexpected changes when\ndeployed. In this scenario, the agent is expected to not only imitate the\ndemonstration but also adapt to unforeseen environmental changes.\n  This motivates us to propose a new topic called imitator learning (ItorL),\nwhich aims to derive an imitator module that can on-the-fly reconstruct the\nimitation policies based on very limited expert demonstrations for different\nunseen tasks, without any extra adjustment. In this work, we focus on imitator\nlearning based on only one expert demonstration. To solve ItorL, we propose\nDemo-Attention Actor-Critic (DAAC), which integrates IL into a\nreinforcement-learning paradigm that can regularize policies' behaviors in\nunexpected situations. Besides, for autonomous imitation policy building, we\ndesign a demonstration-based attention architecture for imitator policy that\ncan effectively output imitated actions by adaptively tracing the suitable\nstates in demonstrations. We develop a new navigation benchmark and a robot\nenvironment for \\topic~and show that DAAC~outperforms previous imitation\nmethods \\textit{with large margins} both on seen and unseen tasks.\n","authors":["Xiong-Hui Chen","Junyin Ye","Hang Zhao","Yi-Chen Li","Haoran Shi","Yu-Yan Xu","Zhihao Ye","Si-Hang Yang","Anqi Huang","Kai Xu","Zongzhang Zhang","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05707v1","updated":"2023-10-09T13:29:37Z","published":"2023-10-09T13:29:37Z","title":"Guiding Language Model Reasoning with Planning Tokens","summary":"  Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\nreasoning. However, most of the existing approaches to enhance this ability\nrely heavily on data-driven methods, while neglecting the structural aspects of\nthe model's reasoning capacity. We find that while LLMs can manage individual\nreasoning steps well, they struggle with maintaining consistency across an\nentire reasoning chain. To solve this, we introduce 'planning tokens' at the\nstart of each reasoning step, serving as a guide for the model. These token\nembeddings are then fine-tuned along with the rest of the model parameters. Our\napproach requires a negligible increase in trainable parameters (just 0.001%)\nand can be applied through either full fine-tuning or a more\nparameter-efficient scheme. We demonstrate our method's effectiveness by\napplying it to three different LLMs, showing notable accuracy improvements\nacross three math word problem datasets w.r.t. plain chain-of-thought\nfine-tuning baselines.\n","authors":["Xinyi Wang","Lucas Caccia","Oleksiy Ostapenko","Xingdi Yuan","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2310.05707v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05703v1","updated":"2023-10-09T13:24:44Z","published":"2023-10-09T13:24:44Z","title":"An Attribution Method for Siamese Encoders","summary":"  Despite the success of Siamese encoder models such as sentence transformers\n(ST), little is known about the aspects of inputs they pay attention to. A\nbarrier is that their predictions cannot be attributed to individual features,\nas they compare two inputs rather than processing a single one. This paper\nderives a local attribution method for Siamese encoders by generalizing the\nprinciple of integrated gradients to models with multiple inputs. The solution\ntakes the form of feature-pair attributions, and can be reduced to a\ntoken-token matrix for STs. Our method involves the introduction of integrated\nJacobians and inherits the advantageous formal properties of integrated\ngradients: it accounts for the model's full computation graph and is guaranteed\nto converge to the actual prediction. A pilot study shows that in an ST few\ntoken-pairs can often explain large fractions of predictions, and it focuses on\nnouns and verbs. For accurate predictions, it however needs to attend to the\nmajority of tokens and parts of speech.\n","authors":["Lucas Mller","Dmitry Nikolaev","Sebastian Pad"],"pdf_url":"https://arxiv.org/pdf/2310.05703v1.pdf","comment":"Accepted to EMNLP'23"},{"id":"http://arxiv.org/abs/2310.05697v1","updated":"2023-10-09T13:16:20Z","published":"2023-10-09T13:16:20Z","title":"Combining recurrent and residual learning for deforestation monitoring\n  using multitemporal SAR images","summary":"  With its vast expanse, exceeding that of Western Europe by twice, the Amazon\nrainforest stands as the largest forest of the Earth, holding immense\nimportance in global climate regulation. Yet, deforestation detection from\nremote sensing data in this region poses a critical challenge, often hindered\nby the persistent cloud cover that obscures optical satellite data for much of\nthe year. Addressing this need, this paper proposes three deep-learning models\ntailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar)\nmultitemporal data moved by its independence on atmospheric conditions.\nSpecifically, the study proposes three novel recurrent fully convolutional\nnetwork architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance\nthe accuracy of deforestation detection. Additionally, this research explores\nreplacing a bitemporal with multitemporal SAR sequences, motivated by the\nhypothesis that deforestation signs quickly fade in SAR images over time. A\ncomprehensive assessment of the proposed approaches was conducted using a\nSentinel-1 multitemporal sequence from a sample site in the Brazilian\nrainforest. The experimental analysis confirmed that analyzing a sequence of\nSAR images over an observation period can reveal deforestation spots\nundetectable in a pair of images. Notably, experimental results underscored the\nsuperiority of the multitemporal approach, yielding approximately a five\npercent enhancement in F1-Score across all tested network architectures.\nParticularly the RRCNN-1 achieved the highest accuracy and also boasted half\nthe processing time of its closest counterpart.\n","authors":["Carla Nascimento Neves","Raul Queiroz Feitosa","Mabel X. Ortega Adarme","Gilson Antonio Giraldi"],"pdf_url":"https://arxiv.org/pdf/2310.05697v1.pdf","comment":"21 pages, 19 Figures"},{"id":"http://arxiv.org/abs/2310.05696v1","updated":"2023-10-09T13:16:10Z","published":"2023-10-09T13:16:10Z","title":"Protecting Sensitive Data through Federated Co-Training","summary":"  In many critical applications, sensitive data is inherently distributed.\nFederated learning trains a model collaboratively by aggregating the parameters\nof locally trained models. This avoids exposing sensitive local data. It is\npossible, though, to infer upon the sensitive data from the shared model\nparameters. At the same time, many types of machine learning models do not lend\nthemselves to parameter aggregation, such as decision trees, or rule ensembles.\nIt has been observed that in many applications, in particular healthcare, large\nunlabeled datasets are publicly available. They can be used to exchange\ninformation between clients by distributed distillation, i.e., co-regularizing\nlocal training via the discrepancy between the soft predictions of each local\nclient on the unlabeled dataset. This, however, still discloses private\ninformation and restricts the types of models to those trainable via\ngradient-based methods. We propose to go one step further and use a form of\nfederated co-training, where local hard labels on the public unlabeled datasets\nare shared and aggregated into a consensus label. This consensus label can be\nused for local training by any supervised machine learning model. We show that\nthis federated co-training approach achieves a model quality comparable to both\nfederated learning and distributed distillation on a set of benchmark datasets\nand real-world medical datasets. It improves privacy over both approaches,\nprotecting against common membership inference attacks to the highest degree.\nFurthermore, we show that federated co-training can collaboratively train\ninterpretable models, such as decision trees and rule ensembles, achieving a\nmodel quality comparable to centralized training.\n","authors":["Amr Abourayya","Jens Kleesiek","Kanishka Rao","Erman Ayday","Bharat Rao","Geoff Webb","Michael Kamp"],"pdf_url":"https://arxiv.org/pdf/2310.05696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05695v1","updated":"2023-10-09T13:15:57Z","published":"2023-10-09T13:15:57Z","title":"Hierarchical Reinforcement Learning for Temporal Pattern Prediction","summary":"  In this work, we explore the use of hierarchical reinforcement learning (HRL)\nfor the task of temporal sequence prediction. Using a combination of deep\nlearning and HRL, we develop a stock agent to predict temporal price sequences\nfrom historical stock price data and a vehicle agent to predict steering angles\nfrom first person, dash cam images. Our results in both domains indicate that a\ntype of HRL, called feudal reinforcement learning, provides significant\nimprovements to training speed and stability and prediction accuracy over\nstandard RL. A key component to this success is the multi-resolution structure\nthat introduces both temporal and spatial abstraction into the network\nhierarchy.\n","authors":["Faith Johnson","Kristin Dana"],"pdf_url":"https://arxiv.org/pdf/2310.05695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04241v2","updated":"2023-10-09T13:02:07Z","published":"2023-10-06T13:22:26Z","title":"Improving Reinforcement Learning Efficiency with Auxiliary Tasks in\n  Non-Visual Environments: A Comparison","summary":"  Real-world reinforcement learning (RL) environments, whether in robotics or\nindustrial settings, often involve non-visual observations and require not only\nefficient but also reliable and thus interpretable and flexible RL approaches.\nTo improve efficiency, agents that perform state representation learning with\nauxiliary tasks have been widely studied in visual observation contexts.\nHowever, for real-world problems, dedicated representation learning modules\nthat are decoupled from RL agents are more suited to meet requirements. This\nstudy compares common auxiliary tasks based on, to the best of our knowledge,\nthe only decoupled representation learning method for low-dimensional\nnon-visual observations. We evaluate potential improvements in sample\nefficiency and returns for environments ranging from a simple pendulum to a\ncomplex simulated robotics task. Our findings show that representation learning\nwith auxiliary tasks only provides performance gains in sufficiently complex\nenvironments and that learning environment dynamics is preferable to predicting\nrewards. These insights can inform future development of interpretable\nrepresentation learning approaches for non-visual observations and advance the\nuse of RL solutions in real-world scenarios.\n","authors":["Moritz Lange","Noah Krystiniak","Raphael C. Engelhardt","Wolfgang Konen","Laurenz Wiskott"],"pdf_url":"https://arxiv.org/pdf/2310.04241v2.pdf","comment":"Accepted at LOD 2023"},{"id":"http://arxiv.org/abs/2208.09287v5","updated":"2023-10-09T12:57:58Z","published":"2022-08-17T20:01:05Z","title":"Detect to Learn: Structure Learning with Attention and Decision Feedback\n  for MIMO-OFDM Receive Processing","summary":"  The limited over-the-air (OTA) pilot symbols in\nmultiple-input-multiple-output orthogonal-frequency-division-multiplexing\n(MIMO-OFDM) systems presents a major challenge for detecting transmitted data\nsymbols at the receiver, especially for machine learning-based approaches.\nWhile it is crucial to explore effective ways to exploit pilots, one can also\ntake advantage of the data symbols to improve detection performance. Thus, this\npaper introduces an online attention-based approach, namely RC-AttStructNet-DF,\nthat can efficiently utilize pilot symbols and be dynamically updated with the\ndetected payload data using the decision feedback (DF) mechanism. Reservoir\ncomputing (RC) is employed in the time domain network to facilitate efficient\nonline training. The frequency domain network adopts the novel 2D multi-head\nattention (MHA) module to capture the time and frequency correlations, and the\nstructural-based StructNet to facilitate the DF mechanism. The attention loss\nis designed to learn the frequency domain network. The DF mechanism further\nenhances detection performance by dynamically tracking the channel changes\nthrough detected data symbols. The effectiveness of the RC-AttStructNet-DF\napproach is demonstrated through extensive experiments in MIMO-OFDM and massive\nMIMO-OFDM systems with different modulation orders and under various scenarios.\n","authors":["Jiarui Xu","Lianjun Li","Lizhong Zheng","Lingjia Liu"],"pdf_url":"https://arxiv.org/pdf/2208.09287v5.pdf","comment":"Accepted to IEEE Transactions on Communications"},{"id":"http://arxiv.org/abs/2310.05682v1","updated":"2023-10-09T12:51:46Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08516v2","updated":"2023-10-09T12:46:17Z","published":"2023-03-15T10:47:48Z","title":"Fair Off-Policy Learning from Observational Data","summary":"  Algorithmic decision-making in practice must be fair for legal, ethical, and\nsocietal reasons. To achieve this, prior research has contributed various\napproaches that ensure fairness in machine learning predictions, while\ncomparatively little effort has focused on fairness in decision-making,\nspecifically off-policy learning. In this paper, we propose a novel framework\nfor fair off-policy learning: we learn decision rules from observational data\nunder different notions of fairness, where we explicitly assume that\nobservational data were collected under a different potentially discriminatory\nbehavioral policy. For this, we first formalize different fairness notions for\noff-policy learning. We then propose a neural network-based framework to learn\noptimal policies under different fairness notions. We further provide\ntheoretical guarantees in the form of generalization bounds for the\nfinite-sample version of our framework. We demonstrate the effectiveness of our\nframework through extensive numerical experiments using both simulated and\nreal-world data. Altogether, our work enables algorithmic decision-making in a\nwide array of practical applications where fairness must be ensured.\n","authors":["Dennis Frauen","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2303.08516v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2310.02724v2","updated":"2023-10-09T12:46:00Z","published":"2023-10-04T10:56:00Z","title":"End-to-End Training of a Neural HMM with Label and Transition\n  Probabilities","summary":"  We investigate a novel modeling approach for end-to-end neural network\ntraining using hidden Markov models (HMM) where the transition probabilities\nbetween hidden states are modeled and learned explicitly. Most contemporary\nsequence-to-sequence models allow for from-scratch training by summing over all\npossible label segmentations in a given topology. In our approach there are\nexplicit, learnable probabilities for transitions between segments as opposed\nto a blank label that implicitly encodes duration statistics. We implement a\nGPU-based forward-backward algorithm that enables the simultaneous training of\nlabel and transition probabilities. We investigate recognition results and\nadditionally Viterbi alignments of our models. We find that while the\ntransition model training does not improve recognition performance, it has a\npositive impact on the alignment quality. The generated alignments are shown to\nbe viable targets in state-of-the-art Viterbi trainings.\n","authors":["Daniel Mann","Tina Raissi","Wilfried Michel","Ralf Schlter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2310.02724v2.pdf","comment":"Accepted for Presentation at ASRU2023"},{"id":"http://arxiv.org/abs/2310.05674v1","updated":"2023-10-09T12:45:13Z","published":"2023-10-09T12:45:13Z","title":"Making Scalable Meta Learning Practical","summary":"  Despite its flexibility to learn diverse inductive biases in machine learning\nprograms, meta learning (i.e., learning to learn) has long been recognized to\nsuffer from poor scalability due to its tremendous compute/memory costs,\ntraining instability, and a lack of efficient distributed training support. In\nthis work, we focus on making scalable meta learning practical by introducing\nSAMA, which combines advances in both implicit differentiation algorithms and\nsystems. Specifically, SAMA is designed to flexibly support a broad range of\nadaptive optimizers in the base level of meta learning programs, while reducing\ncomputational burden by avoiding explicit computation of second-order gradient\ninformation, and exploiting efficient distributed training techniques\nimplemented for first-order gradients. Evaluated on multiple large-scale meta\nlearning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and\n2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU\nsetups compared to other baseline meta learning algorithms. Furthermore, we\nshow that SAMA-based data optimization leads to consistent improvements in text\nclassification accuracy with BERT and RoBERTa large language models, and\nachieves state-of-the-art results in both small- and large-scale data pruning\non image classification tasks, demonstrating the practical applicability of\nscalable meta learning across language and vision domains.\n","authors":["Sang Keun Choe","Sanket Vaibhav Mehta","Hwijeen Ahn","Willie Neiswanger","Pengtao Xie","Emma Strubell","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2310.05674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05672v1","updated":"2023-10-09T12:42:39Z","published":"2023-10-09T12:42:39Z","title":"Multi-timestep models for Model-based Reinforcement Learning","summary":"  In model-based reinforcement learning (MBRL), most algorithms rely on\nsimulating trajectories from one-step dynamics models learned on data. A\ncritical challenge of this approach is the compounding of one-step prediction\nerrors as length of the trajectory grows. In this paper we tackle this issue by\nusing a multi-timestep objective to train one-step models. Our objective is a\nweighted sum of a loss function (e.g., negative log-likelihood) at various\nfuture horizons. We explore and test a range of weights profiles. We find that\nexponentially decaying weights lead to models that significantly improve the\nlong-horizon R2 score. This improvement is particularly noticeable when the\nmodels were evaluated on noisy data. Finally, using a soft actor-critic (SAC)\nagent in pure batch reinforcement learning (RL) and iterated batch RL\nscenarios, we found that our multi-timestep models outperform or match standard\none-step models. This was especially evident in a noisy variant of the\nconsidered environment, highlighting the potential of our approach in\nreal-world applications.\n","authors":["Abdelhakim Benechehab","Giuseppe Paolo","Albert Thomas","Maurizio Filippone","Balzs Kgl"],"pdf_url":"https://arxiv.org/pdf/2310.05672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02911v2","updated":"2023-10-09T12:40:29Z","published":"2023-09-06T11:13:34Z","title":"A Multimodal Learning Framework for Comprehensive 3D Mineral\n  Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships","summary":"  This study presents a novel multimodal fusion model for three-dimensional\nmineral prospectivity mapping (3D MPM), effectively integrating structural and\nfluid information through a deep network architecture. Leveraging Convolutional\nNeural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs\ncanonical correlation analysis (CCA) to align and fuse multimodal features.\nRigorous evaluation on the Jiaojia gold deposit dataset demonstrates the\nmodel's superior performance in distinguishing ore-bearing instances and\npredicting mineral prospectivity, outperforming other models in result\nanalyses. Ablation studies further reveal the benefits of joint feature\nutilization and CCA incorporation. This research not only advances mineral\nprospectivity modeling but also highlights the pivotal role of data integration\nand feature alignment for enhanced exploration decision-making.\n","authors":["Yang Zheng","Hao Deng","Ruisheng Wang","Jingjie Wu"],"pdf_url":"https://arxiv.org/pdf/2309.02911v2.pdf","comment":"Upon careful review, it has come to our attention that inaccuracies\n  exist in the formulation of the structure-fluid relationships, impacting the\n  validity of the presented results"},{"id":"http://arxiv.org/abs/2310.05668v1","updated":"2023-10-09T12:36:16Z","published":"2023-10-09T12:36:16Z","title":"LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised\n  Anomaly Detection","summary":"  Most of current anomaly detection models assume that the normal pattern\nremains same all the time. However, the normal patterns of Web services change\ndramatically and frequently. The model trained on old-distribution data is\noutdated after such changes. Retraining the whole model every time is\nexpensive. Besides, at the beginning of normal pattern changes, there is not\nenough observation data from the new distribution. Retraining a large neural\nnetwork model with limited data is vulnerable to overfitting. Thus, we propose\na Light and Anti-overfitting Retraining Approach (LARA) for deep variational\nauto-encoder based time series anomaly detection methods (VAEs). This work aims\nto make three novel contributions: 1) the retraining process is formulated as a\nconvex problem and can converge at a fast rate as well as prevent overfitting;\n2) designing a ruminate block, which leverages the historical data without the\nneed to store them; 3) mathematically proving that when fine-tuning the latent\nvector and reconstructed data, the linear formations can achieve the least\nadjusting errors between the ground truths and the fine-tuned ones.\n  Moreover, we have performed many experiments to verify that retraining LARA\nwith even 43 time slots of data from new distribution can result in its\ncompetitive F1 Score in comparison with the state-of-the-art anomaly detection\nmodels trained with sufficient data. Besides, we verify its light overhead.\n","authors":["Feiyi Chen","Zhen Qing","Yingying Zhang","Shuiguang Deng","Yi Xiao","Guansong Pang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2310.05668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15187v2","updated":"2023-10-09T12:32:17Z","published":"2023-05-24T14:27:00Z","title":"Using Models Based on Cognitive Theory to Predict Human Behavior in\n  Traffic: A Case Study","summary":"  The development of automated vehicles has the potential to revolutionize\ntransportation, but they are currently unable to ensure a safe and\ntime-efficient driving style. Reliable models predicting human behavior are\nessential for overcoming this issue. While data-driven models are commonly used\nto this end, they can be vulnerable in safety-critical edge cases. This has led\nto an interest in models incorporating cognitive theory, but as such models are\ncommonly developed for explanatory purposes, this approach's effectiveness in\nbehavior prediction has remained largely untested so far. In this article, we\ninvestigate the usefulness of the \\emph{Commotions} model -- a novel\ncognitively plausible model incorporating the latest theories of human\nperception, decision-making, and motor control -- for predicting human behavior\nin gap acceptance scenarios, which entail many important traffic interactions\nsuch as lane changes and intersections. We show that this model can compete\nwith or even outperform well-established data-driven prediction models across\nseveral naturalistic datasets. These results demonstrate the promise of\nincorporating cognitive theory in behavior prediction models for automated\nvehicles.\n","authors":["Julian F. Schumann","Aravinda Ramakrishnan Srinivasan","Jens Kober","Gustav Markkula","Arkady Zgonnikov"],"pdf_url":"https://arxiv.org/pdf/2305.15187v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.05655v1","updated":"2023-10-09T12:10:51Z","published":"2023-10-09T12:10:51Z","title":"Causal structure learning with momentum: Sampling distributions over\n  Markov Equivalence Classes of DAGs","summary":"  In the context of inferring a Bayesian network structure (directed acyclic\ngraph, DAG for short), we devise a non-reversible continuous time Markov chain,\nthe \"Causal Zig-Zag sampler\", that targets a probability distribution over\nclasses of observationally equivalent (Markov equivalent) DAGs. The classes are\nrepresented as completed partially directed acyclic graphs (CPDAGs). The\nnon-reversible Markov chain relies on the operators used in Chickering's Greedy\nEquivalence Search (GES) and is endowed with a momentum variable, which\nimproves mixing significantly as we show empirically. The possible target\ndistributions include posterior distributions based on a prior over DAGs and a\nMarkov equivalent likelihood. We offer an efficient implementation wherein we\ndevelop new algorithms for listing, counting, uniformly sampling, and applying\npossible moves of the GES operators, all of which significantly improve upon\nthe state-of-the-art.\n","authors":["Moritz Schauer","Marcel Wienbst"],"pdf_url":"https://arxiv.org/pdf/2310.05655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11975v2","updated":"2023-10-09T12:05:22Z","published":"2023-01-27T20:22:18Z","title":"Byte Pair Encoding for Symbolic Music","summary":"  When used with deep learning, the symbolic music modality is often coupled\nwith language model architectures. To do so, the music needs to be tokenized,\ni.e. converted into a sequence of discrete tokens. This can be achieved by\ndifferent approaches, as music can be composed of simultaneous tracks, of\nsimultaneous notes with several attributes. Until now, the proposed\ntokenizations rely on small vocabularies of tokens describing the note\nattributes and time events, resulting in fairly long token sequences, and a\nsub-optimal use of the embedding space of language models. Recent research has\nput efforts on reducing the overall sequence length by merging embeddings or\ncombining tokens. In this paper, we show that Byte Pair Encoding, a compression\ntechnique widely used for natural language, significantly decreases the\nsequence length while increasing the vocabulary size. By doing so, we leverage\nthe embedding capabilities of such models with more expressive tokens,\nresulting in both better results and faster inference in generation and\nclassification tasks. The source code is shared on Github, along with a\ncompanion website. Finally, BPE is directly implemented in MidiTok, allowing\nthe reader to easily benefit from this method.\n","authors":["Nathan Fradet","Nicolas Gutowski","Fabien Chhel","Jean-Pierre Briot"],"pdf_url":"https://arxiv.org/pdf/2301.11975v2.pdf","comment":"EMNLP 2023, source code: https://github.com/Natooz/BPE-Symbolic-Music"},{"id":"http://arxiv.org/abs/2310.05651v1","updated":"2023-10-09T12:04:50Z","published":"2023-10-09T12:04:50Z","title":"FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID\n  Detection at Scale In Fantasy Sports","summary":"  Dream11 takes pride in being a unique platform that enables over 190 million\nfantasy sports users to demonstrate their skills and connect deeper with their\nfavorite sports. While managing such a scale, one issue we are faced with is\nduplicate/multiple account creation in the system. This is done by some users\nwith the intent of abusing the platform, typically for bonus offers. The\nchallenge is to detect these multiple accounts before it is too late. We\npropose a graph-based solution to solve this problem in which we first predict\nedges/associations between users. Using the edge information we highlight\nclusters of colluding multiple accounts. In this paper, we talk about our\ndistributed ML system which is deployed to serve and support the inferences\nfrom our detection models. The challenge is to do this in real-time in order to\ntake corrective actions. A core part of this setup also involves\nhuman-in-the-loop components for validation, feedback, and ground-truth\nlabeling.\n","authors":["Akriti Upreti","Kartavya Kothari","Utkarsh Thukral","Vishal Verma"],"pdf_url":"https://arxiv.org/pdf/2310.05651v1.pdf","comment":"7 pages, 7 figures, accepted in AIML Systems 2023"},{"id":"http://arxiv.org/abs/2305.05640v3","updated":"2023-10-09T11:58:55Z","published":"2023-05-09T17:39:45Z","title":"Representation Learning for Person or Entity-centric Knowledge Graphs:\n  An Application in Healthcare","summary":"  Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.\n","authors":["Christos Theodoropoulos","Natasha Mulligan","Thaddeus Stappenbeck","Joao Bettencourt-Silva"],"pdf_url":"https://arxiv.org/pdf/2305.05640v3.pdf","comment":"Accepted into the Twelfth International Conference on Knowledge\n  Capture (K-CAP 2023)"},{"id":"http://arxiv.org/abs/2310.05644v1","updated":"2023-10-09T11:57:46Z","published":"2023-10-09T11:57:46Z","title":"Diagnosing Catastrophe: Large parts of accuracy loss in continual\n  learning can be accounted for by readout misalignment","summary":"  Unlike primates, training artificial neural networks on changing data\ndistributions leads to a rapid decrease in performance on old tasks. This\nphenomenon is commonly referred to as catastrophic forgetting. In this paper,\nwe investigate the representational changes that underlie this performance\ndecrease and identify three distinct processes that together account for the\nphenomenon. The largest component is a misalignment between hidden\nrepresentations and readout layers. Misalignment occurs due to learning on\nadditional tasks and causes internal representations to shift. Representational\ngeometry is partially conserved under this misalignment and only a small part\nof the information is irrecoverably lost. All types of representational changes\nscale with the dimensionality of hidden representations. These insights have\nimplications for deep learning applications that need to be continuously\nupdated, but may also aid aligning ANN models to the rather robust biological\nvision.\n","authors":["Daniel Anthes","Sushrut Thorat","Peter Knig","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2310.05644v1.pdf","comment":"3 pages, 1 figure; published at the 2023 Conference on Cognitive\n  Computational Neuroscience"},{"id":"http://arxiv.org/abs/2310.05632v1","updated":"2023-10-09T11:44:50Z","published":"2023-10-09T11:44:50Z","title":"Binary Classification with Confidence Difference","summary":"  Recently, learning with soft labels has been shown to achieve better\nperformance than learning with hard labels in terms of model generalization,\ncalibration, and robustness. However, collecting pointwise labeling confidence\nfor all training examples can be challenging and time-consuming in real-world\nscenarios. This paper delves into a novel weakly supervised binary\nclassification problem called confidence-difference (ConfDiff) classification.\nInstead of pointwise labeling confidence, we are given only unlabeled data\npairs with confidence difference that specifies the difference in the\nprobabilities of being positive. We propose a risk-consistent approach to\ntackle this problem and show that the estimation error bound achieves the\noptimal convergence rate. We also introduce a risk correction approach to\nmitigate overfitting problems, whose consistency and convergence rate are also\nproven. Extensive experiments on benchmark data sets and a real-world\nrecommender system data set validate the effectiveness of our proposed\napproaches in exploiting the supervision information of the confidence\ndifference.\n","authors":["Wei Wang","Lei Feng","Yuchen Jiang","Gang Niu","Min-Ling Zhang","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2310.05632v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05627v1","updated":"2023-10-09T11:34:18Z","published":"2023-10-09T11:34:18Z","title":"Integrating Stock Features and Global Information via Large Language\n  Models for Enhanced Stock Return Prediction","summary":"  The remarkable achievements and rapid advancements of Large Language Models\n(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in\nquantitative investment. Traders can effectively leverage these LLMs to analyze\nfinancial news and predict stock returns accurately. However, integrating LLMs\ninto existing quantitative models presents two primary challenges: the\ninsufficient utilization of semantic information embedded within LLMs and the\ndifficulties in aligning the latent information within LLMs with pre-existing\nquantitative stock features. We propose a novel framework consisting of two\ncomponents to surmount these challenges. The first component, the Local-Global\n(LG) model, introduces three distinct strategies for modeling global\ninformation. These approaches are grounded respectively on stock features, the\ncapabilities of LLMs, and a hybrid method combining the two paradigms. The\nsecond component, Self-Correlated Reinforcement Learning (SCRL), focuses on\naligning the embeddings of financial news generated by LLMs with stock features\nwithin the same semantic space. By implementing our framework, we have\ndemonstrated superior performance in Rank Information Coefficient and returns,\nparticularly compared to models relying only on stock features in the China\nA-share market.\n","authors":["Yujie Ding","Shuai Jia","Tianyi Ma","Bingcheng Mao","Xiuze Zhou","Liuliu Li","Dongming Han"],"pdf_url":"https://arxiv.org/pdf/2310.05627v1.pdf","comment":"8 pages, International Joint Conferences on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2310.04171v2","updated":"2023-10-09T11:30:59Z","published":"2023-10-06T11:41:38Z","title":"Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection","summary":"  Fraud detection aims to discover fraudsters deceiving other users by, for\nexample, leaving fake reviews or making abnormal transactions. Graph-based\nfraud detection methods consider this task as a classification problem with two\nclasses: frauds or normal. We address this problem using Graph Neural Networks\n(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based\non the observation that many real-world graphs include different types of\nrelations, we propose to learn a node representation per relation and aggregate\nthe node representations using a learnable attention function that assigns a\ndifferent attention coefficient to each relation. Furthermore, we combine the\nnode representations from different layers to consider both the local and\nglobal structures of a target node, which is beneficial to improving the\nperformance of fraud detection on graphs with heterophily. By employing dynamic\ngraph attention in all the aggregation processes, our method adaptively\ncomputes the attention coefficients for each node. Experimental results show\nthat our method, DRAG, outperforms state-of-the-art fraud detection methods on\nreal-world benchmark datasets.\n","authors":["Heehyeon Kim","Jinhyeok Choi","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2310.04171v2.pdf","comment":"5 pages, 3 figures, 3 tables. Machine Learning on Graphs (MLoG)\n  Workshop at the 23rd IEEE International Conference on Data Mining (ICDM 2023)"},{"id":"http://arxiv.org/abs/2310.05624v1","updated":"2023-10-09T11:26:58Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation}","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2309.02428v3","updated":"2023-10-09T11:14:41Z","published":"2023-09-05T17:56:22Z","title":"Enhancing Deep Learning Models through Tensorization: A Comprehensive\n  Survey and Framework","summary":"  The burgeoning growth of public domain data and the increasing complexity of\ndeep learning model architectures have underscored the need for more efficient\ndata representation and analysis techniques. This paper is motivated by the\nwork of (Helal, 2023) and aims to present a comprehensive overview of\ntensorization. This transformative approach bridges the gap between the\ninherently multidimensional nature of data and the simplified 2-dimensional\nmatrices commonly used in linear algebra-based machine learning algorithms.\nThis paper explores the steps involved in tensorization, multidimensional data\nsources, various multiway analysis methods employed, and the benefits of these\napproaches. A small example of Blind Source Separation (BSS) is presented\ncomparing 2-dimensional algorithms and a multiway algorithm in Python. Results\nindicate that multiway analysis is more expressive. Contrary to the intuition\nof the dimensionality curse, utilising multidimensional datasets in their\nnative form and applying multiway analysis methods grounded in multilinear\nalgebra reveal a profound capacity to capture intricate interrelationships\namong various dimensions while, surprisingly, reducing the number of model\nparameters and accelerating processing. A survey of the multi-away analysis\nmethods and integration with various Deep Neural Networks models is presented\nusing case studies in different application domains.\n","authors":["Manal Helal"],"pdf_url":"https://arxiv.org/pdf/2309.02428v3.pdf","comment":"34 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.05615v1","updated":"2023-10-09T11:08:34Z","published":"2023-10-09T11:08:34Z","title":"Adaptive Multi-head Contrastive Learning","summary":"  In contrastive learning, two views of an original image generated by\ndifferent augmentations are considered as a positive pair whose similarity is\nrequired to be high. Moreover, two views of two different images are considered\nas a negative pair, and their similarity is encouraged to be low. Normally, a\nsingle similarity measure given by a single projection head is used to evaluate\npositive and negative sample pairs, respectively. However, due to the various\naugmentation strategies and varying intra-sample similarity, augmented views\nfrom the same image are often not similar. Moreover, due to inter-sample\nsimilarity, augmented views of two different images may be more similar than\naugmented views from the same image. As such, enforcing a high similarity for\npositive pairs and a low similarity for negative pairs may not always be\nachievable, and in the case of some pairs, forcing so may be detrimental to the\nperformance. To address this issue, we propose to use multiple projection\nheads, each producing a separate set of features. Our loss function for\npre-training emerges from a solution to the maximum likelihood estimation over\nhead-wise posterior distributions of positive samples given observations. The\nloss contains the similarity measure over positive and negative pairs, each\nre-weighted by an individual adaptive temperature that is regularized to\nprevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can\nbe applied to and experimentally improves several popular contrastive learning\nmethods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent\nunder various backbones and linear probing epoches and is more significant when\nmultiple augmentation methods are used.\n","authors":["Lei Wang","Piotr Koniusz","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05615v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2310.03902v2","updated":"2023-10-09T11:00:47Z","published":"2023-10-05T21:16:55Z","title":"Provable benefits of annealing for estimating normalizing constants:\n  Importance Sampling, Noise-Contrastive Estimation, and beyond","summary":"  Recent research has developed several Monte Carlo methods for estimating the\nnormalization constant (partition function) based on the idea of annealing.\nThis means sampling successively from a path of distributions that interpolate\nbetween a tractable \"proposal\" distribution and the unnormalized \"target\"\ndistribution. Prominent estimators in this family include annealed importance\nsampling and annealed noise-contrastive estimation (NCE). Such methods hinge on\na number of design choices: which estimator to use, which path of distributions\nto use and whether to use a path at all; so far, there is no definitive theory\non which choices are efficient. Here, we evaluate each design choice by the\nasymptotic estimation error it produces. First, we show that using NCE is more\nefficient than the importance sampling estimator, but in the limit of\ninfinitesimal path steps, the difference vanishes. Second, we find that using\nthe geometric path brings down the estimation error from an exponential to a\npolynomial function of the parameter distance between the target and proposal\ndistributions. Third, we find that the arithmetic path, while rarely used, can\noffer optimality properties over the universally-used geometric path. In fact,\nin a particular limit, the optimal path is arithmetic. Based on this theory, we\nfinally propose a two-step estimator to approximate the optimal path in an\nefficient way.\n","authors":["Omar Chehab","Aapo Hyvarinen","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2310.03902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00086v3","updated":"2023-10-09T10:48:20Z","published":"2023-07-28T10:33:12Z","title":"An unsupervised machine-learning-based shock sensor for high-order\n  supersonic flow solvers","summary":"  We present a novel unsupervised machine-learning sock sensor based on\nGaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable\naccuracy in detecting shocks and is robust across diverse test cases with\nsignificantly less parameter tuning than other options. We compare the\nGMM-based sensor with state-of-the-art alternatives. All methods are integrated\ninto a high-order compressible discontinuous Galerkin solver, where two\nstabilization approaches are coupled to the sensor to provide examples of\npossible applications. The Sedov blast and double Mach reflection cases\ndemonstrate that our proposed sensor can enhance hybrid sub-cell\nflux-differencing formulations by providing accurate information of the nodes\nthat require low-order blending. Besides, supersonic test cases including high\nReynolds numbers showcase the sensor performance when used to introduce\nentropy-stable artificial viscosity to capture shocks, demonstrating the same\neffectiveness as fine-tuned state-of-the-art sensors. The adaptive nature and\nability to function without extensive training datasets make this GMM-based\nsensor suitable for complex geometries and varied flow configurations. Our\nstudy reveals the potential of unsupervised machine-learning methods,\nexemplified by this GMM sensor, to improve the robustness and efficiency of\nadvanced CFD codes.\n","authors":["Andrs Mateo-Gabn","Kenza Tlales","Eusebio Valero","Esteban Ferrer","Gonzalo Rubio"],"pdf_url":"https://arxiv.org/pdf/2308.00086v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05598v1","updated":"2023-10-09T10:34:42Z","published":"2023-10-09T10:34:42Z","title":"On Prediction-Modelers and Decision-Makers: Why Fairness Requires More\n  Than a Fair Prediction Model","summary":"  An implicit ambiguity in the field of prediction-based decision-making\nregards the relation between the concepts of prediction and decision. Much of\nthe literature in the field tends to blur the boundaries between the two\nconcepts and often simply speaks of 'fair prediction.' In this paper, we point\nout that a differentiation of these concepts is helpful when implementing\nalgorithmic fairness. Even if fairness properties are related to the features\nof the used prediction model, what is more properly called 'fair' or 'unfair'\nis a decision system, not a prediction model. This is because fairness is about\nthe consequences on human lives, created by a decision, not by a prediction. We\nclarify the distinction between the concepts of prediction and decision and\nshow the different ways in which these two elements influence the final\nfairness properties of a prediction-based decision system. In addition to\nexploring this relationship conceptually and practically, we propose a\nframework that enables a better understanding and reasoning of the conceptual\nlogic of creating fairness in prediction-based decision-making. In our\nframework, we specify different roles, namely the 'prediction-modeler' and the\n'decision-maker,' and the information required from each of them for being able\nto implement fairness of the system. Our framework allows for deriving distinct\nresponsibilities for both roles and discussing some insights related to ethical\nand legal requirements. Our contribution is twofold. First, we shift the focus\nfrom abstract algorithmic fairness to context-dependent decision-making,\nrecognizing diverse actors with unique objectives and independent actions.\nSecond, we provide a conceptual framework that can help structure\nprediction-based decision problems with respect to fairness issues, identify\nresponsibilities, and implement fairness governance mechanisms in real-world\nscenarios.\n","authors":["Teresa Scantamburlo","Joachim Baumann","Christoph Heitz"],"pdf_url":"https://arxiv.org/pdf/2310.05598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04292v2","updated":"2023-10-09T10:22:11Z","published":"2023-10-06T14:51:17Z","title":"Towards Foundational Models for Molecular Learning on Large-Scale\n  Multi-Task Datasets","summary":"  Recently, pre-trained foundation models have enabled significant advancements\nin multiple fields. In molecular machine learning, however, where datasets are\noften hand-curated, and hence typically small, the lack of datasets with\nlabeled features, and codebases to manage those datasets, has hindered the\ndevelopment of foundation models. In this work, we present seven novel datasets\ncategorized by size into three distinct categories: ToyMix, LargeMix and\nUltraLarge. These datasets push the boundaries in both the scale and the\ndiversity of supervised labels for molecular learning. They cover nearly 100\nmillion molecules and over 3000 sparsely defined tasks, totaling more than 13\nbillion individual labels of both quantum and biological nature. In comparison,\nour datasets contain 300 times more data points than the widely used OGB-LSC\nPCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In\naddition, to support the development of foundational models based on our\nproposed datasets, we present the Graphium graph machine learning library which\nsimplifies the process of building and training molecular machine learning\nmodels for multi-task and multi-level molecular datasets. Finally, we present a\nrange of baseline results as a starting point of multi-task and multi-level\ntraining on these datasets. Empirically, we observe that performance on\nlow-resource biological datasets show improvement by also training on large\namounts of quantum data. This indicates that there may be potential in\nmulti-task and multi-level training of a foundation model and fine-tuning it to\nresource-constrained downstream tasks.\n","authors":["Dominique Beaini","Shenyang Huang","Joao Alex Cunha","Gabriela Moisescu-Pareja","Oleksandr Dymov","Samuel Maddrell-Mander","Callum McLean","Frederik Wenkel","Luis Mller","Jama Hussein Mohamud","Ali Parviz","Michael Craig","Micha Koziarski","Jiarui Lu","Zhaocheng Zhu","Cristian Gabellini","Kerstin Klaser","Josef Dean","Cas Wognum","Maciej Sypetkowski","Guillaume Rabusseau","Reihaneh Rabbany","Jian Tang","Christopher Morris","Ioannis Koutis","Mirco Ravanelli","Guy Wolf","Prudencio Tossou","Hadrien Mary","Therence Bois","Andrew Fitzgibbon","Baej Banaszewski","Chad Martin","Dominic Masters"],"pdf_url":"https://arxiv.org/pdf/2310.04292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07688v2","updated":"2023-10-09T10:08:00Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05573v1","updated":"2023-10-09T09:54:12Z","published":"2023-10-09T09:54:12Z","title":"ODEFormer: Symbolic Regression of Dynamical Systems with Transformers","summary":"  We introduce ODEFormer, the first transformer able to infer multidimensional\nordinary differential equation (ODE) systems in symbolic form from the\nobservation of a single solution trajectory. We perform extensive evaluations\non two datasets: (i) the existing \"Strogatz\" dataset featuring two-dimensional\nsystems; (ii) ODEBench, a collection of one- to four-dimensional systems that\nwe carefully curated from the literature to provide a more holistic benchmark.\nODEFormer consistently outperforms existing methods while displaying\nsubstantially improved robustness to noisy and irregularly sampled\nobservations, as well as faster inference. We release our code, model and\nbenchmark dataset publicly.\n","authors":["Stphane d'Ascoli","Sren Becker","Alexander Mathis","Philippe Schwaller","Niki Kilbertus"],"pdf_url":"https://arxiv.org/pdf/2310.05573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05572v1","updated":"2023-10-09T09:51:44Z","published":"2023-10-09T09:51:44Z","title":"A Simple and Robust Framework for Cross-Modality Medical Image\n  Segmentation applied to Vision Transformers","summary":"  When it comes to clinical images, automatic segmentation has a wide variety\nof applications and a considerable diversity of input domains, such as\ndifferent types of Magnetic Resonance Images (MRIs) and Computerized Tomography\n(CT) scans. This heterogeneity is a challenge for cross-modality algorithms\nthat should equally perform independently of the input image type fed to them.\nOften, segmentation models are trained using a single modality, preventing\ngeneralization to other types of input data without resorting to transfer\nlearning techniques. Furthermore, the multi-modal or cross-modality\narchitectures proposed in the literature frequently require registered images,\nwhich are not easy to collect in clinical environments, or need additional\nprocessing steps, such as synthetic image generation. In this work, we propose\na simple framework to achieve fair image segmentation of multiple modalities\nusing a single conditional model that adapts its normalization layers based on\nthe input type, trained with non-registered interleaved mixed data. We show\nthat our framework outperforms other cross-modality segmentation methods, when\napplied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart\nSegmentation Challenge. Furthermore, we define the Conditional Vision\nTransformer (C-ViT) encoder, based on the proposed cross-modality framework,\nand we show that it brings significant improvements to the resulting\nsegmentation, up to 6.87\\% of Dice accuracy, with respect to its baseline\nreference. The code to reproduce our experiments and the trained model weights\nare available at https://github.com/matteo-bastico/MI-Seg.\n","authors":["Matteo Bastico","David Ryckelynck","Laurent Cort","Yannick Tillier","Etienne Decencire"],"pdf_url":"https://arxiv.org/pdf/2310.05572v1.pdf","comment":"This paper has been accepted in International Conference on Computer\n  Vision Workshops (ICCVW) 2023"},{"id":"http://arxiv.org/abs/2205.08821v4","updated":"2023-10-09T09:46:41Z","published":"2022-05-18T09:38:37Z","title":"Lessons Learned: Defending Against Property Inference Attacks","summary":"  This work investigates and evaluates multiple defense strategies against\nproperty inference attacks (PIAs), a privacy attack against machine learning\nmodels. Given a trained machine learning model, PIAs aim to extract statistical\nproperties of its underlying training data, e.g., reveal the ratio of men and\nwomen in a medical training data set. While for other privacy attacks like\nmembership inference, a lot of research on defense mechanisms has been\npublished, this is the first work focusing on defending against PIAs. With the\nprimary goal of developing a generic mitigation strategy against white-box\nPIAs, we propose the novel approach property unlearning. Extensive experiments\nwith property unlearning show that while it is very effective when defending\ntarget models against specific adversaries, property unlearning is not able to\ngeneralize, i.e., protect against a whole class of PIAs. To investigate the\nreasons behind this limitation, we present the results of experiments with the\nexplainable AI tool LIME. They show how state-of-the-art property inference\nadversaries with the same objective focus on different parts of the target\nmodel. We further elaborate on this with a follow-up experiment, in which we\nuse the visualization technique t-SNE to exhibit how severely statistical\ntraining data properties are manifested in machine learning models. Based on\nthis, we develop the conjecture that post-training techniques like property\nunlearning might not suffice to provide the desirable generic protection\nagainst PIAs. As an alternative, we investigate the effects of simpler training\ndata preprocessing methods like adding Gaussian noise to images of a training\ndata set on the success rate of PIAs. We conclude with a discussion of the\ndifferent defense approaches, summarize the lessons learned and provide\ndirections for future work.\n","authors":["Joshua Stock","Jens Wettlaufer","Daniel Demmler","Hannes Federrath"],"pdf_url":"https://arxiv.org/pdf/2205.08821v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05566v1","updated":"2023-10-09T09:43:08Z","published":"2023-10-09T09:43:08Z","title":"Aggregated f-average Neural Network for Interpretable Ensembling","summary":"  Ensemble learning leverages multiple models (i.e., weak learners) on a common\nmachine learning task to enhance prediction performance. Basic ensembling\napproaches average the weak learners outputs, while more sophisticated ones\nstack a machine learning model in between the weak learners outputs and the\nfinal prediction. This work fuses both aforementioned frameworks. We introduce\nan aggregated f-average (AFA) shallow neural network which models and combines\ndifferent types of averages to perform an optimal aggregation of the weak\nlearners predictions. We emphasise its interpretable architecture and simple\ntraining strategy, and illustrate its good performance on the problem of\nfew-shot class incremental learning.\n","authors":["Mathieu Vu","Emilie Chouzenoux","Jean-Christophe Pesquet","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2310.05566v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2203.10087v2","updated":"2023-10-09T09:42:02Z","published":"2022-03-18T17:55:39Z","title":"But that's not why: Inference adjustment by interactive prototype\n  revision","summary":"  Despite significant advances in machine learning, decision-making of\nartificial agents is still not perfect and often requires post-hoc human\ninterventions. If the prediction of a model relies on unreasonable factors it\nis desirable to remove their effect. Deep interactive prototype adjustment\nenables the user to give hints and correct the model's reasoning. In this\npaper, we demonstrate that prototypical-part models are well suited for this\ntask as their prediction is based on prototypical image patches that can be\ninterpreted semantically by the user. It shows that even correct\nclassifications can rely on unreasonable prototypes that result from\nconfounding variables in a dataset. Hence, we propose simple yet effective\ninteraction schemes for inference adjustment: The user is consulted\ninteractively to identify faulty prototypes. Non-object prototypes can be\nremoved by prototype masking or a custom mode of deselection training.\nInteractive prototype rejection allows machine learning na\\\"{i}ve users to\nadjust the logic of reasoning without compromising the accuracy.\n","authors":["Michael Gerstenberger","Sebastian Lapuschkin","Peter Eisert","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2203.10087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05549v1","updated":"2023-10-09T09:17:52Z","published":"2023-10-09T09:17:52Z","title":"A New Transformation Approach for Uplift Modeling with Binary Outcome","summary":"  Uplift modeling has been used effectively in fields such as marketing and\ncustomer retention, to target those customers who are more likely to respond\ndue to the campaign or treatment. Essentially, it is a machine learning\ntechnique that predicts the gain from performing some action with respect to\nnot taking it. A popular class of uplift models is the transformation approach\nthat redefines the target variable with the original treatment indicator. These\ntransformation approaches only need to train and predict the difference in\noutcomes directly. The main drawback of these approaches is that in general it\ndoes not use the information in the treatment indicator beyond the construction\nof the transformed outcome and usually is not efficient. In this paper, we\ndesign a novel transformed outcome for the case of the binary target variable\nand unlock the full value of the samples with zero outcome. From a practical\nperspective, our new approach is flexible and easy to use. Experimental results\non synthetic and real-world datasets obviously show that our new approach\noutperforms the traditional one. At present, our new approach has already been\napplied to precision marketing in a China nation-wide financial holdings group.\n","authors":["Kun Li","Jiang Tian","Xiaojia Xiang"],"pdf_url":"https://arxiv.org/pdf/2310.05549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05538v1","updated":"2023-10-09T09:01:53Z","published":"2023-10-09T09:01:53Z","title":"M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion\n  for Polyp Localization in Colonoscopy Images","summary":"  Polyp segmentation is crucial for preventing colorectal cancer a common type\nof cancer. Deep learning has been used to segment polyps automatically, which\nreduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images\nis challenging because of its complex characteristics, such as color,\nocclusion, and various shapes of polyps. To address this challenge, a novel\nfrequency-based fully convolutional neural network, Multi-Frequency Feature\nFusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose\nthe input image into low/high/full-frequency components to use the\ncharacteristics of each component. We used three independent multi-frequency\nencoders to map multiple input images into a high-dimensional feature space. In\nthe Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied\nbetween each frequency component to preserve scale information. Subsequently,\nscalable attention was applied to emphasize polyp regions in a high-dimensional\nfeature space. Finally, we designed three multi-task learning (i.e., region,\nedge, and distance) in four decoder blocks to learn the structural\ncharacteristics of the region. The proposed model outperformed various\nsegmentation models with performance gains of 6.92% and 7.52% on average for\nall metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyeong Park","Nur Suriza Syazwany","Yerim Jung","Yu-Han Im","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05538v1.pdf","comment":"5pages. 2023 IEEE International Conference on Image Processing\n  (ICIP). IEEE, 2023"},{"id":"http://arxiv.org/abs/2310.05537v1","updated":"2023-10-09T09:01:25Z","published":"2023-10-09T09:01:25Z","title":"ParFam -- Symbolic Regression Based on Continuous Global Optimization","summary":"  The problem of symbolic regression (SR) arises in many different\napplications, such as identifying physical laws or deriving mathematical\nequations describing the behavior of financial markets from given data. Various\nmethods exist to address the problem of SR, often based on genetic programming.\nHowever, these methods are usually quite complicated and require a lot of\nhyperparameter tuning and computational resources. In this paper, we present\nour new method ParFam that utilizes parametric families of suitable symbolic\nfunctions to translate the discrete symbolic regression problem into a\ncontinuous one, resulting in a more straightforward setup compared to current\nstate-of-the-art methods. In combination with a powerful global optimizer, this\napproach results in an effective method to tackle the problem of SR.\nFurthermore, it can be easily extended to more advanced algorithms, e.g., by\nadding a deep neural network to find good-fitting parametric families. We prove\nthe performance of ParFam with extensive numerical experiments based on the\ncommon SR benchmark suit SRBench, showing that we achieve state-of-the-art\nresults. Our code and results can be found at\nhttps://github.com/Philipp238/parfam .\n","authors":["Philipp Scholl","Katharina Bieker","Hillary Hauger","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2310.05537v1.pdf","comment":"Code: https://github.com/Philipp238/parfam"},{"id":"http://arxiv.org/abs/2310.05530v1","updated":"2023-10-09T08:51:00Z","published":"2023-10-09T08:51:00Z","title":"NetTiSA: Extended IP Flow with Time-series Features for Universal\n  Bandwidth-constrained High-speed Network Traffic Classification","summary":"  Network traffic monitoring based on IP Flows is a standard monitoring\napproach that can be deployed to various network infrastructures, even the\nlarge IPS-based networks connecting millions of people. Since flow records\ntraditionally contain only limited information (addresses, transport ports, and\namount of exchanged data), they are also commonly extended for additional\nfeatures that enable network traffic analysis with high accuracy. Nevertheless,\nthe flow extensions are often too large or hard to compute, which limits their\ndeployment only to smaller-sized networks. This paper proposes a novel extended\nIP flow called NetTiSA (Network Time Series Analysed), which is based on the\nanalysis of the time series of packet sizes. By thoroughly testing 25 different\nnetwork classification tasks, we show the broad applicability and high\nusability of NetTiSA, which often outperforms the best-performing related\nworks. For practical deployment, we also consider the sizes of flows extended\nfor NetTiSA and evaluate the performance impacts of its computation in the flow\nexporter. The novel feature set proved universal and deployable to high-speed\nISP networks with 100\\,Gbps lines; thus, it enables accurate and widespread\nnetwork security protection.\n","authors":["Josef Koumar","Karel Hynek","Jaroslav Peek","Tom ejka"],"pdf_url":"https://arxiv.org/pdf/2310.05530v1.pdf","comment":"Submitted to The International Journal of Computer and\n  Telecommunications Networking"},{"id":"http://arxiv.org/abs/2310.05526v1","updated":"2023-10-09T08:45:06Z","published":"2023-10-09T08:45:06Z","title":"Projecting infinite time series graphs to finite marginal graphs using\n  number theory","summary":"  In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries.\n","authors":["Andreas Gerhardus","Jonas Wahl","Sofia Faltenbacher","Urmi Ninad","Jakob Runge"],"pdf_url":"https://arxiv.org/pdf/2310.05526v1.pdf","comment":"50 pages (including appendix), 9 figures"},{"id":"http://arxiv.org/abs/2310.05518v1","updated":"2023-10-09T08:33:22Z","published":"2023-10-09T08:33:22Z","title":"On Double-Descent in Reinforcement Learning with LSTD and Random\n  Features","summary":"  Temporal Difference (TD) algorithms are widely used in Deep Reinforcement\nLearning (RL). Their performance is heavily influenced by the size of the\nneural network. While in supervised learning, the regime of\nover-parameterization and its benefits are well understood, the situation in RL\nis much less clear. In this paper, we present a theoretical analysis of the\ninfluence of network size and $l_2$-regularization on performance. We identify\nthe ratio between the number of parameters and the number of visited states as\na crucial factor and define over-parameterization as the regime when it is\nlarger than one. Furthermore, we observe a double-descent phenomenon, i.e., a\nsudden drop in performance around the parameter/state ratio of one. Leveraging\nrandom features and the lazy training regime, we study the regularized\nLeast-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as\nboth the number of parameters and states go to infinity, maintaining a constant\nratio. We derive deterministic limits of both the empirical and the true\nMean-Square Bellman Error (MSBE) that feature correction terms responsible for\nthe double-descent. Correction terms vanish when the $l_2$-regularization is\nincreased or the number of unvisited states goes to zero. Numerical experiments\nwith synthetic and small real-world environments closely match the theoretical\npredictions.\n","authors":["David Brellmann","Elose Berthier","David Filliat","Goran Frehse"],"pdf_url":"https://arxiv.org/pdf/2310.05518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05517v1","updated":"2023-10-09T08:33:19Z","published":"2023-10-09T08:33:19Z","title":"WeatherGNN: Exploiting Complicated Relationships in Numerical Weather\n  Prediction Bias Correction","summary":"  Numerical weather prediction (NWP) may be inaccurate or biased due to\nincomplete atmospheric physical processes, insufficient spatial-temporal\nresolution, and inherent uncertainty of weather. Previous studies have\nattempted to correct biases by using handcrafted features and domain knowledge,\nor by applying general machine learning models naively. They do not fully\nexplore the complicated meteorologic interactions and spatial dependencies in\nthe atmosphere dynamically, which limits their applicability in NWP\nbias-correction. Specifically, weather factors interact with each other in\ncomplex ways, and these interactions can vary regionally. In addition, the\ninteractions between weather factors are further complicated by the spatial\ndependencies between regions, which are influenced by varied terrain and\natmospheric motions. To address these issues, we propose WeatherGNN, an NWP\nbias-correction method that utilizes Graph Neural Networks (GNN) to learn\nmeteorologic and geographic relationships in a unified framework. Our approach\nincludes a factor-wise GNN that captures meteorological interactions within\neach grid (a specific location) adaptively, and a fast hierarchical GNN that\ncaptures spatial dependencies between grids dynamically. Notably, the fast\nhierarchical GNN achieves linear complexity with respect to the number of\ngrids, enhancing model efficiency and scalability. Our experimental results on\ntwo real-world datasets demonstrate the superiority of WeatherGNN in comparison\nwith other SOTA methods, with an average improvement of 40.50\\% on RMSE\ncompared to the original NWP.\n","authors":["Binqing Wu","Weiqi Chen","Wengwei Wang","Bingqing Peng","Liang Sun","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10151v2","updated":"2023-10-09T08:28:17Z","published":"2023-04-20T08:23:58Z","title":"Flexible K Nearest Neighbors Classifier: Derivation and Application for\n  Ion-mobility Spectrometry-based Indoor Localization","summary":"  The K Nearest Neighbors (KNN) classifier is widely used in many fields such\nas fingerprint-based localization or medicine. It determines the class\nmembership of unlabelled sample based on the class memberships of the K\nlabelled samples, the so-called nearest neighbors, that are closest to the\nunlabelled sample. The choice of K has been the topic of various studies and\nproposed KNN-variants. Yet no variant has been proven to outperform all other\nvariants. In this paper a KNN-variant is discussed which ensures that the K\nnearest neighbors are indeed close to the unlabelled sample and finds K along\nthe way. The algorithm is tested and compared to the standard KNN in\ntheoretical scenarios and for indoor localization based on ion-mobility\nspectrometry fingerprints. It achieves a higher classification accuracy than\nthe KNN in the tests, while having the same computational demand.\n","authors":["Philipp Mller"],"pdf_url":"https://arxiv.org/pdf/2304.10151v2.pdf","comment":"11 pages, 3 figures, paper presented at the 2023 International\n  Conference on Indoor Positioning and Indoor Navigation (IPIN)"},{"id":"http://arxiv.org/abs/2310.05506v1","updated":"2023-10-09T08:18:58Z","published":"2023-10-09T08:18:58Z","title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning\n  Generalization","summary":"  In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,\nby complicating and diversifying the queries from GSM8K and sampling multiple\nreasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning\non subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art\non GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the\nscale of 13B). A log-linear relationship is presented between MuggleMath's\nperformance and the amount of augmented data. We also find that MuggleMath is\nweak in out-of-domain math reasoning generalization to MATH. This is attributed\nto the differences in query distribution between AugGSM8K and MATH which\nsuggest that augmentation on a single benchmark could not help with overall\nmath reasoning performance. Codes and AugGSM8K will be uploaded to\nhttps://github.com/OFA-Sys/gsm8k-ScRel.\n","authors":["Chengpeng Li","Zheng Yuan","Guanting Dong","Keming Lu","Jiancan Wu","Chuanqi Tan","Xiang Wang","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05506v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2301.12714v2","updated":"2023-10-09T08:03:14Z","published":"2023-01-30T07:53:53Z","title":"Importance Weighted Actor-Critic for Optimal Conservative Offline\n  Reinforcement Learning","summary":"  We propose A-Crab (Actor-Critic Regularized by Average Bellman error), a new\npractical algorithm for offline reinforcement learning (RL) in complex\nenvironments with insufficient data coverage. Our algorithm combines the\nmarginalized importance sampling framework with the actor-critic paradigm,\nwhere the critic returns evaluations of the actor (policy) that are pessimistic\nrelative to the offline data and have a small average (importance-weighted)\nBellman error. Compared to existing methods, our algorithm simultaneously\noffers a number of advantages: (1) It achieves the optimal statistical rate of\n$1/\\sqrt{N}$ -- where $N$ is the size of offline dataset -- in converging to\nthe best policy covered in the offline dataset, even when combined with general\nfunction approximators. (2) It relies on a weaker average notion of policy\ncoverage (compared to the $\\ell_\\infty$ single-policy concentrability) that\nexploits the structure of policy visitations. (3) It outperforms the\ndata-collection behavior policy over a wide range of specific hyperparameters.\nWe provide both theoretical analysis and experimental results to validate the\neffectiveness of our proposed algorithm.\n","authors":["Hanlin Zhu","Paria Rashidinejad","Jiantao Jiao"],"pdf_url":"https://arxiv.org/pdf/2301.12714v2.pdf","comment":"24 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.05495v1","updated":"2023-10-09T07:56:56Z","published":"2023-10-09T07:56:56Z","title":"A Neural Tangent Kernel View on Federated Averaging for Deep Linear\n  Neural Network","summary":"  Federated averaging (FedAvg) is a widely employed paradigm for\ncollaboratively training models from distributed clients without sharing data.\nNowadays, the neural network has achieved remarkable success due to its\nextraordinary performance, which makes it a preferred choice as the model in\nFedAvg. However, the optimization problem of the neural network is often\nnon-convex even non-smooth. Furthermore, FedAvg always involves multiple\nclients and local updates, which results in an inaccurate updating direction.\nThese properties bring difficulties in analyzing the convergence of FedAvg in\ntraining neural networks. Recently, neural tangent kernel (NTK) theory has been\nproposed towards understanding the convergence of first-order methods in\ntackling the non-convex problem of neural networks. The deep linear neural\nnetwork is a classical model in theoretical subject due to its simple\nformulation. Nevertheless, there exists no theoretical result for the\nconvergence of FedAvg in training the deep linear neural network. By applying\nNTK theory, we make a further step to provide the first theoretical guarantee\nfor the global convergence of FedAvg in training deep linear neural networks.\nSpecifically, we prove FedAvg converges to the global minimum at a linear rate\n$\\mathcal{O}\\big((1-\\eta K /N)^t\\big)$, where $t$ is the number of iterations,\n$\\eta$ is the learning rate, $N$ is the number of clients and $K$ is the number\nof local updates. Finally, experimental evaluations on two benchmark datasets\nare conducted to empirically validate the correctness of our theoretical\nfindings.\n","authors":["Xin Liu","Dazhi Zhan","Wei Tao","Xin Ma","Yu Pan","Yu Ding","Zhisong Pan"],"pdf_url":"https://arxiv.org/pdf/2310.05495v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05492v1","updated":"2023-10-09T07:56:16Z","published":"2023-10-09T07:56:16Z","title":"How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition","summary":"  Large language models (LLMs) with enormous pre-training tokens and parameter\namounts emerge abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each\nability, while proprietary LLMs are versatile for all abilities. It is\nimportant to investigate how to unlock them with multiple abilities via SFT. In\nthis study, we specifically focus on the data composition between mathematical\nreasoning, code generation, and general human-aligning abilities during SFT.\nFrom a scaling perspective, we investigate the relationship between model\nabilities and various factors including data amounts, data composition ratio,\nmodel parameters, and SFT strategies. Our experiments reveal that different\nabilities exhibit different scaling patterns, and larger models generally show\nsuperior performance with the same amount of data. Mathematical reasoning and\ncode generation improve as data amounts increase consistently, while the\ngeneral ability is enhanced with about a thousand samples and improves slowly.\nWe find data composition results in various abilities improvements with low\ndata amounts, while conflicts of abilities with high data amounts. Our\nexperiments further show that composition data amount impacts performance,\nwhile the influence of composition ratio is insignificant. Regarding the SFT\nstrategies, we evaluate sequential learning multiple abilities are prone to\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy learns specialized abilities first and then learns general abilities\nwith a small amount of specialized data to prevent forgetting, offering a\npromising solution to learn multiple abilities with different scaling patterns.\n","authors":["Guanting Dong","Hongyi Yuan","Keming Lu","Chengpeng Li","Mingfeng Xue","Dayiheng Liu","Wei Wang","Zheng Yuan","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05492v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.02854v2","updated":"2023-10-09T07:52:46Z","published":"2023-10-04T14:41:41Z","title":"Multi-Domain Causal Representation Learning via Weak Distributional\n  Invariances","summary":"  Causal representation learning has emerged as the center of action in causal\nmachine learning research. In particular, multi-domain datasets present a\nnatural opportunity for showcasing the advantages of causal representation\nlearning over standard unsupervised representation learning. While recent works\nhave taken crucial steps towards learning causal representations, they often\nlack applicability to multi-domain datasets due to over-simplifying assumptions\nabout the data; e.g. each domain comes from a different single-node perfect\nintervention. In this work, we relax these assumptions and capitalize on the\nfollowing observation: there often exists a subset of latents whose certain\ndistributional properties (e.g., support, variance) remain stable across\ndomains; this property holds when, for example, each domain comes from a\nmulti-node imperfect intervention. Leveraging this observation, we show that\nautoencoders that incorporate such invariances can provably identify the stable\nset of latents from the rest across different settings.\n","authors":["Kartik Ahuja","Amin Mansouri","Yixin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05485v1","updated":"2023-10-09T07:44:37Z","published":"2023-10-09T07:44:37Z","title":"Integration-free Training for Spatio-temporal Multimodal Covariate Deep\n  Kernel Point Processes","summary":"  In this study, we propose a novel deep spatio-temporal point process model,\nDeep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal\ncovariate information. DKMPP is an enhanced version of Deep Mixture Point\nProcesses (DMPP), which uses a more flexible deep kernel to model complex\nrelationships between events and covariate data, improving the model's\nexpressiveness. To address the intractable training procedure of DKMPP due to\nthe non-integrable deep kernel, we utilize an integration-free method based on\nscore matching, and further improve efficiency by adopting a scalable denoising\nscore matching method. Our experiments demonstrate that DKMPP and its\ncorresponding score-based estimators outperform baseline models, showcasing the\nadvantages of incorporating covariate information, utilizing a deep kernel, and\nemploying score-based estimators.\n","authors":["Yixuan Zhang","Quyu Kong","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05484v1","updated":"2023-10-09T07:43:57Z","published":"2023-10-09T07:43:57Z","title":"IDTraffickers: An Authorship Attribution Dataset to link and connect\n  Potential Human-Trafficking Operations on Text Escort Advertisements","summary":"  Human trafficking (HT) is a pervasive global issue affecting vulnerable\nindividuals, violating their fundamental human rights. Investigations reveal\nthat a significant number of HT cases are associated with online advertisements\n(ads), particularly in escort markets. Consequently, identifying and connecting\nHT vendors has become increasingly challenging for Law Enforcement Agencies\n(LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset\nconsisting of 87,595 text ads and 5,244 vendor labels to enable the\nverification and identification of potential HT vendors on online escort\nmarkets. To establish a benchmark for authorship identification, we train a\nDeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set\nclassification environment. Next, we leverage the style representations\nextracted from the trained classifier to conduct authorship verification,\nresulting in a mean r-precision score of 0.8852 in an open-set ranking\nenvironment. Finally, to encourage further research and ensure responsible data\nsharing, we plan to release IDTraffickers for the authorship attribution task\nto researchers under specific conditions, considering the sensitive nature of\nthe data. We believe that the availability of our dataset and benchmarks will\nempower future researchers to utilize our findings, thereby facilitating the\neffective linkage of escort ads and the development of more robust approaches\nfor identifying HT indicators.\n","authors":["Vageesh Saxena","Benjamin Bashpole","Gijs Van Dijck","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2310.05484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14717v2","updated":"2023-10-09T07:39:04Z","published":"2023-09-26T07:22:23Z","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","summary":"  Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.\n","authors":["Yuhui Xu","Lingxi Xie","Xiaotao Gu","Xin Chen","Heng Chang","Hengheng Zhang","Zhengsu Chen","Xiaopeng Zhang","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14717v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2310.05469v1","updated":"2023-10-09T07:26:35Z","published":"2023-10-09T07:26:35Z","title":"Vibroacoustic Frequency Response Prediction with Query-based Operator\n  Networks","summary":"  Understanding vibroacoustic wave propagation in mechanical structures like\nairplanes, cars and houses is crucial to ensure health and comfort of their\nusers. To analyze such systems, designers and engineers primarily consider the\ndynamic response in the frequency domain, which is computed through expensive\nnumerical simulations like the finite element method. In contrast, data-driven\nsurrogate models offer the promise of speeding up these simulations, thereby\nfacilitating tasks like design optimization, uncertainty quantification, and\ndesign space exploration. We present a structured benchmark for a\nrepresentative vibroacoustic problem: Predicting the frequency response for\nvibrating plates with varying forms of beadings. The benchmark features a total\nof 12,000 plate geometries with an associated numerical solution and introduces\nevaluation metrics to quantify the prediction quality. To address the frequency\nresponse prediction task, we propose a novel frequency query operator model,\nwhich is trained to map plate geometries to frequency response functions. By\nintegrating principles from operator learning and implicit models for shape\nencoding, our approach effectively addresses the prediction of resonance peaks\nof frequency responses. We evaluate the method on our vibrating-plates\nbenchmark and find that it outperforms DeepONets, Fourier Neural Operators and\nmore traditional neural network architectures. The code and dataset are\navailable from https://eckerlab.org/code/delden2023_plate.\n","authors":["Jan van Delden","Julius Schultz","Christopher Blech","Sabine C. Langer","Timo Lddecke"],"pdf_url":"https://arxiv.org/pdf/2310.05469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05468v1","updated":"2023-10-09T07:24:04Z","published":"2023-10-09T07:24:04Z","title":"ExIFFI and EIF+: Interpretability and Enhanced Generalizability to\n  Extend the Extended Isolation Forest","summary":"  Anomaly detection, an essential unsupervised machine learning task, involves\nidentifying unusual behaviors within complex datasets and systems. While\nMachine Learning algorithms and decision support systems (DSSs) offer effective\nsolutions for this task, simply pinpointing anomalies often falls short in\nreal-world applications. Users of these systems often require insight into the\nunderlying reasons behind predictions to facilitate Root Cause Analysis and\nfoster trust in the model. However, due to the unsupervised nature of anomaly\ndetection, creating interpretable tools is challenging. This work introduces\nEIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to\nenhance generalization capabilities. Additionally, we present ExIFFI, a novel\napproach that equips Extended Isolation Forest with interpretability features,\nspecifically feature rankings. Experimental results provide a comprehensive\ncomparative analysis of Isolation-based approaches for Anomaly Detection,\nincluding synthetic and real dataset evaluations that demonstrate ExIFFI's\neffectiveness in providing explanations. We also illustrate how ExIFFI serves\nas a valid feature selection technique in unsupervised settings. To facilitate\nfurther research and reproducibility, we also provide open-source code to\nreplicate the results.\n","authors":["Alessio Arcudi","Davide Frizzo","Chiara Masiero","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2310.05468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05467v1","updated":"2023-10-09T07:22:22Z","published":"2023-10-09T07:22:22Z","title":"Temporal Convolutional Explorer Helps Understand 1D-CNN's Learning\n  Behavior in Time Series Classification from Frequency Domain","summary":"  While one-dimensional convolutional neural networks (1D-CNNs) have been\nempirically proven effective in time series classification tasks, we find that\nthere remain undesirable outcomes that could arise in their application,\nmotivating us to further investigate and understand their underlying\nmechanisms. In this work, we propose a Temporal Convolutional Explorer (TCE) to\nempirically explore the learning behavior of 1D-CNNs from the perspective of\nthe frequency domain. Our TCE analysis highlights that deeper 1D-CNNs tend to\ndistract the focus from the low-frequency components leading to the accuracy\ndegradation phenomenon, and the disturbing convolution is the driving factor.\nThen, we leverage our findings to the practical application and propose a\nregulatory framework, which can easily be integrated into existing 1D-CNNs. It\naims to rectify the suboptimal learning behavior by enabling the network to\nselectively bypass the specified disturbing convolutions. Finally, through\ncomprehensive experiments on widely-used UCR, UEA, and UCI benchmarks, we\ndemonstrate that 1) TCE's insight into 1D-CNN's learning behavior; 2) our\nregulatory framework enables state-of-the-art 1D-CNNs to get improved\nperformances with less consumption of memory and computational overhead.\n","authors":["Junru Zhang","Lang Feng","Yang He","Yuhan Wu","Yabo Dong"],"pdf_url":"https://arxiv.org/pdf/2310.05467v1.pdf","comment":"Accepted as CIKM'23 Long Paper"},{"id":"http://arxiv.org/abs/2307.10864v2","updated":"2023-10-09T07:20:00Z","published":"2023-07-20T13:33:28Z","title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing","summary":"  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., ``a cat and a dog''. However, its efficacy declines when dealing with\nmore complex prompts, and it does not explicitly address the problem of\nimproper attribute binding. To address the challenges posed by complex prompts\nor scenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n","authors":["Yumeng Li","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2307.10864v2.pdf","comment":"Accepted at BMVC 2023 as Oral. Code:\n  https://github.com/boschresearch/Divide-and-Bind and project page:\n  https://sites.google.com/view/divide-and-bind"},{"id":"http://arxiv.org/abs/2310.05464v1","updated":"2023-10-09T07:13:40Z","published":"2023-10-09T07:13:40Z","title":"Cost-Sensitive Best Subset Selection for Logistic Regression: A\n  Mixed-Integer Conic Optimization Perspective","summary":"  A key challenge in machine learning is to design interpretable models that\ncan reduce their inputs to the best subset for making transparent predictions,\nespecially in the clinical domain. In this work, we propose a certifiably\noptimal feature selection procedure for logistic regression from a\nmixed-integer conic optimization perspective that can take an auxiliary cost to\nobtain features into account. Based on an extensive review of the literature,\nwe carefully create a synthetic dataset generator for clinical prognostic model\nresearch. This allows us to systematically evaluate different heuristic and\noptimal cardinality- and budget-constrained feature selection procedures. The\nanalysis shows key limitations of the methods for the low-data regime and when\nconfronted with label noise. Our paper not only provides empirical\nrecommendations for suitable methods and dataset designs, but also paves the\nway for future research in the area of meta-learning.\n","authors":["Ricardo Knauer","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2310.05464v1.pdf","comment":"German Conference on Artificial Intelligence (K\\\"unstliche\n  Intelligenz)"},{"id":"http://arxiv.org/abs/2306.11974v2","updated":"2023-10-09T07:06:50Z","published":"2023-06-21T02:02:41Z","title":"Universal adversarial perturbations for multiple classification tasks\n  with quantum classifiers","summary":"  Quantum adversarial machine learning is an emerging field that studies the\nvulnerability of quantum learning systems against adversarial perturbations and\ndevelops possible defense strategies. Quantum universal adversarial\nperturbations are small perturbations, which can make different input samples\ninto adversarial examples that may deceive a given quantum classifier. This is\na field that was rarely looked into but worthwhile investigating because\nuniversal perturbations might simplify malicious attacks to a large extent,\ncausing unexpected devastation to quantum machine learning models. In this\npaper, we take a step forward and explore the quantum universal perturbations\nin the context of heterogeneous classification tasks. In particular, we find\nthat quantum classifiers that achieve almost state-of-the-art accuracy on two\ndifferent classification tasks can be both conclusively deceived by one\ncarefully-crafted universal perturbation. This result is explicitly\ndemonstrated with well-designed quantum continual learning models with elastic\nweight consolidation method to avoid catastrophic forgetting, as well as\nreal-life heterogeneous datasets from hand-written digits and medical MRI\nimages. Our results provide a simple and efficient way to generate universal\nperturbations on heterogeneous classification tasks and thus would provide\nvaluable guidance for future quantum learning technologies.\n","authors":["Yun-Zhong Qiu"],"pdf_url":"https://arxiv.org/pdf/2306.11974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15413v3","updated":"2023-10-09T07:02:43Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation","summary":"  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v3.pdf","comment":"Accepted to NeurIPS 2023. Project Page:\n  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/"},{"id":"http://arxiv.org/abs/2310.05456v1","updated":"2023-10-09T06:59:17Z","published":"2023-10-09T06:59:17Z","title":"Ensemble-based Hybrid Optimization of Bayesian Neural Networks and\n  Traditional Machine Learning Algorithms","summary":"  This research introduces a novel methodology for optimizing Bayesian Neural\nNetworks (BNNs) by synergistically integrating them with traditional machine\nlearning algorithms such as Random Forests (RF), Gradient Boosting (GB), and\nSupport Vector Machines (SVM). Feature integration solidifies these results by\nemphasizing the second-order conditions for optimality, including stationarity\nand positive definiteness of the Hessian matrix. Conversely, hyperparameter\ntuning indicates a subdued impact in improving Expected Improvement (EI),\nrepresented by EI(x). Overall, the ensemble method stands out as a robust,\nalgorithmically optimized approach.\n","authors":["Peiwen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.05456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05446v1","updated":"2023-10-09T06:43:38Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10569v3","updated":"2023-10-09T06:41:19Z","published":"2023-09-19T12:26:56Z","title":"Task Graph offloading via Deep Reinforcement Learning in Mobile Edge\n  Computing","summary":"  Various mobile applications that comprise dependent tasks are gaining\nwidespread popularity and are increasingly complex. These applications often\nhave low-latency requirements, resulting in a significant surge in demand for\ncomputing resources. With the emergence of mobile edge computing (MEC), it\nbecomes the most significant issue to offload the application tasks onto\nsmall-scale devices deployed at the edge of the mobile network for obtaining a\nhigh-quality user experience. However, since the environment of MEC is dynamic,\nmost existing works focusing on task graph offloading, which rely heavily on\nexpert knowledge or accurate analytical models, fail to fully adapt to such\nenvironmental changes, resulting in the reduction of user experience. This\npaper investigates the task graph offloading in MEC, considering the\ntime-varying computation capabilities of edge computing devices. To adapt to\nenvironmental changes, we model the task graph scheduling for computation\noffloading as a Markov Decision Process (MDP). Then, we design a deep\nreinforcement learning algorithm (SATA-DRL) to learn the task scheduling\nstrategy from the interaction with the environment, to improve user experience.\nExtensive simulations validate that SATA-DRL is superior to existing strategies\nin terms of reducing average makespan and deadline violation.\n","authors":["Jiagang Liu","Yun Mi","Xinyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.10569v3.pdf","comment":"13 figures"},{"id":"http://arxiv.org/abs/2305.06361v2","updated":"2023-10-09T06:35:46Z","published":"2023-05-10T14:20:34Z","title":"Efficient Training of Multi-task Combinarotial Neural Solver with\n  Multi-armed Bandits","summary":"  Efficiently training a multi-task neural solver for various combinatorial\noptimization problems (COPs) has been less studied so far. In this paper, we\npropose a general and efficient training paradigm based on multi-armed bandits\nto deliver a unified combinarotial multi-task neural solver. To this end, we\nresort to the theoretical loss decomposition for multiple tasks under an\nencoder-decoder framework, which enables more efficient training via proper\nbandit task-sampling algorithms through an intra-task influence matrix. Our\nmethod achieves much higher overall performance with either limited training\nbudgets or the same training epochs, compared to standard training schedules,\nwhich can be promising for advising efficient training of other multi-task\nlarge models. Additionally, the influence matrix can provide empirical evidence\nof some common practices in the area of learning to optimize, which in turn\nsupports the validity of our approach.\n","authors":["Chenguang Wang","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2305.06361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18355v2","updated":"2023-10-09T06:26:35Z","published":"2023-05-26T16:38:48Z","title":"An Efficient Membership Inference Attack for the Diffusion Model by\n  Proximal Initialization","summary":"  Recently, diffusion models have achieved remarkable success in generating\ntasks, including image and audio generation. However, like other generative\nmodels, diffusion models are prone to privacy issues. In this paper, we propose\nan efficient query-based membership inference attack (MIA), namely Proximal\nInitialization Attack (PIA), which utilizes groundtruth trajectory obtained by\n$\\epsilon$ initialized in $t=0$ and predicted point to infer memberships.\nExperimental results indicate that the proposed method can achieve competitive\nperformance with only two queries on both discrete-time and continuous-time\ndiffusion models. Moreover, previous works on the privacy of diffusion models\nhave focused on vision tasks without considering audio tasks. Therefore, we\nalso explore the robustness of diffusion models to MIA in the text-to-speech\n(TTS) task, which is an audio generation task. To the best of our knowledge,\nthis work is the first to study the robustness of diffusion models to MIA in\nthe TTS task. Experimental results indicate that models with mel-spectrogram\n(image-like) output are vulnerable to MIA, while models with audio output are\nrelatively robust to MIA. {Code is available at\n\\url{https://github.com/kong13661/PIA}}.\n","authors":["Fei Kong","Jinhao Duan","RuiPeng Ma","Hengtao Shen","Xiaofeng Zhu","Xiaoshuang Shi","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2305.18355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05430v1","updated":"2023-10-09T06:06:34Z","published":"2023-10-09T06:06:34Z","title":"Replication of Multi-agent Reinforcement Learning for the \"Hide and\n  Seek\" Problem","summary":"  Reinforcement learning generates policies based on reward functions and\nhyperparameters. Slight changes in these can significantly affect results. The\nlack of documentation and reproducibility in Reinforcement learning research\nmakes it difficult to replicate once-deduced strategies. While previous\nresearch has identified strategies using grounded maneuvers, there is limited\nwork in more complex environments. The agents in this study are simulated\nsimilarly to Open Al's hider and seek agents, in addition to a flying\nmechanism, enhancing their mobility, and expanding their range of possible\nactions and strategies. This added functionality improves the Hider agents to\ndevelop a chasing strategy from approximately 2 million steps to 1.6 million\nsteps and hiders\n","authors":["Haider Kamal","Muaz A. Niazi","Hammad Afzal"],"pdf_url":"https://arxiv.org/pdf/2310.05430v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2306.15749v3","updated":"2023-10-09T06:01:33Z","published":"2023-06-27T19:04:00Z","title":"To Spike or Not To Spike: A Digital Hardware Perspective on Deep\n  Learning Acceleration","summary":"  As deep learning models scale, they become increasingly competitive from\ndomains spanning computer vision to natural language processing; however, this\nhappens at the expense of efficiency since they require increasingly more\nmemory and computing power. The power efficiency of the biological brain\noutperforms the one of any large-scale deep learning (DL) model; thus,\nneuromorphic computing tries to mimic the brain operations, such as spike-based\ninformation processing, to improve the efficiency of DL models. Despite the\nbenefits of the brain, such as efficient information transmission, dense\nneuronal interconnects, and the co-location of computation and memory, the\navailable biological substrate has severely constrained the evolution of\nbiological brains. Electronic hardware does not have the same constraints;\ntherefore, while modeling spiking neural networks (SNNs) might uncover one\npiece of the puzzle, the design of efficient hardware backends for SNNs needs\nfurther investigation, potentially taking inspiration from the available work\ndone on the artificial neural networks (ANN s) side. As such, when is it wise\nto look at the brain while designing new hardware, and when should it be\nignored? To answer this question, we quantitatively compare the digital\nhardware acceleration techniques and platforms of ANNs and SNNs.\n","authors":["Fabrizio Ottati","Chang Gao","Qinyu Chen","Giovanni Brignone","Mario R. Casu","Jason K. Eshraghian","Luciano Lavagno"],"pdf_url":"https://arxiv.org/pdf/2306.15749v3.pdf","comment":"Accepted to IEEE Journal on Emerging and Selected Topics in Circuits\n  and Systems"},{"id":"http://arxiv.org/abs/2308.07761v2","updated":"2023-10-09T05:46:26Z","published":"2023-08-15T13:29:14Z","title":"NeFL: Nested Federated Learning for Heterogeneous Clients","summary":"  Federated learning (FL) is a promising approach in distributed learning\nkeeping privacy. However, during the training pipeline of FL, slow or incapable\nclients (i.e., stragglers) slow down the total training time and degrade\nperformance. System heterogeneity, including heterogeneous computing and\nnetwork bandwidth, has been addressed to mitigate the impact of stragglers.\nPrevious studies tackle the system heterogeneity by splitting a model into\nsubmodels, but with less degree-of-freedom in terms of model architecture. We\npropose nested federated learning (NeFL), a generalized framework that\nefficiently divides a model into submodels using both depthwise and widthwise\nscaling. NeFL is implemented by interpreting forward propagation of models as\nsolving ordinary differential equations (ODEs) with adaptive step sizes. To\naddress the inconsistency that arises when training multiple submodels of\ndifferent architecture, we decouple a few parameters from parameters being\ntrained for each submodel. NeFL enables resource-constrained clients to\neffectively join the FL pipeline and the model to be trained with a larger\namount of data. Through a series of experiments, we demonstrate that NeFL leads\nto significant performance gains, especially for the worst-case submodel.\nFurthermore, we demonstrate NeFL aligns with recent studies in FL, regarding\npre-trained models of FL and the statistical heterogeneity.\n","authors":["Honggu Kang","Seohyeon Cha","Jinwoo Shin","Jongmyeong Lee","Joonhyuk Kang"],"pdf_url":"https://arxiv.org/pdf/2308.07761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05422v1","updated":"2023-10-09T05:37:58Z","published":"2023-10-09T05:37:58Z","title":"Reward-Consistent Dynamics Models are Strongly Generalizable for Offline\n  Reinforcement Learning","summary":"  Learning a precise dynamics model can be crucial for offline reinforcement\nlearning, which, unfortunately, has been found to be quite challenging.\nDynamics models that are learned by fitting historical transitions often\nstruggle to generalize to unseen transitions. In this study, we identify a\nhidden but pivotal factor termed dynamics reward that remains consistent across\ntransitions, offering a pathway to better generalization. Therefore, we propose\nthe idea of reward-consistent dynamics models: any trajectory generated by the\ndynamics model should maximize the dynamics reward derived from the data. We\nimplement this idea as the MOREC (Model-based Offline reinforcement learning\nwith Reward Consistency) method, which can be seamlessly integrated into\nprevious offline model-based reinforcement learning (MBRL) methods. MOREC\nlearns a generalizable dynamics reward function from offline data, which is\nsubsequently employed as a transition filter in any offline MBRL method: when\ngenerating transitions, the dynamics model generates a batch of transitions and\nselects the one with the highest dynamics reward value. On a synthetic task, we\nvisualize that MOREC has a strong generalization ability and can surprisingly\nrecover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL\nbenchmarks, MOREC improves the previous state-of-the-art performance by a\nsignificant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably,\nMOREC is the first method that can achieve above 95% online RL performance in 6\nout of 12 D4RL tasks and 3 out of 9 NeoRL tasks.\n","authors":["Fan-Ming Luo","Tian Xu","Xingchen Cao","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05421v1","updated":"2023-10-09T05:35:10Z","published":"2023-10-09T05:35:10Z","title":"Automating Customer Service using LangChain: Building custom open-source\n  GPT Chatbot for organizations","summary":"  In the digital age, the dynamics of customer service are evolving, driven by\ntechnological advancements and the integration of Large Language Models (LLMs).\nThis research paper introduces a groundbreaking approach to automating customer\nservice using LangChain, a custom LLM tailored for organizations. The paper\nexplores the obsolescence of traditional customer support techniques,\nparticularly Frequently Asked Questions (FAQs), and proposes a paradigm shift\ntowards responsive, context-aware, and personalized customer interactions. The\nheart of this innovation lies in the fusion of open-source methodologies, web\nscraping, fine-tuning, and the seamless integration of LangChain into customer\nservice platforms. This open-source state-of-the-art framework, presented as\n\"Sahaay,\" demonstrates the ability to scale across industries and\norganizations, offering real-time support and query resolution. Key elements of\nthis research encompass data collection via web scraping, the role of\nembeddings, the utilization of Google's Flan T5 XXL, Base and Small language\nmodels for knowledge retrieval, and the integration of the chatbot into\ncustomer service platforms. The results section provides insights into their\nperformance and use cases, here particularly within an educational institution.\nThis research heralds a new era in customer service, where technology is\nharnessed to create efficient, personalized, and responsive interactions.\nSahaay, powered by LangChain, redefines the customer-company relationship,\nelevating customer retention, value extraction, and brand image. As\norganizations embrace LLMs, customer service becomes a dynamic and\ncustomer-centric ecosystem.\n","authors":["Keivalya Pandya","Mehfuza Holia"],"pdf_url":"https://arxiv.org/pdf/2310.05421v1.pdf","comment":"4 pages, 2 figures, Submitted to appear in the Proceedings of the 3rd\n  International Conference on Women in Science & Technology Creating\n  Sustainable Career (ICWSTCSC 2023)"},{"id":"http://arxiv.org/abs/2211.15072v4","updated":"2023-10-09T05:19:22Z","published":"2022-11-28T05:16:20Z","title":"FaiREE: Fair Classification with Finite-Sample and Distribution-Free\n  Guarantee","summary":"  Algorithmic fairness plays an increasingly critical role in machine learning\nresearch. Several group fairness notions and algorithms have been proposed.\nHowever, the fairness guarantee of existing fair classification methods mainly\ndepends on specific data distributional assumptions, often requiring large\nsample sizes, and fairness could be violated when there is a modest number of\nsamples, which is often the case in practice. In this paper, we propose FaiREE,\na fair classification algorithm that can satisfy group fairness constraints\nwith finite-sample and distribution-free theoretical guarantees. FaiREE can be\nadapted to satisfy various group fairness notions (e.g., Equality of\nOpportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal\naccuracy. These theoretical guarantees are further supported by experiments on\nboth synthetic and real data. FaiREE is shown to have favorable performance\nover state-of-the-art algorithms.\n","authors":["Puheng Li","James Zou","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.15072v4.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.02519v2","updated":"2023-10-09T05:15:03Z","published":"2023-10-04T01:34:36Z","title":"Parameterized Convex Minorant for Objective Function Approximation in\n  Amortized Optimization","summary":"  Parameterized convex minorant (PCM) method is proposed for the approximation\nof the objective function in amortized optimization. In the proposed method,\nthe objective function approximator is expressed by the sum of a PCM and a\nnonnegative gap function, where the objective function approximator is bounded\nfrom below by the PCM convex in the optimization variable. The proposed\nobjective function approximator is a universal approximator for continuous\nfunctions, and the global minimizer of the PCM attains the global minimum of\nthe objective function approximator. Therefore, the global minimizer of the\nobjective function approximator can be obtained by a single convex\noptimization. As a realization of the proposed method, extended parameterized\nlog-sum-exp network is proposed by utilizing a parameterized log-sum-exp\nnetwork as the PCM. Numerical simulation is performed for\nnon-parameterized-convex objective function approximation and for\nlearning-based nonlinear model predictive control to demonstrate the\nperformance and characteristics of the proposed method. The simulation results\nsupport that the proposed method can be used to learn objective functions and\nto find the global minimizer reliably and quickly by using convex optimization\nalgorithms.\n","authors":["Jinrae Kim","Youdan Kim"],"pdf_url":"https://arxiv.org/pdf/2310.02519v2.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05401v1","updated":"2023-10-09T04:40:20Z","published":"2023-10-09T04:40:20Z","title":"Entropy-MCMC: Sampling from Flat Basins with Ease","summary":"  Bayesian deep learning counts on the quality of posterior distribution\nestimation. However, the posterior of deep neural networks is highly\nmulti-modal in nature, with local modes exhibiting varying generalization\nperformance. Given a practical budget, sampling from the original posterior can\nlead to suboptimal performance, as some samples may become trapped in \"bad\"\nmodes and suffer from overfitting. Leveraging the observation that \"good\" modes\nwith low generalization error often reside in flat basins of the energy\nlandscape, we propose to bias sampling on the posterior toward these flat\nregions. Specifically, we introduce an auxiliary guiding variable, the\nstationary distribution of which resembles a smoothed posterior free from sharp\nmodes, to lead the MCMC sampler to flat basins. By integrating this guiding\nvariable with the model parameter, we create a simple joint distribution that\nenables efficient sampling with minimal computational overhead. We prove the\nconvergence of our method and further show that it converges faster than\nseveral existing flatness-aware methods in the strongly convex setting.\nEmpirical results demonstrate that our method can successfully sample from flat\nbasins of the posterior, and outperforms all compared baselines on multiple\nbenchmarks including classification, calibration, and out-of-distribution\ndetection.\n","authors":["Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05397v1","updated":"2023-10-09T04:23:11Z","published":"2023-10-09T04:23:11Z","title":"Find Your Optimal Assignments On-the-fly: A Holistic Framework for\n  Clustered Federated Learning","summary":"  Federated Learning (FL) is an emerging distributed machine learning approach\nthat preserves client privacy by storing data on edge devices. However, data\nheterogeneity among clients presents challenges in training models that perform\nwell on all local distributions. Recent studies have proposed clustering as a\nsolution to tackle client heterogeneity in FL by grouping clients with\ndistribution shifts into different clusters. However, the diverse learning\nframeworks used in current clustered FL methods make it challenging to\nintegrate various clustered FL methods, gather their benefits, and make further\nimprovements.\n  To this end, this paper presents a comprehensive investigation into current\nclustered FL methods and proposes a four-tier framework, namely HCFL, to\nencompass and extend existing approaches. Based on the HCFL, we identify the\nremaining challenges associated with current clustering methods in each tier\nand propose an enhanced clustering method called HCFL+ to address these\nchallenges. Through extensive numerical evaluations, we showcase the\neffectiveness of our clustering framework and the improved components. Our code\nwill be publicly available.\n","authors":["Yongxin Guo","Xiaoying Tang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12379v3","updated":"2023-10-09T04:22:46Z","published":"2023-01-29T06:50:45Z","title":"FedRC: Tackling Diverse Distribution Shifts Challenge in Federated\n  Learning by Robust Clustering","summary":"  Federated Learning (FL) is a machine learning paradigm that safeguards\nprivacy by retaining client data on edge devices. However, optimizing FL in\npractice can be challenging due to the diverse and heterogeneous nature of the\nlearning system. Though recent research has focused on improving the\noptimization of FL when distribution shifts occur among clients, ensuring\nglobal performance when multiple types of distribution shifts occur\nsimultaneously among clients -- such as feature distribution shift, label\ndistribution shift, and concept shift -- remain under-explored.\n  In this paper, we identify the learning challenges posed by the simultaneous\noccurrence of diverse distribution shifts and propose a clustering principle to\novercome these challenges. Through our research, we find that existing methods\nfailed to address the clustering principle. Therefore, we propose a novel\nclustering algorithm framework, dubbed as FedRC, which adheres to our proposed\nclustering principle by incorporating a bi-level optimization problem and a\nnovel objective function. Extensive experiments demonstrate that FedRC\nsignificantly outperforms other SOTA cluster-based FL methods. Our code will be\npublicly available.\n","authors":["Yongxin Guo","Xiaoying Tang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2301.12379v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05395v1","updated":"2023-10-09T04:19:27Z","published":"2023-10-09T04:19:27Z","title":"Robust Image Watermarking based on Cross-Attention and Invariant Domain\n  Learning","summary":"  Image watermarking involves embedding and extracting watermarks within a\ncover image, with deep learning approaches emerging to bolster generalization\nand robustness. Predominantly, current methods employ convolution and\nconcatenation for watermark embedding, while also integrating conceivable\naugmentation in the training process. This paper explores a robust image\nwatermarking methodology by harnessing cross-attention and invariant domain\nlearning, marking two novel, significant advancements. First, we design a\nwatermark embedding technique utilizing a multi-head cross attention mechanism,\nenabling information exchange between the cover image and watermark to identify\nsemantically suitable embedding locations. Second, we advocate for learning an\ninvariant domain representation that encapsulates both semantic and\nnoise-invariant information concerning the watermark, shedding light on\npromising avenues for enhancing image watermarking techniques.\n","authors":["Agnibh Dasgupta","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.05395v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.05835v1","updated":"2023-10-09T16:25:36Z","published":"2023-10-09T16:25:36Z","title":"Latent Wander: an Alternative Interface for Interactive and\n  Serendipitous Discovery of Large AV Archives","summary":"  Audiovisual (AV) archives are invaluable for holistically preserving the\npast. Unlike other forms, AV archives can be difficult to explore. This is not\nonly because of its complex modality and sheer volume but also the lack of\nappropriate interfaces beyond keyword search. The recent rise in text-to-video\nretrieval tasks in computer science opens the gate to accessing AV content more\nnaturally and semantically, able to map natural language descriptive sentences\nto matching videos. However, applications of this model are rarely seen. The\ncontribution of this work is threefold. First, working with RTS (T\\'el\\'evision\nSuisse Romande), we identified the key blockers in a real archive for\nimplementing such models. We built a functioning pipeline for encoding raw\narchive videos to the text-to-video feature vectors. Second, we designed and\nverified a method to encode and retrieve videos using emotionally abundant\ndescriptions not supported in the original model. Third, we proposed an initial\nprototype for immersive and interactive exploration of AV archives in a latent\nspace based on the previously mentioned encoding of videos.\n","authors":["Yuchen Yang","Linyida Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05825v1","updated":"2023-10-09T16:10:51Z","published":"2023-10-09T16:10:51Z","title":"Write What You Want: Applying Text-to-video Retrieval to Audiovisual\n  Archives","summary":"  Audiovisual (AV) archives, as an essential reservoir of our cultural assets,\nare suffering from the issue of accessibility. The complex nature of the medium\nitself made processing and interaction an open challenge still in the field of\ncomputer vision, multimodal learning, and human-computer interaction, as well\nas in culture and heritage. In recent years, with the raising of video\nretrieval tasks, methods in retrieving video content with natural language\n(text-to-video retrieval) gained quite some attention and have reached a\nperformance level where real-world application is on the horizon. Appealing as\nit may sound, such methods focus on retrieving videos using plain\nvisual-focused descriptions of what has happened in the video and finding\nvideos such as instructions. It is too early to say such methods would be the\nnew paradigms for accessing and encoding complex video content into\nhigh-dimensional data, but they are indeed innovative attempts and foundations\nto build future exploratory interfaces for AV archives (e.g. allow users to\nwrite stories and retrieve related snippets in the archive, or encoding video\ncontent at high-level for visualisation). This work filled the application gap\nby examining such text-to-video retrieval methods from an implementation point\nof view and proposed and verified a classifier-enhanced workflow to allow\nbetter results when dealing with in-situ queries that might have been different\nfrom the training dataset. Such a workflow is then applied to the real-world\narchive from T\\'el\\'evision Suisse Romande (RTS) to create a demo. At last, a\nhuman-centred evaluation is conducted to understand whether the text-to-video\nretrieval methods improve the overall experience of accessing AV archives.\n","authors":["Yuchen Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05804v1","updated":"2023-10-09T15:43:07Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05737v1","updated":"2023-10-09T14:10:29Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","Jos Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06496v2","updated":"2023-10-09T11:59:38Z","published":"2022-09-14T08:51:15Z","title":"CCOM-HuQin: an Annotated Multimodal Chinese Fiddle Performance Dataset","summary":"  HuQin is a family of traditional Chinese bowed string instruments. Playing\ntechniques(PTs) embodied in various playing styles add abundant emotional\ncoloring and aesthetic feelings to HuQin performance. The complex applied\ntechniques make HuQin music a challenging source for fundamental MIR tasks such\nas pitch analysis, transcription and score-audio alignment. In this paper, we\npresent a multimodal performance dataset of HuQin music that contains\naudio-visual recordings of 11,992 single PT clips and 57 annotated musical\npieces of classical excerpts. We systematically describe the HuQin PT taxonomy\nbased on musicological theory and practical use cases. Then we introduce the\ndataset creation methodology and highlight the annotation principles featuring\nPTs. We analyze the statistics in different aspects to demonstrate the variety\nof PTs played in HuQin subcategories and perform preliminary experiments to\nshow the potential applications of the dataset in various MIR tasks and\ncross-cultural music studies. Finally, we propose future work to be extended on\nthe dataset.\n","authors":["Yu Zhang","Ziya Zhou","Xiaobing Li","Feng Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2209.06496v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2305.18829v3","updated":"2023-10-09T11:59:31Z","published":"2023-05-30T08:23:06Z","title":"UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction","summary":"  Multi-camera 3D perception has emerged as a prominent research field in\nautonomous driving, offering a viable and cost-effective alternative to\nLiDAR-based solutions. The existing multi-camera algorithms primarily rely on\nmonocular 2D pre-training. However, the monocular 2D pre-training overlooks the\nspatial and temporal correlations among the multi-camera system. To address\nthis limitation, we propose the first multi-camera unified pre-training\nframework, called UniScene, which involves initially reconstructing the 3D\nscene as the foundational stage and subsequently fine-tuning the model on\ndownstream tasks. Specifically, we employ Occupancy as the general\nrepresentation for the 3D scene, enabling the model to grasp geometric priors\nof the surrounding world through pre-training. A significant benefit of\nUniScene is its capability to utilize a considerable volume of unlabeled\nimage-LiDAR pairs for pre-training purposes. The proposed multi-camera unified\npre-training framework demonstrates promising results in key tasks such as\nmulti-camera 3D object detection and surrounding semantic scene completion.\nWhen compared to monocular pre-training methods on the nuScenes dataset,\nUniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS\nfor multi-camera 3D object detection, as well as a 3% increase in mIoU for\nsurrounding semantic scene completion. By adopting our unified pre-training\nmethod, a 25% reduction in 3D training annotation costs can be achieved,\noffering significant practical value for the implementation of real-world\nautonomous driving. Codes are publicly available at\nhttps://github.com/chaytonmin/UniScene.\n","authors":["Chen Min","Liang Xiao","Dawei Zhao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2305.18829v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05589v1","updated":"2023-10-09T10:21:42Z","published":"2023-10-09T10:21:42Z","title":"DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) is a task that aims to link ambiguous\nmentions within multimodal contexts to referential entities in a multimodal\nknowledge base. Recent methods for MEL adopt a common framework: they first\ninteract and fuse the text and image to obtain representations of the mention\nand entity respectively, and then compute the similarity between them to\npredict the correct entity. However, these methods still suffer from two\nlimitations: first, as they fuse the features of text and image before\nmatching, they cannot fully exploit the fine-grained alignment relations\nbetween the mention and entity. Second, their alignment is static, leading to\nlow performance when dealing with complex and diverse data. To address these\nissues, we propose a novel framework called Dynamic Relation Interactive\nNetwork (DRIN) for MEL tasks. DRIN explicitly models four different types of\nalignment between a mention and entity and builds a dynamic Graph Convolutional\nNetwork (GCN) to dynamically select the corresponding alignment relations for\ndifferent input samples. Experiments on two datasets show that DRIN outperforms\nstate-of-the-art methods by a large margin, demonstrating the effectiveness of\nour approach.\n","authors":["Shangyu Xing","Fei Zhao","Zhen Wu","Chunhui Li","Jianbing Zhang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2310.05589v1.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.05395v1","updated":"2023-10-09T04:19:27Z","published":"2023-10-09T04:19:27Z","title":"Robust Image Watermarking based on Cross-Attention and Invariant Domain\n  Learning","summary":"  Image watermarking involves embedding and extracting watermarks within a\ncover image, with deep learning approaches emerging to bolster generalization\nand robustness. Predominantly, current methods employ convolution and\nconcatenation for watermark embedding, while also integrating conceivable\naugmentation in the training process. This paper explores a robust image\nwatermarking methodology by harnessing cross-attention and invariant domain\nlearning, marking two novel, significant advancements. First, we design a\nwatermark embedding technique utilizing a multi-head cross attention mechanism,\nenabling information exchange between the cover image and watermark to identify\nsemantically suitable embedding locations. Second, we advocate for learning an\ninvariant domain representation that encapsulates both semantic and\nnoise-invariant information concerning the watermark, shedding light on\npromising avenues for enhancing image watermarking techniques.\n","authors":["Agnibh Dasgupta","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.05395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06138v1","updated":"2023-10-09T20:32:49Z","published":"2023-10-09T20:32:49Z","title":"Layout Sequence Prediction From Noisy Mobile Modality","summary":"  Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.\n","authors":["Haichao Zhang","Yi Xu","Hongsheng Lu","Takayuki Shimizu","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2310.06138v1.pdf","comment":"In Proceedings of the 31st ACM International Conference on Multimedia\n  2023 (MM 23)"}]},"2023-10-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.03639v2","updated":"2023-10-08T23:28:50Z","published":"2023-10-05T16:11:14Z","title":"Evaluating Self-Supervised Speech Representations for Indigenous\n  American Languages","summary":"  The application of self-supervision to speech representation learning has\ngarnered significant interest in recent years, due to its scalability to large\namounts of unlabeled data. However, much progress, both in terms of\npre-training and downstream evaluation, has remained concentrated in\nmonolingual models that only consider English. Few models consider other\nlanguages, and even fewer consider indigenous ones. In our submission to the\nNew Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR\ncorpus for Quechua, an indigenous South American Language. We benchmark the\nefficacy of large SSL models on Quechua, along with 6 other indigenous\nlanguages such as Guarani and Bribri, on low-resource ASR. Our results show\nsurprisingly strong performance by state-of-the-art SSL models, showing the\npotential generalizability of large-scale models to real-world data.\n","authors":["Chih-Chen Chen","William Chen","Rodolfo Zevallos","John E. Ortega"],"pdf_url":"https://arxiv.org/pdf/2310.03639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13829v2","updated":"2023-10-08T23:00:33Z","published":"2023-05-23T08:51:08Z","title":"Learning from Mistakes via Interactive Study Assistant for Large\n  Language Models","summary":"  Large language models (LLMs) have shown promising capabilities to refine\ntheir generation based on feedback. However, LLM refinement based on feedback\nis not always robust and may produce incorrect answers. In this paper, we\npropose Large LAnguage Model (SALAM) to learn and correct from their mistakes.\nOur method introduces a study assistant agent to analyze mistakes and generate\nimprovement guidelines from the main LLM. During inference, it identifies\ncommon misunderstandings based on the mistake collections and provides\nguidelines for LLMs to help them avoid similar mistakes. We further finetune\nthe study assistant using imitation learning with successful feedback\ninteraction. Our experiments on two challenging frameworks (BBH and BBQ)\ndemonstrate that SALAM outperforms baselines by a margin of up to 10.7 in\naccuracy.\n","authors":["Danqing Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2305.13829v2.pdf","comment":"Accepted by EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.01352v2","updated":"2023-10-08T22:05:20Z","published":"2023-10-02T17:16:26Z","title":"RA-DIT: Retrieval-Augmented Dual Instruction Tuning","summary":"  Retrieval-augmented language models (RALMs) improve performance by accessing\nlong-tail and up-to-date knowledge from external data stores, but are\nchallenging to build. Existing approaches require either expensive\nretrieval-specific modifications to LM pre-training or use post-hoc integration\nof the data store that leads to suboptimal performance. We introduce\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\nmethodology that provides a third option by retrofitting any LLM with retrieval\ncapabilities. Our approach operates in two distinct fine-tuning steps: (1) one\nupdates a pre-trained LM to better use retrieved information, while (2) the\nother updates the retriever to return more relevant results, as preferred by\nthe LM. By fine-tuning over tasks that require both knowledge utilization and\ncontextual awareness, we demonstrate that each stage yields significant\nperformance improvements, and using both leads to additional gains. Our best\nmodel, RA-DIT 65B, achieves state-of-the-art performance across a range of\nknowledge-intensive zero- and few-shot learning benchmarks, significantly\noutperforming existing in-context RALM approaches by up to +8.9% in 0-shot\nsetting and +1.4% in 5-shot setting on average.\n","authors":["Xi Victoria Lin","Xilun Chen","Mingda Chen","Weijia Shi","Maria Lomeli","Rich James","Pedro Rodriguez","Jacob Kahn","Gergely Szilvasy","Mike Lewis","Luke Zettlemoyer","Scott Yih"],"pdf_url":"https://arxiv.org/pdf/2310.01352v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2308.01313v2","updated":"2023-10-08T21:56:53Z","published":"2023-08-02T17:57:25Z","title":"More Context, Less Distraction: Zero-shot Visual Classification by\n  Inferring and Conditioning on Contextual Attributes","summary":"  Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninterpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst\ngroup accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n","authors":["Bang An","Sicheng Zhu","Michael-Andrei Panaitescu-Liess","Chaithanya Kumar Mummadi","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2308.01313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05295v1","updated":"2023-10-08T21:45:34Z","published":"2023-10-08T21:45:34Z","title":"Visual Storytelling with Question-Answer Plans","summary":"  Visual storytelling aims to generate compelling narratives from image\nsequences. Existing models often focus on enhancing the representation of the\nimage sequence, e.g., with external knowledge sources or advanced graph\nstructures. Despite recent progress, the stories are often repetitive,\nillogical, and lacking in detail. To mitigate these issues, we present a novel\nframework which integrates visual representations with pretrained language\nmodels and planning. Our model translates the image sequence into a visual\nprefix, a sequence of continuous embeddings which language models can\ninterpret. It also leverages a sequence of question-answer pairs as a blueprint\nplan for selecting salient visual concepts and determining how they should be\nassembled into a narrative. Automatic and human evaluation on the VIST\nbenchmark (Huang et al., 2016) demonstrates that blueprint-based models\ngenerate stories that are more coherent, interesting, and natural compared to\ncompetitive baselines and state-of-the-art systems.\n","authors":["Danyang Liu","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2310.05295v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2306.08804v2","updated":"2023-10-08T21:44:47Z","published":"2023-06-15T01:18:02Z","title":"PEACE: Cross-Platform Hate Speech Detection- A Causality-guided\n  Framework","summary":"  Hate speech detection refers to the task of detecting hateful content that\naims at denigrating an individual or a group based on their religion, gender,\nsexual orientation, or other characteristics. Due to the different policies of\nthe platforms, different groups of people express hate in different ways.\nFurthermore, due to the lack of labeled data in some platforms it becomes\nchallenging to build hate speech detection models. To this end, we revisit if\nwe can learn a generalizable hate speech detection model for the cross platform\nsetting, where we train the model on the data from one (source) platform and\ngeneralize the model across multiple (target) platforms. Existing\ngeneralization models rely on linguistic cues or auxiliary information, making\nthem biased towards certain tags or certain kinds of words (e.g., abusive\nwords) on the source platform and thus not applicable to the target platforms.\nInspired by social and psychological theories, we endeavor to explore if there\nexist inherent causal cues that can be leveraged to learn generalizable\nrepresentations for detecting hate speech across these distribution shifts. To\nthis end, we propose a causality-guided framework, PEACE, that identifies and\nleverages two intrinsic causal cues omnipresent in hateful content: the overall\nsentiment and the aggression in the text. We conduct extensive experiments\nacross multiple platforms (representing the distribution shift) showing if\ncausal cues can help cross-platform generalization.\n","authors":["Paras Sheth","Tharindu Kumarage","Raha Moraffah","Aman Chadha","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2306.08804v2.pdf","comment":"ECML PKDD 2023"},{"id":"http://arxiv.org/abs/2310.05294v1","updated":"2023-10-08T21:44:00Z","published":"2023-10-08T21:44:00Z","title":"Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation\n  with the GeNTE Corpus","summary":"  Gender inequality is embedded in our communication practices and perpetuated\nin translation technologies. This becomes particularly apparent when\ntranslating into grammatical gender languages, where machine translation (MT)\noften defaults to masculine and stereotypical representations by making undue\nbinary gender assumptions. Our work addresses the rising demand for inclusive\nlanguage by focusing head-on on gender-neutral translation from English to\nItalian. We start from the essentials: proposing a dedicated benchmark and\nexploring automated evaluation methods. First, we introduce GeNTE, a natural,\nbilingual test set for gender-neutral translation, whose creation was informed\nby a survey on the perception and use of neutral language. Based on GeNTE, we\nthen overview existing reference-based evaluation approaches, highlight their\nlimits, and propose a reference-free method more suitable to assess\ngender-neutral translation.\n","authors":["Andrea Piergentili","Beatrice Savoldi","Dennis Fucci","Matteo Negri","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2310.05294v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.02229v2","updated":"2023-10-08T21:17:54Z","published":"2023-10-03T17:37:22Z","title":"Extraction of Medication and Temporal Relation from Clinical Text using\n  Neural Language Models","summary":"  Clinical texts, represented in electronic medical records (EMRs), contain\nrich medical information and are essential for disease prediction, personalised\ninformation recommendation, clinical decision support, and medication pattern\nmining and measurement. Relation extractions between medication mentions and\ntemporal information can further help clinicians better understand the\npatients' treatment history. To evaluate the performances of deep learning (DL)\nand large language models (LLMs) in medication extraction and temporal\nrelations classification, we carry out an empirical investigation of\n\\textbf{MedTem} project using several advanced learning structures including\nBiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),\nand BERT-CNN for temporal relation extraction (RE), in addition to the\nexploration of different word embedding techniques. Furthermore, we also\ndesigned a set of post-processing roles to generate structured output on\nmedications and the temporal relation. Our experiments show that CNN-BiLSTM\nslightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding\n75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro\nAverage. BERT-CNN model also produced reasonable evaluation scores 64.48,\n67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction\ntest set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted\nat \\url{https://github.com/HECTA-UoM/MedTem}\n","authors":["Hangyu Tu","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2310.02229v2.pdf","comment":"working paper"},{"id":"http://arxiv.org/abs/2310.05280v1","updated":"2023-10-08T21:03:18Z","published":"2023-10-08T21:03:18Z","title":"Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona\n  Biases in Dialogue Systems","summary":"  Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. Generic personas refer to an individual from a demographic group\n(e.g. an Asian person), whereas specific personas can be actual names of\nhistorical figures. While the adoption of personas allows dialogue systems to\nbe more engaging and approachable to users, it also carries the potential risk\nof exacerbating social biases in model responses, further causing societal\nharms through interactions with users. In this paper, we systematically study\n\"persona biases\", which we define to be the sensitivity of harmful dialogue\nmodel behaviors to different persona adoptions. We categorize persona biases\ninto biases in harmful expression and harmful agreement, as well as establish a\ncomprehensive evaluation framework to measure persona biases in five aspects:\nOffensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic\nAgreement. Additionally, we propose to comprehensively investigate persona\nbiases through experimenting with UniversalPersona, a systematized persona\ndataset with a comprehensive list of both generic and specific model personas.\nThrough benchmarking on four different models, including Blender, ChatGPT,\nAlpaca, and Vicuna, our study uncovers significant persona biases in these\ndialogue systems.Findings of our study underscores the immediate need to\nrevisit the use of persona traits in dialogue agents, to ensure their safe\napplication.\n","authors":["Yixin Wan","Jieyu Zhao","Nanyun Peng","Kai-Wei Chang","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2310.05280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05276v1","updated":"2023-10-08T20:33:55Z","published":"2023-10-08T20:33:55Z","title":"Enhancing Pre-Trained Language Models with Sentence Position Embeddings\n  for Rhetorical Roles Recognition in Legal Opinions","summary":"  The legal domain is a vast and complex field that involves a considerable\namount of text analysis, including laws, legal arguments, and legal opinions.\nLegal practitioners must analyze these texts to understand legal cases,\nresearch legal precedents, and prepare legal documents. The size of legal\nopinions continues to grow, making it increasingly challenging to develop a\nmodel that can accurately predict the rhetorical roles of legal opinions given\ntheir complexity and diversity. In this research paper, we propose a novel\nmodel architecture for automatically predicting rhetorical roles using\npre-trained language models (PLMs) enhanced with knowledge of sentence position\ninformation within a document. Based on an annotated corpus from the\nLegalEval@SemEval2023 competition, we demonstrate that our approach requires\nfewer parameters, resulting in lower computational costs when compared to\ncomplex architectures employing a hierarchical model in a global-context, yet\nit achieves great performance. Moreover, we show that adding more attention to\na hierarchical model based only on BERT in the local-context, along with\nincorporating sentence position information, enhances the results.\n","authors":["Anas Belfathi","Nicolas Hernandez","Laura Monceaux"],"pdf_url":"https://arxiv.org/pdf/2310.05276v1.pdf","comment":"Workshop on Automated Semantic Analysis of Information in Legal Text"},{"id":"http://arxiv.org/abs/2104.04844v4","updated":"2023-10-08T19:22:20Z","published":"2021-04-10T19:20:55Z","title":"On migration to Perpetual Enterprise System","summary":"  This document describes a pragmatic approach on how to migrate an\norganisation computer system towards a new system that could evolve forever,\naddresses the whole organisation and it is integrated.\n  Governance aspects are as important, if not more, than purely technical IT\naspects: human resources, call for tenders, and similar. Migration implies that\none is not starting from a green field.\n","authors":["Manuel Tomas Carrasco Benitez"],"pdf_url":"https://arxiv.org/pdf/2104.04844v4.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2305.14493v2","updated":"2023-10-08T18:45:10Z","published":"2023-05-23T19:45:45Z","title":"Prompt position really matters in few-shot and zero-shot NLU tasks","summary":"  Prompt-based models have made remarkable advancements in the fields of\nzero-shot and few-shot learning, attracting a lot of attention from\nresearchers. Developing an effective prompt template plays a critical role.\nHowever, prior studies have mainly focused on prompt vocabulary selection or\nembedding initialization with the reserved prompt position fixed. In this\nempirical study, we conduct the most comprehensive analysis to date of prompt\nposition option for natural language understanding tasks. Our findings quantify\nthe substantial impact prompt position has on model performance. We observe\nthat the prompt position used in prior studies is often sub-optimal for both\nzero-shot and few-shot settings. These findings suggest prompt position\noptimisation as an interesting research direction alongside the existing focus\non prompt engineering.\n","authors":["Junyu Mao","Stuart E. Middleton","Mahesan Niranjan"],"pdf_url":"https://arxiv.org/pdf/2305.14493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13993v2","updated":"2023-10-08T18:15:20Z","published":"2023-05-23T12:21:38Z","title":"Condensing Multilingual Knowledge with Lightweight Language-Specific\n  Modules","summary":"  Incorporating language-specific (LS) modules is a proven method to boost\nperformance in multilingual machine translation. This approach bears similarity\nto Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the\nscalability of this approach to hundreds of languages (experts) tends to be\nunmanageable due to the prohibitive number of parameters introduced by\nfull-rank matrices in fully-connected layers. In this work, we introduce the\nLanguage-Specific Matrix Synthesis (LMS) method. This approach constructs LS\nmodules by generating low-rank matrices from two significantly smaller matrices\nto approximate the full-rank matrix. Furthermore, we condense multilingual\nknowledge from multiple LS modules into a single shared module with the Fuse\nDistillation (FD) technique to improve the efficiency of inference and model\nserialization. We show that our LMS method significantly outperforms previous\nLS methods and MoE methods with the same amount of extra parameters, e.g., 1.73\nBLEU points over the Switch Transformer on many-to-many multilingual machine\ntranslation. Importantly, LMS is able to have comparable translation\nperformance with much fewer parameters.\n","authors":["Haoran Xu","Weiting Tan","Shuyue Stella Li","Yunmo Chen","Benjamin Van Durme","Philipp Koehn","Kenton Murray"],"pdf_url":"https://arxiv.org/pdf/2305.13993v2.pdf","comment":"Accepted at the main conference of EMNLP 2023"},{"id":"http://arxiv.org/abs/2306.02105v2","updated":"2023-10-08T18:07:56Z","published":"2023-06-03T13:11:37Z","title":"Adapting Pretrained ASR Models to Low-resource Clinical Speech using\n  Epistemic Uncertainty-based Data Selection","summary":"  While there has been significant progress in ASR, African-accented clinical\nASR has been understudied due to a lack of training datasets. Building robust\nASR systems in this domain requires large amounts of annotated or labeled data,\nfor a wide variety of linguistically and morphologically rich accents, which\nare expensive to create. Our study aims to address this problem by reducing\nannotation expenses through informative uncertainty-based data selection. We\nshow that incorporating epistemic uncertainty into our adaptation rounds\noutperforms several baseline results, established using state-of-the-art (SOTA)\nASR models, while reducing the required amount of labeled data, and hence\nreducing annotation costs. Our approach also improves out-of-distribution\ngeneralization for very low-resource accents, demonstrating the viability of\nour approach for building generalizable ASR models in the context of accented\nAfrican clinical ASR, where training datasets are predominantly scarce.\n","authors":["Bonaventure F. P. Dossou","Atnafu Lambebo Tonja","Chris Chinenye Emezue","Tobi Olatunji","Naome A Etori","Salomey Osei","Tosin Adewumi","Sahib Singh"],"pdf_url":"https://arxiv.org/pdf/2306.02105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05253v1","updated":"2023-10-08T18:04:05Z","published":"2023-10-08T18:04:05Z","title":"Explainable Claim Verification via Knowledge-Grounded Reasoning with\n  Large Language Models","summary":"  Claim verification plays a crucial role in combating misinformation. While\nexisting works on claim verification have shown promising results, a crucial\npiece of the puzzle that remains unsolved is to understand how to verify claims\nwithout relying on human-annotated data, which is expensive to create at a\nlarge scale. Additionally, it is important for models to provide comprehensive\nexplanations that can justify their decisions and assist human fact-checkers.\nThis paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)\nReasoning that can verify complex claims and generate explanations without the\nneed for annotated evidence using Large Language Models (LLMs). FOLK leverages\nthe in-context learning ability of LLMs to translate the claim into a\nFirst-Order-Logic (FOL) clause consisting of predicates, each corresponding to\na sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning\nover a set of knowledge-grounded question-and-answer pairs to make veracity\npredictions and generate explanations to justify its decision-making process.\nThis process makes our model highly explanatory, providing clear explanations\nof its reasoning process in human-readable form. Our experiment results\nindicate that FOLK outperforms strong baselines on three datasets encompassing\nvarious claim verification challenges. Our code and data are available.\n","authors":["Haoran Wang","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2310.05253v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05242v1","updated":"2023-10-08T17:23:17Z","published":"2023-10-08T17:23:17Z","title":"ChatRadio-Valuer: A Chat Large Language Model for Generalizable\n  Radiology Report Generation Based on Multi-institution and Multi-system Data","summary":"  Radiology report generation, as a key step in medical image analysis, is\ncritical to the quantitative analysis of clinically informed decision-making\nlevels. However, complex and diverse radiology reports with cross-source\nheterogeneity pose a huge generalizability challenge to the current methods\nunder massive data volume, mainly because the style and normativity of\nradiology reports are obviously distinctive among institutions, body regions\ninspected and radiologists. Recently, the advent of large language models (LLM)\noffers great potential for recognizing signs of health conditions. To resolve\nthe above problem, we collaborate with the Second Xiangya Hospital in China and\npropose ChatRadio-Valuer based on the LLM, a tailored model for automatic\nradiology report generation that learns generalizable representations and\nprovides a basis pattern for model adaptation in sophisticated analysts' cases.\nSpecifically, ChatRadio-Valuer is trained based on the radiology reports from a\nsingle institution by means of supervised fine-tuning, and then adapted to\ndisease diagnosis tasks for human multi-system evaluation (i.e., chest,\nabdomen, muscle-skeleton, head, and maxillofacial $\\&$ neck) from six different\ninstitutions in clinical-level events. The clinical dataset utilized in this\nstudy encompasses a remarkable total of \\textbf{332,673} observations. From the\ncomprehensive results on engineering indicators, clinical efficacy and\ndeployment cost metrics, it can be shown that ChatRadio-Valuer consistently\noutperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and\nGPT-4 et al., in terms of the diseases diagnosis from radiology reports.\nChatRadio-Valuer provides an effective avenue to boost model generalization\nperformance and alleviate the annotation workload of experts to enable the\npromotion of clinical AI applications in radiology reports.\n","authors":["Tianyang Zhong","Wei Zhao","Yutong Zhang","Yi Pan","Peixin Dong","Zuowei Jiang","Xiaoyan Kui","Youlan Shang","Li Yang","Yaonai Wei","Longtao Yang","Hao Chen","Huan Zhao","Yuxiao Liu","Ning Zhu","Yiwei Li","Yisong Wang","Jiaqi Yao","Jiaqi Wang","Ying Zeng","Lei He","Chao Zheng","Zhixue Zhang","Ming Li","Zhengliang Liu","Haixing Dai","Zihao Wu","Lu Zhang","Shu Zhang","Xiaoyan Cai","Xintao Hu","Shijie Zhao","Xi Jiang","Xin Zhang","Xiang Li","Dajiang Zhu","Lei Guo","Dinggang Shen","Junwei Han","Tianming Liu","Jun Liu","Tuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05235v1","updated":"2023-10-08T17:05:00Z","published":"2023-10-08T17:05:00Z","title":"XLS-R fine-tuning on noisy word boundaries for unsupervised speech\n  segmentation into words","summary":"  Due to the absence of explicit word boundaries in the speech stream, the task\nof segmenting spoken sentences into word units without text supervision is\nparticularly challenging. In this work, we leverage the most recent\nself-supervised speech models that have proved to quickly adapt to new tasks\nthrough fine-tuning, even in low resource conditions. Taking inspiration from\nsemi-supervised learning, we fine-tune an XLS-R model to predict word\nboundaries themselves produced by top-tier speech segmentation systems: DPDP,\nVG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer\nnew word boundary labels that are used in turn for another fine-tuning step.\nOur method consistently improves the performance of each system and sets a new\nstate-of-the-art that is, on average 130% higher than the previous one as\nmeasured by the F1 score on correctly discovered word tokens on five corpora\nfeaturing different languages. Finally, our system can segment speech from\nlanguages unseen during fine-tuning in a zero-shot fashion.\n","authors":["Robin Algayres","Pablo Diego-Simon","Benoit Sagot","Emmanuel Dupoux"],"pdf_url":"https://arxiv.org/pdf/2310.05235v1.pdf","comment":"Findings at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05231v1","updated":"2023-10-08T17:00:04Z","published":"2023-10-08T17:00:04Z","title":"MindfulDiary: Harnessing Large Language Model to Support Psychiatric\n  Patients' Journaling","summary":"  In the mental health domain, Large Language Models (LLMs) offer promising new\nopportunities, though their inherent complexity and low controllability have\nraised questions about their suitability in clinical settings. We present\nMindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric\npatients document daily experiences through conversation. Designed in\ncollaboration with mental health professionals (MHPs), MindfulDiary takes a\nstate-based approach to safely comply with the experts' guidelines while\ncarrying on free-form conversations. Through a four-week field study involving\n28 patients with major depressive disorder and five psychiatrists, we found\nthat MindfulDiary supported patients in consistently enriching their daily\nrecords and helped psychiatrists better empathize with their patients through\nan understanding of their thoughts and daily contexts. Drawing on these\nfindings, we discuss the implications of leveraging LLMs in the mental health\ndomain, bridging the technical feasibility and their integration into clinical\nsettings.\n","authors":["Taewan Kim","Seolyeong Bae","Hyun Ah Kim","Su-woo Lee","Hwajung Hong","Chanmo Yang","Young-Ho Kim"],"pdf_url":"https://arxiv.org/pdf/2310.05231v1.pdf","comment":"21 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.05224v1","updated":"2023-10-08T16:46:14Z","published":"2023-10-08T16:46:14Z","title":"Generative Spoken Language Model based on continuous word-sized audio\n  tokens","summary":"  In NLP, text language models based on words or subwords are known to\noutperform their character-based counterparts. Yet, in the speech community,\nthe standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter\nthan a phoneme). Taking inspiration from word-based LM, we introduce a\nGenerative Spoken Language Model (GSLM) based on word-size continuous-valued\naudio embeddings that can generate diverse and expressive language output. This\nis obtained by replacing lookup table for lexical types with a Lexical\nEmbedding function, the cross entropy loss by a contrastive loss, and\nmultinomial sampling by k-NN sampling. The resulting model is the first\ngenerative language model based on word-size continuous embeddings. Its\nperformance is on par with discrete unit GSLMs regarding generation quality as\nmeasured by automatic metrics and subjective human judgements. Moreover, it is\nfive times more memory efficient thanks to its large 200ms units. In addition,\nthe embeddings before and after the Lexical Embedder are phonetically and\nsemantically interpretable.\n","authors":["Robin Algayres","Yossi Adi","Tu Anh Nguyen","Jade Copet","Gabriel Synnaeve","Benoit Sagot","Emmanuel Dupoux"],"pdf_url":"https://arxiv.org/pdf/2310.05224v1.pdf","comment":"Conference paper at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.00299v2","updated":"2023-10-08T16:22:26Z","published":"2023-09-30T08:15:36Z","title":"RelBERT: Embedding Relations with Language Models","summary":"  Many applications need access to background knowledge about how different\nconcepts and entities are related. Although Knowledge Graphs (KG) and Large\nLanguage Models (LLM) can address this need to some extent, KGs are inevitably\nincomplete and their relational schema is often too coarse-grained, while LLMs\nare inefficient and difficult to control. As an alternative, we propose to\nextract relation embeddings from relatively small language models. In\nparticular, we show that masked language models such as RoBERTa can be\nstraightforwardly fine-tuned for this purpose, using only a small amount of\ntraining data. The resulting model, which we call RelBERT, captures relational\nsimilarity in a surprisingly fine-grained way, allowing us to set a new\nstate-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of\nmodelling relations that go well beyond what the model has seen during\ntraining. For instance, we obtained strong results on relations between named\nentities with a model that was only trained on lexical relations between\nconcepts, and we observed that RelBERT can recognise morphological analogies\ndespite not being trained on such examples. Overall, we find that RelBERT\nsignificantly outperforms strategies based on prompting language models that\nare several orders of magnitude larger, including recent GPT-based models and\nopen source models.\n","authors":["Asahi Ushio","Jose Camacho-Collados","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2310.00299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05216v1","updated":"2023-10-08T16:16:21Z","published":"2023-10-08T16:16:21Z","title":"Probing Language Models from A Human Behavioral Perspective","summary":"  Large Language Models (LLMs) have emerged as dominant foundational models in\nmodern NLP. However, the understanding of their prediction process and internal\nmechanisms, such as feed-forward networks and multi-head self-attention,\nremains largely unexplored. In this study, we probe LLMs from a human\nbehavioral perspective, correlating values from LLMs with eye-tracking\nmeasures, which are widely recognized as meaningful indicators of reading\npatterns. Our findings reveal that LLMs exhibit a prediction pattern distinct\nfrom that of RNN-based LMs. Moreover, with the escalation of FFN layers, the\ncapacity for memorization and linguistic knowledge encoding also surges until\nit peaks, subsequently pivoting to focus on comprehension capacity. The\nfunctions of self-attention are distributed across multiple heads. Lastly, we\nscrutinize the gate mechanisms, finding that they control the flow of\ninformation, with some gates promoting, while others eliminating information.\n","authors":["Xintong Wang","Xiaoyu Li","Xingshan Li","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2310.05216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05209v1","updated":"2023-10-08T15:50:36Z","published":"2023-10-08T15:50:36Z","title":"Scaling Laws of RoPE-based Extrapolation","summary":"  The extrapolation capability of Large Language Models (LLMs) based on Rotary\nPosition Embedding is currently a topic of considerable interest. The\nmainstream approach to addressing extrapolation with LLMs involves modifying\nRoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the\noriginal RoPE, with a larger value and providing longer fine-tuning text. In\nthis work, we first observe that fine-tuning a RoPE-based LLM with either a\nsmaller or larger base in pre-training context length could significantly\nenhance its extrapolation performance. After that, we propose\n\\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework\nfrom the periodic perspective, to describe the relationship between the\nextrapolation performance and base value as well as tuning context length. In\nthis process, we also explain the origin of the RoPE-based extrapolation issue\nby \\textbf{\\textit{critical dimension for extrapolation}}. Besides these\nobservations and analyses, we achieve extrapolation up to 1 million context\nlength within only 16K training length on LLaMA2 7B and 13B.\n","authors":["Xiaoran Liu","Hang Yan","Shuo Zhang","Chenxin An","Xipeng Qiu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05209v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2309.15857v2","updated":"2023-10-08T15:32:51Z","published":"2023-09-23T15:21:15Z","title":"A Survey on Image-text Multimodal Models","summary":"  Amidst the evolving landscape of artificial intelligence, the convergence of\nvisual and textual information has surfaced as a crucial frontier, leading to\nthe advent of image-text multimodal models. This paper provides a comprehensive\nreview of the evolution and current state of image-text multimodal models,\nexploring their application value, challenges, and potential research\ntrajectories. Initially, we revisit the basic concepts and developmental\nmilestones of these models, introducing a novel classification that segments\ntheir evolution into three distinct phases, based on their time of introduction\nand subsequent impact on the discipline. Furthermore, based on the tasks'\nsignificance and prevalence in the academic landscape, we propose a\ncategorization of the tasks associated with image-text multimodal models into\nfive major types, elucidating the recent progress and key technologies within\neach category. Despite the remarkable accomplishments of these models, numerous\nchallenges and issues persist. This paper delves into the inherent challenges\nand limitations of image-text multimodal models, fostering the exploration of\nprospective research directions. Our objective is to offer an exhaustive\noverview of the present research landscape of image-text multimodal models and\nto serve as a valuable reference for future scholarly endeavors. We extend an\ninvitation to the broader community to collaborate in enhancing the image-text\nmultimodal model community, accessible at:\n\\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.\n","authors":["Ruifeng Guo","Jingxuan Wei","Linzhuang Sun","Bihui Yu","Guiyong Chang","Dawei Liu","Sibo Zhang","Zhengbing Yao","Mingjun Xu","Liping Bu"],"pdf_url":"https://arxiv.org/pdf/2309.15857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05203v1","updated":"2023-10-08T15:30:44Z","published":"2023-10-08T15:30:44Z","title":"A Comparative Study of Voice Conversion Models with Large-Scale Speech\n  and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge\n  2023","summary":"  This paper presents our systems (denoted as T13) for the singing voice\nconversion challenge (SVCC) 2023. For both in-domain and cross-domain English\nsinging voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a\nrecognition-synthesis approach with self-supervised learning-based\nrepresentation. To achieve data-efficient SVC with a limited amount of target\nsinger/speaker's data (150 to 160 utterances for SVCC 2023), we first train a\ndiffusion-based any-to-any voice conversion model using publicly available\nlarge-scale 750 hours of speech and singing data. Then, we finetune the model\nfor each target singer/speaker of Task 1 and Task 2. Large-scale listening\ntests conducted by SVCC 2023 show that our T13 system achieves competitive\nnaturalness and speaker similarity for the harder cross-domain SVC (Task 2),\nwhich implies the generalization ability of our proposed method. Our objective\nevaluation results show that using large datasets is particularly beneficial\nfor cross-domain SVC.\n","authors":["Ryuichi Yamamoto","Reo Yoneyama","Lester Phillip Violeta","Wen-Chin Huang","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2310.05203v1.pdf","comment":"Accepted to ASRU 2023"},{"id":"http://arxiv.org/abs/2310.05199v1","updated":"2023-10-08T15:14:39Z","published":"2023-10-08T15:14:39Z","title":"Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning\n  from Human Feedback","summary":"  Reinforcement learning from human feedback serves as a crucial bridge,\naligning large language models with human and societal values. This alignment\nrequires a vast corpus of human feedback to learn a reward model, which is\nsubsequently used to finetune language models. However, we have identified that\nthe reward model often finds shortcuts to bypass its intended objectives,\nmisleadingly assuming that humans prefer longer responses. The emergence of\nlength bias often induces the model to favor longer outputs, yet it doesn't\nequate to an increase in helpful information within these outputs. In this\npaper, we propose an innovative solution, applying the Product-of-Experts (PoE)\ntechnique to separate reward modeling from the influence of sequence length. In\nour framework, the main expert concentrates on understanding human intents,\nwhile the biased expert targets the identification and capture of length bias.\nTo further enhance the learning of bias, we introduce perturbations into the\nbias-focused expert, disrupting the flow of semantic information. Experimental\nresults validate the effectiveness of our approach, indicating that language\nmodel performance is improved, irrespective of sequence length.\n","authors":["Wei Shen","Rui Zheng","Wenyu Zhan","Jun Zhao","Shihan Dou","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05191v1","updated":"2023-10-08T15:00:04Z","published":"2023-10-08T15:00:04Z","title":"FABRIC: Automated Scoring and Feedback Generation for Essays","summary":"  Automated essay scoring (AES) provides a useful tool for students and\ninstructors in writing classes by generating essay scores in real-time.\nHowever, previous AES models do not provide more specific rubric-based scores\nnor feedback on how to improve the essays, which can be even more important\nthan the overall scores for learning. We present FABRIC, a pipeline to help\nstudents and instructors in English writing classes by automatically generating\n1) the overall scores, 2) specific rubric-based scores, and 3) detailed\nfeedback on how to improve the essays. Under the guidance of English education\nexperts, we chose the rubrics for the specific scores as content, organization,\nand language. The first component of the FABRIC pipeline is DREsS, a real-world\nDataset for Rubric-based Essay Scoring (DREsS). The second component is CASE, a\nCorruption-based Augmentation Strategy for Essays, with which we can improve\nthe accuracy of the baseline model by 45.44%. The third component is EssayCoT,\nthe Essay Chain-of-Thought prompting strategy which uses scores predicted from\nthe AES model to generate better feedback. We evaluate the effectiveness of the\nnew dataset DREsS and the augmentation strategy CASE quantitatively and show\nsignificant improvements over the models trained with existing datasets. We\nevaluate the feedback generated by EssayCoT with English education experts to\nshow significant improvements in the helpfulness of the feedback across all\nrubrics. Lastly, we evaluate the FABRIC pipeline with students in a college\nEnglish writing class who rated the generated scores and feedback with an\naverage of 6 on the Likert scale from 1 to 7.\n","authors":["Jieun Han","Haneul Yoo","Junho Myung","Minsun Kim","Hyunseung Lim","Yoonsu Kim","Tak Yeon Lee","Hwajung Hong","Juho Kim","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2310.05191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05189v1","updated":"2023-10-08T14:55:02Z","published":"2023-10-08T14:55:02Z","title":"Factuality Challenges in the Era of Large Language Models","summary":"  The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations\". Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.\n","authors":["Isabelle Augenstein","Timothy Baldwin","Meeyoung Cha","Tanmoy Chakraborty","Giovanni Luca Ciampaglia","David Corney","Renee DiResta","Emilio Ferrara","Scott Hale","Alon Halevy","Eduard Hovy","Heng Ji","Filippo Menczer","Ruben Miguez","Preslav Nakov","Dietram Scheufele","Shivam Sharma","Giovanni Zagni"],"pdf_url":"https://arxiv.org/pdf/2310.05189v1.pdf","comment":"Our article offers a comprehensive examination of the challenges and\n  risks associated with Large Language Models (LLMs), focusing on their\n  potential impact on the veracity of information in today's digital landscape\n  (ongoing work)"},{"id":"http://arxiv.org/abs/2310.05185v1","updated":"2023-10-08T14:47:13Z","published":"2023-10-08T14:47:13Z","title":"Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational\n  Knowledge Graph Construction","summary":"  Beyond traditional binary relational facts, n-ary relational knowledge graphs\n(NKGs) are comprised of n-ary relational facts containing more than two\nentities, which are closer to real-world facts with broader applications.\nHowever, the construction of NKGs still significantly relies on manual labor,\nand n-ary relation extraction still remains at a course-grained level, which is\nalways in a single schema and fixed arity of entities. To address these\nrestrictions, we propose Text2NKG, a novel fine-grained n-ary relation\nextraction framework for n-ary relational knowledge graph construction. We\nintroduce a span-tuple classification approach with hetero-ordered merging to\naccomplish fine-grained n-ary relation extraction in different arity.\nFurthermore, Text2NKG supports four typical NKG schemas: hyper-relational\nschema, event-based schema, role-based schema, and hypergraph-based schema,\nwith high flexibility and practicality. Experimental results demonstrate that\nText2NKG outperforms the previous state-of-the-art model by nearly 20\\% points\nin the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in\nthe hyper-relational schema. Our code and datasets are publicly available.\n","authors":["Haoran Luo","Haihong E","Yuhao Yang","Tianyu Yao","Yikai Guo","Zichen Tang","Wentai Zhang","Kaiyang Wan","Shiyao Peng","Meina Song","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05185v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2309.07870v2","updated":"2023-10-08T14:43:38Z","published":"2023-09-14T17:18:25Z","title":"Agents: An Open-source Framework for Autonomous Language Agents","summary":"  Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n","authors":["Wangchunshu Zhou","Yuchen Eleanor Jiang","Long Li","Jialong Wu","Tiannan Wang","Shi Qiu","Jintian Zhang","Jing Chen","Ruipu Wu","Shuai Wang","Shiding Zhu","Jiyu Chen","Wentao Zhang","Ningyu Zhang","Huajun Chen","Peng Cui","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2309.07870v2.pdf","comment":"Code available at https://github.com/aiwaves-cn/agents"},{"id":"http://arxiv.org/abs/2309.15630v3","updated":"2023-10-08T14:34:29Z","published":"2023-09-27T13:02:06Z","title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems","summary":"  Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.\n","authors":["Linxin Song","Jieyu Zhang","Lechao Cheng","Pengyuan Zhou","Tianyi Zhou","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2309.15630v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05178v1","updated":"2023-10-08T14:29:33Z","published":"2023-10-08T14:29:33Z","title":"Optimizing Large Language Models to Expedite the Development of Smart\n  Contracts","summary":"  Programming has always been at the heart of technological innovation in the\n21st century. With the advent of blockchain technologies and the proliferation\nof web3 paradigms of decentralised applications, smart contracts have been very\ninstrumental in enabling developers to build applications that reside on\ndecentralised blockchains. Despite the huge interest and potential of smart\ncontracts, there is still a significant knowledge and skill gap that developers\nneed to cross in order to build web3 applications. In light of this, we\nintroduce MazzumaGPT, a large language model that has been optimised to\ngenerate smart contract code and aid developers to scaffold development and\nimprove productivity. As part of this research, we outline the optimisation and\nfine-tuning parameters, evaluate the model's performance on functional\ncorrectness and address the limitations and broader impacts of our research.\n","authors":["Nii Osae Osae Dade","Margaret Lartey-Quaye","Emmanuel Teye-Kofi Odonkor","Paul Ammah"],"pdf_url":"https://arxiv.org/pdf/2310.05178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05177v1","updated":"2023-10-08T14:26:55Z","published":"2023-10-08T14:26:55Z","title":"Do Large Language Models Know about Facts?","summary":"  Large language models (LLMs) have recently driven striking performance\nimprovements across a range of natural language processing tasks. The factual\nknowledge acquired during pretraining and instruction tuning can be useful in\nvarious downstream tasks, such as question answering, and language generation.\nUnlike conventional Knowledge Bases (KBs) that explicitly store factual\nknowledge, LLMs implicitly store facts in their parameters. Content generated\nby the LLMs can often exhibit inaccuracies or deviations from the truth, due to\nfacts that can be incorrectly induced or become obsolete over time. To this\nend, we aim to comprehensively evaluate the extent and scope of factual\nknowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains\n20K diverse factual questions that span different sources, timelines, domains,\nregions, and languages. Furthermore, we investigate whether LLMs are able to\ncompose multiple facts, update factual knowledge temporally, reason over\nmultiple pieces of facts, identify subtle factual differences, and resist\nadversarial examples. Extensive experiments on different sizes and types of\nLLMs show that existing LLMs still lack factual knowledge and suffer from\nvarious spurious correlations. We believe this is a critical bottleneck for\nrealizing trustworthy artificial intelligence. The dataset Pinocchio and our\ncodes will be publicly available.\n","authors":["Xuming Hu","Junzhe Chen","Xiaochuan Li","Yufei Guo","Lijie Wen","Philip S. Yu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2310.05177v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.14318v2","updated":"2023-10-08T14:12:12Z","published":"2023-05-23T17:51:52Z","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning\n  of Large Language Models","summary":"  Large Language Models (LLMs) have made significant progress in utilizing\ntools, but their ability is limited by API availability and the instability of\nimplicit reasoning, particularly when both planning and execution are involved.\nTo overcome these limitations, we propose CREATOR, a novel framework that\nenables LLMs to create their own tools using documentation and code\nrealization. CREATOR disentangles abstract tool creation and concrete decision\nexecution, resulting in improved performance. We evaluate CREATOR on MATH and\nTabMWP benchmarks, respectively consisting of challenging math competition\nproblems and diverse tabular contents. Remarkably, CREATOR outperforms existing\nchain-of-thought, program-of-thought, and tool-using baselines. Additionally,\nwe introduce the Creation Challenge dataset, featuring 2K diverse questions, to\nemphasize the necessity and benefits of LLMs' tool creation ability. Further\nresearch demonstrates that leveraging LLMs as tool creators facilitates\nknowledge transfer, and LLMs exhibit varying levels of tool creation abilities,\nenabling them to adapt to diverse situations. The tool creation ability\nrevolutionizes the LLM's problem-solving paradigm, driving us closer to the\nnext frontier of artificial intelligence. All the codes and data are released.\n","authors":["Cheng Qian","Chi Han","Yi R. Fung","Yujia Qin","Zhiyuan Liu","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2305.14318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05165v1","updated":"2023-10-08T13:49:51Z","published":"2023-10-08T13:49:51Z","title":"On the Zero-Shot Generalization of Machine-Generated Text Detectors","summary":"  The rampant proliferation of large language models, fluent enough to generate\ntext indistinguishable from human-written language, gives unprecedented\nimportance to the detection of machine-generated text. This work is motivated\nby an important research question: How will the detectors of machine-generated\ntext perform on outputs of a new generator, that the detectors were not trained\non? We begin by collecting generation data from a wide range of LLMs, and train\nneural detectors on data from each generator and test its performance on\nheld-out generators. While none of the detectors can generalize to all\ngenerators, we observe a consistent and interesting pattern that the detectors\ntrained on data from a medium-size LLM can zero-shot generalize to the larger\nversion. As a concrete application, we demonstrate that robust detectors can be\nbuilt on an ensemble of training data from medium-sized models.\n","authors":["Xiao Pu","Jingyu Zhang","Xiaochuang Han","Yulia Tsvetkov","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2310.05165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05163v1","updated":"2023-10-08T13:45:05Z","published":"2023-10-08T13:45:05Z","title":"An Investigation of LLMs' Inefficacy in Understanding Converse Relations","summary":"  Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.\n","authors":["Chengwen Qi","Bowen Li","Binyuan Hui","Bailin Wang","Jinyang Li","Jinwang Wu","Yuanjun Laili"],"pdf_url":"https://arxiv.org/pdf/2310.05163v1.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05161v1","updated":"2023-10-08T13:36:05Z","published":"2023-10-08T13:36:05Z","title":"Recurrent Neural Language Models as Probabilistic Finite-state Automata","summary":"  Studying language models (LMs) in terms of well-understood formalisms allows\nus to precisely characterize their abilities and limitations. Previous work has\ninvestigated the representational capacity of recurrent neural network (RNN)\nLMs in terms of their capacity to recognize unweighted formal languages.\nHowever, LMs do not describe unweighted formal languages -- rather, they define\nprobability distributions over strings. In this work, we study what classes of\nsuch probability distributions RNN LMs can represent, which allows us to make\nmore direct statements about their capabilities. We show that simple RNNs are\nequivalent to a subclass of probabilistic finite-state automata, and can thus\nmodel a strict subset of probability distributions expressible by finite-state\nmodels. Furthermore, we study the space complexity of representing finite-state\nLMs with RNNs. We show that, to represent an arbitrary deterministic\nfinite-state LM with $N$ states over an alphabet $\\Sigma$, an RNN requires\n$\\Omega\\left(N |\\Sigma|\\right)$ neurons. These results present a first step\ntowards characterizing the classes of distributions RNN LMs can represent and\nthus help us understand their capabilities and limitations.\n","authors":["Anej Svete","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2310.05161v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2310.05157v1","updated":"2023-10-08T13:19:52Z","published":"2023-10-08T13:19:52Z","title":"MenatQA: A New Dataset for Testing the Temporal Comprehension and\n  Reasoning Abilities of Large Language Models","summary":"  Large language models (LLMs) have shown nearly saturated performance on many\nnatural language processing (NLP) tasks. As a result, it is natural for people\nto believe that LLMs have also mastered abilities such as time understanding\nand reasoning. However, research on the temporal sensitivity of LLMs has been\ninsufficiently emphasized. To fill this gap, this paper constructs Multiple\nSensitive Factors Time QA (MenatQA), which encompasses three temporal factors\n(scope factor, order factor, counterfactual factor) with total 2,853 samples\nfor evaluating the time comprehension and reasoning abilities of LLMs. This\npaper tests current mainstream LLMs with different parameter sizes, ranging\nfrom billions to hundreds of billions. The results show most LLMs fall behind\nsmaller temporal reasoning models with different degree on these factors. In\nspecific, LLMs show a significant vulnerability to temporal biases and depend\nheavily on the temporal information provided in questions. Furthermore, this\npaper undertakes a preliminary investigation into potential improvement\nstrategies by devising specific prompts and leveraging external tools. These\napproaches serve as valuable baselines or references for future research\nendeavors.\n","authors":["Yifan Wei","Yisong Su","Huanhuan Ma","Xiaoyan Yu","Fangyu Lei","Yuanzhe Zhang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05157v1.pdf","comment":"Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.05155v1","updated":"2023-10-08T13:07:42Z","published":"2023-10-08T13:07:42Z","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on\n  Open-Source Model","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in\nutilizing tools, but their closed-source nature and high inference costs pose\nlimitations on their adaptability, necessitating a valid method that leverages\nsmaller, open-sourced models. In this paper, we introduce Toolink, a\ncomprehensive framework that performs task-solving by first creating a toolkit\nand then integrating the planning and calling of tools through a\nchain-of-solving (CoS) approach. We first validate the efficacy of Toolink in\nharnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we\ncurate CoS-GPT, a chain-of-solving dataset designed for tool-using, and\nfinetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source\nmodel with advanced tool-planning and tool-calling capabilities. Evaluation on\ndiverse tasks from BIG-bench demonstrates its CoS ability matches that of\nChatGPT while its performance surpasses the chain-of-thought approach. Further\nstudies highlight the generalization of LLaMA-CoS to unseen tasks and showcase\nits capability in using toolkits not explicitly tailored for the target task,\naffirming its robustness in real-world scenarios. All codes and data are\nreleased.\n","authors":["Cheng Qian","Chenyan Xiong","Zhenghao Liu","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18907v2","updated":"2023-10-08T13:05:26Z","published":"2023-05-30T10:04:01Z","title":"Multitask learning for recognizing stress and depression in social media","summary":"  Stress and depression are prevalent nowadays across people of all ages due to\nthe quick paces of life. People use social media to express their feelings.\nThus, social media constitute a valuable form of information for the early\ndetection of stress and depression. Although many research works have been\nintroduced targeting the early recognition of stress and depression, there are\nstill limitations. There have been proposed multi-task learning settings, which\nuse depression and emotion (or figurative language) as the primary and\nauxiliary tasks respectively. However, although stress is inextricably linked\nwith depression, researchers face these two tasks as two separate tasks. To\naddress these limitations, we present the first study, which exploits two\ndifferent datasets collected under different conditions, and introduce two\nmultitask learning frameworks, which use depression and stress as the main and\nauxiliary tasks respectively. Specifically, we use a depression dataset and a\nstressful dataset including stressful posts from ten subreddits of five\ndomains. In terms of the first approach, each post passes through a shared BERT\nlayer, which is updated by both tasks. Next, two separate BERT encoder layers\nare exploited, which are updated by each task separately. Regarding the second\napproach, it consists of shared and task-specific layers weighted by attention\nfusion networks. We conduct a series of experiments and compare our approaches\nwith existing research initiatives, single-task learning, and transfer\nlearning. Experiments show multiple advantages of our approaches over\nstate-of-the-art ones.\n","authors":["Loukas Ilias","Dimitris Askounis"],"pdf_url":"https://arxiv.org/pdf/2305.18907v2.pdf","comment":"Online Social Networks and Media"},{"id":"http://arxiv.org/abs/2310.05150v1","updated":"2023-10-08T12:52:09Z","published":"2023-10-08T12:52:09Z","title":"From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for\n  Conversational Exploratory Search","summary":"  Exploratory search is an open-ended information retrieval process that aims\nat discovering knowledge about a topic or domain rather than searching for a\nspecific answer or piece of information. Conversational interfaces are\nparticularly suitable for supporting exploratory search, allowing users to\nrefine queries and examine search results through interactive dialogues. In\naddition to conversational search interfaces, knowledge graphs are also useful\nin supporting information exploration due to their rich semantic representation\nof data items. In this study, we demonstrate the synergistic effects of\ncombining knowledge graphs and conversational interfaces for exploratory\nsearch, bridging the gap between structured and unstructured information\nretrieval. To this end, we propose a knowledge-driven dialogue system for\nexploring news articles by asking natural language questions and using the\ngraph structure to navigate between related topics. Based on a user study with\n54 participants, we empirically evaluate the effectiveness of the graph-based\nexploratory search and discuss design implications for developing such systems.\n","authors":["Phillip Schneider","Nils Rehtanz","Kristiina Jokinen","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2310.05150v1.pdf","comment":"Accepted to PACLIC 2023"},{"id":"http://arxiv.org/abs/2310.05149v1","updated":"2023-10-08T12:50:57Z","published":"2023-10-08T12:50:57Z","title":"Retrieval-Generation Synergy Augmented Large Language Models","summary":"  Large language models augmented with task-relevant documents have\ndemonstrated impressive performance on knowledge-intensive tasks. However,\nregarding how to obtain effective documents, the existing methods are mainly\ndivided into two categories. One is to retrieve from an external knowledge\nbase, and the other is to utilize large language models to generate documents.\nWe propose an iterative retrieval-generation collaborative framework. It is not\nonly able to leverage both parametric and non-parametric knowledge, but also\nhelps to find the correct reasoning path through retrieval-generation\ninteractions, which is very important for tasks that require multi-step\nreasoning. We conduct experiments on four question answering datasets,\nincluding single-hop QA and multi-hop QA tasks. Empirical results show that our\nmethod significantly improves the reasoning ability of large language models\nand outperforms previous baselines.\n","authors":["Zhangyin Feng","Xiaocheng Feng","Dezhi Zhao","Maojin Yang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2310.05149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13809v2","updated":"2023-10-08T12:50:10Z","published":"2023-03-24T05:05:03Z","title":"Error Analysis Prompting Enables Human-Like Translation Evaluation in\n  Large Language Models: A Case Study on ChatGPT","summary":"  Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks, such as machine translation,\ntext summarization. Recent research (Kocmi and Federmann, 2023) has shown that\nutilizing ChatGPT for assessing the quality of machine translation (MT)\nachieves state-of-the-art performance at the system level but performs poorly\nat the segment level. To further improve the performance of LLMs on MT quality\nassessment, we conduct an investigation into several prompting methods, and\npropose a new prompting method called Error Analysis Prompting (EAPrompt) by\ncombining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al.,\n2022). Our results on WMT22 indicate that prompting LLMs like ChatGPT with\nerror analysis can generate human-like MT evaluations at both the system and\nsegment level. Additionally, we first discover some limitations of ChatGPT as\nan MT evaluator, such as changing the order of input may significantly\ninfluence the judgment when providing multiple translations in a single query.\nThis work provides a preliminary experience of prompting LLMs as an evaluator\nto improve the reliability of translation evaluation metrics under the error\nanalysis paradigm.\n","authors":["Qingyu Lu","Baopu Qiu","Liang Ding","Kanjian Zhang","Tom Kocmi","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.13809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01074v2","updated":"2023-10-08T12:45:18Z","published":"2023-10-02T10:35:23Z","title":"Back to the Future: Towards Explainable Temporal Reasoning with Large\n  Language Models","summary":"  Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.\n","authors":["Chenhan Yuan","Qianqian Xie","Jimin Huang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2310.01074v2.pdf","comment":"14 pages, 5 figures, code and dataset:\n  https://github.com/chenhan97/TimeLlama"},{"id":"http://arxiv.org/abs/2310.05146v1","updated":"2023-10-08T12:37:28Z","published":"2023-10-08T12:37:28Z","title":"Large Language Model (LLM) as a System of Multiple Expert Agents: An\n  Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge","summary":"  We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge\nusing Large Language Models (LLMs) as a system of multiple expert agents. Using\nthe flexibility of LLMs to be prompted to do various novel tasks using\nzero-shot, few-shot, context-grounded prompting, we explore the feasibility of\nusing LLMs to solve the ARC Challenge. We firstly convert the input image into\nmultiple suitable text-based abstraction spaces. We then utilise the\nassociative power of LLMs to derive the input-output relationship and map this\nto actions in the form of a working program, similar to Voyager / Ghost in the\nMineCraft. In addition, we use iterative environmental feedback in order to\nguide LLMs to solve the task. Our proposed approach achieves 50 solves out of\n111 training set problems (45%) with just three abstraction spaces - grid,\nobject and pixel - and we believe that with more abstraction spaces and\nlearnable actions, we will be able to solve more.\n","authors":["John Chong Min Tan","Mehul Motani"],"pdf_url":"https://arxiv.org/pdf/2310.05146v1.pdf","comment":"6 main pages, 1 page references, 18 pages appendix"},{"id":"http://arxiv.org/abs/2310.05140v1","updated":"2023-10-08T12:21:24Z","published":"2023-10-08T12:21:24Z","title":"Harnessing the Power of Large Language Models for Empathetic Response\n  Generation: Empirical Investigations and Improvements","summary":"  Empathetic dialogue is an indispensable part of building harmonious social\nrelationships and contributes to the development of a helpful AI. Previous\napproaches are mainly based on fine small-scale language models. With the\nadvent of ChatGPT, the application effect of large language models (LLMs) in\nthis field has attracted great attention. This work empirically investigates\nthe performance of LLMs in generating empathetic responses and proposes three\nimprovement methods of semantically similar in-context learning, two-stage\ninteractive generation, and combination with the knowledge base. Extensive\nexperiments show that LLMs can significantly benefit from our proposed methods\nand is able to achieve state-of-the-art performance in both automatic and human\nevaluations. Additionally, we explore the possibility of GPT-4 simulating human\nevaluators.\n","authors":["Yushan Qian","Wei-Nan Zhang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05140v1.pdf","comment":"the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05135v1","updated":"2023-10-08T12:08:48Z","published":"2023-10-08T12:08:48Z","title":"Are Emily and Greg Still More Employable than Lakisha and Jamal?\n  Investigating Algorithmic Hiring Bias in the Era of ChatGPT","summary":"  Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit\napplicability across numerous tasks. One domain of interest is their use in\nalgorithmic hiring, specifically in matching resumes with job categories. Yet,\nthis introduces issues of bias on protected attributes like gender, race and\nmaternity status. The seminal work of Bertrand & Mullainathan (2003) set the\ngold-standard for identifying hiring bias via field experiments where the\nresponse rate for identical resumes that differ only in protected attributes,\ne.g., racially suggestive names such as Emily or Lakisha, is compared. We\nreplicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and\nLlama) to evaluate bias (or lack thereof) on gender, race, maternity status,\npregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)\nmatching resumes to job categories; and (2) summarizing resumes with employment\nrelevant information. Overall, LLMs are robust across race and gender. They\ndiffer in their performance on pregnancy status and political affiliation. We\nuse contrastive input decoding on open-source LLMs to uncover potential sources\nof bias.\n","authors":["Akshaj Kumar Veldanda","Fabian Grob","Shailja Thakur","Hammond Pearce","Benjamin Tan","Ramesh Karri","Siddharth Garg"],"pdf_url":"https://arxiv.org/pdf/2310.05135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05130v1","updated":"2023-10-08T11:41:28Z","published":"2023-10-08T11:41:28Z","title":"Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text\n  via Conditional Probability Curvature","summary":"  Large language models (LLMs) have shown the ability to produce fluent and\ncogent content, presenting both productivity opportunities and societal risks.\nTo build trustworthy AI systems, it is imperative to distinguish between\nmachine-generated and human-authored content. The leading zero-shot detector,\nDetectGPT, showcases commendable performance but is marred by its intensive\ncomputational costs. In this paper, we introduce the concept of conditional\nprobability curvature to elucidate discrepancies in word choices between LLMs\nand humans within a given context. Utilizing this curvature as a foundational\nmetric, we present Fast-DetectGPT, an optimized zero-shot detector, which\nsubstitutes DetectGPT's perturbation step with a more efficient sampling step.\nOur evaluations on various datasets, source models, and test conditions\nindicate that Fast-DetectGPT not only outperforms DetectGPT in both the\nwhite-box and black-box settings but also accelerates the detection process by\na factor of 340, as detailed in Table 1.\n","authors":["Guangsheng Bao","Yanbin Zhao","Zhiyang Teng","Linyi Yang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05130v1.pdf","comment":"9 pages, 5 figures, 11 tables"},{"id":"http://arxiv.org/abs/2310.05128v1","updated":"2023-10-08T11:36:45Z","published":"2023-10-08T11:36:45Z","title":"Instances and Labels: Hierarchy-aware Joint Supervised Contrastive\n  Learning for Hierarchical Multi-Label Text Classification","summary":"  Hierarchical multi-label text classification (HMTC) aims at utilizing a label\nhierarchy in multi-label classification. Recent approaches to HMTC deal with\nthe problem of imposing an overconstrained premise on the output space by using\ncontrastive learning on generated samples in a semi-supervised manner to bring\ntext and label embeddings closer. However, the generation of samples tends to\nintroduce noise as it ignores the correlation between similar samples in the\nsame batch. One solution to this issue is supervised contrastive learning, but\nit remains an underexplored topic in HMTC due to its complex structured labels.\nTo overcome this challenge, we propose HJCL, a \\textbf{H}ierarchy-aware\n\\textbf{J}oint Supervised \\textbf{C}ontrastive \\textbf{L}earning method that\nbridges the gap between supervised contrastive learning and HMTC. Specifically,\nwe employ both instance-wise and label-wise contrastive learning techniques and\ncarefully construct batches to fulfill the contrastive learning objective.\n","authors":["Simon Chi Lok U","Jie He","Vctor Gutirrez-Basulto","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2310.05128v1.pdf","comment":"Accepted as findings at EMNLP-2023"},{"id":"http://arxiv.org/abs/2310.05116v1","updated":"2023-10-08T11:09:16Z","published":"2023-10-08T11:09:16Z","title":"CARLG: Leveraging Contextual Clues and Role Correlations for Improving\n  Document-level Event Argument Extraction","summary":"  Document-level event argument extraction (EAE) is a crucial but challenging\nsubtask in information extraction. Most existing approaches focus on the\ninteraction between arguments and event triggers, ignoring two critical points:\nthe information of contextual clues and the semantic correlations among\nargument roles. In this paper, we propose the CARLG model, which consists of\ntwo modules: Contextual Clues Aggregation (CCA) and Role-based Latent\nInformation Guidance (RLIG), effectively leveraging contextual clues and role\ncorrelations for improving document-level EAE. The CCA module adaptively\ncaptures and integrates contextual clues by utilizing context attention weights\nfrom a pre-trained encoder. The RLIG module captures semantic correlations\nthrough role-interactive encoding and provides valuable information guidance\nwith latent role representation. Notably, our CCA and RLIG modules are compact,\ntransplantable and efficient, which introduce no more than 1% new parameters\nand can be easily equipped on other span-base methods with significant\nperformance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE\ndatasets demonstrate the superiority of the proposed CARLG model. It\noutperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98\nF1, respectively, while reducing the inference time by 31%. Furthermore, we\nprovide detailed experimental analyses based on the performance gains and\nillustrate the interpretability of our model.\n","authors":["Wanlong Liu","Wenyu Chen","Dingyi Zeng","Li Zhou","Hong Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05116v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.05115v1","updated":"2023-10-08T11:07:19Z","published":"2023-10-08T11:07:19Z","title":"Breaking Down Word Semantics from Pre-trained Language Models through\n  Layer-wise Dimension Selection","summary":"  Contextual word embeddings obtained from pre-trained language model (PLM)\nhave proven effective for various natural language processing tasks at the word\nlevel. However, interpreting the hidden aspects within embeddings, such as\nsyntax and semantics, remains challenging. Disentangled representation learning\nhas emerged as a promising approach, which separates specific aspects into\ndistinct embeddings. Furthermore, different linguistic knowledge is believed to\nbe stored in different layers of PLM. This paper aims to disentangle semantic\nsense from BERT by applying a binary mask to middle outputs across the layers,\nwithout updating pre-trained parameters. The disentangled embeddings are\nevaluated through binary classification to determine if the target word in two\ndifferent sentences has the same meaning. Experiments with cased\nBERT$_{\\texttt{base}}$ show that leveraging layer-wise information is effective\nand disentangling semantic sense further improve performance.\n","authors":["Nayoung Choi"],"pdf_url":"https://arxiv.org/pdf/2310.05115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05103v1","updated":"2023-10-08T10:08:21Z","published":"2023-10-08T10:08:21Z","title":"Zero-Shot Detection of Machine-Generated Codes","summary":"  This work proposes a training-free approach for the detection of\nLLMs-generated codes, mitigating the risks associated with their indiscriminate\nusage. To the best of our knowledge, our research is the first to investigate\nzero-shot detection techniques applied to code generated by advanced black-box\nLLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot\ntext detectors are ineffective in detecting code, likely due to the unique\nstatistical properties found in code structures. We then modify the previous\nzero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing\na surrogate white-box model to estimate the probability of the rightmost\ntokens, allowing us to identify code snippets generated by language models.\nThrough extensive experiments conducted on the python codes of the CodeContest\nand APPS dataset, our approach demonstrates its effectiveness by achieving\nstate-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4\nmodels. Moreover, our method exhibits robustness against revision attacks and\ngeneralizes well to Java codes. We also find that the smaller code language\nmodel like PolyCoder-160M performs as a universal code detector, outperforming\nthe billion-scale counterpart. The codes will be available at\nhttps://github.com/ Xianjun-Yang/Code_detection.git\n","authors":["Xianjun Yang","Kexun Zhang","Haifeng Chen","Linda Petzold","William Yang Wang","Wei Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.05103v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2310.05095v1","updated":"2023-10-08T09:53:46Z","published":"2023-10-08T09:53:46Z","title":"How Reliable Are AI-Generated-Text Detectors? An Assessment Framework\n  Using Evasive Soft Prompts","summary":"  In recent years, there has been a rapid proliferation of AI-generated text,\nprimarily driven by the release of powerful pre-trained language models (PLMs).\nTo address the issue of misuse associated with AI-generated text, various\nhigh-performing detectors have been developed, including the OpenAI detector\nand the Stanford DetectGPT. In our study, we ask how reliable these detectors\nare. We answer the question by designing a novel approach that can prompt any\nPLM to generate text that evades these high-performing detectors. The proposed\napproach suggests a universal evasive prompt, a novel type of soft prompt,\nwhich guides PLMs in producing \"human-like\" text that can mislead the\ndetectors. The novel universal evasive prompt is achieved in two steps: First,\nwe create an evasive soft prompt tailored to a specific PLM through prompt\ntuning; and then, we leverage the transferability of soft prompts to transfer\nthe learned evasive soft prompt from one PLM to another. Employing multiple\nPLMs in various writing tasks, we conduct extensive experiments to evaluate the\nefficacy of the evasive soft prompts in their evasion of state-of-the-art\ndetectors.\n","authors":["Tharindu Kumarage","Paras Sheth","Raha Moraffah","Joshua Garland","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05095v1.pdf","comment":"Accepted to EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2306.07207v2","updated":"2023-10-08T09:49:53Z","published":"2023-06-12T16:11:10Z","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","summary":"  Large language models (LLMs), with their remarkable conversational\ncapabilities, have demonstrated impressive performance across various\napplications and have emerged as formidable AI assistants. In view of this, it\nraises an intuitive question: Can we harness the power of LLMs to build\nmultimodal AI assistants for visual applications? Recently, several multi-modal\nmodels have been developed for this purpose. They typically pre-train an\nadaptation module to align the semantics of the vision encoder and language\nmodel, followed by fine-tuning on instruction-following data. However, despite\nthe success of this pipeline in image and language understanding, its\neffectiveness in joint video and language understanding has not been widely\nexplored. In this paper, we aim to develop a novel multi-modal foundation model\ncapable of comprehending video, image, and language within a general framework.\nTo achieve this goal, we introduce Valley, a Video Assistant with Large\nLanguage model Enhanced abilitY. The Valley consists of a LLM, a temporal\nmodeling module, a visual encoder, and a simple projection module designed to\nbridge visual and textual modes. To empower Valley with video comprehension and\ninstruction-following capabilities, we construct a video instruction dataset\nand adopt a two-stage tuning procedure to train it. Specifically, we employ\nChatGPT to facilitate the construction of task-oriented conversation data\nencompassing various tasks, including multi-shot captions, long video\ndescriptions, action recognition, causal relationship inference, etc.\nSubsequently, we adopt a pre-training-then-instructions-tuned pipeline to align\nvisual and textual modalities and improve the instruction-following capability\nof Valley. Qualitative experiments demonstrate that Valley has the potential to\nfunction as a highly effective video assistant that can make complex video\nunderstanding scenarios easy.\n","authors":["Ruipu Luo","Ziwang Zhao","Min Yang","Junwei Dong","Da Li","Pengcheng Lu","Tao Wang","Linmei Hu","Minghui Qiu","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2306.07207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16044v2","updated":"2023-10-08T09:41:22Z","published":"2022-11-29T09:28:05Z","title":"Model Extraction Attack against Self-supervised Speech Models","summary":"  Self-supervised learning (SSL) speech models generate meaningful\nrepresentations of given clips and achieve incredible performance across\nvarious downstream tasks. Model extraction attack (MEA) often refers to an\nadversary stealing the functionality of the victim model with only query\naccess. In this work, we study the MEA problem against SSL speech model with a\nsmall number of queries. We propose a two-stage framework to extract the model.\nIn the first stage, SSL is conducted on the large-scale unlabeled corpus to\npre-train a small speech model. Secondly, we actively sample a small portion of\nclips from the unlabeled corpus and query the target model with these clips to\nacquire their representations as labels for the small model's second-stage\ntraining. Experiment results show that our sampling methods can effectively\nextract the target model without knowing any information about its model\narchitecture.\n","authors":["Tsu-Yuan Hsu","Chen-An Li","Tung-Yu Wu","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2211.16044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05092v1","updated":"2023-10-08T09:41:18Z","published":"2023-10-08T09:41:18Z","title":"Benchmarking Large Language Models with Augmented Instructions for\n  Fine-grained Information Extraction","summary":"  Information Extraction (IE) is an essential task in Natural Language\nProcessing. Traditional methods have relied on coarse-grained extraction with\nsimple instructions. However, with the emergence of Large Language Models\n(LLMs), there is a need to adapt IE techniques to leverage the capabilities of\nthese models. This paper introduces a fine-grained IE benchmark dataset\ntailored for LLMs, employing augmented instructions for each information type,\nwhich includes task descriptions, extraction rules, output formats, and\nexamples. Through extensive evaluations, we observe that encoder-decoder\nmodels, particularly T5 and FLAN-T5, perform well in generalizing to unseen\ninformation types, while ChatGPT exhibits greater adaptability to new task\nforms. Our results also indicate that performance is not solely dictated by\nmodel scale, and highlight the significance of architecture, data diversity,\nand learning techniques. This work paves the way for a more refined and\nversatile utilization of LLMs in Information Extraction.\n","authors":["Jun Gao","Huan Zhao","Yice Zhang","Wei Wang","Changlong Yu","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05083v1","updated":"2023-10-08T09:16:46Z","published":"2023-10-08T09:16:46Z","title":"FLatS: Principled Out-of-Distribution Detection with Feature-Based\n  Likelihood Ratio Score","summary":"  Detecting out-of-distribution (OOD) instances is crucial for NLP models in\npractical applications. Although numerous OOD detection methods exist, most of\nthem are empirical. Backed by theoretical analysis, this paper advocates for\nthe measurement of the \"OOD-ness\" of a test case $\\boldsymbol{x}$ through the\nlikelihood ratio between out-distribution $\\mathcal P_{\\textit{out}}$ and\nin-distribution $\\mathcal P_{\\textit{in}}$. We argue that the state-of-the-art\n(SOTA) feature-based OOD detection methods, such as Maha and KNN, are\nsuboptimal since they only estimate in-distribution density\n$p_{\\textit{in}}(\\boldsymbol{x})$. To address this issue, we propose FLatS, a\nprincipled solution for OOD detection based on likelihood ratio. Moreover, we\ndemonstrate that FLatS can serve as a general framework capable of enhancing\nother OOD detection methods by incorporating out-distribution density\n$p_{\\textit{out}}(\\boldsymbol{x})$ estimation. Experiments show that FLatS\nestablishes a new SOTA on popular benchmarks. Our code is publicly available at\nhttps://github.com/linhaowei1/FLatS.\n","authors":["Haowei Lin","Yuntian Gu"],"pdf_url":"https://arxiv.org/pdf/2310.05083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05074v1","updated":"2023-10-08T08:52:13Z","published":"2023-10-08T08:52:13Z","title":"DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller\n  Language Models","summary":"  Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.\n","authors":["Chengcheng Han","Xiaowei Du","Che Zhang","Yixin Lian","Xiang Li","Ming Gao","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05074v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05073v1","updated":"2023-10-08T08:47:10Z","published":"2023-10-08T08:47:10Z","title":"Enhancing Argument Structure Extraction with Efficient Leverage of\n  Contextual Information","summary":"  Argument structure extraction (ASE) aims to identify the discourse structure\nof arguments within documents. Previous research has demonstrated that\ncontextual information is crucial for developing an effective ASE model.\nHowever, we observe that merely concatenating sentences in a contextual window\ndoes not fully utilize contextual information and can sometimes lead to\nexcessive attention on less informative sentences. To tackle this challenge, we\npropose an Efficient Context-aware ASE model (ECASE) that fully exploits\ncontextual information by enhancing modeling capacity and augmenting training\ndata. Specifically, we introduce a sequence-attention module and\ndistance-weighted similarity loss to aggregate contextual information and\nargumentative information. Additionally, we augment the training data by\nrandomly masking discourse markers and sentences, which reduces the model's\nreliance on specific words or less informative sentences. Our experiments on\nfive datasets from various domains demonstrate that our model achieves\nstate-of-the-art performance. Furthermore, ablation studies confirm the\neffectiveness of each module in our model.\n","authors":["Yun Luo","Zhen Yang","Fandong Meng","Yingjie Li","Jie Zhou","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05073v1.pdf","comment":"EMNLP 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2304.05538v4","updated":"2023-10-08T23:03:33Z","published":"2023-04-11T23:55:50Z","title":"ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of\n  Zoom and Spatial Biases in Image Classification","summary":"  Image classifiers are information-discarding machines, by design. Yet, how\nthese models discard information remains mysterious. We hypothesize that one\nway for image classifiers to reach high accuracy is to first zoom to the most\ndiscriminative region in the image and then extract features from there to\npredict image labels, discarding the rest of the image. Studying six popular\nnetworks ranging from AlexNet to CLIP, we find that proper framing of the input\nimage can lead to the correct classification of 98.91% of ImageNet images.\nFurthermore, we uncover positional biases in various datasets, especially a\nstrong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally,\nleveraging our insights into the potential of zooming, we propose a test-time\naugmentation (TTA) technique that improves classification accuracy by forcing\nmodels to explicitly perform zoom-in operations before making predictions. Our\nmethod is more interpretable, accurate, and faster than MEMO, a\nstate-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark\nthat challenges SOTA classifiers including large vision-language models even\nwhen optimal zooming is allowed.\n","authors":["Mohammad Reza Taesiri","Giang Nguyen","Sarra Habchi","Cor-Paul Bezemer","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2304.05538v4.pdf","comment":"NeurIPS 2023 Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2310.05306v1","updated":"2023-10-08T22:58:31Z","published":"2023-10-08T22:58:31Z","title":"Progressive Neural Compression for Adaptive Image Offloading under\n  Timing Constraints","summary":"  IoT devices are increasingly the source of data for machine learning (ML)\napplications running on edge servers. Data transmissions from devices to\nservers are often over local wireless networks whose bandwidth is not just\nlimited but, more importantly, variable. Furthermore, in cyber-physical systems\ninteracting with the physical environment, image offloading is also commonly\nsubject to timing constraints. It is, therefore, important to develop an\nadaptive approach that maximizes the inference performance of ML applications\nunder timing constraints and the resource constraints of IoT devices. In this\npaper, we use image classification as our target application and propose\nprogressive neural compression (PNC) as an efficient solution to this problem.\nAlthough neural compression has been used to compress images for different ML\napplications, existing solutions often produce fixed-size outputs that are\nunsuitable for timing-constrained offloading over variable bandwidth. To\naddress this limitation, we train a multi-objective rateless autoencoder that\noptimizes for multiple compression rates via stochastic taildrop to create a\ncompression solution that produces features ordered according to their\nimportance to inference performance. Features are then transmitted in that\norder based on available bandwidth, with classification ultimately performed\nusing the (sub)set of features received by the deadline. We demonstrate the\nbenefits of PNC over state-of-the-art neural compression approaches and\ntraditional compression methods on a testbed comprising an IoT device and an\nedge server connected over a wireless network with varying bandwidth.\n","authors":["Ruiqi Wang","Hanyang Liu","Jiaming Qiu","Moran Xu","Roch Guerin","Chenyang Lu"],"pdf_url":"https://arxiv.org/pdf/2310.05306v1.pdf","comment":"IEEE the 44th Real-Time System Symposium (RTSS), 2023"},{"id":"http://arxiv.org/abs/2310.05304v1","updated":"2023-10-08T22:48:30Z","published":"2023-10-08T22:48:30Z","title":"GestSync: Determining who is speaking without a talking head","summary":"  In this paper we introduce a new synchronisation task, Gesture-Sync:\ndetermining if a person's gestures are correlated with their speech or not. In\ncomparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far\nlooser relationship between the voice and body movement than there is between\nvoice and lip motion. We introduce a dual-encoder model for this task, and\ncompare a number of input representations including RGB frames, keypoint\nimages, and keypoint vectors, assessing their performance and advantages. We\nshow that the model can be trained using self-supervised learning alone, and\nevaluate its performance on the LRS3 dataset. Finally, we demonstrate\napplications of Gesture-Sync for audio-visual synchronisation, and in\ndetermining who is the speaker in a crowd, without seeing their faces. The\ncode, datasets and pre-trained models can be found at:\n\\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.\n","authors":["Sindhu B Hegde","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.05304v1.pdf","comment":"Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7\n  Figures"},{"id":"http://arxiv.org/abs/2012.13633v3","updated":"2023-10-08T22:30:49Z","published":"2020-12-25T21:56:36Z","title":"Detecting Road Obstacles by Erasing Them","summary":"  Vehicles can encounter a myriad of obstacles on the road, and it is\nimpossible to record them all beforehand to train a detector. Instead, we\nselect image patches and inpaint them with the surrounding road texture, which\ntends to remove obstacles from those patches. We then use a network trained to\nrecognize discrepancies between the original patch and the inpainted one, which\nsignals an erased obstacle.\n","authors":["Krzysztof Lis","Sina Honari","Pascal Fua","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2012.13633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05299v1","updated":"2023-10-08T22:08:59Z","published":"2023-10-08T22:08:59Z","title":"Image Compression and Decompression Framework Based on Latent Diffusion\n  Model for Breast Mammography","summary":"  This research presents a novel framework for the compression and\ndecompression of medical images utilizing the Latent Diffusion Model (LDM). The\nLDM represents advancement over the denoising diffusion probabilistic model\n(DDPM) with a potential to yield superior image quality while requiring fewer\ncomputational resources in the image decompression process. A possible\napplication of LDM and Torchvision for image upscaling has been explored using\nmedical image data, serving as an alternative to traditional image compression\nand decompression algorithms. The experimental outcomes demonstrate that this\napproach surpasses a conventional file compression algorithm, and convolutional\nneural network (CNN) models trained with decompressed files perform comparably\nto those trained with original image files. This approach also significantly\nreduces dataset size so that it can be distributed with a smaller size, and\nmedical images take up much less space in medical devices. The research\nimplications extend to noise reduction in lossy compression algorithms and\nsubstitute for complex wavelet-based lossless algorithms.\n","authors":["InChan Hwang","MinJae Woo"],"pdf_url":"https://arxiv.org/pdf/2310.05299v1.pdf","comment":"6 pages IEEE conference"},{"id":"http://arxiv.org/abs/2301.10750v3","updated":"2023-10-08T22:01:52Z","published":"2023-01-25T18:14:49Z","title":"Out of Distribution Performance of State of Art Vision Model","summary":"  The vision transformer (ViT) has advanced to the cutting edge in the visual\nrecognition task. Transformers are more robust than CNN, according to the\nlatest research. ViT's self-attention mechanism, according to the claim, makes\nit more robust than CNN. Even with this, we discover that these conclusions are\nbased on unfair experimental conditions and just comparing a few models, which\ndid not allow us to depict the entire scenario of robustness performance. In\nthis study, we investigate the performance of 58 state-of-the-art computer\nvision models in a unified training setup based not only on attention and\nconvolution mechanisms but also on neural networks based on a combination of\nconvolution and attention mechanisms, sequence-based model, complementary\nsearch, and network-based method. Our research demonstrates that robustness\ndepends on the training setup and model types, and performance varies based on\nout-of-distribution type. Our research will aid the community in better\nunderstanding and benchmarking the robustness of computer vision models.\n","authors":["Salman Rahman","Wonkwon Lee"],"pdf_url":"https://arxiv.org/pdf/2301.10750v3.pdf","comment":"incomplete work - need to complete it"},{"id":"http://arxiv.org/abs/2308.01313v2","updated":"2023-10-08T21:56:53Z","published":"2023-08-02T17:57:25Z","title":"More Context, Less Distraction: Zero-shot Visual Classification by\n  Inferring and Conditioning on Contextual Attributes","summary":"  Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninterpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst\ngroup accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n","authors":["Bang An","Sicheng Zhu","Michael-Andrei Panaitescu-Liess","Chaithanya Kumar Mummadi","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2308.01313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01508v2","updated":"2023-10-08T21:51:46Z","published":"2023-08-03T02:34:01Z","title":"Circumventing Concept Erasure Methods For Text-to-Image Generative\n  Models","summary":"  Text-to-image generative models can produce photo-realistic images for an\nextremely broad range of concepts, and their usage has proliferated widely\namong the general public. On the flip side, these models have numerous\ndrawbacks, including their potential to generate images featuring sexually\nexplicit content, mirror artistic styles without permission, or even\nhallucinate (or deepfake) the likenesses of celebrities. Consequently, various\nmethods have been proposed in order to \"erase\" sensitive concepts from\ntext-to-image models. In this work, we examine five recently proposed concept\nerasure methods, and show that targeted concepts are not fully excised from any\nof these methods. Specifically, we leverage the existence of special learned\nword embeddings that can retrieve \"erased\" concepts from the sanitized models\nwith no alterations to their weights. Our results highlight the brittleness of\npost hoc concept erasure methods, and call into question their use in the\nalgorithmic toolkit for AI safety.\n","authors":["Minh Pham","Kelly O. Marshall","Niv Cohen","Govind Mittal","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2308.01508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05290v1","updated":"2023-10-08T21:32:30Z","published":"2023-10-08T21:32:30Z","title":"MSight: An Edge-Cloud Infrastructure-based Perception System for\n  Connected Automated Vehicles","summary":"  As vehicular communication and networking technologies continue to advance,\ninfrastructure-based roadside perception emerges as a pivotal tool for\nconnected automated vehicle (CAV) applications. Due to their elevated\npositioning, roadside sensors, including cameras and lidars, often enjoy\nunobstructed views with diminished object occlusion. This provides them a\ndistinct advantage over onboard perception, enabling more robust and accurate\ndetection of road objects. This paper presents MSight, a cutting-edge roadside\nperception system specifically designed for CAVs. MSight offers real-time\nvehicle detection, localization, tracking, and short-term trajectory\nprediction. Evaluations underscore the system's capability to uphold lane-level\naccuracy with minimal latency, revealing a range of potential applications to\nenhance CAV safety and efficiency. Presently, MSight operates 24/7 at a\ntwo-lane roundabout in the City of Ann Arbor, Michigan.\n","authors":["Rusheng Zhang","Depu Meng","Shengyin Shen","Zhengxia Zou","Houqiang Li","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05290v1.pdf","comment":"Submitted to IEEE T-ITS"},{"id":"http://arxiv.org/abs/2309.16118v2","updated":"2023-10-08T21:17:51Z","published":"2023-09-28T02:50:16Z","title":"D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable\n  Robotic Manipulation","summary":"  Scene representation has been a crucial design choice in robotic manipulation\nsystems. An ideal representation should be 3D, dynamic, and semantic to meet\nthe demands of diverse manipulation tasks. However, previous works often lack\nall three properties simultaneously. In this work, we introduce D$^3$Fields -\ndynamic 3D descriptor fields. These fields capture the dynamics of the\nunderlying 3D environment and encode both semantic features and instance masks.\nSpecifically, we project arbitrary 3D points in the workspace onto multi-view\n2D visual observations and interpolate features derived from foundational\nmodels. The resulting fused descriptor fields allow for flexible goal\nspecifications using 2D images with varied contexts, styles, and instances. To\nevaluate the effectiveness of these descriptor fields, we apply our\nrepresentation to a wide range of robotic manipulation tasks in a zero-shot\nmanner. Through extensive evaluation in both real-world scenarios and\nsimulations, we demonstrate that D$^3$Fields are both generalizable and\neffective for zero-shot robotic manipulation tasks. In quantitative comparisons\nwith state-of-the-art dense descriptors, such as Dense Object Nets and DINO,\nD$^3$Fields exhibit significantly better generalization abilities and\nmanipulation accuracy.\n","authors":["Yixuan Wang","Zhuoran Li","Mingtong Zhang","Katherine Driggs-Campbell","Jiajun Wu","Li Fei-Fei","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2309.16118v2.pdf","comment":"Project Page: https://robopil.github.io/d3fields/"},{"id":"http://arxiv.org/abs/2310.05270v1","updated":"2023-10-08T19:59:42Z","published":"2023-10-08T19:59:42Z","title":"Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using\n  a Novel Distributed Denoising CNN (DDCNN)","summary":"  Art restoration is crucial for preserving cultural heritage, but traditional\nmethods have limitations in faithfully reproducing original artworks while\naddressing issues like fading, staining, and damage. We present an innovative\napproach using deep learning, specifically Convolutional Neural Networks\n(CNNs), and Computer Vision techniques to revolutionize art restoration. We\nstart by creating a diverse dataset of deteriorated art images with various\ndistortions and degradation levels. This dataset trains a Distributed Denoising\nCNN (DDCNN) to remove distortions while preserving intricate details. Our\nmethod is adaptable to different distortion types and levels, making it\nsuitable for various deteriorated artworks, including paintings, sketches, and\nphotographs. Extensive experiments demonstrate our approach's efficiency and\neffectiveness compared to other Denoising CNN models. We achieve a substantial\nreduction in distortion, transforming deteriorated artworks into masterpieces.\nQuantitative evaluations confirm our method's superiority over traditional\ntechniques, reshaping the art restoration field and preserving cultural\nheritage. In summary, our paper introduces an AI-powered solution that combines\nComputer Vision and deep learning with DDCNN to restore artworks accurately,\novercoming limitations and paving the way for future advancements in art\nrestoration.\n","authors":["Sankar B.","Mukil Saravanan","Kalaivanan Kumar","Siri Dubbaka"],"pdf_url":"https://arxiv.org/pdf/2310.05270v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.05264v1","updated":"2023-10-08T19:02:46Z","published":"2023-10-08T19:02:46Z","title":"The Emergence of Reproducibility and Consistency in Diffusion Models","summary":"  Recently, diffusion models have emerged as powerful deep generative models,\nshowcasing cutting-edge performance across various applications such as image\ngeneration, solving inverse problems, and text-to-image synthesis. These models\ngenerate new data (e.g., images) by transforming random noise inputs through a\nreverse diffusion process. In this work, we uncover a distinct and prevalent\nphenomenon within diffusion models in contrast to most other generative models,\nwhich we refer to as ``consistent model reproducibility''. To elaborate, our\nextensive experiments have consistently shown that when starting with the same\ninitial noise input and sampling with a deterministic solver, diffusion models\ntend to produce nearly identical output content. This consistency holds true\nregardless of the choices of model architectures and training procedures.\nAdditionally, our research has unveiled that this exceptional model\nreproducibility manifests in two distinct training regimes: (i) ``memorization\nregime,'' characterized by a significantly overparameterized model which\nattains reproducibility mainly by memorizing the training data; (ii)\n``generalization regime,'' in which the model is trained on an extensive\ndataset, and its reproducibility emerges with the model's generalization\ncapabilities. Our analysis provides theoretical justification for the model\nreproducibility in ``memorization regime''. Moreover, our research reveals that\nthis valuable property generalizes to many variants of diffusion models,\nincluding conditional diffusion models, diffusion models for solving inverse\nproblems, and fine-tuned diffusion models. A deeper understanding of this\nphenomenon has the potential to yield more interpretable and controllable data\ngenerative processes based on diffusion models.\n","authors":["Huijie Zhang","Jinfan Zhou","Yifu Lu","Minzhe Guo","Liyue Shen","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05264v1.pdf","comment":"41 pages, 21 figures"},{"id":"http://arxiv.org/abs/2310.05262v1","updated":"2023-10-08T18:51:33Z","published":"2023-10-08T18:51:33Z","title":"Structure-Preserving Instance Segmentation via Skeleton-Aware Distance\n  Transform","summary":"  Objects with complex structures pose significant challenges to existing\ninstance segmentation methods that rely on boundary or affinity maps, which are\nvulnerable to small errors around contacting pixels that cause noticeable\nconnectivity change. While the distance transform (DT) makes instance interiors\nand boundaries more distinguishable, it tends to overlook the intra-object\nconnectivity for instances with varying width and result in over-segmentation.\nTo address these challenges, we propose a skeleton-aware distance transform\n(SDT) that combines the merits of object skeleton in preserving connectivity\nand DT in modeling geometric arrangement to represent instances with arbitrary\nstructures. Comprehensive experiments on histopathology image segmentation\ndemonstrate that SDT achieves state-of-the-art performance.\n","authors":["Zudi Lin","Donglai Wei","Aarush Gupta","Xingyu Liu","Deqing Sun","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2310.05262v1.pdf","comment":"MICCAI 2023 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2305.02034v3","updated":"2023-10-08T18:15:18Z","published":"2023-05-03T10:58:07Z","title":"SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment\n  Anything Model","summary":"  The success of the Segment Anything Model (SAM) demonstrates the significance\nof data-centric machine learning. However, due to the difficulties and high\ncosts associated with annotating Remote Sensing (RS) images, a large amount of\nvaluable RS data remains unlabeled, particularly at the pixel level. In this\nstudy, we leverage SAM and existing RS object detection datasets to develop an\nefficient pipeline for generating a large-scale RS segmentation dataset, dubbed\nSAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances,\nsurpassing existing high-resolution RS segmentation datasets in size by several\norders of magnitude. It provides object category, location, and instance\ninformation that can be used for semantic segmentation, instance segmentation,\nand object detection, either individually or in combination. We also provide a\ncomprehensive analysis of SAMRS from various aspects. Moreover, preliminary\nexperiments highlight the importance of conducting segmentation pre-training\nwith SAMRS to address task discrepancies and alleviate the limitations posed by\nlimited training data during fine-tuning. The code and dataset will be\navailable at https://github.com/ViTAE-Transformer/SAMRS.\n","authors":["Di Wang","Jing Zhang","Bo Du","Minqiang Xu","Lin Liu","Dacheng Tao","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.02034v3.pdf","comment":"Accepted by NeurIPS 2023 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2310.05255v1","updated":"2023-10-08T18:07:15Z","published":"2023-10-08T18:07:15Z","title":"Persis: A Persian Font Recognition Pipeline Using Convolutional Neural\n  Networks","summary":"  What happens if we encounter a suitable font for our design work but do not\nknow its name? Visual Font Recognition (VFR) systems are used to identify the\nfont typeface in an image. These systems can assist graphic designers in\nidentifying fonts used in images. A VFR system also aids in improving the speed\nand accuracy of Optical Character Recognition (OCR) systems. In this paper, we\nintroduce the first publicly available datasets in the field of Persian font\nrecognition and employ Convolutional Neural Networks (CNN) to address this\nproblem. The results show that the proposed pipeline obtained 78.0% top-1\naccuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the\nKAFD dataset. Furthermore, the average time spent in the entire pipeline for\none sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,\nrespectively. We conclude that CNN methods can be used to recognize Persian\nfonts without the need for additional pre-processing steps such as feature\nextraction, binarization, normalization, etc.\n","authors":["Mehrdad Mohammadian","Neda Maleki","Tobias Olsson","Fredrik Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2310.05255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05241v1","updated":"2023-10-08T17:19:58Z","published":"2023-10-08T17:19:58Z","title":"SCANet: Scene Complexity Aware Network for Weakly-Supervised Video\n  Moment Retrieval","summary":"  Video moment retrieval aims to localize moments in video corresponding to a\ngiven language query. To avoid the expensive cost of annotating the temporal\nmoments, weakly-supervised VMR (wsVMR) systems have been studied. For such\nsystems, generating a number of proposals as moment candidates and then\nselecting the most appropriate proposal has been a popular approach. These\nproposals are assumed to contain many distinguishable scenes in a video as\ncandidates. However, existing proposals of wsVMR systems do not respect the\nvarying numbers of scenes in each video, where the proposals are heuristically\ndetermined irrespective of the video. We argue that the retrieval system should\nbe able to counter the complexities caused by varying numbers of scenes in each\nvideo. To this end, we present a novel concept of a retrieval system referred\nto as Scene Complexity Aware Network (SCANet), which measures the `scene\ncomplexity' of multiple scenes in each video and generates adaptive proposals\nresponding to variable complexities of scenes in each video. Experimental\nresults on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)\nachieve state-of-the-art performances and demonstrate the effectiveness of\nincorporating the scene complexity.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Dahyun Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2310.05241v1.pdf","comment":"11 pages, Accepted in ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05237v1","updated":"2023-10-08T17:11:14Z","published":"2023-10-08T17:11:14Z","title":"Latent Diffusion Model for Medical Image Standardization and Enhancement","summary":"  Computed tomography (CT) serves as an effective tool for lung cancer\nscreening, diagnosis, treatment, and prognosis, providing a rich source of\nfeatures to quantify temporal and spatial tumor changes. Nonetheless, the\ndiversity of CT scanners and customized acquisition protocols can introduce\nsignificant inconsistencies in texture features, even when assessing the same\npatient. This variability poses a fundamental challenge for subsequent research\nthat relies on consistent image features. Existing CT image standardization\nmodels predominantly utilize GAN-based supervised or semi-supervised learning,\nbut their performance remains limited. We present DiffusionCT, an innovative\nscore-based DDPM model that operates in the latent space to transform disparate\nnon-standard distributions into a standardized form. The architecture comprises\na U-Net-based encoder-decoder, augmented by a DDPM model integrated at the\nbottleneck position. First, the encoder-decoder is trained independently,\nwithout embedding DDPM, to capture the latent representation of the input data.\nSecond, the latent DDPM model is trained while keeping the encoder-decoder\nparameters fixed. Finally, the decoder uses the transformed latent\nrepresentation to generate a standardized CT image, providing a more consistent\nbasis for downstream analysis. Empirical tests on patient CT images indicate\nnotable improvements in image standardization using DiffusionCT. Additionally,\nthe model significantly reduces image noise in SPAD images, further validating\nthe effectiveness of DiffusionCT for advanced imaging tasks.\n","authors":["Md Selim","Jie Zhang","Faraneh Fathi","Michael A. Brooks","Ge Wang","Guoqiang Yu","Jin Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16772v3","updated":"2023-10-08T16:32:45Z","published":"2023-09-28T18:09:40Z","title":"XVO: Generalized Visual Odometry via Cross-Modal Self-Training","summary":"  We propose XVO, a semi-supervised learning method for training generalized\nmonocular Visual Odometry (VO) models with robust off-the-self operation across\ndiverse datasets and settings. In contrast to standard monocular VO approaches\nwhich often study a known calibration within a single dataset, XVO efficiently\nlearns to recover relative pose with real-world scale from visual scene\nsemantics, i.e., without relying on any known camera parameters. We optimize\nthe motion estimation model via self-training from large amounts of\nunconstrained and heterogeneous dash camera videos available on YouTube. Our\nkey contribution is twofold. First, we empirically demonstrate the benefits of\nsemi-supervised training for learning a general-purpose direct VO regression\nnetwork. Second, we demonstrate multi-modal supervision, including\nsegmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate\ngeneralized representations for the VO task. Specifically, we find audio\nprediction task to significantly enhance the semi-supervised learning process\nwhile alleviating noisy pseudo-labels, particularly in highly dynamic and\nout-of-domain video data. Our proposed teacher network achieves\nstate-of-the-art performance on the commonly used KITTI benchmark despite no\nmulti-frame optimization or knowledge of camera parameters. Combined with the\nproposed semi-supervised step, XVO demonstrates off-the-shelf knowledge\ntransfer across diverse conditions on KITTI, nuScenes, and Argoverse without\nfine-tuning.\n","authors":["Lei Lai","Zhongkai Shangguan","Jimuyang Zhang","Eshed Ohn-Bar"],"pdf_url":"https://arxiv.org/pdf/2309.16772v3.pdf","comment":"ICCV 2023, Paris https://genxvo.github.io/"},{"id":"http://arxiv.org/abs/2310.05212v1","updated":"2023-10-08T16:05:17Z","published":"2023-10-08T16:05:17Z","title":"Interpretable Semiotics Networks Representing Awareness","summary":"  Humans perceive objects daily and communicate their perceptions using various\nchannels. Here, we describe a computational model that track and simulate\nobjects' perception, and their representations as they pass in communication.\n  We describe two key components of our internal representation ('observed' and\n'seen') and relate them to familiar computer vision terms (encoding and\ndecoding). These elements joined together to form semiotic networks, which\nsimulate awareness in object perception and human communication.\n  Nowadays, most neural networks are uninterpretable. On the other hand, our\nmodel is free from this disadvantages. We performed several experiments and\ndemonstrated the visibility of our model.\n  We describe how our network may be used as preprocessing unit to any\nclassification network. In our experiments the compound network overperforms in\naverage the classification network at datasets with small training data.\n  Future work would leverage our model to gain better understanding of human\ncommunications and personal representations.\n","authors":["David Kupeev","Eyal Nitcany"],"pdf_url":"https://arxiv.org/pdf/2310.05212v1.pdf","comment":"58 pages"},{"id":"http://arxiv.org/abs/2208.04173v2","updated":"2023-10-08T15:55:35Z","published":"2022-08-08T14:26:35Z","title":"SIAD: Self-supervised Image Anomaly Detection System","summary":"  Recent trends in AIGC effectively boosted the application of visual\ninspection. However, most of the available systems work in a human-in-the-loop\nmanner and can not provide long-term support to the online application. To make\na step forward, this paper outlines an automatic annotation system called SsaA,\nworking in a self-supervised learning manner, for continuously making the\nonline visual inspection in the manufacturing automation scenarios. Benefit\nfrom the self-supervised learning, SsaA is effective to establish a visual\ninspection application for the whole life-cycle of manufacturing. In the early\nstage, with only the anomaly-free data, the unsupervised algorithms are adopted\nto process the pretext task and generate coarse labels for the following data.\nThen supervised algorithms are trained for the downstream task. With\nuser-friendly web-based interfaces, SsaA is very convenient to integrate and\ndeploy both of the unsupervised and supervised algorithms. So far, the SsaA\nsystem has been adopted for some real-life industrial applications.\n","authors":["Jiawei Li","Chenxi Lan","Xinyi Zhang","Bolin Jiang","Yuqiu Xie","Naiqi Li","Yan Liu","Yaowei Li","Enze Huo","Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2208.04173v2.pdf","comment":"4 pages, 3 figures, ICCV 2023 Demo Track"},{"id":"http://arxiv.org/abs/2310.05207v1","updated":"2023-10-08T15:49:26Z","published":"2023-10-08T15:49:26Z","title":"Boosting Facial Action Unit Detection Through Jointly Learning Facial\n  Landmark Detection and Domain Separation and Reconstruction","summary":"  Recently how to introduce large amounts of unlabeled facial images in the\nwild into supervised Facial Action Unit (AU) detection frameworks has become a\nchallenging problem. In this paper, we propose a new AU detection framework\nwhere multi-task learning is introduced to jointly learn AU domain separation\nand reconstruction and facial landmark detection by sharing the parameters of\nhomostructural facial extraction modules. In addition, we propose a new feature\nalignment scheme based on contrastive learning by simple projectors and an\nimproved contrastive loss, which adds four additional intermediate supervisors\nto promote the feature reconstruction process. Experimental results on two\nbenchmarks demonstrate our superiority against the state-of-the-art methods for\nAU detection in the wild.\n","authors":["Ziqiao Shang","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05207v1.pdf","comment":"5 pages, 1 figure, published to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05202v1","updated":"2023-10-08T15:28:01Z","published":"2023-10-08T15:28:01Z","title":"Enhancing Cross-Dataset Performance of Distracted Driving Detection With\n  Score-Softmax Classifier","summary":"  Deep neural networks enable real-time monitoring of in-vehicle driver,\nfacilitating the timely prediction of distractions, fatigue, and potential\nhazards. This technology is now integral to intelligent transportation systems.\nRecent research has exposed unreliable cross-dataset end-to-end driver behavior\nrecognition due to overfitting, often referred to as ``shortcut learning\",\nresulting from limited data samples. In this paper, we introduce the\nScore-Softmax classifier, which addresses this issue by enhancing inter-class\nindependence and Intra-class uncertainty. Motivated by human rating patterns,\nwe designed a two-dimensional supervisory matrix based on marginal Gaussian\ndistributions to train the classifier. Gaussian distributions help amplify\nintra-class uncertainty while ensuring the Score-Softmax classifier learns\naccurate knowledge. Furthermore, leveraging the summation of independent\nGaussian distributed random variables, we introduced a multi-channel\ninformation fusion method. This strategy effectively resolves the\nmulti-information fusion challenge for the Score-Softmax classifier.\nConcurrently, we substantiate the necessity of transfer learning and\nmulti-dataset combination. We conducted cross-dataset experiments using the\nSFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax\nimproves cross-dataset performance without modifying the model architecture.\nThis provides a new approach for enhancing neural network generalization.\nAdditionally, our information fusion approach outperforms traditional methods.\n","authors":["Cong Duan","Zixuan Liu","Jiahao Xia","Minghai Zhang","Jiacai Liao","Libo Cao"],"pdf_url":"https://arxiv.org/pdf/2310.05202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11642v3","updated":"2023-10-08T15:19:18Z","published":"2022-12-22T12:15:37Z","title":"Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for\n  Video Prediction","summary":"  We present a multi-scale predictive coding model for future video frames\nprediction. Drawing inspiration on the ``Predictive Coding\" theories in\ncognitive science, it is updated by a combination of bottom-up and top-down\ninformation flows, which can enhance the interaction between different network\nlevels. However, traditional predictive coding models only predict what is\nhappening hierarchically rather than predicting the future. To address the\nproblem, our model employs a multi-scale approach (Coarse to Fine), where the\nhigher level neurons generate coarser predictions (lower resolution), while the\nlower level generate finer predictions (higher resolution). In terms of network\narchitecture, we directly incorporate the encoder-decoder network within the\nLSTM module and share the final encoded high-level semantic information across\ndifferent network levels. This enables comprehensive interaction between the\ncurrent input and the historical states of LSTM compared with the traditional\nEncoder-LSTM-Decoder architecture, thus learning more believable temporal and\nspatial dependencies. Furthermore, to tackle the instability in adversarial\ntraining and mitigate the accumulation of prediction errors in long-term\nprediction, we propose several improvements to the training strategy. Our\napproach achieves good performance on datasets such as KTH, Moving MNIST and\nCaltech Pedestrian. Code is available at https://github.com/Ling-CF/MSPN.\n","authors":["Chaofan Ling","Junpei Zhong","Weihua Li"],"pdf_url":"https://arxiv.org/pdf/2212.11642v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05195v1","updated":"2023-10-08T15:04:50Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v1.pdf","comment":"Work in progress. The code will be released"},{"id":"http://arxiv.org/abs/2310.05193v1","updated":"2023-10-08T15:01:54Z","published":"2023-10-08T15:01:54Z","title":"Improving Discriminative Multi-Modal Learning with Large-Scale\n  Pre-Trained Models","summary":"  This paper investigates how to better leverage large-scale pre-trained\nuni-modal models to further enhance discriminative multi-modal learning. Even\nwhen fine-tuned with only uni-modal data, these models can outperform previous\nmulti-modal models in certain tasks. It's clear that their incorporation into\nmulti-modal learning would significantly improve performance. However,\nmulti-modal learning with these models still suffers from insufficient learning\nof uni-modal features, which weakens the resulting multi-modal model's\ngeneralization ability. While fine-tuning uni-modal models separately and then\naggregating their predictions is straightforward, it doesn't allow for adequate\nadaptation between modalities, also leading to sub-optimal results. To this\nend, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By\nfreezing the weights of uni-modal fine-tuned models, adding extra trainable\nrank decomposition matrices to them, and subsequently performing multi-modal\njoint training, our method enhances adaptation between modalities and boosts\noverall performance. We demonstrate the effectiveness of MMLoRA on three\ndataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D),\nvision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).\n","authors":["Chenzhuang Du","Yue Zhao","Chonghua Liao","Jiacheng You","Jie Fu","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.05193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05192v1","updated":"2023-10-08T15:00:38Z","published":"2023-10-08T15:00:38Z","title":"HOD: A Benchmark Dataset for Harmful Object Detection","summary":"  Recent multi-media data such as images and videos have been rapidly spread\nout on various online services such as social network services (SNS). With the\nexplosive growth of online media services, the number of image content that may\nharm users is also growing exponentially. Thus, most recent online platforms\nsuch as Facebook and Instagram have adopted content filtering systems to\nprevent the prevalence of harmful content and reduce the possible risk of\nadverse effects on users. Unfortunately, computer vision research on detecting\nharmful content has not yet attracted attention enough. Users of each platform\nstill manually click the report button to recognize patterns of harmful content\nthey dislike when exposed to harmful content. However, the problem with manual\nreporting is that users are already exposed to harmful content. To address\nthese issues, our research goal in this work is to develop automatic harmful\nobject detection systems for online services. We present a new benchmark\ndataset for harmful object detection. Unlike most related studies focusing on a\nsmall subset of object categories, our dataset addresses various categories.\nSpecifically, our proposed dataset contains more than 10,000 images across 6\ncategories that might be harmful, consisting of not only normal cases but also\nhard cases that are difficult to detect. Moreover, we have conducted extensive\nexperiments to evaluate the effectiveness of our proposed dataset. We have\nutilized the recently proposed state-of-the-art (SOTA) object detection\narchitectures and demonstrated our proposed dataset can be greatly useful for\nthe real-time harmful object detection task. The whole source codes and\ndatasets are publicly accessible at\nhttps://github.com/poori-nuna/HOD-Benchmark-Dataset.\n","authors":["Eungyeom Ha","Heemook Kim","Sung Chul Hong","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2310.05192v1.pdf","comment":"13 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.05258v1","updated":"2023-10-08T18:28:17Z","published":"2023-10-08T18:28:17Z","title":"A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and\n  Locations in the Healthcare Domain","summary":"  Efficiently finding doctors and locations is an important search problem for\npatients in the healthcare domain, for which traditional information retrieval\nmethods tend not to work optimally. In the last ten years, knowledge graphs\n(KGs) have emerged as a powerful way to combine the benefits of gleaning\ninsights from semi-structured data using semantic modeling, natural language\nprocessing techniques like information extraction, and robust querying using\nstructured query languages like SPARQL and Cypher. In this short paper, we\npresent a KG-based search engine architecture for robustly finding doctors and\nlocations in the healthcare domain. Early results demonstrate that our approach\ncan lead to significantly higher coverage for complex queries without degrading\nquality.\n","authors":["Mayank Kejriwal","Hamid Haidarian","Min-Hsueh Chiu","Andy Xiang","Deep Shrestha","Faizan Javed"],"pdf_url":"https://arxiv.org/pdf/2310.05258v1.pdf","comment":"Presented as an applied data science poster in KDD 2023"},{"id":"http://arxiv.org/abs/2310.05195v1","updated":"2023-10-08T15:04:50Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v1.pdf","comment":"Work in progress. The code will be released"},{"id":"http://arxiv.org/abs/2310.05150v1","updated":"2023-10-08T12:52:09Z","published":"2023-10-08T12:52:09Z","title":"From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for\n  Conversational Exploratory Search","summary":"  Exploratory search is an open-ended information retrieval process that aims\nat discovering knowledge about a topic or domain rather than searching for a\nspecific answer or piece of information. Conversational interfaces are\nparticularly suitable for supporting exploratory search, allowing users to\nrefine queries and examine search results through interactive dialogues. In\naddition to conversational search interfaces, knowledge graphs are also useful\nin supporting information exploration due to their rich semantic representation\nof data items. In this study, we demonstrate the synergistic effects of\ncombining knowledge graphs and conversational interfaces for exploratory\nsearch, bridging the gap between structured and unstructured information\nretrieval. To this end, we propose a knowledge-driven dialogue system for\nexploring news articles by asking natural language questions and using the\ngraph structure to navigate between related topics. Based on a user study with\n54 participants, we empirically evaluate the effectiveness of the graph-based\nexploratory search and discuss design implications for developing such systems.\n","authors":["Phillip Schneider","Nils Rehtanz","Kristiina Jokinen","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2310.05150v1.pdf","comment":"Accepted to PACLIC 2023"},{"id":"http://arxiv.org/abs/2310.05116v1","updated":"2023-10-08T11:09:16Z","published":"2023-10-08T11:09:16Z","title":"CARLG: Leveraging Contextual Clues and Role Correlations for Improving\n  Document-level Event Argument Extraction","summary":"  Document-level event argument extraction (EAE) is a crucial but challenging\nsubtask in information extraction. Most existing approaches focus on the\ninteraction between arguments and event triggers, ignoring two critical points:\nthe information of contextual clues and the semantic correlations among\nargument roles. In this paper, we propose the CARLG model, which consists of\ntwo modules: Contextual Clues Aggregation (CCA) and Role-based Latent\nInformation Guidance (RLIG), effectively leveraging contextual clues and role\ncorrelations for improving document-level EAE. The CCA module adaptively\ncaptures and integrates contextual clues by utilizing context attention weights\nfrom a pre-trained encoder. The RLIG module captures semantic correlations\nthrough role-interactive encoding and provides valuable information guidance\nwith latent role representation. Notably, our CCA and RLIG modules are compact,\ntransplantable and efficient, which introduce no more than 1% new parameters\nand can be easily equipped on other span-base methods with significant\nperformance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE\ndatasets demonstrate the superiority of the proposed CARLG model. It\noutperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98\nF1, respectively, while reducing the inference time by 31%. Furthermore, we\nprovide detailed experimental analyses based on the performance gains and\nillustrate the interpretability of our model.\n","authors":["Wanlong Liu","Wenyu Chen","Dingyi Zeng","Li Zhou","Hong Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05116v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2308.02925v2","updated":"2023-10-08T08:56:56Z","published":"2023-08-05T17:33:17Z","title":"ConvFormer: Revisiting Transformer for Sequential User Modeling","summary":"  Sequential user modeling, a critical task in personalized recommender\nsystems, focuses on predicting the next item a user would prefer, requiring a\ndeep understanding of user behavior sequences. Despite the remarkable success\nof Transformer-based models across various domains, their full potential in\ncomprehending user behavior remains untapped. In this paper, we re-examine\nTransformer-like architectures aiming to advance state-of-the-art performance.\nWe start by revisiting the core building blocks of Transformer-based methods,\nanalyzing the effectiveness of the item-to-item mechanism within the context of\nsequential user modeling. After conducting a thorough experimental analysis, we\nidentify three essential criteria for devising efficient sequential user\nmodels, which we hope will serve as practical guidelines to inspire and shape\nfuture designs. Following this, we introduce ConvFormer, a simple but powerful\nmodification to the Transformer architecture that meets these criteria,\nyielding state-of-the-art results. Additionally, we present an acceleration\ntechnique to minimize the complexity associated with processing extremely long\nsequences. Experiments on four public datasets showcase ConvFormer's\nsuperiority and confirm the validity of our proposed criteria.\n","authors":["Hao Wang","Jianxun Lian","Mingqi Wu","Haoxuan Li","Jiajun Fan","Wanyue Xu","Chaozhuo Li","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2308.02925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06674v2","updated":"2023-10-08T02:45:12Z","published":"2023-02-13T20:27:26Z","title":"PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded\n  Dialogue","summary":"  Identifying relevant persona or knowledge for conversational systems is\ncritical to grounded dialogue response generation. However, each grounding has\nbeen mostly researched in isolation with more practical multi-context dialogue\ntasks introduced in recent works. We define Persona and Knowledge Dual Context\nIdentification as the task to identify persona and knowledge jointly for a\ngiven dialogue, which could be of elevated importance in complex multi-context\ndialogue settings. We develop a novel grounding retrieval method that utilizes\nall contexts of dialogue simultaneously. Our method requires less computational\npower via utilizing neural QA retrieval models. We further introduce our novel\nnull-positive rank test which measures ranking performance on semantically\ndissimilar samples (i.e. hard negatives) in relation to data augmentation.\n","authors":["Minsik Oh","Joosung Lee","Jiwei Li","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2302.06674v2.pdf","comment":"Accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.04954v1","updated":"2023-10-08T00:35:54Z","published":"2023-10-08T00:35:54Z","title":"A framework to generate sparsity-inducing regularizers for enhanced\n  low-rank matrix completion","summary":"  Applying half-quadratic optimization to loss functions can yield the\ncorresponding regularizers, while these regularizers are usually not\nsparsity-inducing regularizers (SIRs). To solve this problem, we devise a\nframework to generate an SIR with closed-form proximity operator. Besides, we\nspecify our framework using several commonly-used loss functions, and produce\nthe corresponding SIRs, which are then adopted as nonconvex rank surrogates for\nlow-rank matrix completion. Furthermore, algorithms based on the alternating\ndirection method of multipliers are developed. Extensive numerical results show\nthe effectiveness of our methods in terms of recovery performance and runtime.\n","authors":["Zhi-Yong Wang","Hing Cheung So"],"pdf_url":"https://arxiv.org/pdf/2310.04954v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.09075v4","updated":"2023-10-08T18:37:54Z","published":"2023-09-16T18:48:40Z","title":"Music Generation based on Generative Adversarial Networks with\n  Transformer","summary":"  Autoregressive models based on Transformers have become the prevailing\napproach for generating music compositions that exhibit comprehensive musical\nstructure. These models are typically trained by minimizing the negative\nlog-likelihood (NLL) of the observed sequence in an autoregressive manner.\nHowever, when generating long sequences, the quality of samples from these\nmodels tends to significantly deteriorate due to exposure bias. To address this\nissue, we leverage classifiers trained to differentiate between real and\nsampled sequences to identify these failures. This observation motivates our\nexploration of adversarial losses as a complement to the NLL objective. We\nemploy a pre-trained Span-BERT model as the discriminator in the Generative\nAdversarial Network (GAN) framework, which enhances training stability in our\nexperiments. To optimize discrete sequences within the GAN framework, we\nutilize the Gumbel-Softmax trick to obtain a differentiable approximation of\nthe sampling process. Additionally, we partition the sequences into smaller\nchunks to ensure that memory constraints are met. Through human evaluations and\nthe introduction of a novel discriminative metric, we demonstrate that our\napproach outperforms a baseline model trained solely on likelihood\nmaximization.\n","authors":["Ziyi Jiang","Ruoxue Wu","Zhenghan Chen","Xiaoxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2309.09075v4.pdf","comment":"error upload"},{"id":"http://arxiv.org/abs/2309.15857v2","updated":"2023-10-08T15:32:51Z","published":"2023-09-23T15:21:15Z","title":"A Survey on Image-text Multimodal Models","summary":"  Amidst the evolving landscape of artificial intelligence, the convergence of\nvisual and textual information has surfaced as a crucial frontier, leading to\nthe advent of image-text multimodal models. This paper provides a comprehensive\nreview of the evolution and current state of image-text multimodal models,\nexploring their application value, challenges, and potential research\ntrajectories. Initially, we revisit the basic concepts and developmental\nmilestones of these models, introducing a novel classification that segments\ntheir evolution into three distinct phases, based on their time of introduction\nand subsequent impact on the discipline. Furthermore, based on the tasks'\nsignificance and prevalence in the academic landscape, we propose a\ncategorization of the tasks associated with image-text multimodal models into\nfive major types, elucidating the recent progress and key technologies within\neach category. Despite the remarkable accomplishments of these models, numerous\nchallenges and issues persist. This paper delves into the inherent challenges\nand limitations of image-text multimodal models, fostering the exploration of\nprospective research directions. Our objective is to offer an exhaustive\noverview of the present research landscape of image-text multimodal models and\nto serve as a valuable reference for future scholarly endeavors. We extend an\ninvitation to the broader community to collaborate in enhancing the image-text\nmultimodal model community, accessible at:\n\\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.\n","authors":["Ruifeng Guo","Jingxuan Wei","Linzhuang Sun","Bihui Yu","Guiyong Chang","Dawei Liu","Sibo Zhang","Zhengbing Yao","Mingjun Xu","Liping Bu"],"pdf_url":"https://arxiv.org/pdf/2309.15857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05195v1","updated":"2023-10-08T15:04:50Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v1.pdf","comment":"Work in progress. The code will be released"},{"id":"http://arxiv.org/abs/2310.00068v2","updated":"2023-10-08T14:31:02Z","published":"2023-09-29T18:18:32Z","title":"Emotional Listener Portrait: Neural Listener Head Generation with\n  Emotion","summary":"  Listener head generation centers on generating non-verbal behaviors (e.g.,\nsmile) of a listener in reference to the information delivered by a speaker. A\nsignificant challenge when generating such responses is the non-deterministic\nnature of fine-grained facial expressions during a conversation, which varies\ndepending on the emotions and attitudes of both the speaker and the listener.\nTo tackle this problem, we propose the Emotional Listener Portrait (ELP), which\ntreats each fine-grained facial motion as a composition of several discrete\nmotion-codewords and explicitly models the probability distribution of the\nmotions under different emotion in conversation. Benefiting from the\n``explicit'' and ``discrete'' design, our ELP model can not only automatically\ngenerate natural and diverse responses toward a given speaker via sampling from\nthe learned distribution but also generate controllable responses with a\npredetermined attitude. Under several quantitative metrics, our ELP exhibits\nsignificant improvements compared to previous methods.\n","authors":["Luchuan Song","Guojun Yin","Zhenchao Jin","Xiaoyi Dong","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.00068v2.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2310.05158v1","updated":"2023-10-08T13:22:20Z","published":"2023-10-08T13:22:20Z","title":"ITRE: Low-light Image Enhancement Based on Illumination Transmission\n  Ratio Estimation","summary":"  Noise, artifacts, and over-exposure are significant challenges in the field\nof low-light image enhancement. Existing methods often struggle to address\nthese issues simultaneously. In this paper, we propose a novel Retinex-based\nmethod, called ITRE, which suppresses noise and artifacts from the origin of\nthe model, prevents over-exposure throughout the enhancement process.\nSpecifically, we assume that there must exist a pixel which is least disturbed\nby low light within pixels of same color. First, clustering the pixels on the\nRGB color space to find the Illumination Transmission Ratio (ITR) matrix of the\nwhole image, which determines that noise is not over-amplified easily. Next, we\nconsider ITR of the image as the initial illumination transmission map to\nconstruct a base model for refined transmission map, which prevents artifacts.\nAdditionally, we design an over-exposure module that captures the fundamental\ncharacteristics of pixel over-exposure and seamlessly integrate it into the\nbase model. Finally, there is a possibility of weak enhancement when\ninter-class distance of pixels with same color is too small. To counteract\nthis, we design a Robust-Guard module that safeguards the robustness of the\nimage enhancement process. Extensive experiments demonstrate the effectiveness\nof our approach in suppressing noise, preventing artifacts, and controlling\nover-exposure level simultaneously. Our method performs superiority in\nqualitative and quantitative performance evaluations by comparing with\nstate-of-the-art methods.\n","authors":["Yu Wang","Yihong Wang","Tong Liu","Xiubao Sui","Qian Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05099v1","updated":"2023-10-08T10:01:28Z","published":"2023-10-08T10:01:28Z","title":"Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive\n  Telemedicine Applications","summary":"  Telemedicine applications have recently received substantial potential and\ninterest, especially after the COVID-19 pandemic. Remote experience will help\npeople get their complex surgery done or transfer knowledge to local surgeons,\nwithout the need to travel abroad. Even with breakthrough improvements in\ninternet speeds, the delay in video streaming is still a hurdle in telemedicine\napplications. This imposes using image compression and region of interest (ROI)\ntechniques to reduce the data size and transmission needs. This paper proposes\na Deep Reinforcement Learning (DRL) model that intelligently adapts the ROI\nsize and non-ROI quality depending on the estimated throughput. The delay and\nstructural similarity index measure (SSIM) comparison are used to assess the\nDRL model. The comparison findings and the practical application reveal that\nDRL is capable of reducing the delay by 13% and keeping the overall quality in\nan acceptable range. Since the latency has been significantly reduced, these\nfindings are a valuable enhancement to telemedicine applications.\n","authors":["Abdulrahman Soliman","Amr Mohamed","Elias Yaacoub","Nikhil V. Navkar","Aiman Erbad"],"pdf_url":"https://arxiv.org/pdf/2310.05099v1.pdf","comment":"7 pages"}]},"2023-10-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2308.02580v2","updated":"2023-10-07T19:41:00Z","published":"2023-08-03T16:13:46Z","title":"Gaussian-based Probabilistic Deep Supervision Network for\n  Noise-Resistant QoS Prediction","summary":"  Quality of Service (QoS) prediction is an essential task in recommendation\nsystems, where accurately predicting unknown QoS values can improve user\nsatisfaction. However, existing QoS prediction techniques may perform poorly in\nthe presence of noise data, such as fake location information or virtual\ngateways. In this paper, we propose the Probabilistic Deep Supervision Network\n(PDS-Net), a novel framework for QoS prediction that addresses this issue.\nPDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate\nlayers and learns probability spaces for both known features and true labels.\nMoreover, PDS-Net employs a condition-based multitasking loss function to\nidentify objects with noise data and applies supervision directly to deep\nfeatures sampled from the probability space by optimizing the Kullback-Leibler\ndistance between the probability space of these objects and the real-label\nprobability space. Thus, PDS-Net effectively reduces errors resulting from the\npropagation of corrupted data, leading to more accurate QoS predictions.\nExperimental evaluations on two real-world QoS datasets demonstrate that the\nproposed PDS-Net outperforms state-of-the-art baselines, validating the\neffectiveness of our approach.\n","authors":["Ziliang Wang","Xiaohong Zhang","Sheng Huang","Wei Zhang","Dan Yang","Meng Yan"],"pdf_url":"https://arxiv.org/pdf/2308.02580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04892v1","updated":"2023-10-07T18:29:33Z","published":"2023-10-07T18:29:33Z","title":"Commercialized Generative AI: A Critical Study of the Feasibility and\n  Ethics of Generating Native Advertising Using Large Language Models in\n  Conversational Web Search","summary":"  How will generative AI pay for itself? Unless charging users for access,\nselling advertising is the only alternative. Especially in the multi-billion\ndollar web search market with ads as the main source of revenue, the\nintroduction of a subscription model seems unlikely. The recent disruption of\nsearch by generative large language models could thus ultimately be accompanied\nby generated ads. Our concern is that the commercialization of generative AI in\ngeneral and large language models in particular could lead to native\nadvertising in the form of quite subtle brand or product placements. In web\nsearch, the evolution of search engine results pages (SERPs) from traditional\nlists of ``ten blue links'' (lists SERPs) to generated text with web page\nreferences (text SERPs) may further blur the line between advertising-based and\norganic search results, making it difficult for users to distinguish between\nthe two, depending on how advertising is integrated and disclosed. To raise\nawareness of this potential development, we conduct a pilot study analyzing the\ncapabilities of current large language models to blend ads with organic search\nresults. Although the models still struggle to subtly frame ads in an unrelated\ncontext, their potential is evident when integrating ads into related topics\nwhich calls for further investigation.\n","authors":["Ines Zelch","Matthias Hagen","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2310.04892v1.pdf","comment":"Presented at OSSYM 2023"},{"id":"http://arxiv.org/abs/2310.04878v1","updated":"2023-10-07T17:24:41Z","published":"2023-10-07T17:24:41Z","title":"Hybrid Recommendation System using Graph Neural Network and BERT\n  Embeddings","summary":"  Recommender systems have emerged as a crucial component of the modern web\necosystem. The effectiveness and accuracy of such systems are critical for\nproviding users with personalized recommendations that meet their specific\ninterests and needs. In this paper, we introduce a novel model that utilizes a\nGraph Neural Network (GNN) in conjunction with sentence transformer embeddings\nto predict anime recommendations for different users. Our model employs the\ntask of link prediction to create a recommendation system that considers both\nthe features of anime and user interactions with different anime. The\nhybridization of the GNN and transformer embeddings enables us to capture both\ninter-level and intra-level features of anime data.Our model not only\nrecommends anime to users but also predicts the rating a specific user would\ngive to an anime. We utilize the GraphSAGE network for model building and\nweighted root mean square error (RMSE) to evaluate the performance of the\nmodel. Our approach has the potential to significantly enhance the accuracy and\neffectiveness of anime recommendation systems and can be extended to other\ndomains that require personalized recommendations.\n","authors":["Shashidhar Reddy Javaji","Krutika Sarode"],"pdf_url":"https://arxiv.org/pdf/2310.04878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04865v1","updated":"2023-10-07T16:21:04Z","published":"2023-10-07T16:21:04Z","title":"ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding","summary":"  Developing text mining approaches to mine aspects from customer reviews has\nbeen well-studied due to its importance in understanding customer needs and\nproduct attributes. In contrast, it remains unclear how to predict the future\nemerging aspects of a new product that currently has little review information.\nThis task, which we named product aspect forecasting, is critical for\nrecommending new products, but also challenging because of the missing reviews.\nHere, we propose ForeSeer, a novel textual mining and product embedding\napproach progressively trained on temporal product graphs for this novel\nproduct aspect forecasting task. ForeSeer transfers reviews from similar\nproducts on a large product graph and exploits these reviews to predict aspects\nthat might emerge in future reviews. A key novelty of our method is to jointly\nprovide review, product, and aspect embeddings that are both time-sensitive and\nless affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer\non a real-world product review system containing 11,536,382 reviews and 11,000\nproducts over 3 years. We observe that ForeSeer substantially outperformed\nexisting approaches with at least 49.1\\% AUPRC improvement under the real\nsetting where aspect associations are not given. ForeSeer further improves\nfuture link prediction on the product graph and the review aspect association\nprediction. Collectively, Foreseer offers a novel framework for review\nforecasting by effectively integrating review text, product network, and\ntemporal information, opening up new avenues for online shopping recommendation\nand e-commerce applications.\n","authors":["Zixuan Liu","Gaurush Hiranandani","Kun Qian","Eddie W. Huang","Yi Xu","Belinda Zeng","Karthik Subbian","Sheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04804v1","updated":"2023-10-07T13:45:13Z","published":"2023-10-07T13:45:13Z","title":"Ten Challenges in Industrial Recommender Systems","summary":"  Huawei's vision and mission is to build a fully connected intelligent world.\nSince 2013, Huawei Noah's Ark Lab has helped many products build recommender\nsystems and search engines for getting the right information to the right\nusers. Every day, our recommender systems serve hundreds of millions of mobile\nphone users and recommend different kinds of content and services such as apps,\nnews feeds, songs, videos, books, themes, and instant services. The big data\nand various scenarios provide us with great opportunities to develop advanced\nrecommendation technologies. Furthermore, we have witnessed the technical trend\nof recommendation models in the past ten years, from the shallow and simple\nmodels like collaborative filtering, linear models, low rank models to deep and\ncomplex models like neural networks, pre-trained language models. Based on the\nmission, opportunities and technological trends, we have also met several hard\nproblems in our recommender systems. In this talk, we will share ten important\nand interesting challenges and hope that the RecSys community can get inspired\nand create better recommender systems.\n","authors":["Zhenhua Dong","Jieming Zhu","Weiwen Liu","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2310.04804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04735v1","updated":"2023-10-07T08:22:27Z","published":"2023-10-07T08:22:27Z","title":"Investigating the Influence of Legal Case Retrieval Systems on Users'\n  Decision Process","summary":"  Given a specific query case, legal case retrieval systems aim to retrieve a\nset of case documents relevant to the case at hand. Previous studies on user\nbehavior analysis have shown that information retrieval (IR) systems can\nsignificantly influence users' decisions by presenting results in varying\norders and formats. However, whether such influence exists in legal case\nretrieval remains largely unknown. This study presents the first investigation\ninto the influence of legal case retrieval systems on the decision-making\nprocess of legal users. We conducted an online user study involving more than\nninety participants, and our findings suggest that the result distribution of\nlegal case retrieval systems indeed affect users' judgements on the sentences\nin cases. Notably, when users are presented with biased results that involve\nharsher sentences, they tend to impose harsher sentences on the current case as\nwell. This research highlights the importance of optimizing the unbiasedness of\nlegal case retrieval systems.\n","authors":["Beining Wang","Ruizhe Zhang","Yueyue Wu","Qingyao Ai","Min Zhang","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.04735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04678v1","updated":"2023-10-07T03:25:06Z","published":"2023-10-07T03:25:06Z","title":"DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based\n  Queries","summary":"  In scientific research, the ability to effectively retrieve relevant\ndocuments based on complex, multifaceted queries is critical. Existing\nevaluation datasets for this task are limited, primarily due to the high cost\nand effort required to annotate resources that effectively represent complex\nqueries. To address this, we propose a novel task, Scientific DOcument\nRetrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed\nto handle the complex nature of user queries in scientific research. We\ndeveloped a benchmark dataset within the field of computer science, consisting\nof 100 human-authored complex query cases. For each complex query, we assembled\na collection of 100 relevant documents and produced annotated relevance scores\nfor ranking them. Recognizing the significant labor of expert annotation, we\nalso introduce Anno-GPT, a scalable framework for validating the performance of\nLarge Language Models (LLMs) on expert-level dataset annotation tasks. LLM\nannotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,\nwithout compromising quality. Furthermore, due to the multi-tiered structure of\nthese complex queries, the DORIS-MAE dataset can be extended to over 4,000\nsub-query test cases without requiring additional annotation. We evaluated 17\nrecent retrieval methods on DORIS-MAE, observing notable performance drops\ncompared to traditional datasets. This highlights the need for better\napproaches to handle complex, multifaceted queries in scientific research. Our\ndataset and codebase are available at\nhttps://github.com/Real-Doris-Mae/Doris-Mae-Dataset.\n","authors":["Jianyou Wang","Kaicheng Wang","Xiaoyue Wang","Prudhviraj Naidu","Leon Bergen","Ramamohan Paturi"],"pdf_url":"https://arxiv.org/pdf/2310.04678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17866v2","updated":"2023-10-07T01:57:13Z","published":"2023-05-29T03:13:39Z","title":"Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation","summary":"  Traditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.\n","authors":["Jingjin Liu","Hankz Hankui Zhuo","Kebing Jin","Jiamin Yuan","Zhimin Yang","Zhengan Yao"],"pdf_url":"https://arxiv.org/pdf/2305.17866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04633v1","updated":"2023-10-07T01:00:40Z","published":"2023-10-07T01:00:40Z","title":"Unbiased and Robust: External Attention-enhanced Graph Contrastive\n  Learning for Cross-domain Sequential Recommendation","summary":"  Cross-domain sequential recommenders (CSRs) are gaining considerable research\nattention as they can capture user sequential preference by leveraging side\ninformation from multiple domains. However, these works typically follow an\nideal setup, i.e., different domains obey similar data distribution, which\nignores the bias brought by asymmetric interaction densities (a.k.a. the\ninter-domain density bias). Besides, the frequently adopted mechanism (e.g.,\nthe self-attention network) in sequence encoder only focuses on the\ninteractions within a local view, which overlooks the global correlations\nbetween different training batches. To this end, we propose an External\nAttention-enhanced Graph Contrastive Learning framework, namely EA-GCL.\nSpecifically, to remove the impact of the inter-domain density bias, an\nauxiliary Self-Supervised Learning (SSL) task is attached to the traditional\ngraph encoder under a multi-task learning manner. To robustly capture users'\nbehavioral patterns, we develop an external attention-based sequence encoder\nthat contains an MLP-based memory-sharing structure. Unlike the self-attention\nmechanism, such a structure can effectively alleviate the bias interference\nfrom the batch-based training scheme. Extensive experiments on two real-world\ndatasets demonstrate that EA-GCL outperforms several state-of-the-art baselines\non CSR tasks. The source codes and relevant datasets are available at\nhttps://github.com/HoupingY/EA-GCL.\n","authors":["Xinhua Wang","Houping Yue","Zizheng Wang","Liancheng Xu","Jinyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04633v1.pdf","comment":"9 pages, 4 figures, accepted by ICDM 2023 (workshop-GML4Rec)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.04771v1","updated":"2023-10-07T10:36:41Z","published":"2023-10-07T10:36:41Z","title":"Embodied Cognition Guides Virtual-Real Interaction Design to Help\n  Yicheng Flower Drum Intangible Cultural Heritage Dissemination","summary":"  In order to make the non-heritage culture of Yicheng Flower Drum more\nrelevant to the trend of the digital era and promote its dissemination and\ninheritance, the design and application of gesture recognition and virtual\nreality technologies guided by embodied cognition theory in the process of\nnon-heritage culture dissemination is studied. At the same time, it will\nenhance the interaction between people and NRM culture, stimulate the\naudience's interest in understanding NRM and spreading NRM, and create\nawareness of preserving NRM culture. Using embodied cognition as a theoretical\nguide, expanding the unidirectional communication mode through human-computer\ninteraction close to natural behavior and cooperating with multisensory\ninformation reception channels, so as to construct an embodied and immersive\ninteractive atmosphere for the participants and enable them to naturally form\nthe cognition and understanding of the traditional culture in the process of\ninteraction. The dissemination of the non-heritage culture Yicheng Flower Drum\ncan take the theory of embodied cognition as an entry point, and through the\nvirtual and real scenes of Yicheng Flower Drum and the immersive experience, we\ncan empower the interaction design of non-heritage culture dissemination of the\nvirtual and real, and provide a new method for the research of digital design\nof non-heritage culture.\n","authors":["Yuhan Ma","Weiran Zhao","Xiaolin Zhang","Ze Gao"],"pdf_url":"https://arxiv.org/pdf/2310.04771v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10369v2","updated":"2023-10-07T08:35:35Z","published":"2023-03-18T09:04:55Z","title":"Blind Multimodal Quality Assessment of Low-light Images","summary":"  Blind image quality assessment (BIQA) aims at automatically and accurately\nforecasting objective scores for visual signals, which has been widely used to\nmonitor product and service quality in low-light applications, covering\nsmartphone photography, video surveillance, autonomous driving, etc. Recent\ndevelopments in this field are dominated by unimodal solutions inconsistent\nwith human subjective rating patterns, where human visual perception is\nsimultaneously reflected by multiple sensory information. In this article, we\npresent a unique blind multimodal quality assessment (BMQA) of low-light images\nfrom subjective evaluation to objective score. To investigate the multimodal\nmechanism, we first establish a multimodal low-light image quality (MLIQ)\ndatabase with authentic low-light distortions, containing image-text modality\npairs. Further, we specially design the key modules of BMQA, considering\nmultimodal quality representation, latent feature alignment and fusion, and\nhybrid self-supervised and supervised learning. Extensive experiments show that\nour BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark\ndatabase. In particular, we also build an independent single-image modality\nDark-4K database, which is used to verify its applicability and generalization\nperformance in mainstream unimodal applications. Qualitative and quantitative\nresults on Dark-4K show that BMQA achieves superior performance to existing\nBIQA approaches as long as a pre-trained model is provided to generate text\ndescription. The proposed framework and two databases as well as the collected\nBIQA methods and evaluation metrics are made publicly available on here.\n","authors":["Miaohui Wang","Zhuowei Xu","Mai Xu","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10369v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2305.11019v4","updated":"2023-10-07T07:57:15Z","published":"2023-05-18T14:52:45Z","title":"Annotation-free Audio-Visual Segmentation","summary":"  The objective of Audio-Visual Segmentation (AVS) is to localise the sounding\nobjects within visual scenes by accurately predicting pixel-wise segmentation\nmasks. To tackle the task, it involves a comprehensive consideration of both\nthe data and model aspects. In this paper, first, we initiate a novel pipeline\nfor generating artificial data for the AVS task without extra manual\nannotations. We leverage existing image segmentation and audio datasets and\nmatch the image-mask pairs with its corresponding audio samples using category\nlabels in segmentation datasets, that allows us to effortlessly compose (image,\naudio, mask) triplets for training AVS models. The pipeline is annotation-free\nand scalable to cover a large number of categories. Additionally, we introduce\na lightweight model SAMA-AVS which adapts the pre-trained segment anything\nmodel~(SAM) to the AVS task. By introducing only a small number of trainable\nparameters with adapters, the proposed model can effectively achieve adequate\naudio-visual fusion and interaction in the encoding stage with vast majority of\nparameters fixed. We conduct extensive experiments, and the results show our\nproposed model remarkably surpasses other competing methods. Moreover, by using\nthe proposed model pretrained with our synthetic data, the performance on real\nAVSBench data is further improved, achieving 83.17 mIoU on S4 subset and 66.95\nmIoU on MS3 set. The project page is\nhttps://jinxiang-liu.github.io/anno-free-AVS/.\n","authors":["Jinxiang Liu","Yu Wang","Chen Ju","Chaofan Ma","Ya Zhang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2305.11019v4.pdf","comment":"Camera-ready version for WACV 2024; project page is\n  https://jinxiang-liu.github.io/anno-free-AVS/"},{"id":"http://arxiv.org/abs/2305.12333v3","updated":"2023-10-07T04:25:55Z","published":"2023-05-21T03:50:44Z","title":"GRACE: Loss-Resilient Real-Time Video through Neural Codecs","summary":"  In real-time video communication, retransmitting lost packets over\nhigh-latency networks is not viable due to strict latency requirements. To\ncounter packet losses without retransmission, two primary strategies are\nemployed -- encoder-based forward error correction (FEC) and decoder-based\nerror concealment. The former encodes data with redundancy before transmission,\nyet determining the optimal redundancy level in advance proves challenging. The\nlatter reconstructs video from partially received frames, but dividing a frame\ninto independently coded partitions inherently compromises compression\nefficiency, and the lost information cannot be effectively recovered by the\ndecoder without adapting the encoder.\n  We present a loss-resilient real-time video system called GRACE, which\npreserves the user's quality of experience (QoE) across a wide range of packet\nlosses through a new neural video codec. Central to GRACE's enhanced loss\nresilience is its joint training of the neural encoder and decoder under a\nspectrum of simulated packet losses. In lossless scenarios, GRACE achieves\nvideo quality on par with conventional codecs (e.g., H.265). As the loss rate\nescalates, GRACE exhibits a more graceful, less pronounced decline in quality,\nconsistently outperforming other loss-resilient schemes. Through extensive\nevaluation on various videos and real network traces, we demonstrate that GRACE\nreduces undecodable frames by 95% and stall duration by 90% compared with FEC,\nwhile markedly boosting video quality over error concealment methods. In a user\nstudy with 240 crowdsourced participants and 960 subjective ratings, GRACE\nregisters a 38% higher mean opinion score (MOS) than other baselines.\n","authors":["Yihua Cheng","Ziyi Zhang","Hanchen Li","Anton Arapin","Yue Zhang","Qizheng Zhang","Yuhan Liu","Xu Zhang","Francis Y. Yan","Amrita Mazumdar","Nick Feamster","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2305.12333v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04673v1","updated":"2023-10-07T03:17:59Z","published":"2023-10-07T03:17:59Z","title":"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","summary":"  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks. However, there has\nbeen limited research on applying similar frameworks to audio tasks. Previously\nproposed large language models for audio tasks either lack sufficient\nquantitative evaluations, or are limited to tasks for recognizing and\nunderstanding audio content, or significantly underperform existing\nstate-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified\nGPT model for audio recognition, understanding, and generation. LauraGPT is a\nversatile language model that can process both audio and text inputs and\ngenerate outputs in either modalities. It can perform a wide range of tasks\nrelated to content, semantics, paralinguistics, and audio-signal analysis. Some\nof its noteworthy tasks include automatic speech recognition, speech-to-text\ntranslation, text-to-speech synthesis, machine translation, speech enhancement,\nautomated audio captioning, speech emotion recognition, and spoken language\nunderstanding. To achieve this goal, we use a combination of continuous and\ndiscrete features for audio. We encode input audio into continuous\nrepresentations using an audio encoder and decode output audio from discrete\ncodec codes. We then fine-tune a large decoder-only Transformer-based language\nmodel on multiple audio-to-text, text-to-audio, audio-to-audio, and\ntext-to-text tasks using a supervised multitask learning approach. Extensive\nexperiments show that LauraGPT achieves competitive or superior performance\ncompared to existing SOTA models on various audio processing benchmarks.\n","authors":["Jiaming Wang^","Zhihao Du^","Qian Chen","Yunfei Chu","Zhifu Gao","Zerui Li","Kai Hu","Xiaohuan Zhou","Jin Xu","Ziyang Ma","Wen Wang","Siqi Zheng","Chang Zhou","Zhijie Yan","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04673v1.pdf","comment":"10 pages, under review"}]},"2023-10-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.06839v1","updated":"2023-10-10T17:59:58Z","published":"2023-10-10T17:59:58Z","title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression","summary":"  In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational/financial cost, longer latency, and inferior\nperformance. Some studies reveal that the performance of LLMs depends on both\nthe density and the position of the key information (question relevant) in the\ninput prompt. Inspired by these findings, we propose LongLLMLingua for prompt\ncompression towards improving LLMs' perception of the key information to\nsimultaneously address the three challenges. We conduct evaluation on a wide\nrange of long context scenarios including single-/multi-document QA, few-shot\nlearning, summarization, synthetic tasks, and code completion. The experimental\nresults show that LongLLMLingua compressed prompt can derive higher performance\nwith much less cost. The latency of the end-to-end system is also reduced. For\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost\nof up to 17.1% over the original prompt with ~4x fewer tokens as input to\nGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000\nsamples from the LongBench and ZeroScrolls benchmark, respectively.\nAdditionally, when compressing prompts of ~10k tokens at a compression rate of\n2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our\ncode is available at https://aka.ms/LLMLingua.\n","authors":["Huiqiang Jiang","Qianhui Wu","Xufang Luo","Dongsheng Li","Chin-Yew Lin","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.06839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06837v1","updated":"2023-10-10T17:59:51Z","published":"2023-10-10T17:59:51Z","title":"Generating and Evaluating Tests for K-12 Students with Language Model\n  Simulations: A Case Study on Sentence Reading Efficiency","summary":"  Developing an educational test can be expensive and time-consuming, as each\nitem must be written by experts and then evaluated by collecting hundreds of\nstudent responses. Moreover, many tests require multiple distinct sets of\nquestions administered throughout the school year to closely monitor students'\nprogress, known as parallel tests. In this study, we focus on tests of silent\nsentence reading efficiency, used to assess students' reading ability over\ntime. To generate high-quality parallel tests, we propose to fine-tune large\nlanguage models (LLMs) to simulate how previous students would have responded\nto unseen items. With these simulated responses, we can estimate each item's\ndifficulty and ambiguity. We first use GPT-4 to generate new test items\nfollowing a list of expert-developed rules and then apply a fine-tuned LLM to\nfilter the items based on criteria from psychological measurements. We also\npropose an optimal-transport-inspired technique for generating parallel tests\nand show the generated tests closely correspond to the original test's\ndifficulty and reliability based on crowdworker responses. Our evaluation of a\ngenerated test with 234 students from grades 2 to 8 produces test scores highly\ncorrelated (r=0.93) to those of a standard test form written by human experts\nand evaluated across thousands of K-12 students.\n","authors":["Eric Zelikman","Wanjing Anya Ma","Jasmine E. Tran","Diyi Yang","Jason D. Yeatman","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2310.06837v1.pdf","comment":"Accepted to EMNLP 2023 (Main)"},{"id":"http://arxiv.org/abs/2310.06830v1","updated":"2023-10-10T17:57:45Z","published":"2023-10-10T17:57:45Z","title":"Lemur: Harmonizing Natural Language and Code for Language Agents","summary":"  We introduce Lemur and Lemur-Chat, openly accessible language models\noptimized for both natural language and coding capabilities to serve as the\nbackbone of versatile language agents. The evolution from language chat models\nto functional language agents demands that models not only master human\ninteraction, reasoning, and planning but also ensure grounding in the relevant\nenvironments. This calls for a harmonious blend of language and coding\ncapabilities in the models. Lemur and Lemur-Chat are proposed to address this\nnecessity, demonstrating balanced proficiencies in both domains, unlike\nexisting open-source models that tend to specialize in either. Through\nmeticulous pre-training using a code-intensive corpus and instruction\nfine-tuning on text and code data, our models achieve state-of-the-art averaged\nperformance across diverse text and coding benchmarks among open-source models.\nComprehensive experiments demonstrate Lemur's superiority over existing\nopen-source models and its proficiency across various agent tasks involving\nhuman communication, tool usage, and interaction under fully- and partially-\nobservable environments. The harmonization between natural and programming\nlanguages enables Lemur-Chat to significantly narrow the gap with proprietary\nmodels on agent abilities, providing key insights into developing advanced\nopen-source agents adept at reasoning, planning, and operating seamlessly\nacross environments. https://github.com/OpenLemur/Lemur\n","authors":["Yiheng Xu","Hongjin Su","Chen Xing","Boyu Mi","Qian Liu","Weijia Shi","Binyuan Hui","Fan Zhou","Yitao Liu","Tianbao Xie","Zhoujun Cheng","Siheng Zhao","Lingpeng Kong","Bailin Wang","Caiming Xiong","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2310.06830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06827v1","updated":"2023-10-10T17:57:00Z","published":"2023-10-10T17:57:00Z","title":"Teaching Language Models to Hallucinate Less with Synthetic Tasks","summary":"  Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.\n","authors":["Erik Jones","Hamid Palangi","Clarisse Simes","Varun Chandrasekaran","Subhabrata Mukherjee","Arindam Mitra","Ahmed Awadallah","Ece Kamar"],"pdf_url":"https://arxiv.org/pdf/2310.06827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06825v1","updated":"2023-10-10T17:54:58Z","published":"2023-10-10T17:54:58Z","title":"Mistral 7B","summary":"  We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.\n","authors":["Albert Q. Jiang","Alexandre Sablayrolles","Arthur Mensch","Chris Bamford","Devendra Singh Chaplot","Diego de las Casas","Florian Bressand","Gianna Lengyel","Guillaume Lample","Lucile Saulnier","Llio Renard Lavaud","Marie-Anne Lachaux","Pierre Stock","Teven Le Scao","Thibaut Lavril","Thomas Wang","Timothe Lacroix","William El Sayed"],"pdf_url":"https://arxiv.org/pdf/2310.06825v1.pdf","comment":"Models and code are available at\n  https://mistral.ai/news/announcing-mistral-7b/"},{"id":"http://arxiv.org/abs/2310.06816v1","updated":"2023-10-10T17:39:03Z","published":"2023-10-10T17:39:03Z","title":"Text Embeddings Reveal (Almost) As Much As Text","summary":"  How much private information do text embeddings reveal about the original\ntext? We investigate the problem of embedding \\textit{inversion},\nreconstructing the full text represented in dense text embeddings. We frame the\nproblem as controlled generation: generating text that, when reembedded, is\nclose to a fixed point in latent space. We find that although a na\\\"ive model\nconditioned on the embedding performs poorly, a multi-step method that\niteratively corrects and re-embeds text is able to recover $92\\%$ of\n$32\\text{-token}$ text inputs exactly. We train our model to decode text\nembeddings from two state-of-the-art embedding models, and also show that our\nmodel can recover important personal information (full names) from a dataset of\nclinical notes. Our code is available on Github:\n\\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.\n","authors":["John X. Morris","Volodymyr Kuleshov","Vitaly Shmatikov","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2310.06816v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.13160v2","updated":"2023-10-10T17:34:15Z","published":"2023-05-22T15:47:31Z","title":"Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via\n  Debate","summary":"  Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive\nperformance in complex reasoning tasks. However, it is difficult to know\nwhether the models are reasoning based on deep understandings of truth and\nlogic, or leveraging their memorized patterns in a relatively superficial way.\nIn this work, we explore testing LLMs' reasoning by engaging with them in a\ndebate-like conversation, where given a question, the LLM and the user need to\ndiscuss to make the correct decision starting from opposing arguments. Upon\nmitigating the Clever Hans effect, our task requires the LLM to not only\nachieve the correct answer on its own, but also be able to hold and defend its\nbelief instead of blindly believing or getting misled by the user's (invalid)\narguments and critiques, thus testing in greater depth whether the LLM grasps\nthe essence of the reasoning required to solve the problem. Across a range of\ncomplex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench\ntasks, we find that despite their impressive performance as reported in\nexisting work on generating correct step-by-step solutions in the beginning,\nLLMs like ChatGPT cannot maintain their beliefs in truth for a significant\nportion of examples when challenged by oftentimes absurdly invalid arguments.\nOur work points to danger zones of model alignment, and also suggests more\ncareful treatments and interpretations of the recent findings that LLMs can\nimprove their responses based on feedback.\n","authors":["Boshi Wang","Xiang Yue","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2305.13160v2.pdf","comment":"EMNLP-23 (findings)"},{"id":"http://arxiv.org/abs/2310.05914v2","updated":"2023-10-10T17:31:00Z","published":"2023-10-09T17:58:34Z","title":"NEFTune: Noisy Embeddings Improve Instruction Finetuning","summary":"  We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.\n","authors":["Neel Jain","Ping-yeh Chiang","Yuxin Wen","John Kirchenbauer","Hong-Min Chu","Gowthami Somepalli","Brian R. Bartoldson","Bhavya Kailkhura","Avi Schwarzschild","Aniruddha Saha","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2310.05914v2.pdf","comment":"25 pages, Code is available on Github:\n  https://github.com/neelsjain/NEFTune"},{"id":"http://arxiv.org/abs/2306.08749v2","updated":"2023-10-10T17:29:52Z","published":"2023-06-14T21:17:31Z","title":"Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology\n  Reports","summary":"  Despite the reduction in turn-around times in radiology reports with the use\nof speech recognition software, persistent communication errors can\nsignificantly impact the interpretation of the radiology report. Pre-filling a\nradiology report holds promise in mitigating reporting errors, and despite\nefforts in the literature to generate medical reports, there exists a lack of\napproaches that exploit the longitudinal nature of patient visit records in the\nMIMIC-CXR dataset. To address this gap, we propose to use longitudinal\nmulti-modal data, i.e., previous patient visit CXR, current visit CXR, and\nprevious visit report, to pre-fill the 'findings' section of a current patient\nvisit report. We first gathered the longitudinal visit information for 26,625\npatients from the MIMIC-CXR dataset and created a new dataset called\nLongitudinal-MIMIC. With this new dataset, a transformer-based model was\ntrained to capture the information from longitudinal patient visit records\ncontaining multi-modal data (CXR images + reports) via a cross-attention-based\nmulti-modal fusion module and a hierarchical memory-driven decoder. In contrast\nto previous work that only uses current visit data as input to train a model,\nour work exploits the longitudinal information available to pre-fill the\n'findings' section of radiology reports. Experiments show that our approach\noutperforms several recent approaches. Code will be published at\nhttps://github.com/CelestialShine/Longitudinal-Chest-X-Ray.\n","authors":["Qingqing Zhu","Tejas Sudharshan Mathai","Pritam Mukherjee","Yifan Peng","Ronald M. Summers","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2306.08749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06803v1","updated":"2023-10-10T17:21:03Z","published":"2023-10-10T17:21:03Z","title":"Advancing Transformer's Capabilities in Commonsense Reasoning","summary":"  Recent advances in general purpose pre-trained language models have shown\ngreat potential in commonsense reasoning. However, current works still perform\npoorly on standard commonsense reasoning benchmarks including the Com2Sense\nDataset. We argue that this is due to a disconnect with current cutting-edge\nmachine learning methods. In this work, we aim to bridge the gap by introducing\ncurrent ML-based methods to improve general purpose pre-trained language models\nin the task of commonsense reasoning. Specifically, we experiment with and\nsystematically evaluate methods including knowledge transfer, model ensemble,\nand introducing an additional pairwise contrastive objective. Our best model\noutperforms the strongest previous works by ~15\\% absolute gains in Pairwise\nAccuracy and ~8.7\\% absolute gains in Standard Accuracy.\n","authors":["Yu Zhou","Yunqiu Han","Hanyu Zhou","Yulun Wu"],"pdf_url":"https://arxiv.org/pdf/2310.06803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06786v1","updated":"2023-10-10T16:57:28Z","published":"2023-10-10T16:57:28Z","title":"OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text","summary":"  There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.\n","authors":["Keiran Paster","Marco Dos Santos","Zhangir Azerbayev","Jimmy Ba"],"pdf_url":"https://arxiv.org/pdf/2310.06786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06773v1","updated":"2023-10-10T16:49:21Z","published":"2023-10-10T16:49:21Z","title":"Uni3D: Exploring Unified 3D Representation at Scale","summary":"  Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.\n","authors":["Junsheng Zhou","Jinsheng Wang","Baorui Ma","Yu-Shen Liu","Tiejun Huang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06773v1.pdf","comment":"Code and Demo: https://github.com/baaivision/Uni3D"},{"id":"http://arxiv.org/abs/2310.06770v1","updated":"2023-10-10T16:47:29Z","published":"2023-10-10T16:47:29Z","title":"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?","summary":"  Language models have outpaced our ability to evaluate them effectively, but\nfor their future development it is essential to study the frontier of their\ncapabilities. We consider real-world software engineering to be a rich,\nsustainable, and challenging testbed for evaluating the next generation of\nlanguage models. We therefore introduce SWE-bench, an evaluation framework\nincluding $2,294$ software engineering problems drawn from real GitHub issues\nand corresponding pull requests across $12$ popular Python repositories. Given\na codebase along with a description of an issue to be resolved, a language\nmodel is tasked with editing the codebase to address the issue. Resolving\nissues in SWE-bench frequently requires understanding and coordinating changes\nacross multiple functions, classes, and even files simultaneously, calling for\nmodels to interact with execution environments, process extremely long contexts\nand perform complex reasoning that goes far beyond traditional code generation.\nOur evaluations show that both state-of-the-art proprietary models and our\nfine-tuned model SWE-Llama can resolve only the simplest issues. Claude 2 and\nGPT-4 solve a mere $4.8$% and $1.7$% of instances respectively, even when\nprovided with an oracle retriever. Advances on SWE-bench represent steps\ntowards LMs that are more practical, intelligent, and autonomous.\n","authors":["Carlos E. Jimenez","John Yang","Alexander Wettig","Shunyu Yao","Kexin Pei","Ofir Press","Karthik Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2310.06770v1.pdf","comment":"Data, code, and leaderboard are available at https://www.swebench.com"},{"id":"http://arxiv.org/abs/2307.10236v2","updated":"2023-10-10T16:44:02Z","published":"2023-07-16T08:28:04Z","title":"Look Before You Leap: An Exploratory Study of Uncertainty Measurement\n  for Large Language Models","summary":"  The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.\n","authors":["Yuheng Huang","Jiayang Song","Zhijie Wang","Huaming Chen","Felix Juefei-Xu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2307.10236v2.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06764v1","updated":"2023-10-10T16:40:00Z","published":"2023-10-10T16:40:00Z","title":"OmniLingo: Listening- and speaking-based language learning","summary":"  In this demo paper we present OmniLingo, an architecture for distributing\ndata for listening- and speaking-based language learning applications and a\ndemonstration client built using the architecture. The architecture is based on\nthe Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty\nover data.\n","authors":["Francis M. Tyers","Nicholas Howell"],"pdf_url":"https://arxiv.org/pdf/2310.06764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06762v1","updated":"2023-10-10T16:38:49Z","published":"2023-10-10T16:38:49Z","title":"TRACE: A Comprehensive Benchmark for Continual Learning in Large\n  Language Models","summary":"  Aligned large language models (LLMs) demonstrate exceptional capabilities in\ntask-solving, following instructions, and ensuring safety. However, the\ncontinual learning aspect of these aligned LLMs has been largely overlooked.\nExisting continual learning benchmarks lack sufficient challenge for leading\naligned LLMs, owing to both their simplicity and the models' potential exposure\nduring instruction tuning. In this paper, we introduce TRACE, a novel benchmark\ndesigned to evaluate continual learning in LLMs. TRACE consists of 8 distinct\ndatasets spanning challenging tasks including domain-specific tasks,\nmultilingual capabilities, code generation, and mathematical reasoning. All\ndatasets are standardized into a unified format, allowing for effortless\nautomatic evaluation of LLMs. Our experiments show that after training on\nTRACE, aligned LLMs exhibit significant declines in both general ability and\ninstruction-following capabilities. For example, the accuracy of llama2-chat\n13B on gsm8k dataset declined precipitously from 28.8\\% to 2\\% after training\non our datasets. This highlights the challenge of finding a suitable tradeoff\nbetween achieving performance on specific tasks while preserving the original\nprowess of LLMs. Empirical findings suggest that tasks inherently equipped with\nreasoning paths contribute significantly to preserving certain capabilities of\nLLMs against potential declines. Motivated by this, we introduce the\nReasoning-augmented Continual Learning (RCL) approach. RCL integrates\ntask-specific cues with meta-rationales, effectively reducing catastrophic\nforgetting in LLMs while expediting convergence on novel tasks.\n","authors":["Xiao Wang","Yuansen Zhang","Tianze Chen","Songyang Gao","Senjie Jin","Xianjun Yang","Zhiheng Xi","Rui Zheng","Yicheng Zou","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14210v2","updated":"2023-10-10T16:23:33Z","published":"2023-05-23T16:28:29Z","title":"Skill-Based Few-Shot Selection for In-Context Learning","summary":"  In-context learning is the paradigm that adapts large language models to\ndownstream tasks by providing a few examples. Few-shot selection -- selecting\nappropriate examples for each test instance separately -- is important for\nin-context learning. In this paper, we propose Skill-KNN, a skill-based\nfew-shot selection method for in-context learning. The key advantages of\nSkill-KNN include: (1) it addresses the problem that existing methods based on\npre-trained embeddings can be easily biased by surface natural language\nfeatures that are not important for the target task; (2) it does not require\ntraining or fine-tuning of any models, making it suitable for frequently\nexpanding or changing example banks. The key insight is to optimize the inputs\nfed into the embedding model, rather than tuning the model itself. Technically,\nSkill-KNN generates the skill-based descriptions for each test case and\ncandidate example by utilizing a pre-processing few-shot prompting, thus\neliminating unimportant surface features. Experimental results across five\ncross-domain semantic parsing datasets and six backbone models show that\nSkill-KNN significantly outperforms existing methods.\n","authors":["Shengnan An","Bo Zhou","Zeqi Lin","Qiang Fu","Bei Chen","Nanning Zheng","Weizhu Chen","Jian-Guang Lou"],"pdf_url":"https://arxiv.org/pdf/2305.14210v2.pdf","comment":"Accepted by EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.06714v1","updated":"2023-10-10T15:41:26Z","published":"2023-10-10T15:41:26Z","title":"Exploring Memorization in Fine-tuned Language Models","summary":"  LLMs have shown great capabilities in various tasks but also exhibited\nmemorization of training data, thus raising tremendous privacy and copyright\nconcerns. While prior work has studied memorization during pre-training, the\nexploration of memorization during fine-tuning is rather limited. Compared with\npre-training, fine-tuning typically involves sensitive data and diverse\nobjectives, thus may bring unique memorization behaviors and distinct privacy\nrisks. In this work, we conduct the first comprehensive analysis to explore\nLMs' memorization during fine-tuning across tasks. Our studies with\nopen-sourced and our own fine-tuned LMs across various tasks indicate that\nfine-tuned memorization presents a strong disparity among tasks. We provide an\nunderstanding of this task disparity via sparse coding theory and unveil a\nstrong correlation between memorization and attention score distribution. By\ninvestigating its memorization behavior, multi-task fine-tuning paves a\npotential strategy to mitigate fine-tuned memorization.\n","authors":["Shenglai Zeng","Yaxin Li","Jie Ren","Yiding Liu","Han Xu","Pengfei He","Yue Xing","Shuaiqiang Wang","Jiliang Tang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.06714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06707v1","updated":"2023-10-10T15:33:51Z","published":"2023-10-10T15:33:51Z","title":"Quality Control at Your Fingertips: Quality-Aware Translation Models","summary":"  Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy\nfor neural machine translation (NMT) models. The underlying assumption is that\nmodel probability correlates well with human judgment, with better translations\nbeing more likely. However, research has shown that this assumption does not\nalways hold, and decoding strategies which directly optimize a utility\nfunction, like Minimum Bayes Risk (MBR) or Quality-Aware decoding can\nsignificantly improve translation quality over standard MAP decoding. The main\ndisadvantage of these methods is that they require an additional model to\npredict the utility, and additional steps during decoding, which makes the\nentire process computationally demanding. In this paper, we propose to make the\nNMT models themselves quality-aware by training them to estimate the quality of\ntheir own output. During decoding, we can use the model's own quality estimates\nto guide the generation process and produce the highest-quality translations\npossible. We demonstrate that the model can self-evaluate its own output during\ntranslation, eliminating the need for a separate quality estimation model.\nMoreover, we show that using this quality signal as a prompt during MAP\ndecoding can significantly improve translation quality. When using the internal\nquality estimate to prune the hypothesis space during MBR decoding, we can not\nonly further improve translation quality, but also reduce inference speed by\ntwo orders of magnitude.\n","authors":["Christian Tomani","David Vilar","Markus Freitag","Colin Cherry","Subhajit Naskar","Mara Finkelstein","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.06707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06702v1","updated":"2023-10-10T15:25:33Z","published":"2023-10-10T15:25:33Z","title":"Temporally Aligning Long Audio Interviews with Questions: A Case Study\n  in Multimodal Data Integration","summary":"  The problem of audio-to-text alignment has seen significant amount of\nresearch using complete supervision during training. However, this is typically\nnot in the context of long audio recordings wherein the text being queried does\nnot appear verbatim within the audio file. This work is a collaboration with a\nnon-governmental organization called CARE India that collects long audio health\nsurveys from young mothers residing in rural parts of Bihar, India. Given a\nquestion drawn from a questionnaire that is used to guide these surveys, we aim\nto locate where the question is asked within a long audio recording. This is of\ngreat value to African and Asian organizations that would otherwise have to\npainstakingly go through long and noisy audio recordings to locate questions\n(and answers) of interest. Our proposed framework, INDENT, uses a\ncross-attention-based model and prior information on the temporal ordering of\nsentences to learn speech embeddings that capture the semantics of the\nunderlying spoken text. These learnt embeddings are used to retrieve the\ncorresponding audio segment based on text queries at inference time. We\nempirically demonstrate the significant effectiveness (improvement in R-avg of\nabout 3%) of our model over those obtained using text-based heuristics. We also\nshow how noisy ASR, generated using state-of-the-art ASR models for Indian\nlanguages, yields better results when used in place of speech. INDENT, trained\nonly on Hindi data is able to cater to all languages supported by the\n(semantically) shared text space. We illustrate this empirically on 11 Indic\nlanguages.\n","authors":["Piyush Singh Pasi","Karthikeya Battepati","Preethi Jyothi","Ganesh Ramakrishnan","Tanmay Mahapatra","Manoj Singh"],"pdf_url":"https://arxiv.org/pdf/2310.06702v1.pdf","comment":"Work Accepted in IJCAI-23- AI and Social Good Track"},{"id":"http://arxiv.org/abs/2310.03304v3","updated":"2023-10-10T15:15:54Z","published":"2023-10-05T04:15:48Z","title":"Learning Personalized Story Evaluation","summary":"  While large language models (LLMs) have shown impressive results for more\nobjective tasks such as QA and retrieval, it remains nontrivial to evaluate\ntheir performance on open-ended text generation for reasons including (1) data\ncontamination; (2) multi-dimensional evaluation criteria; and (3)\nsubjectiveness stemming from reviewers' personal preferences. To address such\nissues, we propose to model personalization in an uncontaminated open-ended\ngeneration assessment. We create two new datasets Per-MPST and Per-DOC for\npersonalized story evaluation, by re-purposing existing datasets with proper\nanonymization and new personalized labels. We further develop a personalized\nstory evaluation model PERSE to infer reviewer preferences and provide a\npersonalized evaluation. Specifically, given a few exemplary reviews from a\nparticular reviewer, PERSE predicts either a detailed review or fine-grained\ncomparison in several aspects (such as interestingness and surprise) for that\nreviewer on a new text input. Experimental results show that PERSE outperforms\nGPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on\npairwise preference prediction accuracy. Both datasets and code will be\nreleased.\n","authors":["Danqing Wang","Kevin Yang","Hanlin Zhu","Xiaomeng Yang","Andrew Cohen","Lei Li","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.03304v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2310.06694v1","updated":"2023-10-10T15:13:30Z","published":"2023-10-10T15:13:30Z","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n  Pruning","summary":"  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA\nmodels, on a wide range of downstream and instruction tuning evaluations, while\nrequiring only 3% of compute compared to training such models from scratch.\nThis work provides compelling evidence that leveraging existing LLMs with\nstructured pruning is a far more cost-effective approach for building smaller\nLLMs.\n","authors":["Mengzhou Xia","Tianyu Gao","Zhiyuan Zeng","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06694v1.pdf","comment":"The code and models are available at\n  https://github.com/princeton-nlp/LLM-Shearing"},{"id":"http://arxiv.org/abs/2310.06692v1","updated":"2023-10-10T15:10:03Z","published":"2023-10-10T15:10:03Z","title":"Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task\n  Scenarios with Large Language Models","summary":"  Large language models (LLMs) have unveiled remarkable reasoning capabilities\nby exploiting chain-of-thought (CoT) prompting, which generates intermediate\nreasoning chains to serve as the rationale for deriving the answer. However,\ncurrent CoT methods either simply employ general prompts such as Let's think\nstep by step, or heavily rely on handcrafted task-specific demonstrations to\nattain preferable performances, thereby engendering an inescapable gap between\nperformance and generalization. To bridge this gap, we propose Meta-CoT, a\ngeneralizable CoT prompting method in mixed-task scenarios where the type of\ninput questions is unknown. Meta-CoT firstly categorizes the scenario based on\nthe input question and subsequently constructs diverse demonstrations from the\ncorresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys\nremarkable performances on ten public benchmark reasoning tasks and superior\ngeneralization capabilities. Notably, Meta-CoT achieves the state-of-the-art\nresult on SVAMP (93.7%) without any additional program-aided methods. Our\nfurther experiments on five out-of-distribution datasets verify the stability\nand generality of Meta-CoT.\n","authors":["Anni Zou","Zhuosheng Zhang","Hai Zhao","Xiangru Tang"],"pdf_url":"https://arxiv.org/pdf/2310.06692v1.pdf","comment":"22 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2210.03493"},{"id":"http://arxiv.org/abs/2310.05035v2","updated":"2023-10-10T15:03:35Z","published":"2023-10-08T06:36:26Z","title":"Self-Convinced Prompting: Few-Shot Question Answering with Repeated\n  Introspection","summary":"  While large language models (LLMs) such as ChatGPT and PaLM have demonstrated\nremarkable performance in various language understanding and generation tasks,\ntheir capabilities in complex reasoning and intricate knowledge utilization\nstill fall short of human-level proficiency. Recent studies have established\nthe effectiveness of prompts in steering LLMs towards generating desired\noutputs. Building on these insights, we introduce a novel framework that\nharnesses the potential of large-scale pre-trained language models, to\niteratively enhance performance of the LLMs. Our framework incorporates three\ncomponents: \\textit{Normal CoT}, a \\textit{Convincer}, and an\n\\textit{Answerer}. It processes the output of a typical few-shot\nchain-of-thought prompt, assesses the correctness of the response, scrutinizes\nthe answer, refines the reasoning, and ultimately produces a new solution.\nExperimental results on the 7 datasets of miscellaneous problems validate the\nefficacy of the Self-Convince framework, achieving substantial improvements\ncompared to the baselines. This study contributes to the burgeoning body of\nresearch focused on integrating pre-trained language models with tailored\nprompts and iterative refinement processes to augment their performance in\ncomplex tasks.\n","authors":["Haodi Zhang","Min Cai","Xinhe Zhang","Chen Jason Zhang","Rui Mao","Kaishun Wu"],"pdf_url":"https://arxiv.org/pdf/2310.05035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06684v1","updated":"2023-10-10T14:59:22Z","published":"2023-10-10T14:59:22Z","title":"Learning Multiplex Embeddings on Text-rich Networks with One Text\n  Encoder","summary":"  In real-world scenarios, texts in a network are often linked by multiple\nsemantic relations (e.g., papers in an academic network are referenced by other\npublications, written by the same author, or published in the same venue),\nwhere text documents and their relations form a multiplex text-rich network.\nMainstream text representation learning methods use pretrained language models\n(PLMs) to generate one embedding for each text unit, expecting that all types\nof relations between texts can be captured by these single-view embeddings.\nHowever, this presumption does not hold particularly in multiplex text-rich\nnetworks. Along another line of work, multiplex graph neural networks (GNNs)\ndirectly initialize node attributes as a feature vector for node representation\nlearning, but they cannot fully capture the semantics of the nodes' associated\ntexts. To bridge these gaps, we propose METERN, a new framework for learning\nMultiplex Embeddings on TExt-Rich Networks. In contrast to existing methods,\nMETERN uses one text encoder to model the shared knowledge across relations and\nleverages a small number of parameters per relation to derive relation-specific\nrepresentations. This allows the encoder to effectively capture the multiplex\nstructures in the network while also preserving parameter efficiency. We\nconduct experiments on nine downstream tasks in five networks from both\nacademic and e-commerce domains, where METERN outperforms baselines\nsignificantly and consistently. The code is available at\nhttps://github.com/PeterGriffinJin/METERN-submit.\n","authors":["Bowen Jin","Wentao Zhang","Yu Zhang","Yu Meng","Han Zhao","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.06684v1.pdf","comment":"9 pages, 11 appendix pages"},{"id":"http://arxiv.org/abs/2310.06675v1","updated":"2023-10-10T14:50:20Z","published":"2023-10-10T14:50:20Z","title":"SEER: A Knapsack approach to Exemplar Selection for In-Context HybridQA","summary":"  Question answering over hybrid contexts is a complex task, which requires the\ncombination of information extracted from unstructured texts and structured\ntables in various ways. Recently, In-Context Learning demonstrated significant\nperformance advances for reasoning tasks. In this paradigm, a large language\nmodel performs predictions based on a small set of supporting exemplars. The\nperformance of In-Context Learning depends heavily on the selection procedure\nof the supporting exemplars, particularly in the case of HybridQA, where\nconsidering the diversity of reasoning chains and the large size of the hybrid\ncontexts becomes crucial. In this work, we present Selection of ExEmplars for\nhybrid Reasoning (SEER), a novel method for selecting a set of exemplars that\nis both representative and diverse. The key novelty of SEER is that it\nformulates exemplar selection as a Knapsack Integer Linear Program. The\nKnapsack framework provides the flexibility to incorporate diversity\nconstraints that prioritize exemplars with desirable attributes, and capacity\nconstraints that ensure that the prompt size respects the provided capacity\nbudgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two\nreal-world benchmarks for HybridQA, where it outperforms previous exemplar\nselection methods.\n","authors":["Jonathan Tonglet","Manon Reusens","Philipp Borchert","Bart Baesens"],"pdf_url":"https://arxiv.org/pdf/2310.06675v1.pdf","comment":"Accepted to EMNLP 2023 main conference. Code available at\n  github.com/jtonglet/SEER"},{"id":"http://arxiv.org/abs/2310.06671v1","updated":"2023-10-10T14:47:09Z","published":"2023-10-10T14:47:09Z","title":"Making Large Language Models Perform Better in Knowledge Graph\n  Completion","summary":"  Large language model (LLM) based knowledge graph completion (KGC) aims to\npredict the missing triples in the KGs with LLMs and enrich the KGs to become\nbetter web infrastructure, which can benefit a lot of web-based automatic\nservices. However, research about LLM-based KGC is limited and lacks effective\nutilization of LLM's inference capabilities, which ignores the important\nstructural information in KGs and prevents LLMs from acquiring accurate factual\nknowledge. In this paper, we discuss how to incorporate the helpful KG\nstructural information into the LLMs, aiming to achieve structrual-aware\nreasoning in the LLMs. We first transfer the existing LLM paradigms to\nstructural-aware settings and further propose a knowledge prefix adapter (KoPA)\nto fulfill this stated goal. KoPA employs structural embedding pre-training to\ncapture the structural information of entities and relations in the KG. Then\nKoPA informs the LLMs of the knowledge prefix adapter which projects the\nstructural embeddings into the textual space and obtains virtual knowledge\ntokens as a prefix of the input prompt. We conduct comprehensive experiments on\nthese structural-aware LLM-based KGC methods and provide an in-depth analysis\ncomparing how the introduction of structural information would be better for\nLLM's knowledge reasoning ability. Our code is released at\nhttps://github.com/zjukg/KoPA.\n","authors":["Yichi Zhang","Zhuo Chen","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06671v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2310.06666v1","updated":"2023-10-10T14:41:38Z","published":"2023-10-10T14:41:38Z","title":"Unlock the Potential of Counterfactually-Augmented Data in\n  Out-Of-Distribution Generalization","summary":"  Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip\nthe corresponding labels -- has the potential to improve the\nOut-Of-Distribution (OOD) generalization capability of language models, as CAD\ninduces language models to exploit domain-independent causal features and\nexclude spurious correlations. However, the empirical results of CAD's OOD\ngeneralization are not as efficient as anticipated. In this study, we attribute\nthe inefficiency to the myopia phenomenon caused by CAD: language models only\nfocus on causal features that are edited in the augmentation operation and\nexclude other non-edited causal features. Therefore, the potential of CAD is\nnot fully exploited. To address this issue, we analyze the myopia phenomenon in\nfeature space from the perspective of Fisher's Linear Discriminant, then we\nintroduce two additional constraints based on CAD's structural properties\n(dataset-level and sentence-level) to help language models extract more\ncomplete causal features in CAD, thereby mitigating the myopia phenomenon and\nimproving OOD generalization capability. We evaluate our method on two tasks:\nSentiment Analysis and Natural Language Inference, and the experimental results\ndemonstrate that our method could unlock the potential of CAD and improve the\nOOD generalization performance of language models by 1.0% to 5.9%.\n","authors":["Caoyun Fan","Wenqing Chen","Jidong Tian","Yitian Li","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2310.06666v1.pdf","comment":"Expert Systems With Applications 2023. arXiv admin note: text overlap\n  with arXiv:2302.09345"},{"id":"http://arxiv.org/abs/2212.03749v2","updated":"2023-10-10T14:32:43Z","published":"2022-12-07T16:20:50Z","title":"Memorization of Named Entities in Fine-tuned BERT Models","summary":"  Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets, which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differentially Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a detrimental\neffect on the text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.\n","authors":["Andor Diera","Nicolas Lell","Aygul Garifullina","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2212.03749v2.pdf","comment":"accepted at CD-MAKE 2023"},{"id":"http://arxiv.org/abs/2310.05028v2","updated":"2023-10-10T14:09:13Z","published":"2023-10-08T06:17:39Z","title":"Revisiting Large Language Models as Zero-shot Relation Extractors","summary":"  Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.\n","authors":["Guozheng Li","Peng Wang","Wenjun Ke"],"pdf_url":"https://arxiv.org/pdf/2310.05028v2.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06645v1","updated":"2023-10-10T14:07:49Z","published":"2023-10-10T14:07:49Z","title":"Self-Supervised Representation Learning for Online Handwriting Text\n  Classification","summary":"  Self-supervised learning offers an efficient way of extracting rich\nrepresentations from various types of unlabeled data while avoiding the cost of\nannotating large-scale datasets. This is achievable by designing a pretext task\nto form pseudo labels with respect to the modality and domain of the data.\nGiven the evolving applications of online handwritten texts, in this study, we\npropose the novel Part of Stroke Masking (POSM) as a pretext task for\npretraining models to extract informative representations from the online\nhandwriting of individuals in English and Chinese languages, along with two\nsuggested pipelines for fine-tuning the pretrained models. To evaluate the\nquality of the extracted representations, we use both intrinsic and extrinsic\nevaluation methods. The pretrained models are fine-tuned to achieve\nstate-of-the-art results in tasks such as writer identification, gender\nclassification, and handedness classification, also highlighting the\nsuperiority of utilizing the pretrained models over the models trained from\nscratch.\n","authors":["Pouya Mehralian","Bagher BabaAli","Ashena Gorgan Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2310.06645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08937v2","updated":"2023-10-10T13:48:12Z","published":"2023-06-15T08:21:15Z","title":"DocumentNet: Bridging the Data Gap in Document Pre-Training","summary":"  Document understanding tasks, in particular, Visually-rich Document Entity\nRetrieval (VDER), have gained significant attention in recent years thanks to\ntheir broad applications in enterprise AI. However, publicly available data\nhave been scarce for these tasks due to strict privacy constraints and high\nannotation costs. To make things worse, the non-overlapping entity spaces from\ndifferent datasets hinder the knowledge transfer between document types. In\nthis paper, we propose a method to collect massive-scale and weakly labeled\ndata from the web to benefit the training of VDER models. The collected\ndataset, named DocumentNet, does not depend on specific document types or\nentity sets, making it universally applicable to all VDER tasks. The current\nDocumentNet consists of 30M documents spanning nearly 400 document types\norganized in a four-level ontology. Experiments on a set of broadly adopted\nVDER tasks show significant improvements when DocumentNet is incorporated into\nthe pre-training for both classic and few-shot learning settings. With the\nrecent emergence of large language models (LLMs), DocumentNet provides a large\ndata source to extend their multi-modal capabilities for VDER.\n","authors":["Lijun Yu","Jin Miao","Xiaoyu Sun","Jiayi Chen","Alexander G. Hauptmann","Hanjun Dai","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2306.08937v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06627v1","updated":"2023-10-10T13:45:59Z","published":"2023-10-10T13:45:59Z","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models","summary":"  Counterfactual reasoning ability is one of the core abilities of human\nintelligence. This reasoning process involves the processing of alternatives to\nobserved states or past events, and this process can improve our ability for\nplanning and decision-making. In this work, we focus on benchmarking the\ncounterfactual reasoning ability of multi-modal large language models. We take\nthe question and answer pairs from the VQAv2 dataset and add one counterfactual\npresupposition to the questions, with the answer being modified accordingly.\nAfter generating counterfactual questions and answers using ChatGPT, we\nmanually examine all generated questions and answers to ensure correctness.\nOver 2k counterfactual question and answer pairs are collected this way. We\nevaluate recent vision language models on our newly collected test dataset and\nfound that all models exhibit a large performance drop compared to the results\ntested on questions without the counterfactual presupposition. This result\nindicates that there still exists space for developing vision language models.\nApart from the vision language models, our proposed dataset can also serves as\na benchmark for evaluating the ability of code generation LLMs, results\ndemonstrate a large gap between GPT-4 and current open-source models. Our code\nand dataset are available at \\url{https://github.com/Letian2003/C-VQA}.\n","authors":["Letian Zhang","Xiaotong Zhai","Zhongkai Zhao","Xin Wen","Yongshuo Zong","Bingchen Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06627v1.pdf","comment":"Short paper accepted at ICCV 2023 VLAR workshop"},{"id":"http://arxiv.org/abs/2310.06626v1","updated":"2023-10-10T13:45:24Z","published":"2023-10-10T13:45:24Z","title":"Topic-DPR: Topic-based Prompts for Dense Passage Retrieval","summary":"  Prompt-based learning's efficacy across numerous natural language processing\ntasks has led to its integration into dense passage retrieval. Prior research\nhas mainly focused on enhancing the semantic understanding of pre-trained\nlanguage models by optimizing a single vector as a continuous prompt. This\napproach, however, leads to a semantic space collapse; identical semantic\ninformation seeps into all representations, causing their distributions to\nconverge in a restricted region. This hinders differentiation between relevant\nand irrelevant passages during dense retrieval. To tackle this issue, we\npresent Topic-DPR, a dense passage retrieval model that uses topic-based\nprompts. Unlike the single prompt method, multiple topic-based prompts are\nestablished over a probabilistic simplex and optimized simultaneously through\ncontrastive learning. This encourages representations to align with their topic\ndistributions, improving space uniformity. Furthermore, we introduce a novel\npositive and negative sampling strategy, leveraging semi-structured data to\nboost dense retrieval efficiency. Experimental results from two datasets affirm\nthat our method surpasses previous state-of-the-art retrieval techniques.\n","authors":["Qingfa Xiao","Shuangyin Li","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06626v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06590v1","updated":"2023-10-10T12:55:22Z","published":"2023-10-10T12:55:22Z","title":"No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech\n  Recognition through Pitch Manipulation","summary":"  Automatic speech recognition (ASR) systems are known to be sensitive to the\nsociolinguistic variability of speech data, in which gender plays a crucial\nrole. This can result in disparities in recognition accuracy between male and\nfemale speakers, primarily due to the under-representation of the latter group\nin the training data. While in the context of hybrid ASR models several\nsolutions have been proposed, the gender bias issue has not been explicitly\naddressed in end-to-end neural architectures. To fill this gap, we propose a\ndata augmentation technique that manipulates the fundamental frequency (f0) and\nformants. This technique reduces the data unbalance among genders by simulating\nvoices of the under-represented female speakers and increases the variability\nwithin each gender group. Experiments on spontaneous English speech show that\nour technique yields a relative WER improvement up to 9.87% for utterances by\nfemale speakers, with larger gains for the least-represented f0 ranges.\n","authors":["Dennis Fucci","Marco Gaido","Matteo Negri","Mauro Cettolo","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2310.06590v1.pdf","comment":"Accepted at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.06588v1","updated":"2023-10-10T12:53:48Z","published":"2023-10-10T12:53:48Z","title":"FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics","summary":"  Despite the massive success of fine-tuning large Pre-trained Language Models\n(PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain\nsusceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM)\nis a simple yet effective dual-model approach that enhances the robustness of\nfine-tuned PLMs, which involves fine-tuning a model on the original training\nset (i.e. reference model), selecting a specified fraction of important\ntraining examples according to the training dynamics of the reference model,\nand fine-tuning the same model on these selected examples (i.e. main model).\nHowever, it suffers from the drawback of requiring fine-tuning the same model\ntwice, which is computationally expensive for large models. In this paper, we\nfirst show that 1) training dynamics are highly transferable across different\nmodel sizes and different pre-training methods, and that 2) main models\nfine-tuned using DM learn faster than when using conventional Empirical Risk\nMinimization (ERM). Building on these observations, we propose a novel\nfine-tuning approach based on the DM method: Fine-Tuning by transFerring\nTraining dynamics (FTFT). Compared with DM, FTFT uses more efficient reference\nmodels and then fine-tunes more capable main models for fewer steps. Our\nexperiments show that FTFT achieves better generalization robustness than ERM\nwhile spending less than half of the training cost.\n","authors":["Yupei Du","Albert Gatt","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.06588v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.06555v1","updated":"2023-10-10T12:10:40Z","published":"2023-10-10T12:10:40Z","title":"On Temporal References in Emergent Communication","summary":"  As humans, we use linguistic elements referencing time, such as before or\ntomorrow, to easily share past experiences and future predictions. While\ntemporal aspects of the language have been considered in computational\nlinguistics, no such exploration has been done within the field of emergent\ncommunication. We research this gap, providing the first reported temporal\nvocabulary within emergent communication literature. Our experimental analysis\nshows that a different agent architecture is sufficient for the natural\nemergence of temporal references, and that no additional losses are necessary.\nOur readily transferable architectural insights provide the basis for the\nincorporation of temporal referencing into other emergent communication\nenvironments.\n","authors":["Olaf Lipinski","Adam J. Sobey","Federico Cerutti","Timothy J. Norman"],"pdf_url":"https://arxiv.org/pdf/2310.06555v1.pdf","comment":"26 pages, 13 figures. Code available at\n  https://anonymous.4open.science/r/TRG-E137/README.md"},{"id":"http://arxiv.org/abs/2308.15126v3","updated":"2023-10-10T11:57:26Z","published":"2023-08-29T08:51:24Z","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n","authors":["Junyang Wang","Yiyang Zhou","Guohai Xu","Pengcheng Shi","Chenlin Zhao","Haiyang Xu","Qinghao Ye","Ming Yan","Ji Zhang","Jihua Zhu","Jitao Sang","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2308.15126v3.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06552v1","updated":"2023-10-10T11:56:48Z","published":"2023-10-10T11:56:48Z","title":"Automated clinical coding using off-the-shelf large language models","summary":"  The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment.\nUnsupervised pre-training alone does not guarantee precise knowledge of the ICD\nontology and specialist clinical coding task, therefore we frame the task as\ninformation extraction, providing a description of each coded concept and\nasking the model to retrieve related mentions. For efficiency, rather than\niterating over all codes, we leverage the hierarchical nature of the ICD\nontology to sparsely search for relevant codes. Then, in a second stage, which\nwe term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant\nlabels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4\non the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search\nmethod achieves state-of-the-art performance on rarer classes, achieving the\nbest macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157,\ncompared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our\nknowledge, this is the first method for automated ICD coding requiring no\ntask-specific learning.\n","authors":["Joseph S. Boyle","Antanas Kascenas","Pat Lok","Maria Liakata","Alison Q. O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.06552v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06547v1","updated":"2023-10-10T11:50:27Z","published":"2023-10-10T11:50:27Z","title":"Rationale-Enhanced Language Models are Better Continual Relation\n  Learners","summary":"  Continual relation extraction (CRE) aims to solve the problem of catastrophic\nforgetting when learning a sequence of newly emerging relations. Recent CRE\nstudies have found that catastrophic forgetting arises from the model's lack of\nrobustness against future analogous relations. To address the issue, we\nintroduce rationale, i.e., the explanations of relation classification results\ngenerated by large language models (LLM), into CRE task. Specifically, we\ndesign the multi-task rationale tuning strategy to help the model learn current\nrelations robustly. We also conduct contrastive rationale replay to further\ndistinguish analogous relations. Experimental results on two standard\nbenchmarks demonstrate that our method outperforms the state-of-the-art CRE\nmodels.\n","authors":["Weimin Xiong","Yifan Song","Peiyi Wang","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2310.06547v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06546v1","updated":"2023-10-10T11:50:16Z","published":"2023-10-10T11:50:16Z","title":"AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual\n  Voice Conversion","summary":"  This paper proposes a simple and robust zero-shot voice conversion system\nwith a cycle structure and mel-spectrogram pre-processing. Previous works\nsuffer from information loss and poor synthesis quality due to their reliance\non a carefully designed bottleneck structure. Moreover, models relying solely\non self-reconstruction loss struggled with reproducing different speakers'\nvoices. To address these issues, we suggested a cycle-consistency loss that\nconsiders conversion back and forth between target and source speakers.\nAdditionally, stacked random-shuffled mel-spectrograms and a label smoothing\nmethod are utilized during speaker encoder training to extract a\ntime-independent global speaker representation from speech, which is the key to\na zero-shot conversion. Our model outperforms existing state-of-the-art results\nin both subjective and objective evaluations. Furthermore, it facilitates\ncross-lingual voice conversions and enhances the quality of synthesized speech.\n","authors":["Haeyun Choi","Jio Gim","Yuho Lee","Youngin Kim","Young-Joo Suh"],"pdf_url":"https://arxiv.org/pdf/2310.06546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06540v1","updated":"2023-10-10T11:38:16Z","published":"2023-10-10T11:38:16Z","title":"A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo:\n  A Romanian Clickbait Corpus of News Articles","summary":"  To increase revenue, news websites often resort to using deceptive news\ntitles, luring users into clicking on the title and reading the full news.\nClickbait detection is the task that aims to automatically detect this form of\nfalse advertisement and avoid wasting the precious time of online users.\nDespite the importance of the task, to the best of our knowledge, there is no\npublicly available clickbait corpus for the Romanian language. To this end, we\nintroduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news\nsamples which are manually annotated with clickbait and non-clickbait labels.\nFurthermore, we conduct experiments with four machine learning methods, ranging\nfrom handcrafted models to recurrent and transformer-based neural networks, to\nestablish a line-up of competitive baselines. We also carry out experiments\nwith a weighted voting ensemble. Among the considered baselines, we propose a\nnovel BERT-based contrastive learning model that learns to encode news titles\nand contents into a deep metric space such that titles and contents of\nnon-clickbait news have high cosine similarity, while titles and contents of\nclickbait news have low cosine similarity. Our data set and code to reproduce\nthe baselines are publicly available for download at\nhttps://github.com/dariabroscoteanu/RoCliCo.\n","authors":["Daria-Mihaela Broscoteanu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2310.06540v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.12660v2","updated":"2023-10-10T11:36:08Z","published":"2023-05-22T03:04:06Z","title":"Beneath Surface Similarity: Large Language Models Make Reasonable\n  Scientific Analogies after Structure Abduction","summary":"  The vital role of analogical reasoning in human cognition allows us to grasp\nnovel concepts by linking them with familiar ones through shared relational\nstructures. Despite the attention previous research has given to word\nanalogies, this work suggests that Large Language Models (LLMs) often overlook\nthe structures that underpin these analogies, raising questions about the\nefficacy of word analogies as a measure of analogical reasoning skills akin to\nhuman cognition. In response to this, our paper introduces a task of analogical\nstructure abduction, grounded in cognitive psychology, designed to abduce\nstructures that form an analogy between two systems. In support of this task,\nwe establish a benchmark called SCAR, containing 400 scientific analogies from\n13 distinct fields, tailored for evaluating analogical reasoning with structure\nabduction. The empirical evidence underlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need\nfor future exploration to enhance their abilities.\n","authors":["Siyu Yuan","Jiangjie Chen","Xuyang Ge","Yanghua Xiao","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2305.12660v2.pdf","comment":"Accepted to EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2310.06536v1","updated":"2023-10-10T11:31:11Z","published":"2023-10-10T11:31:11Z","title":"EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer\n  Service Dialogues on Twitter","summary":"  Due to the rise of user-generated content, social media is increasingly\nadopted as a channel to deliver customer service. Given the public character of\nthese online platforms, the automatic detection of emotions forms an important\napplication in monitoring customer satisfaction and preventing negative\nword-of-mouth. This paper introduces EmoTwiCS, a corpus of 9,489 Dutch customer\nservice dialogues on Twitter that are annotated for emotion trajectories. In\nour business-oriented corpus, we view emotions as dynamic attributes of the\ncustomer that can change at each utterance of the conversation. The term\n`emotion trajectory' refers therefore not only to the fine-grained emotions\nexperienced by customers (annotated with 28 labels and\nvalence-arousal-dominance scores), but also to the event happening prior to the\nconversation and the responses made by the human operator (both annotated with\n8 categories). Inter-annotator agreement (IAA) scores on the resulting dataset\nare substantial and comparable with related research, underscoring its high\nquality. Given the interplay between the different layers of annotated\ninformation, we perform several in-depth analyses to investigate (i) static\nemotions in isolated tweets, (ii) dynamic emotions and their shifts in\ntrajectory, and (iii) the role of causes and response strategies in emotion\ntrajectories. We conclude by listing the advantages and limitations of our\ndataset, after which we give some suggestions on the different types of\npredictive modelling tasks and open research questions to which EmoTwiCS can be\napplied. The dataset is available upon request and will be made publicly\navailable upon acceptance of the paper.\n","authors":["Sofie Labat","Thomas Demeester","Vronique Hoste"],"pdf_url":"https://arxiv.org/pdf/2310.06536v1.pdf","comment":"Preprint to Language Resources and Evaluation Journal"},{"id":"http://arxiv.org/abs/2305.14189v2","updated":"2023-10-10T11:27:49Z","published":"2023-05-23T16:11:00Z","title":"Beyond Shared Vocabulary: Increasing Representational Word Similarities\n  across Languages for Multilingual Machine Translation","summary":"  Using a vocabulary that is shared across languages is common practice in\nMultilingual Neural Machine Translation (MNMT). In addition to its simple\ndesign, shared tokens play an important role in positive knowledge transfer,\nassuming that shared tokens refer to similar meanings across languages.\nHowever, when word overlap is small, especially due to different writing\nsystems, transfer is inhibited. In this paper, we define word-level information\ntransfer pathways via word equivalence classes and rely on graph networks to\nfuse word embeddings across languages. Our experiments demonstrate the\nadvantages of our approach: 1) embeddings of words with similar meanings are\nbetter aligned across languages, 2) our method achieves consistent BLEU\nimprovements of up to 2.3 points for high- and low-resource MNMT, and 3) less\nthan 1.0\\% additional trainable parameters are required with a limited increase\nin computational costs, while inference time remains identical to the baseline.\nWe release the codebase to the community.\n","authors":["Di Wu","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2305.14189v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.06517v1","updated":"2023-10-10T11:00:23Z","published":"2023-10-10T11:00:23Z","title":"Toward Semantic Publishing in Non-Invasive Brain Stimulation: A\n  Comprehensive Analysis of rTMS Studies","summary":"  Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation\ntechniques that can influence brain excitability. These techniques have the\npotential to treat conditions like depression, anxiety, and chronic pain, and\nto provide insights into brain function. However, a lack of standardized\nreporting practices limits its reproducibility and full clinical potential.\nThis paper aims to foster interinterdisciplinarity toward adopting Computer\nScience Semantic reporting methods for the standardized documentation of\nNeuroscience NIBS studies making them explicitly Findable, Accessible,\nInteroperable, and Reusable (FAIR).\n  In a large-scale systematic review of 600 repetitive transcranial magnetic\nstimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that\nallow for structured descriptions and comparisons of the studies. This paper\nshowcases the semantic publishing of NIBS in the ecosphere of\nknowledge-graph-based next-generation scholarly digital libraries.\nSpecifically, the FAIR Semantic Web resource(s)-based publishing paradigm is\nimplemented for the 600 reviewed rTMS studies in the Open Research Knowledge\nGraph.\n","authors":["Swathi Anil","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2310.06517v1.pdf","comment":"8 pages, 2 figures. Accepted as a Practice Paper at The 25th\n  International Conference on Asia-Pacific Digital Libraries (ICADL 2023)\n  https://icadl.net/icadl2023/index.html#accepted"},{"id":"http://arxiv.org/abs/2310.06505v1","updated":"2023-10-10T10:25:56Z","published":"2023-10-10T10:25:56Z","title":"Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion","summary":"  Since its launch in November 2022, ChatGPT has had a transformative effect on\neducation where students are using it to help with homework assignments and\nteachers are actively employing it in their teaching practices. This includes\nusing ChatGPT as a tool for writing teachers to grade and generate feedback on\nstudents' essays. In this study, we evaluated the quality of the feedback\ngenerated by ChatGPT regarding the coherence and cohesion of the essays written\nby English Language Learners (ELLs) students. We selected 50 argumentative\nessays and generated feedback on coherence and cohesion using the ELLIPSE\nrubric. During the feedback evaluation, we used a two-step approach: first,\neach sentence in the feedback was classified into subtypes based on its\nfunction (e.g., positive reinforcement, problem statement). Next, we evaluated\nits accuracy and usability according to these types. Both the analysis of\nfeedback types and the evaluation of accuracy and usability revealed that most\nfeedback sentences were highly abstract and generic, failing to provide\nconcrete suggestions for improvement. The accuracy in detecting major problems,\nsuch as repetitive ideas and the inaccurate use of cohesive devices, depended\non superficial linguistic features and was often incorrect. In conclusion,\nChatGPT, without specific training for the feedback generation task, does not\noffer effective feedback on ELL students' coherence and cohesion.\n","authors":["Su-Youn Yoon","Eva Miszoglad","Lisa R. Pierce"],"pdf_url":"https://arxiv.org/pdf/2310.06505v1.pdf","comment":"24 pages, 1 figures"},{"id":"http://arxiv.org/abs/2310.06504v1","updated":"2023-10-10T10:22:05Z","published":"2023-10-10T10:22:05Z","title":"Revisit Input Perturbation Problems for LLMs: A Unified Robustness\n  Evaluation Framework for Noisy Slot Filling Task","summary":"  With the increasing capabilities of large language models (LLMs), these\nhigh-performance models have achieved state-of-the-art results on a wide range\nof natural language processing (NLP) tasks. However, the models' performance on\ncommonly-used benchmark datasets often fails to accurately reflect their\nreliability and robustness when applied to real-world noisy data. To address\nthese challenges, we propose a unified robustness evaluation framework based on\nthe slot-filling task to systematically evaluate the dialogue understanding\ncapability of LLMs in diverse input perturbation scenarios. Specifically, we\nconstruct a input perturbation evaluation dataset, Noise-LLM, which contains\nfive types of single perturbation and four types of mixed perturbation data.\nFurthermore, we utilize a multi-level data augmentation method (character,\nword, and sentence levels) to construct a candidate data pool, and carefully\ndesign two ways of automatic task demonstration construction strategies\n(instance-level and entity-level) with various prompt templates. Our aim is to\nassess how well various robustness methods of LLMs perform in real-world noisy\nscenarios. The experiments have demonstrated that the current open-source LLMs\ngenerally achieve limited perturbation robustness performance. Based on these\nexperimental observations, we make some forward-looking suggestions to fuel the\nresearch in this direction.\n","authors":["Guanting Dong","Jinxu Zhao","Tingfeng Hui","Daichi Guo","Wenlong Wan","Boqi Feng","Yueyan Qiu","Zhuoma Gongque","Keqing He","Zechen Wang","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2310.06504v1.pdf","comment":"Accepted at NLPCC 2023 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2310.06502v1","updated":"2023-10-10T10:19:58Z","published":"2023-10-10T10:19:58Z","title":"The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment\n  Quadruples: A Comparative Analysis","summary":"  Recently, ChatGPT has attracted great attention from both industry and\nacademia due to its surprising abilities in natural language understanding and\ngeneration. We are particularly curious about whether it can achieve promising\nperformance on one of the most complex tasks in aspect-based sentiment\nanalysis, i.e., extracting aspect-category-opinion-sentiment quadruples from\ntexts. To this end, in this paper we develop a specialized prompt template that\nenables ChatGPT to effectively tackle this complex quadruple extraction task.\nFurther, we propose a selection method on few-shot examples to fully exploit\nthe in-context learning ability of ChatGPT and uplift its effectiveness on this\ncomplex task. Finally, we provide a comparative evaluation on ChatGPT against\nexisting state-of-the-art quadruple extraction models based on four public\ndatasets and highlight some important findings regarding the capability\nboundaries of ChatGPT in the quadruple extraction.\n","authors":["Xiancai Xu","Jia-Dong Zhang","Rongchang Xiao","Lei Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.06502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04444v2","updated":"2023-10-10T10:15:14Z","published":"2023-10-02T22:35:40Z","title":"What's the Magic Word? A Control Theory of LLM Prompting","summary":"  Prompt engineering is effective and important in the deployment of LLMs but\nis poorly understood mathematically. Here, we formalize prompt engineering as\nan optimal control problem on LLMs -- where the prompt is considered a control\nvariable for modulating the output distribution of the LLM. Within this\nframework, we ask a simple question: given a sequence of tokens, does there\nalways exist a prompt we can prepend that will steer the LLM toward accurately\npredicting the final token? We call such an optimal prompt the magic word since\nprepending the prompt causes the LLM to output the correct answer. If magic\nwords exist, can we find them? If so, what are their properties? We offer\nanalytic analysis on the controllability of the self-attention head where we\nprove a bound on controllability as a function of the singular values of its\nweight matrices. We take inspiration from control theory to propose a metric\ncalled $k-\\epsilon$ controllability to characterize LLM steerability. We\ncompute the $k-\\epsilon$ controllability of a panel of large language models,\nincluding Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language\nmodeling tasks. Remarkably, we find that magic words of 10 tokens or less exist\nfor over 97% of WikiText instances surveyed for each model.\n","authors":["Aman Bhargava","Cameron Witkowski","Manav Shah","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2310.04444v2.pdf","comment":"18 pages, 8 figures. Under review for ICLR 2024"},{"id":"http://arxiv.org/abs/2310.06498v1","updated":"2023-10-10T10:14:59Z","published":"2023-10-10T10:14:59Z","title":"A New Benchmark and Reverse Validation Method for Passage-level\n  Hallucination Detection","summary":"  Large Language Models (LLMs) have demonstrated their ability to collaborate\neffectively with humans in real-world scenarios. However, LLMs are apt to\ngenerate hallucinations, i.e., makeup incorrect text and unverified\ninformation, which can cause significant damage when deployed for\nmission-critical tasks. In this paper, we propose a self-check approach based\non reverse validation to detect factual errors automatically in a zero-resource\nfashion. To facilitate future studies and assess different methods, we\nconstruct a hallucination detection benchmark, which is generated by ChatGPT\nand annotated by human annotators. Contrasting previous studies of\nzero-resource hallucination detection, our method and benchmark concentrate on\npassage-level detection instead of sentence-level. We empirically evaluate our\nmethod and existing zero-resource detection methods on different domains of\nbenchmark to explore the implicit relation between hallucination and training\ndata. Furthermore, we manually analyze some hallucination cases that LLM failed\nto capture, revealing the shared limitation of zero-resource methods.\n","authors":["Shiping Yang","Renliang Sun","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2310.06498v1.pdf","comment":"Findings of EMNLP 2023;Camera-ready version will be updated soon"},{"id":"http://arxiv.org/abs/2310.06488v1","updated":"2023-10-10T09:57:17Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15004v2","updated":"2023-10-10T09:56:31Z","published":"2023-05-24T10:45:16Z","title":"LLMDet: A Third Party Large Language Models Generated Text Detection\n  Tool","summary":"  Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x3.5 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.\n","authors":["Kangxi Wu","Liang Pang","Huawei Shen","Xueqi Cheng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2305.15004v2.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2307.09378v2","updated":"2023-10-10T09:50:48Z","published":"2023-07-13T16:01:58Z","title":"Adapting an ASR Foundation Model for Spoken Language Assessment","summary":"  A crucial part of an accurate and reliable spoken language assessment system\nis the underlying ASR model. Recently, large-scale pre-trained ASR foundation\nmodels such as Whisper have been made available. As the output of these models\nis designed to be human readable, punctuation is added, numbers are presented\nin Arabic numeric form and abbreviations are included. Additionally, these\nmodels have a tendency to skip disfluencies and hesitations in the output.\nThough useful for readability, these attributes are not helpful for assessing\nthe ability of a candidate and providing feedback. Here a precise transcription\nof what a candidate said is needed. In this paper, we give a detailed analysis\nof Whisper outputs and propose two solutions: fine-tuning and soft prompt\ntuning. Experiments are conducted on both public speech corpora and an English\nlearner dataset. Results show that we can effectively alter the decoding\nbehaviour of Whisper to generate the exact words spoken in the response.\n","authors":["Rao Ma","Mengjie Qian","Mark J. F. Gales","Kate M. Knill"],"pdf_url":"https://arxiv.org/pdf/2307.09378v2.pdf","comment":"Proceedings of SLaTE"},{"id":"http://arxiv.org/abs/2303.00456v3","updated":"2023-10-10T09:45:13Z","published":"2023-03-01T12:32:34Z","title":"N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses\n  and Constrained Decoding Space","summary":"  Error correction models form an important part of Automatic Speech\nRecognition (ASR) post-processing to improve the readability and quality of\ntranscriptions. Most prior works use the 1-best ASR hypothesis as input and\ntherefore can only perform correction by leveraging the context within one\nsentence. In this work, we propose a novel N-best T5 model for this task, which\nis fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By\ntransferring knowledge from the pre-trained language model and obtaining richer\ninformation from the ASR decoding space, the proposed approach outperforms a\nstrong Conformer-Transducer baseline. Another issue with standard error\ncorrection is that the generation process is not well-guided. To address this a\nconstrained decoding process, either based on the N-best list or an ASR\nlattice, is used which allows additional information to be propagated.\n","authors":["Rao Ma","Mark J. F. Gales","Kate M. Knill","Mengjie Qian"],"pdf_url":"https://arxiv.org/pdf/2303.00456v3.pdf","comment":"Proceedings of INTERSPEECH"},{"id":"http://arxiv.org/abs/2306.01208v2","updated":"2023-10-10T09:44:24Z","published":"2023-06-01T23:54:11Z","title":"Adapting an Unadaptable ASR System","summary":"  As speech recognition model sizes and training data requirements grow, it is\nincreasingly common for systems to only be available via APIs from online\nservice providers rather than having direct access to models themselves. In\nthis scenario it is challenging to adapt systems to a specific target domain.\nTo address this problem we consider the recently released OpenAI Whisper ASR as\nan example of a large-scale ASR system to assess adaptation methods. An error\ncorrection based approach is adopted, as this does not require access to the\nmodel, but can be trained from either 1-best or N-best outputs that are\nnormally available via the ASR API. LibriSpeech is used as the primary target\ndomain for adaptation. The generalization ability of the system in two distinct\ndimensions are then evaluated. First, whether the form of correction model is\nportable to other speech recognition domains, and secondly whether it can be\nused for ASR models having a different architecture.\n","authors":["Rao Ma","Mengjie Qian","Mark J. F. Gales","Kate M. Knill"],"pdf_url":"https://arxiv.org/pdf/2306.01208v2.pdf","comment":"Proceedings of INTERSPEECH"},{"id":"http://arxiv.org/abs/2310.06474v1","updated":"2023-10-10T09:44:06Z","published":"2023-10-10T09:44:06Z","title":"Multilingual Jailbreak Challenges in Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English data. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risk\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\nhttps://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This\npaper contains examples with potentially harmful content.\n","authors":["Yue Deng","Wenxuan Zhang","Sinno Jialin Pan","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2310.06474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04480v2","updated":"2023-10-10T09:31:04Z","published":"2023-10-06T09:12:35Z","title":"Auto-survey Challenge","summary":"  We present a novel platform for evaluating the capability of Large Language\nModels (LLMs) to autonomously compose and critique survey papers spanning a\nvast array of disciplines including sciences, humanities, education, and law.\nWithin this framework, AI systems undertake a simulated peer-review mechanism\nakin to traditional scholarly journals, with human organizers serving in an\neditorial oversight capacity. Within this framework, we organized a competition\nfor the AutoML conference 2023. Entrants are tasked with presenting stand-alone\nmodels adept at authoring articles from designated prompts and subsequently\nappraising them. Assessment criteria include clarity, reference\nappropriateness, accountability, and the substantive value of the content. This\npaper presents the design of the competition, including the implementation\nbaseline submissions and methods of evaluation.\n","authors":["Thanh Gia Hieu Khuong","Benedictus Kent Rachmat"],"pdf_url":"https://arxiv.org/pdf/2310.04480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06458v1","updated":"2023-10-10T09:29:38Z","published":"2023-10-10T09:29:38Z","title":"Cultural Compass: Predicting Transfer Learning Success in Offensive\n  Language Detection with Cultural Features","summary":"  The increasing ubiquity of language technology necessitates a shift towards\nconsidering cultural diversity in the machine learning realm, particularly for\nsubjective tasks that rely heavily on cultural nuances, such as Offensive\nLanguage Detection (OLD). Current understanding underscores that these tasks\nare substantially influenced by cultural values, however, a notable gap exists\nin determining if cultural features can accurately predict the success of\ncross-cultural transfer learning for such subjective tasks. Addressing this,\nour study delves into the intersection of cultural features and transfer\nlearning effectiveness. The findings reveal that cultural value surveys indeed\npossess a predictive power for cross-cultural transfer learning success in OLD\ntasks and that it can be further improved using offensive word distance. Based\non these results, we advocate for the integration of cultural information into\ndatasets. Additionally, we recommend leveraging data sources rich in cultural\ninformation, such as surveys, to enhance cultural adaptability. Our research\nsignifies a step forward in the quest for more inclusive, culturally sensitive\nlanguage technologies.\n","authors":["Li Zhou","Antonia Karamolegkou","Wenyu Chen","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2310.06458v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06452v1","updated":"2023-10-10T09:25:44Z","published":"2023-10-10T09:25:44Z","title":"Understanding the Effects of RLHF on LLM Generalisation and Diversity","summary":"  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the trade-off between generalisation\nand diversity.\n","authors":["Robert Kirk","Ishita Mediratta","Christoforos Nalmpantis","Jelena Luketina","Eric Hambro","Edward Grefenstette","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2310.06452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06450v1","updated":"2023-10-10T09:20:14Z","published":"2023-10-10T09:20:14Z","title":"Constructive Large Language Models Alignment with Diverse Feedback","summary":"  In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.\n","authors":["Tianshu Yu","Ting-En Lin","Yuchuan Wu","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.06450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15649v2","updated":"2023-10-10T09:10:58Z","published":"2023-09-27T13:36:03Z","title":"Generative Speech Recognition Error Correction with Large Language\n  Models and Task-Activating Prompting","summary":"  We explore the ability of large language models (LLMs) to act as speech\nrecognition post-processors that perform rescoring and error correction. Our\nfirst focus is on instruction prompting to let LLMs perform these task without\nfine-tuning, for which we evaluate different prompting schemes, both zero- and\nfew-shot in-context learning, and a novel task activation prompting method that\ncombines causal instructions and demonstration to increase its context windows.\nNext, we show that rescoring only by in-context learning with frozen LLMs\nachieves results that are competitive with rescoring by domain-tuned LMs, using\na pretrained first-pass recognition system and rescoring output on two\nout-of-domain tasks (ATIS and WSJ). By combining prompting techniques with\nfine-tuning we achieve error rates below the N-best oracle level, showcasing\nthe generalization power of the LLMs.\n","authors":["Chao-Han Huck Yang","Yile Gu","Yi-Chieh Liu","Shalini Ghosh","Ivan Bulyko","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2309.15649v2.pdf","comment":"Accepted to IEEE Automatic Speech Recognition and Understanding\n  (ASRU) 2023. 8 pages. 2nd version revised from Sep 29th's version"},{"id":"http://arxiv.org/abs/2310.06436v1","updated":"2023-10-10T09:06:08Z","published":"2023-10-10T09:06:08Z","title":"MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer\n  for Document Question Answering","summary":"  We introduce MemSum-DQA, an efficient system for document question answering\n(DQA) that leverages MemSum, a long document extractive summarizer. By\nprefixing each text block in the parsed document with the provided question and\nquestion type, MemSum-DQA selectively extracts text blocks as answers from\ndocuments. On full-document answering tasks, this approach yields a 9%\nimprovement in exact match accuracy over prior state-of-the-art baselines.\nNotably, MemSum-DQA excels in addressing questions related to\nchild-relationship understanding, underscoring the potential of extractive\nsummarization techniques for DQA tasks.\n","authors":["Nianlong Gu","Yingqiang Gao","Richard H. R. Hahnloser"],"pdf_url":"https://arxiv.org/pdf/2310.06436v1.pdf","comment":"This paper is the technical research paper of CIKM 2023 DocIU\n  challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored\n  by Google, Microsoft, and the Centre for data-driven geoscience"},{"id":"http://arxiv.org/abs/2310.06434v1","updated":"2023-10-10T09:04:33Z","published":"2023-10-10T09:04:33Z","title":"Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n  for Speech Recognition","summary":"  We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.\n","authors":["Srijith Radhakrishnan","Chao-Han Huck Yang","Sumeer Ahmad Khan","Rohit Kumar","Narsis A. Kiani","David Gomez-Cabrero","Jesper N. Tegner"],"pdf_url":"https://arxiv.org/pdf/2310.06434v1.pdf","comment":"Accepted to EMNLP 2023. 10 pages. This work has been done in October\n  2022 and was submitted to EMNLP 23 once the draft was finalized. GitHub:\n  https://github.com/Srijith-rkr/Whispering-LLaMA"},{"id":"http://arxiv.org/abs/2310.06433v1","updated":"2023-10-10T09:03:01Z","published":"2023-10-10T09:03:01Z","title":"Retromorphic Testing: A New Approach to the Test Oracle Problem","summary":"  A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.\n","authors":["Boxi Yu","Qiuyang Mang","Qingshuo Guo","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2310.06433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00322v2","updated":"2023-10-10T09:00:40Z","published":"2023-09-30T09:35:50Z","title":"Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language\n  Models","summary":"  Deployable Large Language Models (LLMs) must conform to the criterion of\nhelpfulness and harmlessness, thereby achieving consistency between LLMs\noutputs and human values. Red-teaming techniques constitute a critical way\ntowards this criterion. Existing work rely solely on manual red team designs\nand heuristic adversarial prompts for vulnerability detection and optimization.\nThese approaches lack rigorous mathematical formulation, thus limiting the\nexploration of diverse attack strategy within quantifiable measure and\noptimization of LLMs under convergence guarantees. In this paper, we present\nRed-teaming Game (RTG), a general game-theoretic framework without manual\nannotation. RTG is designed for analyzing the multi-turn attack and defense\ninteractions between Red-team language Models (RLMs) and Blue-team Language\nModel (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with\ndiversity measure of the semantic space. GRTS is an automated red teaming\ntechnique to solve RTG towards Nash equilibrium through meta-game analysis,\nwhich corresponds to the theoretically guaranteed optimization direction of\nboth RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that\nGRTS autonomously discovered diverse attack strategies and effectively improved\nsecurity of LLMs, outperforming existing heuristic red-team designs. Overall,\nRTG has established a foundational framework for red teaming tasks and\nconstructed a new scalable oversight technique for alignment.\n","authors":["Chengdong Ma","Ziran Yang","Minquan Gao","Hai Ci","Jun Gao","Xuehai Pan","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2310.00322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06422v1","updated":"2023-10-10T08:46:10Z","published":"2023-10-10T08:46:10Z","title":"Large Language Models for Propaganda Detection","summary":"  The prevalence of propaganda in our digital society poses a challenge to\nsocietal harmony and the dissemination of truth. Detecting propaganda through\nNLP in text is challenging due to subtle manipulation techniques and contextual\ndependencies. To address this issue, we investigate the effectiveness of modern\nLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.\nWe conduct experiments using the SemEval-2020 task 11 dataset, which features\nnews articles labeled with 14 propaganda techniques as a multi-label\nclassification problem. Five variations of GPT-3 and GPT-4 are employed,\nincorporating various prompt engineering and fine-tuning strategies across the\ndifferent models. We evaluate the models' performance by assessing metrics such\nas $F1$ score, $Precision$, and $Recall$, comparing the results with the\ncurrent state-of-the-art approach using RoBERTa. Our findings demonstrate that\nGPT-4 achieves comparable results to the current state-of-the-art. Further,\nthis study analyzes the potential and challenges of LLMs in complex tasks like\npropaganda detection.\n","authors":["Kilian Sprenkamp","Daniel Gordon Jones","Liudmila Zavolokina"],"pdf_url":"https://arxiv.org/pdf/2310.06422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10916v3","updated":"2023-10-10T08:45:00Z","published":"2023-09-19T20:28:24Z","title":"What Learned Representations and Influence Functions Can Tell Us About\n  Adversarial Examples","summary":"  Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n  In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.\n","authors":["Shakila Mahjabin Tonni","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2309.10916v3.pdf","comment":"20 pages, Accepted in IJCNLP_AACL 2023"},{"id":"http://arxiv.org/abs/2310.06408v1","updated":"2023-10-10T08:24:28Z","published":"2023-10-10T08:24:28Z","title":"Humans and language models diverge when predicting repeating text","summary":"  Language models that are trained on the next-word prediction task have been\nshown to accurately model human behavior in word prediction and reading speed.\nIn contrast with these findings, we present a scenario in which the performance\nof humans and LMs diverges. We collected a dataset of human next-word\npredictions for five stimuli that are formed by repeating spans of text. Human\nand GPT-2 LM predictions are strongly aligned in the first presentation of a\ntext span, but their performance quickly diverges when memory (or in-context\nlearning) begins to play a role. We traced the cause of this divergence to\nspecific attention heads in a middle layer. Adding a power-law recency bias to\nthese attention heads yielded a model that performs much more similarly to\nhumans. We hope that this scenario will spur future work in bringing LMs closer\nto human behavior.\n","authors":["Aditya R. Vaidya","Javier Turek","Alexander G. Huth"],"pdf_url":"https://arxiv.org/pdf/2310.06408v1.pdf","comment":"To appear in the 26th Conference on Computational Natural Language\n  Learning (CoNLL 2023)"},{"id":"http://arxiv.org/abs/2310.06404v1","updated":"2023-10-10T08:15:24Z","published":"2023-10-10T08:15:24Z","title":"Hexa: Self-Improving for Knowledge-Grounded Dialogue System","summary":"  A common practice in knowledge-grounded dialogue generation is to explicitly\nutilize intermediate steps (e.g., web-search, memory retrieval) with modular\napproaches. However, data for such steps are often inaccessible compared to\nthose of dialogue responses as they are unobservable in an ordinary dialogue.\nTo fill in the absence of these data, we develop a self-improving method to\nimprove the generative performances of intermediate steps without the ground\ntruth data. In particular, we propose a novel bootstrapping scheme with a\nguided prompt and a modified loss function to enhance the diversity of\nappropriate self-generated responses. Through experiments on various benchmark\ndatasets, we empirically demonstrate that our method successfully leverages a\nself-improving mechanism in generating intermediate and final responses and\nimproves the performances on the task of knowledge-grounded dialogue\ngeneration.\n","authors":["Daejin Jo","Daniel Wontae Nam","Gunsoo Han","Kyoung-Woon On","Taehwan Kwon","Seungeun Rho","Sungwoong Kim"],"pdf_url":"https://arxiv.org/pdf/2310.06404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01405v3","updated":"2023-10-10T08:00:53Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v3.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2310.06391v1","updated":"2023-10-10T07:54:24Z","published":"2023-10-10T07:54:24Z","title":"Improved prompting and process for writing user personas with LLMs,\n  using qualitative interviews: Capturing behaviour and personality traits of\n  users","summary":"  This draft paper presents a workflow for creating User Personas with Large\nLanguage Models, using the results of a Thematic Analysis of qualitative\ninterviews. The proposed workflow uses improved prompting and a larger pool of\nThemes, compared to previous work conducted by the author for the same task.\nThis is possible due to the capabilities of a recently released LLM which\nallows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to\nthe possibility to offer a refined prompting for the creation of Personas. The\npaper offers details of performing Phase 2 and 3 of Thematic Analysis, and then\ndiscusses the improved workflow for creating Personas. The paper also offers\nsome reflections on the relationship between the proposed process and existing\napproaches to Personas such as the data-driven and qualitative Personas.\nMoreover, the paper offers reflections on the capacity of LLMs to capture user\nbehaviours and personality traits, from the underlying dataset of qualitative\ninterviews used for the analysis.\n","authors":["Stefano De Paoli"],"pdf_url":"https://arxiv.org/pdf/2310.06391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06390v1","updated":"2023-10-10T07:53:36Z","published":"2023-10-10T07:53:36Z","title":"P5: Plug-and-Play Persona Prompting for Personalized Response Selection","summary":"  The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.\n","authors":["Joosung Lee","Minsik Oh","Donghun Lee"],"pdf_url":"https://arxiv.org/pdf/2310.06390v1.pdf","comment":"EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2309.10400v2","updated":"2023-10-10T07:51:31Z","published":"2023-09-19T08:03:38Z","title":"PoSE: Efficient Context Window Extension of LLMs via Positional\n  Skip-wise Training","summary":"  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n","authors":["Dawei Zhu","Nan Yang","Liang Wang","Yifan Song","Wenhao Wu","Furu Wei","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2309.10400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06387v1","updated":"2023-10-10T07:50:29Z","published":"2023-10-10T07:50:29Z","title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context\n  Demonstrations","summary":"  Large Language Models (LLMs) have shown remarkable success in various tasks,\nbut concerns about their safety and the potential for generating malicious\ncontent have emerged. In this paper, we explore the power of In-Context\nLearning (ICL) in manipulating the alignment ability of LLMs. We find that by\nproviding just few in-context demonstrations without fine-tuning, LLMs can be\nmanipulated to increase or decrease the probability of jailbreaking, i.e.\nanswering malicious prompts. Based on these observations, we propose In-Context\nAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding\naligned language model purposes. ICA crafts malicious contexts to guide models\nin generating harmful outputs, while ICD enhances model robustness by\ndemonstrations of rejecting to answer harmful prompts. Our experiments show the\neffectiveness of ICA and ICD in increasing or reducing the success rate of\nadversarial jailbreaking attacks. Overall, we shed light on the potential of\nICL to influence LLM behavior and provide a new perspective for enhancing the\nsafety and alignment of LLMs.\n","authors":["Zeming Wei","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15011v2","updated":"2023-10-10T07:46:44Z","published":"2023-05-24T10:50:31Z","title":"Bactrian-X: Multilingual Replicable Instruction-Following Models with\n  Low-Rank Adaptation","summary":"  Instruction tuning has shown great promise in improving the performance of\nlarge language models. However, research on multilingual instruction tuning has\nbeen limited due to the scarcity of high-quality instruction-response datasets\nacross different languages. To bridge this gap, we present Bactrian-X, a\ncomprehensive multilingual parallel dataset of 3.4 million instruction-response\npairs across 52 languages. Leveraging this dataset, we train a set of adapters\nusing low-rank adaptation (LoRA), which are lightweight components that\nseamlessly integrate with large language models. These adapters have a\nsubstantially lower parameter count than the base model, making them easily\nreplaceable and usable as plug-ins for different languages or language groups.\nExtensive experiments in various multilingual evaluation settings demonstrate\nthat models derived from LoRA-based training over Bactrian-X outperform both\nthe vanilla models and existing instruction-tuned models. The code and models\nare publicly available at https://github.com/mbzuai-nlp/bactrian-x\n","authors":["Haonan Li","Fajri Koto","Minghao Wu","Alham Fikri Aji","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2305.15011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06374v1","updated":"2023-10-10T07:34:45Z","published":"2023-10-10T07:34:45Z","title":"Rethinking Model Selection and Decoding for Keyphrase Generation with\n  Pre-trained Sequence-to-Sequence Models","summary":"  Keyphrase Generation (KPG) is a longstanding task in NLP with widespread\napplications. The advent of sequence-to-sequence (seq2seq) pre-trained language\nmodels (PLMs) has ushered in a transformative era for KPG, yielding promising\nperformance improvements. However, many design decisions remain unexplored and\nare often made arbitrarily. This paper undertakes a systematic analysis of the\ninfluence of model selection and decoding strategies on PLM-based KPG. We begin\nby elucidating why seq2seq PLMs are apt for KPG, anchored by an\nattention-driven hypothesis. We then establish that conventional wisdom for\nselecting seq2seq PLMs lacks depth: (1) merely increasing model size or\nperforming task-specific adaptation is not parameter-efficient; (2) although\ncombining in-domain pre-training with task adaptation benefits KPG, it does\npartially hinder generalization. Regarding decoding, we demonstrate that while\ngreedy search delivers strong F1 scores, it lags in recall compared with\nsampling-based methods. From our insights, we propose DeSel, a likelihood-based\ndecode-select algorithm that improves greedy search by an average of 4.7%\nsemantic F1 across five datasets. Our collective findings pave the way for\ndeeper future investigations into PLM-based KPG.\n","authors":["Di Wu","Wasi Uddin Ahmad","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2310.06374v1.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.04484v2","updated":"2023-10-10T07:17:32Z","published":"2023-10-06T13:28:04Z","title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning","summary":"  Generating diverse and sophisticated instructions for downstream tasks by\nLarge Language Models (LLMs) is pivotal for advancing the effect. Current\napproaches leverage closed-source LLMs, employing in-context prompting for\ninstruction generation. However, in this paper, we found that in-context\nprompting cannot generate complex instructions with length $\\ge 100$ for tasks\nlike code completion.\n  To solve this problem, we introduce Ada-Instruct, an adaptive instruction\ngenerator developed by fine-tuning open-source LLMs. Our pivotal finding\nillustrates that fine-tuning open-source LLMs with a mere ten samples generates\nlong instructions that maintain distributional consistency for complex\nreasoning tasks. We empirically validated Ada-Instruct's efficacy across\ndifferent applications, including code completion, mathematical reasoning, and\ncommonsense reasoning. The results underscore Ada-Instruct's superiority,\nevidencing its improvements over its base models, current self-instruct\nmethods, and other state-of-the-art models.\n","authors":["Wanyun Cui","Qianle Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06365v1","updated":"2023-10-10T07:06:06Z","published":"2023-10-10T07:06:06Z","title":"Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity\n  Alignment","summary":"  Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify\nequivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,\nthis task faces challenges due to the presence of different types of\ninformation, including neighboring entities, multi-modal attributes, and entity\ntypes. Directly incorporating the above information (e.g., concatenation or\nattention) can lead to an unaligned information space. To address these\nchallenges, we propose a novel MMEA transformer, called MoAlign, that\nhierarchically introduces neighbor features, multi-modal attributes, and entity\ntypes to enhance the alignment task. Taking advantage of the transformer's\nability to better integrate multiple information, we design a hierarchical\nmodifiable self-attention block in a transformer encoder to preserve the unique\nsemantics of different information. Furthermore, we design two entity-type\nprefix injection methods to integrate entity-type information using type\nprefixes, which help to restrict the global information of entities not present\nin the MMKGs. Our extensive experiments on benchmark datasets demonstrate that\nour approach outperforms strong competitors and achieves excellent entity\nalignment performance.\n","authors":["Qian Li","Cheng Ji","Shu Guo","Zhaoji Liang","Lihong Wang","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2310.06365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06362v1","updated":"2023-10-10T07:00:13Z","published":"2023-10-10T07:00:13Z","title":"InfoCL: Alleviating Catastrophic Forgetting in Continual Text\n  Classification from An Information Theoretic Perspective","summary":"  Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. We focus on continual text\nclassification under the class-incremental setting. Recent CL studies have\nidentified the severe performance decrease on analogous classes as a key factor\nfor catastrophic forgetting. In this paper, through an in-depth exploration of\nthe representation learning process in CL, we discover that the compression\neffect of the information bottleneck leads to confusion on analogous classes.\nTo enable the model learn more sufficient representations, we propose a novel\nreplay-based continual text classification method, InfoCL. Our approach\nutilizes fast-slow and current-past contrastive learning to perform mutual\ninformation maximization and better recover the previously learned\nrepresentations. In addition, InfoCL incorporates an adversarial memory\naugmentation strategy to alleviate the overfitting problem of replay.\nExperimental results demonstrate that InfoCL effectively mitigates forgetting\nand achieves state-of-the-art performance on three text classification tasks.\nThe code is publicly available at https://github.com/Yifan-Song793/InfoCL.\n","authors":["Yifan Song","Peiyi Wang","Weimin Xiong","Dawei Zhu","Tianyu Liu","Zhifang Sui","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2310.06362v1.pdf","comment":"Findings of EMNLP 2023. An improved version of arXiv:2305.07289"},{"id":"http://arxiv.org/abs/2309.16583v2","updated":"2023-10-10T06:50:38Z","published":"2023-09-28T16:43:35Z","title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the\n  Evolutionary Path towards GPT-4 and Beyond","summary":"  With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n","authors":["Shen Zheng","Yuyu Zhang","Yijie Zhu","Chenguang Xi","Pengyang Gao","Xun Zhou","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2309.16583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06356v1","updated":"2023-10-10T06:49:43Z","published":"2023-10-10T06:49:43Z","title":"A Semantic Invariant Robust Watermark for Large Language Models","summary":"  Watermark algorithms for large language models (LLMs) have achieved extremely\nhigh accuracy in detecting text generated by LLMs. Such algorithms typically\ninvolve adding extra watermark logits to the LLM's logits at each generation\nstep. However, prior algorithms face a trade-off between attack robustness and\nsecurity robustness. This is because the watermark logits for a token are\ndetermined by a certain number of preceding tokens; a small number leads to low\nsecurity robustness, while a large number results in insufficient attack\nrobustness. In this work, we propose a semantic invariant watermarking method\nfor LLMs that provides both attack robustness and security robustness. The\nwatermark logits in our work are determined by the semantics of all preceding\ntokens. Specifically, we utilize another embedding LLM to generate semantic\nembeddings for all preceding tokens, and then these semantic embeddings are\ntransformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our\nmethod in semantically invariant settings: synonym substitution and text\nparaphrasing settings. Finally, we also show that our watermark possesses\nadequate security robustness. Our code and data are available at\nhttps://github.com/THU-BPM/Robust_Watermark.\n","authors":["Aiwei Liu","Leyi Pan","Xuming Hu","Shiao Meng","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2310.06356v1.pdf","comment":"16 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2309.15223v2","updated":"2023-10-10T06:30:47Z","published":"2023-09-26T19:41:34Z","title":"Low-rank Adaptation of Large Language Model Rescoring for\n  Parameter-Efficient Speech Recognition","summary":"  We propose a neural language modeling system based on low-rank adaptation\n(LoRA) for speech recognition output rescoring. Although pretrained language\nmodels (LMs) like BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pretraining stage and\nadapting the pretrained models to specific domains limit their practical use in\nrescoring. Here we present a method based on low-rank decomposition to train a\nrescoring BERT model and adapt it to new domains using only a fraction (0.08%)\nof the pretrained parameters. These inserted matrices are optimized through a\ndiscriminative training objective along with a correlation-based regularization\nloss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is\nevaluated on LibriSpeech and internal datasets with decreased training times by\nfactors between 5.4 and 3.6.\n","authors":["Yu Yu","Chao-Han Huck Yang","Jari Kolehmainen","Prashanth G. Shivakumar","Yile Gu","Sungho Ryu","Roger Ren","Qi Luo","Aditya Gourav","I-Fan Chen","Yi-Chieh Liu","Tuan Dinh","Ankur Gandhe","Denis Filimonov","Shalini Ghosh","Andreas Stolcke","Ariya Rastow","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2309.15223v2.pdf","comment":"Accepted to IEEE ASRU 2023. Internal Review Approved. Revised 2nd\n  version with Andreas and Huck. The first version is in Sep 29th. 8 pages"},{"id":"http://arxiv.org/abs/2310.01444v2","updated":"2023-10-10T05:38:05Z","published":"2023-10-01T07:50:30Z","title":"Adapting LLM Agents Through Communication","summary":"  Recent advancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Recent\nadvancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Through\niterative exploration and PPO training, LTC empowers the agent to assimilate\nshort-term experiences into long-term memory. To optimize agent interactions\nfor task-specific learning, we introduce three structured communication\npatterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as\ndecision-making, knowledge-intensive reasoning, and numerical reasoning. We\nevaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA\n(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,\nit exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,\nLTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it\noutperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,\nLTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results\nshowcase the versatility and efficiency of the LTC approach across diverse\ndomains. We will open-source our code to promote further development of the\ncommunity.\n","authors":["Kuan Wang","Yadong Lu","Michael Santacroce","Yeyun Gong","Chao Zhang","Yelong Shen"],"pdf_url":"https://arxiv.org/pdf/2310.01444v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.11366v4","updated":"2023-10-10T05:21:45Z","published":"2023-03-20T18:08:50Z","title":"Reflexion: Language Agents with Verbal Reinforcement Learning","summary":"  Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.\n","authors":["Noah Shinn","Federico Cassano","Edward Berman","Ashwin Gopinath","Karthik Narasimhan","Shunyu Yao"],"pdf_url":"https://arxiv.org/pdf/2303.11366v4.pdf","comment":"v4 contains a few additional experiments"},{"id":"http://arxiv.org/abs/2305.19011v2","updated":"2023-10-10T05:17:01Z","published":"2023-05-30T13:07:33Z","title":"MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models","summary":"  SUPERB was proposed to evaluate the generalizability of self-supervised\nlearning (SSL) speech models across various tasks. However, it incurs high\ncomputational costs due to the large datasets and diverse tasks. In this paper,\nwe introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL\nspeech models with comparable results to SUPERB but lower computational costs\nsignificantly. We carefully select representative tasks, sample datasets, and\nextract model representations offline. Our approach achieves a Spearman's rank\ncorrelation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge,\nrespectively. Additionally, we reduce the computational cost by 97% in terms of\nMultiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech\nmodels in few-shot scenarios and observe significant variations in their\nperformance. To our knowledge, this is the first study to examine both the\ncomputational cost of the model itself and the cost of evaluating it on a\nbenchmark.\n","authors":["Yu-Hsiang Wang","Huang-Yu Chen","Kai-Wei Chang","Winston Hsu","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2305.19011v2.pdf","comment":"Accepted to IEEE ASRU 2023"},{"id":"http://arxiv.org/abs/2310.01271v2","updated":"2023-10-10T04:34:56Z","published":"2023-10-02T15:16:31Z","title":"LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System","summary":"  As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nfirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .\n","authors":["Xue Zongyue","Liu Huanghai","Hu Yiran","Kong Kangle","Wang Chenlu","Liu Yun","Shen Weixing"],"pdf_url":"https://arxiv.org/pdf/2310.01271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06302v1","updated":"2023-10-10T04:31:41Z","published":"2023-10-10T04:31:41Z","title":"Selective Demonstrations for Cross-domain Text-to-SQL","summary":"  Large language models (LLMs) with in-context learning have demonstrated\nimpressive generalization capabilities in the cross-domain text-to-SQL task,\nwithout the use of in-domain annotations. However, incorporating in-domain\ndemonstration examples has been found to greatly enhance LLMs' performance. In\nthis paper, we delve into the key factors within in-domain examples that\ncontribute to the improvement and explore whether we can harness these benefits\nwithout relying on in-domain annotations. Based on our findings, we propose a\ndemonstration selection framework ODIS which utilizes both out-of-domain\nexamples and synthetically generated in-domain examples to construct\ndemonstrations. By retrieving demonstrations from hybrid sources, ODIS\nleverages the advantages of both, showcasing its effectiveness compared to\nbaseline methods that rely on a single data source. Furthermore, ODIS\noutperforms state-of-the-art approaches on two cross-domain text-to-SQL\ndatasets, with improvements of 1.1 and 11.8 points in execution accuracy,\nrespectively.\n","authors":["Shuaichen Chang","Eric Fosler-Lussier"],"pdf_url":"https://arxiv.org/pdf/2310.06302v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.04678v2","updated":"2023-10-10T04:13:36Z","published":"2023-10-07T03:25:06Z","title":"DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based\n  Queries","summary":"  In scientific research, the ability to effectively retrieve relevant\ndocuments based on complex, multifaceted queries is critical. Existing\nevaluation datasets for this task are limited, primarily due to the high cost\nand effort required to annotate resources that effectively represent complex\nqueries. To address this, we propose a novel task, Scientific DOcument\nRetrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed\nto handle the complex nature of user queries in scientific research. We\ndeveloped a benchmark dataset within the field of computer science, consisting\nof 100 human-authored complex query cases. For each complex query, we assembled\na collection of 100 relevant documents and produced annotated relevance scores\nfor ranking them. Recognizing the significant labor of expert annotation, we\nalso introduce Anno-GPT, a scalable framework for validating the performance of\nLarge Language Models (LLMs) on expert-level dataset annotation tasks. LLM\nannotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,\nwithout compromising quality. Furthermore, due to the multi-tiered structure of\nthese complex queries, the DORIS-MAE dataset can be extended to over 4,000\nsub-query test cases without requiring additional annotation. We evaluated 17\nrecent retrieval methods on DORIS-MAE, observing notable performance drops\ncompared to traditional datasets. This highlights the need for better\napproaches to handle complex, multifaceted queries in scientific research. Our\ndataset and codebase are available at\nhttps://github.com/Real-Doris-Mae/Doris-Mae-Dataset.\n","authors":["Jianyou Wang","Kaicheng Wang","Xiaoyue Wang","Prudhviraj Naidu","Leon Bergen","Ramamohan Paturi"],"pdf_url":"https://arxiv.org/pdf/2310.04678v2.pdf","comment":"To appear in NeurIPS 2023 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2305.11186v2","updated":"2023-10-10T04:01:30Z","published":"2023-05-17T20:45:13Z","title":"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM\n  Inference with Transferable Prompt","summary":"  While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.\n","authors":["Zhaozhuo Xu","Zirui Liu","Beidi Chen","Yuxin Tang","Jue Wang","Kaixiong Zhou","Xia Hu","Anshumali Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2305.11186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11940v2","updated":"2023-10-10T03:35:35Z","published":"2023-08-23T06:21:46Z","title":"Audio Generation with Multiple Conditional Diffusion Model","summary":"  Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/\n","authors":["Zhifang Guo","Jianguo Mao","Rui Tao","Long Yan","Kazushige Ouchi","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11940v2.pdf","comment":"Submitted to AAAI 2024"},{"id":"http://arxiv.org/abs/2310.05189v2","updated":"2023-10-10T03:34:46Z","published":"2023-10-08T14:55:02Z","title":"Factuality Challenges in the Era of Large Language Models","summary":"  The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.\n","authors":["Isabelle Augenstein","Timothy Baldwin","Meeyoung Cha","Tanmoy Chakraborty","Giovanni Luca Ciampaglia","David Corney","Renee DiResta","Emilio Ferrara","Scott Hale","Alon Halevy","Eduard Hovy","Heng Ji","Filippo Menczer","Ruben Miguez","Preslav Nakov","Dietram Scheufele","Shivam Sharma","Giovanni Zagni"],"pdf_url":"https://arxiv.org/pdf/2310.05189v2.pdf","comment":"Our article offers a comprehensive examination of the challenges and\n  risks associated with Large Language Models (LLMs), focusing on their\n  potential impact on the veracity of information in today's digital landscape"},{"id":"http://arxiv.org/abs/2310.04270v2","updated":"2023-10-10T03:26:16Z","published":"2023-10-06T14:16:28Z","title":"A Comprehensive Evaluation of Large Language Models on Benchmark\n  Biomedical Text Processing Tasks","summary":"  Recently, Large Language Models (LLM) have demonstrated impressive capability\nto solve a wide range of tasks. However, despite their success across various\ntasks, no prior work has investigated their capability in the biomedical domain\nyet. To this end, this paper aims to evaluate the performance of LLMs on\nbenchmark biomedical tasks. For this purpose, we conduct a comprehensive\nevaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.\nTo the best of our knowledge, this is the first work that conducts an extensive\nevaluation and comparison of various LLMs in the biomedical domain.\nInterestingly, we find based on our evaluation that in biomedical datasets that\nhave smaller training sets, zero-shot LLMs even outperform the current\nstate-of-the-art fine-tuned biomedical models. This suggests that pretraining\non large text corpora makes LLMs quite specialized even in the biomedical\ndomain. We also find that not a single LLM can outperform other LLMs in all\ntasks, with the performance of different LLMs may vary depending on the task.\nWhile their performance is still quite poor in comparison to the biomedical\nmodels that were fine-tuned on large training sets, our findings demonstrate\nthat LLMs have the potential to be a valuable tool for various biomedical tasks\nthat lack large annotated data.\n","authors":["Israt Jahan","Md Tahmid Rahman Laskar","Chun Peng","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2310.04270v2.pdf","comment":"Extended version of the following BioNLP paper:\n  https://aclanthology.org/2023.bionlp-1.30/ (arXiv:2306.04504). arXiv admin\n  note: substantial text overlap with arXiv:2306.04504"},{"id":"http://arxiv.org/abs/2310.05036v2","updated":"2023-10-10T03:26:15Z","published":"2023-10-08T06:37:08Z","title":"From Text to Tactic: Evaluating LLMs Playing the Game of Avalon","summary":"  In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.\n","authors":["Jonathan Light","Min Cai","Sheng Shen","Ziniu Hu"],"pdf_url":"https://arxiv.org/pdf/2310.05036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01498v3","updated":"2023-10-10T03:19:16Z","published":"2023-05-02T15:18:18Z","title":"Summarizing Multiple Documents with Conversational Structure for\n  Meta-Review Generation","summary":"  We present PeerSum, a novel dataset for generating meta-reviews of scientific\npapers. The meta-reviews can be interpreted as abstractive summaries of\nreviews, multi-turn discussions and the paper abstract. These source documents\nhave rich inter-document relationships with an explicit hierarchical\nconversational structure, cross-references and (occasionally) conflicting\ninformation. To introduce the structural inductive bias into pre-trained\nlanguage models, we introduce Rammer ( Relationship-aware Multi-task\nMeta-review Generator), a model that uses sparse attention based on the\nconversational structure and a multi-task training objective that predicts\nmetadata features (e.g., review ratings). Our experimental results show that\nRammer outperforms other strong baseline models in terms of a suite of\nautomatic evaluation metrics. Further analyses, however, reveal that RAMMER and\nother models struggle to handle conflicts in source documents of PeerSum,\nsuggesting meta-review generation is a challenging task and a promising avenue\nfor further research.\n","authors":["Miao Li","Eduard Hovy","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2305.01498v3.pdf","comment":"Long paper; Accepted to EMNLP 2023; Soundness: 3, 3, 4; Excitement:\n  3, 4, 4"},{"id":"http://arxiv.org/abs/2309.09507v2","updated":"2023-10-10T03:13:38Z","published":"2023-09-18T06:38:24Z","title":"Pruning Large Language Models via Accuracy Predictor","summary":"  Large language models(LLMs) containing tens of billions of parameters (or\neven more) have demonstrated impressive capabilities in various NLP tasks.\nHowever, substantial model size poses challenges to training, inference, and\ndeployment so that it is necessary to compress the model. At present, most\nmodel compression for LLMs requires manual design of pruning features, which\nhas problems such as complex optimization pipeline and difficulty in retaining\nthe capabilities of certain parts of the model.Therefore, we propose a novel\npruning approach: firstly, a training set of a certain number of\narchitecture-accuracy pairs is established, and then a non-neural model is\ntrained as an accuracy predictor. Using the accuracy predictor to further\noptimize the search space and search, the optimal model can be automatically\nselected. Experiments show that our proposed approach is effective and\nefficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB\ndropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU\nincreased by 6.28%.\n","authors":["Yupeng Ji","Yibo Cao","Jiucai Liu"],"pdf_url":"https://arxiv.org/pdf/2309.09507v2.pdf","comment":"6 pages, 4 figs, submitted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05074v2","updated":"2023-10-10T03:09:31Z","published":"2023-10-08T08:52:13Z","title":"DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller\n  Language Models","summary":"  Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.\n","authors":["Chengcheng Han","Xiaowei Du","Che Zhang","Yixin Lian","Xiang Li","Ming Gao","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05074v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06272v1","updated":"2023-10-10T03:06:38Z","published":"2023-10-10T03:06:38Z","title":"Let Models Speak Ciphers: Multiagent Debate through Embeddings","summary":"  Discussion and debate among Large Language Models (LLMs) have gained\nconsiderable attention due to their potential to enhance the reasoning ability\nof LLMs. Although natural language is an obvious choice for communication due\nto LLM's language understanding capability, the token sampling step needed when\ngenerating natural language poses a potential risk of information loss, as it\nuses only one token to represent the model's belief across the entire\nvocabulary. In this paper, we introduce a communication regime named CIPHER\n(Communicative Inter-Model Protocol Through Embedding Representation) to\naddress this issue. Specifically, we remove the token sampling step from LLMs\nand let them communicate their beliefs across the vocabulary through the\nexpectation of the raw transformer output embeddings. Remarkably, by deviating\nfrom natural language, CIPHER offers an advantage of encoding a broader\nspectrum of information without any modification to the model weights. While\nthe state-of-the-art LLM debate methods using natural language outperforms\ntraditional inference by a margin of 1.5-8%, our experiment results show that\nCIPHER debate further extends this lead by 1-3.5% across five reasoning tasks\nand multiple open-source LLMs of varying sizes. This showcases the superiority\nand robustness of embeddings as an alternative \"language\" for communication\namong LLMs.\n","authors":["Chau Pham","Boyi Liu","Yingxiang Yang","Zhengyu Chen","Tianyi Liu","Jianbo Yuan","Bryan A. Plummer","Zhaoran Wang","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06271v1","updated":"2023-10-10T03:05:44Z","published":"2023-10-10T03:05:44Z","title":"Towards Mitigating Hallucination in Large Language Models via\n  Self-Reflection","summary":"  Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.\n","authors":["Ziwei Ji","Tiezheng Yu","Yan Xu","Nayeon Lee","Etsuko Ishii","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2310.06271v1.pdf","comment":"Accepted by the findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05317v2","updated":"2023-10-10T03:04:18Z","published":"2023-10-09T00:20:59Z","title":"Enhancing Long-form Text Generation in Mental Health with Task-adaptive\n  Tokenization","summary":"  We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.\n","authors":["Siyang Liu","Naihao Deng","Sahand Sabour","Yilin Jia","Minlie Huang","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2310.05317v2.pdf","comment":"Accepted at the main conference of The 2023 Conference on Empirical\n  Methods in Natural Language Processing; 8 pages"},{"id":"http://arxiv.org/abs/2212.07220v2","updated":"2023-10-10T03:00:35Z","published":"2022-12-14T13:41:49Z","title":"Understanding Translationese in Cross-Lingual Summarization","summary":"  Given a document in a source language, cross-lingual summarization (CLS) aims\nat generating a concise summary in a different target language. Unlike\nmonolingual summarization (MS), naturally occurring source-language documents\npaired with target-language summaries are rare. To collect large-scale CLS\ndata, existing datasets typically involve translation in their creation.\nHowever, the translated text is distinguished from the text originally written\nin that language, i.e., translationese. In this paper, we first confirm that\ndifferent approaches of constructing CLS datasets will lead to different\ndegrees of translationese. Then we systematically investigate how\ntranslationese affects CLS model evaluation and performance when it appears in\nsource documents or target summaries. In detail, we find that (1) the\ntranslationese in documents or summaries of test sets might lead to the\ndiscrepancy between human judgment and automatic evaluation; (2) the\ntranslationese in training sets would harm model performance in real-world\napplications; (3) though machine-translated documents involve translationese,\nthey are very useful for building CLS systems on low-resource languages under\nspecific training strategies. Lastly, we give suggestions for future CLS\nresearch including dataset and model developments. We hope that our work could\nlet researchers notice the phenomenon of translationese in CLS and take it into\naccount in the future.\n","authors":["Jiaan Wang","Fandong Meng","Yunlong Liang","Tingyi Zhang","Jiarong Xu","Zhixu Li","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07220v2.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.00212v3","updated":"2023-10-10T02:32:08Z","published":"2023-09-30T01:23:22Z","title":"Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for\n  LLM Alignment","summary":"  Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.\n","authors":["Tianhao Wu","Banghua Zhu","Ruoyu Zhang","Zhaojin Wen","Kannan Ramchandran","Jiantao Jiao"],"pdf_url":"https://arxiv.org/pdf/2310.00212v3.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05364v2","updated":"2023-10-10T02:30:12Z","published":"2023-10-09T02:50:54Z","title":"Universal Multi-modal Entity Alignment via Iteratively Fusing Modality\n  Similarity Paths","summary":"  The objective of Entity Alignment (EA) is to identify equivalent entity pairs\nfrom multiple Knowledge Graphs (KGs) and create a more comprehensive and\nunified KG. The majority of EA methods have primarily focused on the structural\nmodality of KGs, lacking exploration of multi-modal information. A few\nmulti-modal EA methods have made good attempts in this field. Still, they have\ntwo shortcomings: (1) inconsistent and inefficient modality modeling that\ndesigns complex and distinct models for each modality; (2) ineffective modality\nfusion due to the heterogeneous nature of modalities in EA. To tackle these\nchallenges, we propose PathFusion, consisting of two main components: (1) MSP,\na unified modeling approach that simplifies the alignment process by\nconstructing paths connecting entities and modality nodes to represent multiple\nmodalities; (2) IRF, an iterative fusion method that effectively combines\ninformation from different modalities using the path as an information carrier.\nExperimental results on real-world datasets demonstrate the superiority of\nPathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement\non Hits@1, and 0.194-0.245 absolute improvement on MRR.\n","authors":["Bolin Zhu","Xiaoze Liu","Xin Mao","Zhuo Chen","Lingbing Guo","Tao Gui","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06260v1","updated":"2023-10-10T02:07:24Z","published":"2023-10-10T02:07:24Z","title":"An experiment on an automated literature survey of data-driven speech\n  enhancement methods","summary":"  The increasing number of scientific publications in acoustics, in general,\npresents difficulties in conducting traditional literature surveys. This work\nexplores the use of a generative pre-trained transformer (GPT) model to\nautomate a literature survey of 116 articles on data-driven speech enhancement\nmethods. The main objective is to evaluate the capabilities and limitations of\nthe model in providing accurate responses to specific queries about the papers\nselected from a reference human-based survey. While we see great potential to\nautomate literature surveys in acoustics, improvements are needed to address\ntechnical questions more clearly and accurately.\n","authors":["Arthur dos Santos","Jayr Pereira","Rodrigo Nogueira","Bruno Masiero","Shiva Sander-Tavallaey","Elias Zea"],"pdf_url":"https://arxiv.org/pdf/2310.06260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06254v1","updated":"2023-10-10T02:00:00Z","published":"2023-10-10T02:00:00Z","title":"Get the gist? Using large language models for few-shot\n  decontextualization","summary":"  In many NLP applications that involve interpreting sentences within a rich\ncontext -- for instance, information retrieval systems or dialogue systems --\nit is desirable to be able to preserve the sentence in a form that can be\nreadily understood without context, for later reuse -- a process known as\n``decontextualization''. While previous work demonstrated that generative\nSeq2Seq models could effectively perform decontextualization after being\nfine-tuned on a specific dataset, this approach requires expensive human\nannotations and may not transfer to other domains. We propose a few-shot method\nof decontextualization using a large language model, and present preliminary\nresults showing that this method achieves viable performance on multiple\ndomains using only a small set of examples.\n","authors":["Benjamin Kane","Lenhart Schubert"],"pdf_url":"https://arxiv.org/pdf/2310.06254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06245v1","updated":"2023-10-10T01:44:47Z","published":"2023-10-10T01:44:47Z","title":"We are what we repeatedly do: Inducing and deploying habitual schemas in\n  persona-based responses","summary":"  Many practical applications of dialogue technology require the generation of\nresponses according to a particular developer-specified persona. While a\nvariety of personas can be elicited from recent large language models, the\nopaqueness and unpredictability of these models make it desirable to be able to\nspecify personas in an explicit form. In previous work, personas have typically\nbeen represented as sets of one-off pieces of self-knowledge that are retrieved\nby the dialogue system for use in generation. However, in realistic human\nconversations, personas are often revealed through story-like narratives that\ninvolve rich habitual knowledge -- knowledge about kinds of events that an\nagent often participates in (e.g., work activities, hobbies, sporting\nactivities, favorite entertainments, etc.), including typical goals,\nsub-events, preconditions, and postconditions of those events. We capture such\nhabitual knowledge using an explicit schema representation, and propose an\napproach to dialogue generation that retrieves relevant schemas to condition a\nlarge language model to generate persona-based responses. Furthermore, we\ndemonstrate a method for bootstrapping the creation of such schemas by first\ngenerating generic passages from a set of simple facts, and then inducing\nschemas from the generated passages.\n","authors":["Benjamin Kane","Lenhart Schubert"],"pdf_url":"https://arxiv.org/pdf/2310.06245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06239v1","updated":"2023-10-10T01:27:08Z","published":"2023-10-10T01:27:08Z","title":"Model Tuning or Prompt Tuning? A Study of Large Language Models for\n  Clinical Concept and Relation Extraction","summary":"  Objective To develop soft prompt-based learning algorithms for large language\nmodels (LLMs), examine the shape of prompts, prompt-tuning using\nfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.\nMethods We developed a soft prompt-based LLM model and compared 4 training\nstrategies including (1) fine-tuning without prompts; (2) hard-prompt with\nunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with\nfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for\nclinical concept and relation extraction on two benchmark datasets. We\nevaluated the transfer learning ability of the prompt-based learning algorithms\nin a cross-institution setting. We also assessed the few-shot learning ability.\nResults and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft\nprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept\nextraction, outperforming the traditional fine-tuning and hard prompt-based\nmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft\nprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end\nrelation extraction, outperforming the other two models by 0.2~2% and\n0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million\nparameters) LLMs have a big gap to be competitive with unfrozen models; scaling\nLLMs up to billions of parameters makes frozen LLMs competitive with unfrozen\nLLMs. For cross-institute evaluation, soft prompting with a frozen\nGatorTron-8.9B model achieved the best performance. This study demonstrates\nthat (1) machines can learn soft prompts better than humans, (2) frozen LLMs\nhave better few-shot learning ability and transfer learning ability to\nfacilitate muti-institution applications, and (3) frozen LLMs require large\nmodels.\n","authors":["Cheng Peng","Xi Yang","Kaleb E Smith","Zehao Yu","Aokun Chen","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2310.06239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00297v2","updated":"2023-10-10T01:22:47Z","published":"2023-09-30T08:13:49Z","title":"Understanding In-Context Learning from Repetitions","summary":"  This paper explores the elusive mechanism underpinning in-context learning in\nLarge Language Models (LLMs). Our work provides a novel perspective by\nexamining in-context learning via the lens of surface repetitions. We\nquantitatively investigate the role of surface features in text generation, and\nempirically establish the existence of \\emph{token co-occurrence\nreinforcement}, a principle that strengthens the relationship between two\ntokens based on their contextual co-occurrences. By investigating the dual\nimpacts of these features, our research illuminates the internal workings of\nin-context learning and expounds on the reasons for its failures. This paper\nprovides an essential contribution to the understanding of in-context learning\nand its potential limitations, providing a fresh perspective on this exciting\ncapability.\n","authors":["Jianhao Yan","Jin Xu","Chiyu Song","Chenming Wu","Yafu Li","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.00297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06238v1","updated":"2023-10-10T01:22:41Z","published":"2023-10-10T01:22:41Z","title":"Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for\n  Unbiased Question-Answering","summary":"  In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.\n","authors":["Xiulong Liu","Zhikang Dong","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05242v2","updated":"2023-10-10T01:22:16Z","published":"2023-10-08T17:23:17Z","title":"ChatRadio-Valuer: A Chat Large Language Model for Generalizable\n  Radiology Report Generation Based on Multi-institution and Multi-system Data","summary":"  Radiology report generation, as a key step in medical image analysis, is\ncritical to the quantitative analysis of clinically informed decision-making\nlevels. However, complex and diverse radiology reports with cross-source\nheterogeneity pose a huge generalizability challenge to the current methods\nunder massive data volume, mainly because the style and normativity of\nradiology reports are obviously distinctive among institutions, body regions\ninspected and radiologists. Recently, the advent of large language models (LLM)\noffers great potential for recognizing signs of health conditions. To resolve\nthe above problem, we collaborate with the Second Xiangya Hospital in China and\npropose ChatRadio-Valuer based on the LLM, a tailored model for automatic\nradiology report generation that learns generalizable representations and\nprovides a basis pattern for model adaptation in sophisticated analysts' cases.\nSpecifically, ChatRadio-Valuer is trained based on the radiology reports from a\nsingle institution by means of supervised fine-tuning, and then adapted to\ndisease diagnosis tasks for human multi-system evaluation (i.e., chest,\nabdomen, muscle-skeleton, head, and maxillofacial $\\&$ neck) from six different\ninstitutions in clinical-level events. The clinical dataset utilized in this\nstudy encompasses a remarkable total of \\textbf{332,673} observations. From the\ncomprehensive results on engineering indicators, clinical efficacy and\ndeployment cost metrics, it can be shown that ChatRadio-Valuer consistently\noutperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and\nGPT-4 et al., in terms of the diseases diagnosis from radiology reports.\nChatRadio-Valuer provides an effective avenue to boost model generalization\nperformance and alleviate the annotation workload of experts to enable the\npromotion of clinical AI applications in radiology reports.\n","authors":["Tianyang Zhong","Wei Zhao","Yutong Zhang","Yi Pan","Peixin Dong","Zuowei Jiang","Xiaoyan Kui","Youlan Shang","Li Yang","Yaonai Wei","Longtao Yang","Hao Chen","Huan Zhao","Yuxiao Liu","Ning Zhu","Yiwei Li","Yisong Wang","Jiaqi Yao","Jiaqi Wang","Ying Zeng","Lei He","Chao Zheng","Zhixue Zhang","Ming Li","Zhengliang Liu","Haixing Dai","Zihao Wu","Lu Zhang","Shu Zhang","Xiaoyan Cai","Xintao Hu","Shijie Zhao","Xi Jiang","Xin Zhang","Xiang Li","Dajiang Zhu","Lei Guo","Dinggang Shen","Junwei Han","Tianming Liu","Jun Liu","Tuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06228v1","updated":"2023-10-10T00:41:38Z","published":"2023-10-10T00:41:38Z","title":"Evolution of Natural Language Processing Technology: Not Just Language\n  Processing Towards General Purpose AI","summary":"  Since the invention of computers, communication through natural language\n(actual human language) has been a dream technology. However, natural language\nis extremely difficult to mathematically formulate, making it difficult to\nrealize as an algorithm without considering programming. While there have been\nnumerous technological developments, one cannot say that any results allowing\nfree utilization have been achieved thus far. In the case of language learning\nin humans, for instance when learning one's mother tongue or foreign language,\none must admit that this process is similar to the adage \"practice makes\nperfect\" in principle, even though the learning method is significant up to a\npoint. Deep learning has played a central role in contemporary AI technology in\nrecent years. When applied to natural language processing (NLP), this produced\nunprecedented results. Achievements exceeding the initial predictions have been\nreported from the results of learning vast amounts of textual data using deep\nlearning. For instance, four arithmetic operations could be performed without\nexplicit learning, thereby enabling the explanation of complex images and the\ngeneration of images from corresponding explanatory texts. It is an accurate\nexample of the learner embodying the concept of \"practice makes perfect\" by\nusing vast amounts of textual data. This report provides a technological\nexplanation of how cutting-edge NLP has made it possible to realize the\n\"practice makes perfect\" principle. Additionally, examples of how this can be\napplied to business are provided. We reported in June 2022 in Japanese on the\nNLP movement from late 2021 to early 2022. We would like to summarize this as a\nmemorandum since this is just the initial movement leading to the current large\nlanguage models (LLMs).\n","authors":["Masahiro Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2310.06228v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2310.06213v1","updated":"2023-10-10T00:03:23Z","published":"2023-10-10T00:03:23Z","title":"GeoLLM: Extracting Geospatial Knowledge from Large Language Models","summary":"  The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well.\n","authors":["Rohin Manvi","Samar Khanna","Gengchen Mai","Marshall Burke","David Lobell","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2310.06213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07081v1","updated":"2023-10-10T23:47:25Z","published":"2023-10-10T23:47:25Z","title":"Crossing the Threshold: Idiomatic Machine Translation through Retrieval\n  Augmentation and Loss Weighting","summary":"  Idioms are common in everyday language, but often pose a challenge to\ntranslators because their meanings do not follow from the meanings of their\nparts. Despite significant advances, machine translation systems still struggle\nto translate idiomatic expressions. We provide a simple characterization of\nidiomatic translation and related issues. This allows us to conduct a synthetic\nexperiment revealing a tipping point at which transformer-based machine\ntranslation models correctly default to idiomatic translations. To expand\nmultilingual resources, we compile a dataset of ~4k natural sentences\ncontaining idiomatic expressions in French, Finnish, and Japanese. To improve\ntranslation of natural idioms, we introduce two straightforward yet effective\ntechniques: the strategic upweighting of training loss on potentially idiomatic\nsentences, and using retrieval-augmented models. This not only improves the\naccuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in\nabsolute accuracy, but also holds potential benefits for non-idiomatic\nsentences.\n","authors":["Emmy Liu","Aditi Chaudhary","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2310.07081v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.14761v3","updated":"2023-10-10T23:39:25Z","published":"2023-05-24T06:11:17Z","title":"UniChart: A Universal Vision-language Pretrained Model for Chart\n  Comprehension and Reasoning","summary":"  Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n","authors":["Ahmed Masry","Parsa Kavehzadeh","Xuan Long Do","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2305.14761v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07075v1","updated":"2023-10-10T23:37:53Z","published":"2023-10-10T23:37:53Z","title":"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State\n  Decoding","summary":"  Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.\n","authors":["Kexun Zhang","Hongqiao Chen","Lei Li","William Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14718v3","updated":"2023-10-10T23:34:02Z","published":"2023-05-24T04:42:17Z","title":"Improving Language Models with Advantage-based Offline Policy Gradients","summary":"  Language Models (LMs) achieve substantial language capabilities when\nfinetuned using Reinforcement Learning with Human Feedback (RLHF). However,\nRLHF is an unstable and data-hungry process that continually requires new\nhigh-quality LM-generated data for finetuning. We introduce Advantage-Leftover\nLunch RL (A-LoL), a new class of offline policy gradient algorithms that enable\nRL training on any pre-existing data. By assuming the entire LM output sequence\nas a single action, A-LoL allows incorporating sequence-level classifiers or\nhuman-designed scoring functions as rewards. Subsequently, by using LM's\ninternal sequence-level value estimate, A-LoL filters negative advantage\n(low-quality) data points during training, making it resilient to noise.\nOverall, A-LoL is an easy-to-implement LM training recipe that is\nsample-efficient and stable.\n  We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data. We\nalso release our experimental code. https://github.com/abaheti95/LoL-RL\n","authors":["Ashutosh Baheti","Ximing Lu","Faeze Brahman","Ronan Le Bras","Maarten Sap","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2305.14718v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07064v1","updated":"2023-10-10T23:07:01Z","published":"2023-10-10T23:07:01Z","title":"Large Language Models can Learn Rules","summary":"  When prompted with a few examples and intermediate steps, large language\nmodels (LLMs) have demonstrated impressive performance in various reasoning\ntasks. However, prompting methods that rely on implicit knowledge in an LLM\noften hallucinate incorrect answers when the implicit knowledge is wrong or\ninconsistent with the task. To tackle this problem, we present\nHypotheses-to-Theories (HtT), a framework that learns a rule library for\nreasoning with LLMs. HtT contains two stages, an induction stage and a\ndeduction stage. In the induction stage, an LLM is first asked to generate and\nverify rules over a set of training examples. Rules that appear and lead to\ncorrect answers sufficiently often are collected to form a rule library. In the\ndeduction stage, the LLM is then prompted to employ the learned rule library to\nperform reasoning to answer test questions. Experiments on both numerical\nreasoning and relational reasoning problems show that HtT improves existing\nprompting methods, with an absolute gain of 11-27% in accuracy. The learned\nrules are also transferable to different models and to different forms of the\nsame problem.\n","authors":["Zhaocheng Zhu","Yuan Xue","Xinyun Chen","Denny Zhou","Jian Tang","Dale Schuurmans","Hanjun Dai"],"pdf_url":"https://arxiv.org/pdf/2310.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07059v1","updated":"2023-10-10T22:53:15Z","published":"2023-10-10T22:53:15Z","title":"DKEC: Domain Knowledge Enhanced Multi-Label Classification for\n  Electronic Health Records","summary":"  Multi-label text classification (MLTC) tasks in the medical domain often face\nlong-tail label distribution, where rare classes have fewer training samples\nthan frequent classes. Although previous works have explored different model\narchitectures and hierarchical label structures to find important features,\nmost of them neglect to incorporate the domain knowledge from medical\nguidelines. In this paper, we present DKEC, Domain Knowledge Enhanced\nClassifier for medical diagnosis prediction with two innovations: (1) a\nlabel-wise attention mechanism that incorporates a heterogeneous graph and\ndomain ontologies to capture the semantic relationships between medical\nentities, (2) a simple yet effective group-wise training method based on\nsimilarity of labels to increase samples of rare classes. We evaluate DKEC on\ntwo real-world medical datasets: the RAA dataset, a collection of 4,417 patient\ncare reports from emergency medical services (EMS) incidents, and a subset of\n53,898 reports from the MIMIC-III dataset. Experimental results show that our\nmethod outperforms the state-of-the-art, particularly for the few-shot (tail)\nclasses. More importantly, we study the applicability of DKEC to different\nlanguage models and show that DKEC can help the smaller language models achieve\ncomparable performance to large language models.\n","authors":["Xueren Ge","Ronald Dean Williams","John A. Stankovic","Homa Alemzadeh"],"pdf_url":"https://arxiv.org/pdf/2310.07059v1.pdf","comment":"Submitted to AAAI 2024"},{"id":"http://arxiv.org/abs/2307.14539v2","updated":"2023-10-10T22:17:17Z","published":"2023-07-26T23:11:15Z","title":"Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal\n  Language Models","summary":"  We introduce new jailbreak attacks on vision language models (VLMs), which\nuse aligned LLMs and are resilient to text-only jailbreak attacks.\nSpecifically, we develop cross-modality attacks on alignment where we pair\nadversarial images going through the vision encoder with textual prompts to\nbreak the alignment of the language model. Our attacks employ a novel\ncompositional strategy that combines an image, adversarially targeted towards\ntoxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the\nLLM draws the context to answer the generic prompt from the adversarial image.\nThe generation of benign-appearing adversarial images leverages a novel\nembedding-space-based methodology, operating with no access to the LLM model.\nInstead, the attacks require access only to the vision encoder and utilize one\nof our four embedding space targeting strategies. By not requiring access to\nthe LLM, the attacks lower the entry barrier for attackers, particularly when\nvision encoders such as CLIP are embedded in closed-source LLMs. The attacks\nachieve a high success rate across different VLMs, highlighting the risk of\ncross-modality alignment vulnerabilities, and the need for new alignment\napproaches for multi-modal models.\n","authors":["Erfan Shayegani","Yue Dong","Nael Abu-Ghazaleh"],"pdf_url":"https://arxiv.org/pdf/2307.14539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07023v1","updated":"2023-10-10T21:23:47Z","published":"2023-10-10T21:23:47Z","title":"Automatic Macro Mining from Interaction Traces at Scale","summary":"  Macros are building block tasks of our everyday smartphone activity (e.g.,\n\"login\", or \"booking a flight\"). Effectively extracting macros is important for\nunderstanding mobile interaction and enabling task automation. These macros are\nhowever difficult to extract at scale as they can be comprised of multiple\nsteps yet hidden within programmatic components of the app. In this paper, we\nintroduce a novel approach based on Large Language Models (LLMs) to\nautomatically extract semantically meaningful macros from both random and\nuser-curated mobile interaction traces. The macros produced by our approach are\nautomatically tagged with natural language descriptions and are fully\nexecutable. To examine the quality of extraction, we conduct multiple studies,\nincluding user evaluation, comparative analysis against human-curated tasks,\nand automatic execution of these macros. These experiments and analyses show\nthe effectiveness of our approach and the usefulness of extracted macros in\nvarious downstream applications.\n","authors":["Forrest Huang","Gang Li","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07018v1","updated":"2023-10-10T21:08:51Z","published":"2023-10-10T21:08:51Z","title":"NEWTON: Are Large Language Models Capable of Physical Reasoning?","summary":"  Large Language Models (LLMs), through their contextualized representations,\nhave been empirically proven to encapsulate syntactic, semantic, word sense,\nand common-sense knowledge. However, there has been limited exploration of\ntheir physical reasoning abilities, specifically concerning the crucial\nattributes for comprehending everyday objects. To address this gap, we\nintroduce NEWTON, a repository and benchmark for evaluating the physics\nreasoning skills of LLMs. Further, to enable domain-specific adaptation of this\nbenchmark, we present a pipeline to enable researchers to generate a variant of\nthis benchmark that has been customized to the objects and attributes relevant\nfor their application. The NEWTON repository comprises a collection of 2800\nobject-attribute pairs, providing the foundation for generating infinite-scale\nassessment templates. The NEWTON benchmark consists of 160K QA questions,\ncurated using the NEWTON repository to investigate the physical reasoning\ncapabilities of several mainstream language models across foundational,\nexplicit, and implicit reasoning tasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs for physical reasoning. We find\nthat LLMs like GPT-4 demonstrate strong reasoning capabilities in\nscenario-based tasks but exhibit less consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing language models, paving the way for\ntheir integration into physically grounded settings, such as robotic\nmanipulation. Project site: https://newtonreasoning.github.io\n","authors":["Yi Ru Wang","Jiafei Duan","Dieter Fox","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2310.07018v1.pdf","comment":"EMNLP 2023 Findings; 8 pages, 3 figures, 7 tables; Project page:\n  https://newtonreasoning.github.io"},{"id":"http://arxiv.org/abs/2310.07008v1","updated":"2023-10-10T20:49:43Z","published":"2023-10-10T20:49:43Z","title":"Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs","summary":"  Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.\n","authors":["Mikhail Salnikov","Maria Lysyuk","Pavel Braslavski","Anton Razzhigaev","Valentin Malykh","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2310.07008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06987v1","updated":"2023-10-10T20:15:54Z","published":"2023-10-10T20:15:54Z","title":"Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation","summary":"  The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.\n","authors":["Yangsibo Huang","Samyak Gupta","Mengzhou Xia","Kai Li","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06983v1","updated":"2023-10-10T20:05:13Z","published":"2023-10-10T20:05:13Z","title":"Violation of Expectation via Metacognitive Prompting Reduces Theory of\n  Mind Prediction Error in Large Language Models","summary":"  Recent research shows that Large Language Models (LLMs) exhibit a compelling\nlevel of proficiency in Theory of Mind (ToM) tasks. This ability to impute\nunobservable mental states to others is vital to human social cognition and may\nprove equally important in principal-agent relations between individual humans\nand Artificial Intelligences (AIs). In this paper, we explore how a mechanism\nstudied in developmental psychology known as Violation of Expectation (VoE) can\nbe implemented to reduce errors in LLM prediction about users by leveraging\nemergent ToM affordances. And we introduce a \\textit{metacognitive prompting}\nframework to apply VoE in the context of an AI tutor. By storing and retrieving\nfacts derived in cases where LLM expectation about the user was violated, we\nfind that LLMs are able to learn about users in ways that echo theories of\nhuman learning. Finally, we discuss latent hazards and augmentative\nopportunities associated with modeling user psychology and propose ways to\nmitigate risk along with possible directions for future inquiry.\n","authors":["Courtland Leer","Vincent Trost","Vineeth Voruganti"],"pdf_url":"https://arxiv.org/pdf/2310.06983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06977v1","updated":"2023-10-10T19:56:10Z","published":"2023-10-10T19:56:10Z","title":"Why bother with geometry? On the relevance of linear decompositions of\n  Transformer embeddings","summary":"  A recent body of work has demonstrated that Transformer embeddings can be\nlinearly decomposed into well-defined sums of factors, that can in turn be\nrelated to specific network inputs or components. There is however still a\ndearth of work studying whether these mathematical reformulations are\nempirically meaningful. In the present work, we study representations from\nmachine-translation decoders using two of such embedding decomposition methods.\nOur results indicate that, while decomposition-derived indicators effectively\ncorrelate with model performance, variation across different runs suggests a\nmore nuanced take on this question. The high variability of our measurements\nindicate that geometry reflects model-specific characteristics more than it\ndoes sentence-specific computations, and that similar training conditions do\nnot guarantee similar vector spaces.\n","authors":["Timothee Mickus","Ral Vzquez"],"pdf_url":"https://arxiv.org/pdf/2310.06977v1.pdf","comment":"Accepted to BlackBoxNLP 2023"},{"id":"http://arxiv.org/abs/2310.05797v2","updated":"2023-10-10T19:33:38Z","published":"2023-10-09T15:31:03Z","title":"Are Large Language Models Post Hoc Explainers?","summary":"  Large Language Models (LLMs) are increasingly used as powerful tools for a\nplethora of natural language processing (NLP) applications. A recent\ninnovation, in-context learning (ICL), enables LLMs to learn new tasks by\nsupplying a few examples in the prompt during inference time, thereby\neliminating the need for model fine-tuning. While LLMs have been utilized in\nseveral applications, their applicability in explaining the behavior of other\nmodels remains relatively unexplored. Despite the growing number of new\nexplanation techniques, many require white-box access to the model and/or are\ncomputationally expensive, highlighting a need for next-generation post hoc\nexplainers. In this work, we present the first framework to study the\neffectiveness of LLMs in explaining other predictive models. More specifically,\nwe propose a novel framework encompassing multiple prompting strategies: i)\nPerturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,\nand iv) Explanation-based ICL, with varying levels of information about the\nunderlying ML model and the local neighborhood of the test sample. We conduct\nextensive experiments with real-world benchmark datasets to demonstrate that\nLLM-generated explanations perform on par with state-of-the-art post hoc\nexplainers using their ability to leverage ICL examples and their internal\nknowledge in generating model explanations. On average, across four datasets\nand two ML models, we observe that LLMs identify the most important feature\nwith 72.19% accuracy, opening up new frontiers in explainable artificial\nintelligence (XAI) to explore LLM-based explanation frameworks.\n","authors":["Nicholas Kroeger","Dan Ley","Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.05797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05057v2","updated":"2023-10-10T19:30:31Z","published":"2023-10-08T07:46:01Z","title":"BRAINTEASER: Lateral Thinking Puzzles for Large Language Models","summary":"  The success of language models has inspired the NLP community to attend to\ntasks that require implicit and complex reasoning, relying on human-like\ncommonsense mechanisms. While such vertical thinking tasks have been relatively\npopular, lateral thinking puzzles have received little attention. To bridge\nthis gap, we devise BRAINTEASER: a multiple-choice Question Answering task\ndesigned to test the model's ability to exhibit lateral thinking and defy\ndefault commonsense associations. We design a three-step procedure for creating\nthe first lateral thinking benchmark, consisting of data collection, distractor\ngeneration, and generation of adversarial examples, leading to 1,100 puzzles\nwith high-quality annotations. To assess the consistency of lateral reasoning\nby models, we enrich BRAINTEASER based on a semantic and contextual\nreconstruction of its questions. Our experiments with state-of-the-art\ninstruction- and commonsense language models reveal a significant gap between\nhuman and model performance, which is further widened when consistency across\nadversarial formats is considered. We make all of our code and data available\nto stimulate work on developing and evaluating lateral thinking models.\n","authors":["Yifan Jiang","Filip Ilievski","Kaixin Ma","Zhivar Sourati"],"pdf_url":"https://arxiv.org/pdf/2310.05057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08614v8","updated":"2023-10-10T19:06:20Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over RDF data like knowledge graphs has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, the IR and NLP\ncommunities have addressed QA over text, but such systems barely utilize\nsemantic data and knowledge. This paper presents a method for complex questions\nthat can seamlessly operate over a mixture of RDF datasets and text corpora, or\nindividual sources, in a unified framework. Our method, called UNIQORN, builds\na context graph on-the-fly, by retrieving question-relevant evidences from the\nRDF data and/or a text corpus, using fine-tuned BERT models. The resulting\ngraph typically contains all question-relevant evidences but also a lot of\nnoise. UNIQORN copes with this input by a graph algorithm for Group Steiner\nTrees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA -- in a full training mode, as\nwell as in zero-shot settings. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v8.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2305.09900v2","updated":"2023-10-10T19:01:41Z","published":"2023-05-17T02:20:34Z","title":"Efficient Equivariant Transfer Learning from Pretrained Models","summary":"  Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of Basu\net al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and\noptimization-based methods, respectively, over features from group-transformed\ninputs to obtain equivariant outputs from non-equivariant neural networks.\nWhile Kaba et al. (2022) are only concerned with training from scratch, we find\nthat equitune performs poorly on equivariant zero-shot tasks despite good\nfinetuning results. We hypothesize that this is because pretrained models\nprovide better quality features for certain transformations than others and\nsimply averaging them is deleterious. Hence, we propose {\\lambda}-equitune that\naverages the features using importance weights, {\\lambda}s. These weights are\nlearned directly from the data using a small neural network, leading to\nexcellent zero-shot and finetuned results that outperform equitune. Further, we\nprove that {\\lambda}-equitune is equivariant and a universal approximator of\nequivariant functions. Additionally, we show that the method of Kaba et al.\n(2022) used with appropriate loss functions, which we call equizero, also gives\nexcellent zero-shot and finetuned performance. Both equitune and equizero are\nspecial cases of {\\lambda}-equitune. To show the simplicity and generality of\nour method, we validate on a wide range of diverse applications and models such\nas 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.\n","authors":["Sourya Basu","Pulkit Katdare","Prasanna Sattigeri","Vijil Chenthamarakshan","Katherine Driggs-Campbell","Payel Das","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2305.09900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05862v2","updated":"2023-10-10T18:54:43Z","published":"2023-05-10T03:13:54Z","title":"Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text\n  Analytics? A Study on Several Typical Tasks","summary":"  The most recent large language models(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of generalist models, achieving state-of-the-art\nperformance on a wide range of NLP tasks with little or no adaptation. How\neffective are such models in the financial domain? Understanding this basic\nquestion would have a significant impact on many downstream financial\nanalytical tasks. In this paper, we conduct an empirical study and provide\nexperimental evidences of their performance on a wide variety of financial text\nanalytical problems, using eight benchmark datasets from five categories of\ntasks. We report both the strengths and limitations of the current models by\ncomparing them to the state-of-the-art fine-tuned approaches and the recently\nreleased domain-specific pretrained models. We hope our study can help\nunderstand the capability of the existing models in the financial domain and\nfacilitate further improvements.\n","authors":["Xianzhi Li","Samuel Chan","Xiaodan Zhu","Yulong Pei","Zhiqiang Ma","Xiaomo Liu","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2305.05862v2.pdf","comment":"Add more experiments, accepted to EMNLP 2023 Industry track"},{"id":"http://arxiv.org/abs/2310.06940v1","updated":"2023-10-10T18:53:21Z","published":"2023-10-10T18:53:21Z","title":"Document-Level Supervision for Multi-Aspect Sentiment Analysis Without\n  Fine-grained Labels","summary":"  Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often\ntrained through supervision from human annotations of opinionated texts. These\nfine-grained annotations include identifying aspects towards which a user\nexpresses their sentiment, and their associated polarities (aspect-based\nsentiments). Such fine-grained annotations can be expensive and often\ninfeasible to obtain in real-world settings. There is, however, an abundance of\nscenarios where user-generated text contains an overall sentiment, such as a\nrating of 1-5 in user reviews or user-generated feedback, which may be\nleveraged for this task. In this paper, we propose a VAE-based topic modeling\napproach that performs ABSA using document-level supervision and without\nrequiring fine-grained labels for either aspects or sentiments. Our approach\nallows for the detection of multiple aspects in a document, thereby allowing\nfor the possibility of reasoning about how sentiment expressed through multiple\naspects comes together to form an observable overall document-level sentiment.\nWe demonstrate results on two benchmark datasets from two different domains,\nsignificantly outperforming a state-of-the-art baseline.\n","authors":["Kasturi Bhattacharjee","Rashmi Gangadharaiah"],"pdf_url":"https://arxiv.org/pdf/2310.06940v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2310.06927v1","updated":"2023-10-10T18:28:38Z","published":"2023-10-10T18:28:38Z","title":"Sparse Finetuning for Inference Acceleration of Large Language Models","summary":"  We consider the problem of accurate sparse finetuning of large language\nmodels (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while\ninducing sparsity in their weights. On the accuracy side, we observe that\nstandard loss-based finetuning may fail to recover accuracy, especially at high\nsparsities. To address this, we perform a detailed study of distillation-type\nlosses, determining an L2-based distillation approach we term SquareHead which\nenables accurate recovery even at higher sparsities, across all model types. On\nthe practical efficiency side, we show that sparse LLMs can be executed with\nspeedups by taking advantage of sparsity, for both CPU and GPU runtimes. While\nthe standard approach is to leverage sparsity for computational reduction, we\nobserve that in the case of memory-bound LLMs sparsity can also be leveraged\nfor reducing memory bandwidth. We exhibit end-to-end results showing speedups\ndue to sparsity, while recovering accuracy, on T5 (language translation),\nWhisper (speech translation), and open GPT-type (MPT for text generation). For\nMPT text generation, we show for the first time that sparse finetuning can\nreach 75% sparsity without accuracy drops, provide notable end-to-end speedups\nfor both CPU and GPU inference, and highlight that sparsity is also compatible\nwith quantization approaches. Models and software for reproducing our results\nare provided in Section 6.\n","authors":["Eldar Kurtic","Denis Kuznedelev","Elias Frantar","Michael Goin","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2310.06927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06918v1","updated":"2023-10-10T18:15:24Z","published":"2023-10-10T18:15:24Z","title":"Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE","summary":"  The recent success of SimCSE has greatly advanced state-of-the-art sentence\nrepresentations. However, the original formulation of SimCSE does not fully\nexploit the potential of hard negative samples in contrastive learning. This\nstudy introduces an unsupervised contrastive learning framework that combines\nSimCSE with hard negative mining, aiming to enhance the quality of sentence\nembeddings. The proposed focal-InfoNCE function introduces self-paced\nmodulation terms in the contrastive objective, downweighting the loss\nassociated with easy negatives and encouraging the model focusing on hard\nnegatives. Experimentation on various STS benchmarks shows that our method\nimproves sentence embeddings in terms of Spearman's correlation and\nrepresentation alignment and uniformity.\n","authors":["Pengyue Hou","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2310.06918v1.pdf","comment":"emnlp 2023 findings"},{"id":"http://arxiv.org/abs/2310.06913v1","updated":"2023-10-10T18:09:32Z","published":"2023-10-10T18:09:32Z","title":"A Comparative Study of Transformer-based Neural Text Representation\n  Techniques on Bug Triaging","summary":"  Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.\n","authors":["Atish Kumar Dipongkor","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2310.06913v1.pdf","comment":"12 pages, to appear in the Proceedings of 38th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE'23)"},{"id":"http://arxiv.org/abs/2307.10236v2","updated":"2023-10-10T16:44:02Z","published":"2023-07-16T08:28:04Z","title":"Look Before You Leap: An Exploratory Study of Uncertainty Measurement\n  for Large Language Models","summary":"  The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.\n","authors":["Yuheng Huang","Jiayang Song","Zhijie Wang","Shengming Zhao","Huaming Chen","Felix Juefei-Xu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2307.10236v2.pdf","comment":"20 pages, 4 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.06838v1","updated":"2023-10-10T17:59:53Z","published":"2023-10-10T17:59:53Z","title":"AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description","summary":"  Audio Description (AD) is the task of generating descriptions of visual\ncontent, at suitable time intervals, for the benefit of visually impaired\naudiences. For movies, this presents notable challenges -- AD must occur only\nduring existing pauses in dialogue, should refer to characters by name, and\nought to aid understanding of the storyline as a whole. To this end, we develop\na new model for automatically generating movie AD, given CLIP visual features\nof the frames, the cast list, and the temporal locations of the speech;\naddressing all three of the 'who', 'when', and 'what' questions: (i) who -- we\nintroduce a character bank consisting of the character's name, the actor that\nplayed the part, and a CLIP feature of their face, for the principal cast of\neach movie, and demonstrate how this can be used to improve naming in the\ngenerated AD; (ii) when -- we investigate several models for determining\nwhether an AD should be generated for a time interval or not, based on the\nvisual content of the interval and its neighbours; and (iii) what -- we\nimplement a new vision-language model for this task, that can ingest the\nproposals from the character bank, whilst conditioning on the visual features\nusing cross-attention, and demonstrate how this improves over previous\narchitectures for AD text generation in an apples-to-apples comparison.\n","authors":["Tengda Han","Max Bain","Arsha Nagrani","Gl Varol","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.06838v1.pdf","comment":"ICCV2023. Project page:\n  https://www.robots.ox.ac.uk/vgg/research/autoad/"},{"id":"http://arxiv.org/abs/2310.06836v1","updated":"2023-10-10T17:59:28Z","published":"2023-10-10T17:59:28Z","title":"What Does Stable Diffusion Know about the 3D Scene?","summary":"  Recent advances in generative models like Stable Diffusion enable the\ngeneration of highly photo-realistic images. Our objective in this paper is to\nprobe the diffusion network to determine to what extent it 'understands'\ndifferent properties of the 3D scene depicted in an image. To this end, we make\nthe following contributions: (i) We introduce a protocol to evaluate whether a\nnetwork models a number of physical 'properties' of the 3D scene by probing for\nexplicit features that represent these properties. The probes are applied on\ndatasets of real images with annotations for the property. (ii) We apply this\nprotocol to properties covering scene geometry, scene material, support\nrelations, lighting, and view dependent measures. (iii) We find that Stable\nDiffusion is good at a number of properties including scene geometry, support\nrelations, shadows and depth, but less performant for occlusion. (iv) We also\napply the probes to other models trained at large-scale, including DINO and\nCLIP, and find their performance inferior to that of Stable Diffusion.\n","authors":["Guanqi Zhan","Chuanxia Zheng","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.06836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v1","updated":"2023-10-10T17:53:36Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Moun Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.06822v1","updated":"2023-10-10T17:50:09Z","published":"2023-10-10T17:50:09Z","title":"Neural Bounding","summary":"  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free and empty. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods.\n","authors":["Wenxin Liu","Michael Fischer","Paul D. Yoo","Tobias Ritschel"],"pdf_url":"https://arxiv.org/pdf/2310.06822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13655v2","updated":"2023-10-10T17:46:49Z","published":"2023-05-23T03:59:06Z","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image\n  Diffusion Models with Large Language Models","summary":"  Recent advancements in text-to-image diffusion models have yielded impressive\nresults in generating realistic and diverse images. However, these models still\nstruggle with complex prompts, such as those that involve numeracy and spatial\nreasoning. This work proposes to enhance prompt understanding capabilities in\ndiffusion models. Our method leverages a pretrained large language model (LLM)\nfor grounded generation in a novel two-stage process. In the first stage, the\nLLM generates a scene layout that comprises captioned bounding boxes from a\ngiven prompt describing the desired image. In the second stage, a novel\ncontroller guides an off-the-shelf diffusion model for layout-grounded image\ngeneration. Both stages utilize existing pretrained models without additional\nmodel parameter optimization. Our method significantly outperforms the base\ndiffusion model and several strong baselines in accurately generating images\naccording to prompts that require various capabilities, doubling the generation\naccuracy across four tasks on average. Furthermore, our method enables\ninstruction-based multi-round scene specification and can handle prompts in\nlanguages not supported by the underlying diffusion model. We anticipate that\nour method will unleash users' creativity by accurately following more complex\nprompts.\n","authors":["Long Lian","Boyi Li","Adam Yala","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2305.13655v2.pdf","comment":"Project page: https://llm-grounded-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2202.06095v2","updated":"2023-10-10T17:27:05Z","published":"2022-02-12T16:22:46Z","title":"A Review of Deep Learning-based Approaches for Deepfake Content\n  Detection","summary":"  Recent advancements in deep learning generative models have raised concerns\nas they can create highly convincing counterfeit images and videos. This poses\na threat to people's integrity and can lead to social instability. To address\nthis issue, there is a pressing need to develop new computational models that\ncan efficiently detect forged content and alert users to potential image and\nvideo manipulations. This paper presents a comprehensive review of recent\nstudies for deepfake content detection using deep learning-based approaches. We\naim to broaden the state-of-the-art research by systematically reviewing the\ndifferent categories of fake content detection. Furthermore, we report the\nadvantages and drawbacks of the examined works and future directions towards\nthe issues and shortcomings still unsolved on deepfake detection.\n","authors":["Leandro A. Passos","Danilo Jodas","Kelton A. P. da Costa","Luis A. Souza Jnior","Douglas Rodrigues","Javier Del Ser","David Camacho","Joo Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2202.06095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05803v2","updated":"2023-10-10T17:13:03Z","published":"2023-05-09T23:24:09Z","title":"Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly\n  Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) aims to bypass the need for\nlaborious pixel-level annotation by using only image-level annotation. Most\nexisting methods rely on Class Activation Maps (CAM) to derive pixel-level\npseudo-labels and use them to train a fully supervised semantic segmentation\nmodel. Although these pseudo-labels are class-aware, indicating the coarse\nregions for particular classes, they are not object-aware and fail to delineate\naccurate object boundaries. To address this, we introduce a simple yet\neffective method harnessing the Segment Anything Model (SAM), a class-agnostic\nfoundation model capable of producing fine-grained instance masks of objects,\nparts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM\nmasks, resulting in high-quality pseudo-labels that are both class-aware and\nobject-aware. Our approach is highly versatile and can be easily integrated\ninto existing WSSS methods without any modification. Despite its simplicity,\nour approach shows consistent gain over the state-of-the-art WSSS methods on\nboth PASCAL VOC and MS-COCO datasets.\n","authors":["Tianle Chen","Zheda Mai","Ruiwen Li","Wei-lun Chao"],"pdf_url":"https://arxiv.org/pdf/2305.05803v2.pdf","comment":"Tianle Chen and Zheda Mai contributed equally to this work. Our code\n  is available at \\url{https://github.com/cskyl/SAM_WSSS}"},{"id":"http://arxiv.org/abs/2310.06773v1","updated":"2023-10-10T16:49:21Z","published":"2023-10-10T16:49:21Z","title":"Uni3D: Exploring Unified 3D Representation at Scale","summary":"  Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.\n","authors":["Junsheng Zhou","Jinsheng Wang","Baorui Ma","Yu-Shen Liu","Tiejun Huang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06773v1.pdf","comment":"Code and Demo: https://github.com/baaivision/Uni3D"},{"id":"http://arxiv.org/abs/2309.10791v2","updated":"2023-10-10T16:44:34Z","published":"2023-09-19T17:40:23Z","title":"Multi-spectral Entropy Constrained Neural Compression of Solar Imagery","summary":"  Missions studying the dynamic behaviour of the Sun are defined to capture\nmulti-spectral images of the sun and transmit them to the ground station in a\ndaily basis. To make transmission efficient and feasible, image compression\nsystems need to be exploited. Recently successful end-to-end optimized neural\nnetwork-based image compression systems have shown great potential to be used\nin an ad-hoc manner. In this work we have proposed a transformer-based\nmulti-spectral neural image compressor to efficiently capture redundancies both\nintra/inter-wavelength. To unleash the locality of window-based self attention\nmechanism, we propose an inter-window aggregated token multi head self\nattention. Additionally to make the neural compressor autoencoder shift\ninvariant, a randomly shifted window attention mechanism is used which makes\nthe transformer blocks insensitive to translations in their input domain. We\ndemonstrate that the proposed approach not only outperforms the conventional\ncompression algorithms but also it is able to better decorrelates images along\nthe multiple wavelengths compared to single spectral compression.\n","authors":["Ali Zafari","Atefeh Khoshkhahtinat","Piyush M. Mehta","Nasser M. Nasrabadi","Barbara J. Thompson","Michael S. F. Kirk","Daniel da Silva"],"pdf_url":"https://arxiv.org/pdf/2309.10791v2.pdf","comment":"Accepted to IEEE 22$^{nd}$ International Conference on Machine\n  Learning and Applications 2023 (ICMLA)"},{"id":"http://arxiv.org/abs/2304.05292v3","updated":"2023-10-10T16:40:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v3.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2310.06753v1","updated":"2023-10-10T16:24:51Z","published":"2023-10-10T16:24:51Z","title":"TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning","summary":"  Topology reasoning aims to comprehensively understand road scenes and present\ndrivable routes in autonomous driving. It requires detecting road centerlines\n(lane) and traffic elements, further reasoning their topology relationship,\ni.e., lane-lane topology, and lane-traffic topology. In this work, we first\npresent that the topology score relies heavily on detection performance on lane\nand traffic elements. Therefore, we introduce a powerful 3D lane detector and\nan improved 2D traffic element detector to extend the upper limit of topology\nperformance. Further, we propose TopoMLP, a simple yet high-performance\npipeline for driving topology reasoning. Based on the impressive detection\nperformance, we develop two simple MLP-based heads for topology generation.\nTopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e.,\n41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane\nTopology in Autonomous Driving Challenge. We hope such simple and strong\npipeline can provide some new insights to the community. Code is at\nhttps://github.com/wudongming97/TopoMLP.\n","authors":["Dongming Wu","Jiahao Chang","Fan Jia","Yingfei Liu","Tiancai Wang","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2310.06753v1.pdf","comment":"The 1st solution for 1st OpenLane Topology in Autonomous Driving\n  Challenge. Code is at https://github.com/wudongming97/TopoMLP"},{"id":"http://arxiv.org/abs/2305.10983v3","updated":"2023-10-10T16:23:34Z","published":"2023-05-18T13:55:28Z","title":"Assessor360: Multi-sequence Network for Blind Omnidirectional Image\n  Quality Assessment","summary":"  Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively\nassess the human perceptual quality of omnidirectional images (ODIs) without\nrelying on pristine-quality image information. It is becoming more significant\nwith the increasing advancement of virtual reality (VR) technology. However,\nthe quality assessment of ODIs is severely hampered by the fact that the\nexisting BOIQA pipeline lacks the modeling of the observer's browsing process.\nTo tackle this issue, we propose a novel multi-sequence network for BOIQA\ncalled Assessor360, which is derived from the realistic multi-assessor ODI\nquality assessment procedure. Specifically, we propose a generalized Recursive\nProbability Sampling (RPS) method for the BOIQA task, combining content and\ndetails information to generate multiple pseudo-viewport sequences from a given\nstarting point. Additionally, we design a Multi-scale Feature Aggregation (MFA)\nmodule with a Distortion-aware Block (DAB) to fuse distorted and semantic\nfeatures of each viewport. We also devise Temporal Modeling Module (TMM) to\nlearn the viewport transition in the temporal domain. Extensive experimental\nresults demonstrate that Assessor360 outperforms state-of-the-art methods on\nmultiple OIQA datasets. The code and models are available at\nhttps://github.com/TianheWu/Assessor360.\n","authors":["Tianhe Wu","Shuwei Shi","Haoming Cai","Mingdeng Cao","Jing Xiao","Yinqiang Zheng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.10983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06744v1","updated":"2023-10-10T16:14:20Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in text-to-image diffusion models have enabled 3D generation\nfrom a single image. However, current image-to-3D methods often produce\nsuboptimal results for novel views, with blurred textures and deviations from\nthe reference image, limiting their practical applications. In this paper, we\nintroduce HiFi-123, a method designed for high-fidelity and multi-view\nconsistent 3D generation. Our contributions are twofold: First, we propose a\nreference-guided novel view enhancement technique that substantially reduces\nthe quality gap between synthesized and reference views. Second, capitalizing\non the novel view enhancement, we present a novel reference-guided state\ndistillation loss. When incorporated into the optimization-based image-to-3D\npipeline, our method significantly improves 3D generation quality, achieving\nstate-of-the-art performance. Comprehensive evaluations demonstrate the\neffectiveness of our approach over existing methods, both qualitatively and\nquantitatively.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06737v1","updated":"2023-10-10T16:07:23Z","published":"2023-10-10T16:07:23Z","title":"Multi-domain improves out-of-distribution and data-limited scenarios for\n  medical image analysis","summary":"  Current machine learning methods for medical image analysis primarily focus\non developing models tailored for their specific tasks, utilizing data within\ntheir target domain. These specialized models tend to be data-hungry and often\nexhibit limitations in generalizing to out-of-distribution samples. Recently,\nfoundation models have been proposed, which combine data from various domains\nand demonstrate excellent generalization capabilities. Building upon this, this\nwork introduces the incorporation of diverse medical image domains, including\ndifferent imaging modalities like X-ray, MRI, CT, and ultrasound images, as\nwell as various viewpoints such as axial, coronal, and sagittal views. We refer\nto this approach as multi-domain model and compare its performance to that of\nspecialized models. Our findings underscore the superior generalization\ncapabilities of multi-domain models, particularly in scenarios characterized by\nlimited data availability and out-of-distribution, frequently encountered in\nhealthcare applications. The integration of diverse data allows multi-domain\nmodels to utilize shared information across domains, enhancing the overall\noutcomes significantly. To illustrate, for organ recognition, multi-domain\nmodel can enhance accuracy by up to 10% compared to conventional specialized\nmodels.\n","authors":["Ece Ozkan","Xavier Boix"],"pdf_url":"https://arxiv.org/pdf/2310.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08559v2","updated":"2023-10-10T15:49:45Z","published":"2022-11-15T23:04:15Z","title":"Robust Alzheimer's Progression Modeling using Cross-Domain\n  Self-Supervised Deep Learning","summary":"  Developing successful artificial intelligence systems in practice depends on\nboth robust deep learning models and large, high-quality data. However,\nacquiring and labeling data can be prohibitively expensive and time-consuming\nin many real-world applications, such as clinical disease models.\nSelf-supervised learning has demonstrated great potential in increasing model\naccuracy and robustness in small data regimes. In addition, many clinical\nimaging and disease modeling applications rely heavily on regression of\ncontinuous quantities. However, the applicability of self-supervised learning\nfor these medical-imaging regression tasks has not been extensively studied. In\nthis study, we develop a cross-domain self-supervised learning approach for\ndisease prognostic modeling as a regression problem using medical images as\ninput. We demonstrate that self-supervised pretraining can improve the\nprediction of Alzheimer's Disease progression from brain MRI. We also show that\npretraining on extended (but not labeled) brain MRI data outperforms\npretraining on natural images. We further observe that the highest performance\nis achieved when both natural images and extended brain-MRI data are used for\npretraining.\n","authors":["Saba Dadsetan","Mohsen Hejrati","Shandong Wu","Somaye Hashemifar"],"pdf_url":"https://arxiv.org/pdf/2211.08559v2.pdf","comment":"This work has been published at the Transactions on Machine Learning\n  Research (TMLR) journal"},{"id":"http://arxiv.org/abs/2306.00042v2","updated":"2023-10-10T14:48:59Z","published":"2023-05-31T13:21:54Z","title":"Graph-based methods coupled with specific distributional distances for\n  adversarial attack detection","summary":"  Artificial neural networks are prone to being fooled by carefully perturbed\ninputs which cause an egregious misclassification. These \\textit{adversarial}\nattacks have been the focus of extensive research. Likewise, there has been an\nabundance of research in ways to detect and defend against them. We introduce a\nnovel approach of detection and interpretation of adversarial attacks from a\ngraph perspective. For an input image, we compute an associated sparse graph\nusing the layer-wise relevance propagation algorithm \\cite{bach15}.\nSpecifically, we only keep edges of the neural network with the highest\nrelevance values. Three quantities are then computed from the graph which are\nthen compared against those computed from the training set. The result of the\ncomparison is a classification of the image as benign or adversarial. To make\nthe comparison, two classification methods are introduced: 1) an explicit\nformula based on Wasserstein distance applied to the degree of node and 2) a\nlogistic regression. Both classification methods produce strong results which\nlead us to believe that a graph-based interpretation of adversarial attacks is\nvaluable.\n","authors":["Dwight Nwaigwe","Lucrezia Carboni","Martial Mermillod","Sophie Achard","Michel Dojat"],"pdf_url":"https://arxiv.org/pdf/2306.00042v2.pdf","comment":"published in Neural Networks"},{"id":"http://arxiv.org/abs/2310.06670v1","updated":"2023-10-10T14:46:22Z","published":"2023-10-10T14:46:22Z","title":"Domain Generalization by Rejecting Extreme Augmentations","summary":"  Data augmentation is one of the most effective techniques for regularizing\ndeep learning models and improving their recognition performance in a variety\nof tasks and domains. However, this holds for standard in-domain settings, in\nwhich the training and test data follow the same distribution. For the\nout-of-domain case, where the test data follow a different and unknown\ndistribution, the best recipe for data augmentation is unclear. In this paper,\nwe show that for out-of-domain and domain generalization settings, data\naugmentation can provide a conspicuous and robust improvement in performance.\nTo do that, we propose a simple training procedure: (i) use uniform sampling on\nstandard data augmentation transformations; (ii) increase the strength\ntransformations to account for the higher data variance expected when working\nout-of-domain, and (iii) devise a new reward function to reject extreme\ntransformations that can harm the training. With this procedure, our data\naugmentation scheme achieves a level of accuracy that is comparable to or\nbetter than state-of-the-art methods on benchmark domain generalization\ndatasets. Code: \\url{https://github.com/Masseeh/DCAug}\n","authors":["Masih Aminbeidokhti","Fidel A. Guerrero Pea","Heitor Rapela Medeiros","Thomas Dubail","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2310.06670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06668v1","updated":"2023-10-10T14:42:34Z","published":"2023-10-10T14:42:34Z","title":"Latent Diffusion Counterfactual Explanations","summary":"  Counterfactual explanations have emerged as a promising method for\nelucidating the behavior of opaque black-box models. Recently, several works\nleveraged pixel-space diffusion models for counterfactual generation. To handle\nnoisy, adversarial gradients during counterfactual generation -- causing\nunrealistic artifacts or mere adversarial perturbations -- they required either\nauxiliary adversarially robust models or computationally intensive guidance\nschemes. However, such requirements limit their applicability, e.g., in\nscenarios with restricted access to the model's training data. To address these\nlimitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).\nLDCE harnesses the capabilities of recent class- or text-conditional foundation\nlatent diffusion models to expedite counterfactual generation and focus on the\nimportant, semantic parts of the data. Furthermore, we propose a novel\nconsensus guidance mechanism to filter out noisy, adversarial gradients that\nare misaligned with the diffusion model's implicit classifier. We demonstrate\nthe versatility of LDCE across a wide spectrum of models trained on diverse\ndatasets with different learning paradigms. Finally, we showcase how LDCE can\nprovide insights into model errors, enhancing our understanding of black-box\nmodel behavior.\n","authors":["Karim Farid","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.06668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06667v1","updated":"2023-10-10T14:42:32Z","published":"2023-10-10T14:42:32Z","title":"SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space","summary":"  Generative Adversarial Networks (GANs) can synthesize realistic images, with\nthe learned latent space shown to encode rich semantic information with various\ninterpretable directions. However, due to the unstructured nature of the\nlearned latent space, it inherits the bias from the training data where\nspecific groups of visual attributes that are not causally related tend to\nappear together, a phenomenon also known as spurious correlations, e.g., age\nand eyeglasses or women and lipsticks. Consequently, the learned distribution\noften lacks the proper modelling of the missing examples. The interpolation\nfollowing editing directions for one attribute could result in entangled\nchanges with other attributes. To address this problem, previous works\ntypically adjust the learned directions to minimize the changes in other\nattributes, yet they still fail on strongly correlated features. In this work,\nwe study the entanglement issue in both the training data and the learned\nlatent space for the StyleGAN2-FFHQ model. We propose a novel framework\nSC$^2$GAN that achieves disentanglement by re-projecting low-density latent\ncode samples in the original latent space and correcting the editing directions\nbased on both the high-density and low-density regions. By leveraging the\noriginal meaningful directions and semantic region-specific layers, our\nframework interpolates the original latent codes to generate images with\nattribute combination that appears infrequently, then inverts these samples\nback to the original latent space. We apply our framework to pre-existing\nmethods that learn meaningful latent directions and showcase its strong\ncapability to disentangle the attributes with small amounts of low-density\nregion samples added.\n","authors":["Zikun Chen","Han Zhao","Parham Aarabi","Ruowei Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.06667v1.pdf","comment":"Accepted to the Out Of Distribution Generalization in Computer Vision\n  workshop at ICCV2023"},{"id":"http://arxiv.org/abs/2310.06654v1","updated":"2023-10-10T14:22:56Z","published":"2023-10-10T14:22:56Z","title":"Evaluating Explanation Methods for Vision-and-Language Navigation","summary":"  The ability to navigate robots with natural language instructions in an\nunknown environment is a crucial step for achieving embodied artificial\nintelligence (AI). With the improving performance of deep neural models\nproposed in the field of vision-and-language navigation (VLN), it is equally\ninteresting to know what information the models utilize for their\ndecision-making in the navigation tasks. To understand the inner workings of\ndeep neural models, various explanation methods have been developed for\npromoting explainable AI (XAI). But they are mostly applied to deep neural\nmodels for image or text classification tasks and little work has been done in\nexplaining deep neural models for VLN tasks. In this paper, we address these\nproblems by building quantitative benchmarks to evaluate explanation methods\nfor VLN models in terms of faithfulness. We propose a new erasure-based\nevaluation pipeline to measure the step-wise textual explanation in the\nsequential decision-making setting. We evaluate several explanation methods for\ntwo representative VLN models on two popular VLN datasets and reveal valuable\nfindings through our experiments.\n","authors":["Guanqi Chen","Lei Yang","Guanhua Chen","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2310.06654v1.pdf","comment":"Accepted by ECAI 2023"},{"id":"http://arxiv.org/abs/2202.10115v4","updated":"2023-10-10T14:12:43Z","published":"2022-02-21T10:57:16Z","title":"An Efficient Smoothing and Thresholding Image Segmentation Framework\n  with Weighted Anisotropic-Isotropic Total Variation","summary":"  In this paper, we design an efficient, multi-stage image segmentation\nframework that incorporates a weighted difference of anisotropic and isotropic\ntotal variation (AITV). The segmentation framework generally consists of two\nstages: smoothing and thresholding, thus referred to as SaT. In the first\nstage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS)\nmodel, which can be solved efficiently by the alternating direction method of\nmultipliers (ADMM) with a closed-form solution of a proximal operator of the\n$\\ell_1 -\\alpha \\ell_2$ regularizer. Convergence of the ADMM algorithm is\nanalyzed. In the second stage, we threshold the smoothed image by $K$-means\nclustering to obtain the final segmentation result. Numerical experiments\ndemonstrate that the proposed segmentation framework is versatile for both\ngrayscale and color images, efficient in producing high-quality segmentation\nresults within a few seconds, and robust to input images that are corrupted\nwith noise, blur, or both. We compare the AITV method with its original convex\nTV and nonconvex TV$^p (0<p<1)$ counterparts, showcasing the qualitative and\nquantitative advantages of our proposed method.\n","authors":["Kevin Bui","Yifei Lou","Fredrick Park","Jack Xin"],"pdf_url":"https://arxiv.org/pdf/2202.10115v4.pdf","comment":"accepted to Springer CAMC"},{"id":"http://arxiv.org/abs/2309.08066v2","updated":"2023-10-10T14:10:05Z","published":"2023-09-14T23:28:58Z","title":"Morphologically-Aware Consensus Computation via Heuristics-based\n  IterATive Optimization (MACCHIatO)","summary":"  The extraction of consensus segmentations from several binary or\nprobabilistic masks is important to solve various tasks such as the analysis of\ninter-rater variability or the fusion of several neural network outputs. One of\nthe most widely used methods to obtain such a consensus segmentation is the\nSTAPLE algorithm. In this paper, we first demonstrate that the output of that\nalgorithm is heavily impacted by the background size of images and the choice\nof the prior. We then propose a new method to construct a binary or a\nprobabilistic consensus segmentation based on the Fr\\'{e}chet means of\ncarefully chosen distances which makes it totally independent of the image\nbackground size. We provide a heuristic approach to optimize this criterion\nsuch that a voxel's class is fully determined by its voxel-wise distance to the\ndifferent masks, the connected component it belongs to and the group of raters\nwho segmented it. We compared extensively our method on several datasets with\nthe STAPLE method and the naive segmentation averaging method, showing that it\nleads to binary consensus masks of intermediate size between Majority Voting\nand STAPLE and to different posterior probabilities than Mask Averaging and\nSTAPLE methods. Our code is available at\nhttps://gitlab.inria.fr/dhamzaou/jaccardmap .\n","authors":["Dimitri Hamzaoui","Sarah Montagne","Raphale Renard-Penna","Nicholas Ayache","Herv Delingette"],"pdf_url":"https://arxiv.org/pdf/2309.08066v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2023:013"},{"id":"http://arxiv.org/abs/2310.06641v1","updated":"2023-10-10T14:04:32Z","published":"2023-10-10T14:04:32Z","title":"How (not) to ensemble LVLMs for VQA","summary":"  This paper studies ensembling in the era of Large Vision-Language Models\n(LVLMs). Ensembling is a classical method to combine different models to get\nincreased performance. In the recent work on Encyclopedic-VQA the authors\nexamine a wide variety of models to solve their task: from vanilla LVLMs, to\nmodels including the caption as extra context, to models augmented with\nLens-based retrieval of Wikipedia pages. Intuitively these models are highly\ncomplementary, which should make them ideal for ensembling. Indeed, an oracle\nexperiment shows potential gains from 48.8% accuracy (the best single model)\nall the way up to 67% (best possible ensemble). So it is a trivial exercise to\ncreate an ensemble with substantial real gains. Or is it?\n","authors":["Lisa Alazraki","Lluis Castrejon","Mostafa Dehghani","Fantine Huot","Jasper Uijlings","Thomas Mensink"],"pdf_url":"https://arxiv.org/pdf/2310.06641v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2310.06633v1","updated":"2023-10-10T13:51:24Z","published":"2023-10-10T13:51:24Z","title":"Blind Dates: Examining the Expression of Temporality in Historical\n  Photographs","summary":"  This paper explores the capacity of computer vision models to discern\ntemporal information in visual content, focusing specifically on historical\nphotographs. We investigate the dating of images using OpenCLIP, an open-source\nimplementation of CLIP, a multi-modal language and vision model. Our experiment\nconsists of three steps: zero-shot classification, fine-tuning, and analysis of\nvisual content. We use the \\textit{De Boer Scene Detection} dataset, containing\n39,866 gray-scale historical press photographs from 1950 to 1999. The results\nshow that zero-shot classification is relatively ineffective for image dating,\nwith a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a\nlogistic classifier improves performance and eliminates the bias. Additionally,\nour analysis reveals that images featuring buses, cars, cats, dogs, and people\nare more accurately dated, suggesting the presence of temporal markers. The\nstudy highlights the potential of machine learning models like OpenCLIP in\ndating images and emphasizes the importance of fine-tuning for accurate\ntemporal analysis. Future research should explore the application of these\nfindings to color photographs and diverse datasets.\n","authors":["Alexandra Barancov","Melvin Wevers","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2310.06633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06629v1","updated":"2023-10-10T13:48:18Z","published":"2023-10-10T13:48:18Z","title":"EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention","summary":"  Because of the advancement of deep learning technology, vision transformer\nhas demonstrated competitive performance in various computer vision tasks.\nUnfortunately, vision transformer still faces some challenges such as high\ncomputational complexity and absence of desirable inductive bias. To alleviate\nthese problems, this study proposes a novel Bi-Fovea Self-Attention (BFSA)\ninspired by the physiological structure and characteristics of bi-fovea vision\nin eagle eyes. This BFSA can simulate the shallow fovea and deep fovea\nfunctions of eagle vision, enabling the network to extract feature\nrepresentations of targets from coarse to fine, facilitating the interaction of\nmulti-scale feature representations. Additionally, this study designs a Bionic\nEagle Vision (BEV) block based on BFSA and CNN. It combines CNN and Vision\nTransformer, to enhance the network's local and global representation ability\nfor targets. Furthermore, this study develops a unified and efficient general\npyramid backbone network family, named Eagle Vision Transformers (EViTs) by\nstacking the BEV blocks. Experimental results on various computer vision tasks\nincluding image classification, object detection, instance segmentation and\nother transfer learning tasks show that the proposed EViTs perform\nsignificantly better than the baselines under similar model sizes, which\nexhibits faster speed on graphics processing unit compared to other models.\nCode will be released at https://github.com/nkusyl.\n","authors":["Yulong Shi","Mingwei Sun","Yongshuai Wang","Rui Wang","Hui Sun","Zengqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06629v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06628v1","updated":"2023-10-10T13:46:11Z","published":"2023-10-10T13:46:11Z","title":"Deep Cardiac MRI Reconstruction with ADMM","summary":"  Cardiac magnetic resonance imaging is a valuable non-invasive tool for\nidentifying cardiovascular diseases. For instance, Cine MRI is the benchmark\nmodality for assessing the cardiac function and anatomy. On the other hand,\nmulti-contrast (T1 and T2) mapping has the potential to assess pathologies and\nabnormalities in the myocardium and interstitium. However, voluntary\nbreath-holding and often arrhythmia, in combination with MRI's slow imaging\nspeed, can lead to motion artifacts, hindering real-time acquisition image\nquality. Although performing accelerated acquisitions can facilitate dynamic\nimaging, it induces aliasing, causing low reconstructed image quality in Cine\nMRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by\nrelated work in accelerated MRI reconstruction, we present a deep learning\n(DL)-based method for accelerated cine and multi-contrast reconstruction in the\ncontext of dynamic cardiac imaging. We formulate the reconstruction problem as\na least squares regularized optimization task, and employ vSHARP, a\nstate-of-the-art DL-based inverse problem solver, which incorporates\nhalf-quadratic variable splitting and the alternating direction method of\nmultipliers with neural networks. We treat the problem in two setups; a 2D\nreconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep\nlearning networks, respectively. Our method optimizes in both the image and\nk-space domains, allowing for high reconstruction fidelity. Although the target\ndata is undersampled with a Cartesian equispaced scheme, we train our model\nusing both Cartesian and simulated non-Cartesian undersampling schemes to\nenhance generalization of the model to unseen data. Furthermore, our model\nadopts a deep neural network to learn and refine the sensitivity maps of\nmulti-coil k-space data. Lastly, our method is jointly trained on both,\nundersampled cine and multi-contrast data.\n","authors":["George Yiasemis","Nikita Moriakov","Jan-Jakob Sonke","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2310.06628v1.pdf","comment":"12 pages, 3 figures, 2 tables. CMRxRecon Challenge, MICCAI 2023"},{"id":"http://arxiv.org/abs/2310.06627v1","updated":"2023-10-10T13:45:59Z","published":"2023-10-10T13:45:59Z","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models","summary":"  Counterfactual reasoning ability is one of the core abilities of human\nintelligence. This reasoning process involves the processing of alternatives to\nobserved states or past events, and this process can improve our ability for\nplanning and decision-making. In this work, we focus on benchmarking the\ncounterfactual reasoning ability of multi-modal large language models. We take\nthe question and answer pairs from the VQAv2 dataset and add one counterfactual\npresupposition to the questions, with the answer being modified accordingly.\nAfter generating counterfactual questions and answers using ChatGPT, we\nmanually examine all generated questions and answers to ensure correctness.\nOver 2k counterfactual question and answer pairs are collected this way. We\nevaluate recent vision language models on our newly collected test dataset and\nfound that all models exhibit a large performance drop compared to the results\ntested on questions without the counterfactual presupposition. This result\nindicates that there still exists space for developing vision language models.\nApart from the vision language models, our proposed dataset can also serves as\na benchmark for evaluating the ability of code generation LLMs, results\ndemonstrate a large gap between GPT-4 and current open-source models. Our code\nand dataset are available at \\url{https://github.com/Letian2003/C-VQA}.\n","authors":["Letian Zhang","Xiaotong Zhai","Zhongkai Zhao","Xin Wen","Yongshuo Zong","Bingchen Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06627v1.pdf","comment":"Short paper accepted at ICCV 2023 VLAR workshop"},{"id":"http://arxiv.org/abs/2303.09756v2","updated":"2023-10-10T13:31:22Z","published":"2023-03-17T03:44:15Z","title":"Video Action Recognition with Attentive Semantic Units","summary":"  Visual-Language Models (VLMs) have significantly advanced action video\nrecognition. Supervised by the semantics of action labels, recent works adapt\nthe visual branch of VLMs to learn video representations. Despite the\neffectiveness proved by these works, we believe that the potential of VLMs has\nyet to be fully harnessed. In light of this, we exploit the semantic units (SU)\nhiding behind the action labels and leverage their correlations with\nfine-grained items in frames for more accurate action recognition. SUs are\nentities extracted from the language descriptions of the entire action set,\nincluding body parts, objects, scenes, and motions. To further enhance the\nalignments between visual contents and the SUs, we introduce a multi-region\nmodule (MRA) to the visual branch of the VLM. The MRA allows the perception of\nregion-aware visual features beyond the original global feature. Our method\nadaptively attends to and selects relevant SUs with visual features of frames.\nWith a cross-modal decoder, the selected SUs serve to decode spatiotemporal\nvideo representations. In summary, the SUs as the medium can boost\ndiscriminative ability and transferability. Specifically, in fully-supervised\nlearning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2\nfew-shot experiments, our method surpassed the previous state-of-the-art by\n+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.\n","authors":["Yifei Chen","Dapeng Chen","Ruijin Liu","Hao Li","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.09756v2.pdf","comment":"Accepted at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.06603v1","updated":"2023-10-10T13:12:03Z","published":"2023-10-10T13:12:03Z","title":"V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric\n  Heterogenous Distillation Network","summary":"  Object detection is the central issue of intelligent traffic systems, and\nrecent advancements in single-vehicle lidar-based 3D detection indicate that it\ncan provide accurate position information for intelligent agents to make\ndecisions and plan. Compared with single-vehicle perception, multi-view\nvehicle-road cooperation perception has fundamental advantages, such as the\nelimination of blind spots and a broader range of perception, and has become a\nresearch hotspot. However, the current perception of cooperation focuses on\nimproving the complexity of fusion while ignoring the fundamental problems\ncaused by the absence of single-view outlines. We propose a multi-view\nvehicle-road cooperation perception system, vehicle-to-everything cooperative\nperception (V2X-AHD), in order to enhance the identification capability,\nparticularly for predicting the vehicle's shape. At first, we propose an\nasymmetric heterogeneous distillation network fed with different training data\nto improve the accuracy of contour recognition, with multi-view teacher\nfeatures transferring to single-view student features. While the point cloud\ndata are sparse, we propose Spara Pillar, a spare convolutional-based plug-in\nfeature extraction backbone, to reduce the number of parameters and improve and\nenhance feature extraction capabilities. Moreover, we leverage the multi-head\nself-attention (MSA) to fuse the single-view feature, and the lightweight\ndesign makes the fusion feature a smooth expression. The results of applying\nour algorithm to the massive open dataset V2Xset demonstrate that our method\nachieves the state-of-the-art result. The V2X-AHD can effectively improve the\naccuracy of 3D object detection and reduce the number of network parameters,\naccording to this study, which serves as a benchmark for cooperative\nperception. The code for this article is available at\nhttps://github.com/feeling0414-lab/V2X-AHD.\n","authors":["Caizhen He","Hai Wang","Long Chen","Tong Luo","Yingfeng Cai"],"pdf_url":"https://arxiv.org/pdf/2310.06603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06600v1","updated":"2023-10-10T13:08:50Z","published":"2023-10-10T13:08:50Z","title":"Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy\n  Labels","summary":"  Label noise is a pervasive problem in deep learning that often compromises\nthe generalization performance of trained models. Recently, leveraging\nprivileged information (PI) -- information available only during training but\nnot at test time -- has emerged as an effective approach to mitigate this\nissue. Yet, existing PI-based methods have failed to consistently outperform\ntheir no-PI counterparts in terms of preventing overfitting to label noise. To\naddress this deficiency, we introduce Pi-DUAL, an architecture designed to\nharness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the\noutput logits into a prediction term, based on conventional input features, and\na noise-fitting term influenced solely by PI. A gating mechanism steered by PI\nadaptively shifts focus between these terms, allowing the model to implicitly\nseparate the learning paths of clean and wrong labels. Empirically, Pi-DUAL\nachieves significant performance improvements on key PI benchmarks (e.g., +6.8%\non ImageNet-PI), establishing a new state-of-the-art test set accuracy.\nAdditionally, Pi-DUAL is a potent method for identifying noisy samples\npost-training, outperforming other strong methods at this task. Overall,\nPi-DUAL is a simple, scalable and practical approach for mitigating the effects\nof label noise in a variety of real-world scenarios with PI.\n","authors":["Ke Wang","Guillermo Ortiz-Jimenez","Rodolphe Jenatton","Mark Collier","Efi Kokiopoulou","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2310.06600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06594v1","updated":"2023-10-10T13:01:38Z","published":"2023-10-10T13:01:38Z","title":"REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning\n  Datasets","summary":"  There is an emerging line of research on multimodal instruction tuning, and a\nline of benchmarks have been proposed for evaluating these models recently.\nInstead of evaluating the models directly, in this paper we try to evaluate the\nVision-Language Instruction-Tuning (VLIT) datasets themselves and further seek\nthe way of building a dataset for developing an all-powerful VLIT model, which\nwe believe could also be of utility for establishing a grounded protocol for\nbenchmarking VLIT models. For effective analysis of VLIT datasets that remains\nan open question, we propose a tune-cross-evaluation paradigm: tuning on one\ndataset and evaluating on the others in turn. For each single tune-evaluation\nexperiment set, we define the Meta Quality (MQ) as the mean score measured by a\nseries of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the\nquality of a certain dataset or a sample. On this basis, to evaluate the\ncomprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering\nall tune-evaluation sets. To lay the foundation for building a comprehensive\ndataset and developing an all-powerful model for practical applications, we\nfurther define the Sample Quality (SQ) to quantify the all-sided quality of\neach sample. Extensive experiments validate the rationality of the proposed\nevaluation paradigm. Based on the holistic evaluation, we build a new dataset,\nREVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples\nwith higher SQ from each dataset. With only half of the full data, the model\ntrained on REVO-LION can achieve performance comparable to simply adding all\nVLIT datasets up. In addition to developing an all-powerful model, REVO-LION\nalso includes an evaluation set, which is expected to serve as a convenient\nevaluation benchmark for future research.\n","authors":["Ning Liao","Shaofeng Zhang","Renqiu Xia","Bo Zhang","Min Cao","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06582v1","updated":"2023-10-10T12:47:31Z","published":"2023-10-10T12:47:31Z","title":"Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and\n  Leaves","summary":"  Advancements in machine vision that enable detailed inferences to be made\nfrom images have the potential to transform many sectors including agriculture.\nPrecision agriculture, where data analysis enables interventions to be\nprecisely targeted, has many possible applications. Precision spraying, for\nexample, can limit the application of herbicide only to weeds, or limit the\napplication of fertiliser only to undernourished crops, instead of spraying the\nentire field. The approach promises to maximise yields, whilst minimising\nresource use and harms to the surrounding environment. To this end, we propose\na hierarchical panoptic segmentation method to simultaneously identify\nindicators of plant growth and locate weeds within an image. We adapt\nMask2Former, a state-of-the-art architecture for panoptic segmentation, to\npredict crop, weed and leaf masks. We achieve a PQ{\\dag} of 75.99.\nAdditionally, we explore approaches to make the architecture more compact and\ntherefore more suitable for time and compute constrained applications. With our\nmore compact architecture, inference is up to 60% faster and the reduction in\nPQ{\\dag} is less than 1%.\n","authors":["Madeleine Darbyshire","Elizabeth Sklar","Simon Parsons"],"pdf_url":"https://arxiv.org/pdf/2310.06582v1.pdf","comment":"6 pages, 5 figures, 2 tables, for code, see\n  https://github.com/madeleinedarbyshire/HierarchicalMask2Former"},{"id":"http://arxiv.org/abs/2310.06578v1","updated":"2023-10-10T12:39:10Z","published":"2023-10-10T12:39:10Z","title":"Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking\n  Neural Network","summary":"  Human vision incorporates non-uniform resolution retina, efficient eye\nmovement strategy, and spiking neural network (SNN) to balance the requirements\nin visual field size, visual resolution, energy cost, and inference latency.\nThese properties have inspired interest in developing human-like computer\nvision. However, existing models haven't fully incorporated the three features\nof human vision, and their learned eye movement strategies haven't been\ncompared with human's strategy, making the models' behavior difficult to\ninterpret. Here, we carry out experiments to examine human visual search\nbehaviors and establish the first SNN-based visual search model. The model\ncombines an artificial retina with spiking feature extraction, memory, and\nsaccade decision modules, and it employs population coding for fast and\nefficient saccade decisions. The model can learn either a human-like or a\nnear-optimal fixation strategy, outperform humans in search speed and accuracy,\nand achieve high energy efficiency through short saccade decision latency and\nsparse activation. It also suggests that the human search strategy is\nsuboptimal in terms of search speed. Our work connects modeling of vision in\nneuroscience and machine learning and sheds light on developing more\nenergy-efficient computer vision algorithms.\n","authors":["Yunhui Zhou","Dongqi Han","Yuguo Yu"],"pdf_url":"https://arxiv.org/pdf/2310.06578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06577v1","updated":"2023-10-10T12:38:34Z","published":"2023-10-10T12:38:34Z","title":"SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D\n  Human Reconstruction","summary":"  Reconstructing 3D human shapes from 2D images has received increasing\nattention recently due to its fundamental support for many high-level 3D\napplications. Compared with natural images, freehand sketches are much more\nflexible to depict various shapes, providing a high potential and valuable way\nfor 3D human reconstruction. However, such a task is highly challenging. The\nsparse abstract characteristics of sketches add severe difficulties, such as\narbitrariness, inaccuracy, and lacking image details, to the already badly\nill-posed problem of 2D-to-3D reconstruction. Although current methods have\nachieved great success in reconstructing 3D human bodies from a single-view\nimage, they do not work well on freehand sketches. In this paper, we propose a\nnovel sketch-driven multi-faceted decoder network termed SketchBodyNet to\naddress this task. Specifically, the network consists of a backbone and three\nseparate attention decoder branches, where a multi-head self-attention module\nis exploited in each decoder to obtain enhanced features, followed by a\nmulti-layer perceptron. The multi-faceted decoders aim to predict the camera,\nshape, and pose parameters, respectively, which are then associated with the\nSMPL model to reconstruct the corresponding 3D human mesh. In learning,\nexisting 3D meshes are projected via the camera parameters into 2D synthetic\nsketches with joints, which are combined with the freehand sketches to optimize\nthe model. To verify our method, we collect a large-scale dataset of about 26k\nfreehand sketches and their corresponding 3D meshes containing various poses of\nhuman bodies from 14 different angles. Extensive experimental results\ndemonstrate our SketchBodyNet achieves superior performance in reconstructing\n3D human meshes from freehand sketches.\n","authors":["Fei Wang","Kongzhang Tang","Hefeng Wu","Baoquan Zhao","Hao Cai","Teng Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06577v1.pdf","comment":"9 pages, to appear in Pacific Graphics 2023"},{"id":"http://arxiv.org/abs/2310.05873v2","updated":"2023-10-10T12:32:32Z","published":"2023-10-09T17:13:10Z","title":"Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion\n  Models","summary":"  Fine-tuning diffusion models through personalized datasets is an acknowledged\nmethod for improving generation quality across downstream tasks, which,\nhowever, often inadvertently generates unintended concepts such as watermarks\nand QR codes, attributed to the limitations in image sources and collecting\nmethods within specific downstream tasks. Existing solutions suffer from\neliminating these unintentionally learned implicit concepts, primarily due to\nthe dependency on the model's ability to recognize concepts that it actually\ncannot discern. In this work, we introduce Geom-Erasing, a novel approach that\nsuccessfully removes the implicit concepts with either an additional accessible\nclassifier or detector model to encode geometric information of these concepts\ninto text domain. Moreover, we propose Implicit Concept, a novel image-text\ndataset imbued with three implicit concepts (i.e., watermarks, QR codes, and\ntext) for training and evaluation. Experimental results demonstrate that\nGeom-Erasing not only identifies but also proficiently eradicates implicit\nconcepts, revealing a significant improvement over the existing methods. The\nintegration of geometric information marks a substantial progression in the\nprecise removal of implicit concepts in diffusion models.\n","authors":["Zhili Liu","Kai Chen","Yifan Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.05873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06566v1","updated":"2023-10-10T12:25:52Z","published":"2023-10-10T12:25:52Z","title":"Efficient Retrieval of Images with Irregular Patterns using\n  Morphological Image Analysis: Applications to Industrial and Healthcare\n  datasets","summary":"  Image retrieval is the process of searching and retrieving images from a\ndatabase based on their visual content and features. Recently, much attention\nhas been directed towards the retrieval of irregular patterns within industrial\nor medical images by extracting features from the images, such as deep\nfeatures, colour-based features, shape-based features and local features. This\nhas applications across a spectrum of industries, including fault inspection,\ndisease diagnosis, and maintenance prediction. This paper proposes an image\nretrieval framework to search for images containing similar irregular patterns\nby extracting a set of morphological features (DefChars) from images; the\ndatasets employed in this paper contain wind turbine blade images with defects,\nchest computerised tomography scans with COVID-19 infection, heatsink images\nwith defects, and lake ice images. The proposed framework was evaluated with\ndifferent feature extraction methods (DefChars, resized raw image, local binary\npattern, and scale-invariant feature transforms) and distance metrics to\ndetermine the most efficient parameters in terms of retrieval performance\nacross datasets. The retrieval results show that the proposed framework using\nthe DefChars and the Manhattan distance metric achieves a mean average\nprecision of 80% and a low standard deviation of 0.09 across classes of\nirregular patterns, outperforming alternative feature-metric combinations\nacross all datasets. Furthermore, the low standard deviation between each class\nhighlights DefChars' capability for a reliable image retrieval task, even in\nthe presence of class imbalances or small-sized datasets.\n","authors":["Jiajun Zhang","Georgina Cosma","Sarah Bugby","Jason Watkins"],"pdf_url":"https://arxiv.org/pdf/2310.06566v1.pdf","comment":"35 pages, 5 figures, 19 tables (17 tables in appendix), submitted to\n  Special Issue: Advances and Challenges in Multimodal Machine Learning 2nd\n  Edition, Journal of Imaging, MDPI"},{"id":"http://arxiv.org/abs/2310.06562v1","updated":"2023-10-10T12:19:39Z","published":"2023-10-10T12:19:39Z","title":"Compositional Representation Learning for Brain Tumour Segmentation","summary":"  For brain tumour segmentation, deep learning models can achieve human\nexpert-level performance given a large amount of data and pixel-level\nannotations. However, the expensive exercise of obtaining pixel-level\nannotations for large amounts of data is not always feasible, and performance\nis often heavily reduced in a low-annotated data regime. To tackle this\nchallenge, we adapt a mixed supervision framework, vMFNet, to learn robust\ncompositional representations using unsupervised learning and weak supervision\nalongside non-exhaustive pixel-level pathology labels. In particular, we use\nthe BraTS dataset to simulate a collection of 2-point expert pathology\nannotations indicating the top and bottom slice of the tumour (or tumour\nsub-regions: peritumoural edema, GD-enhancing tumour, and the necrotic /\nnon-enhancing tumour) in each MRI volume, from which weak image-level labels\nthat indicate the presence or absence of the tumour (or the tumour sub-regions)\nin the image are constructed. Then, vMFNet models the encoded image features\nwith von-Mises-Fisher (vMF) distributions, via learnable and compositional vMF\nkernels which capture information about structures in the images. We show that\ngood tumour segmentation performance can be achieved with a large amount of\nweakly labelled data but only a small amount of fully-annotated data.\nInterestingly, emergent learning of anatomical structures occurs in the\ncompositional representation even given only supervision relating to pathology\n(tumour).\n","authors":["Xiao Liu","Antanas Kascenas","Hannah Watson","Sotirios A. Tsaftaris","Alison Q. O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.06562v1.pdf","comment":"Accepted by DART workshop, MICCAI 2023"},{"id":"http://arxiv.org/abs/2310.06557v1","updated":"2023-10-10T12:13:38Z","published":"2023-10-10T12:13:38Z","title":"Data efficient deep learning for medical image analysis: A survey","summary":"  The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.\n","authors":["Suruchi Kumari","Pravendra Singh"],"pdf_url":"https://arxiv.org/pdf/2310.06557v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2204.14240v2","updated":"2023-10-10T12:04:53Z","published":"2022-04-29T17:10:01Z","title":"EndoMapper dataset of complete calibrated endoscopy procedures","summary":"  Computer-assisted systems are becoming broadly used in medicine. In\nendoscopy, most research focuses on the automatic detection of polyps or other\npathologies, but localization and navigation of the endoscope are completely\nperformed manually by physicians. To broaden this research and bring spatial\nArtificial Intelligence to endoscopies, data from complete procedures is\nneeded. This paper introduces the Endomapper dataset, the first collection of\ncomplete endoscopy sequences acquired during regular medical practice, making\nsecondary use of medical data. Its main purpose is to facilitate the\ndevelopment and evaluation of Visual Simultaneous Localization and Mapping\n(VSLAM) methods in real endoscopy data. The dataset contains more than 24 hours\nof video. It is the first endoscopic dataset that includes endoscope\ncalibration as well as the original calibration videos. Meta-data and\nannotations associated with the dataset vary from the anatomical landmarks,\nprocedure labeling, segmentations, reconstructions, simulated sequences with\nground truth and same patient procedures. The software used in this paper is\npublicly available.\n","authors":["Pablo Azagra","Carlos Sostres","ngel Ferrandez","Luis Riazuelo","Clara Tomasini","Oscar Len Barbed","Javier Morlana","David Recasens","Victor M. Batlle","Juan J. Gmez-Rodrguez","Richard Elvira","Julia Lpez","Cristina Oriol","Javier Civera","Juan D. Tards","Ana Cristina Murillo","Angel Lanas","Jos M. M. Montiel"],"pdf_url":"https://arxiv.org/pdf/2204.14240v2.pdf","comment":"17 pages, 14 figures, 8 tables"},{"id":"http://arxiv.org/abs/2308.15126v3","updated":"2023-10-10T11:57:26Z","published":"2023-08-29T08:51:24Z","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n","authors":["Junyang Wang","Yiyang Zhou","Guohai Xu","Pengcheng Shi","Chenlin Zhao","Haiyang Xu","Qinghao Ye","Ming Yan","Ji Zhang","Jihua Zhu","Jitao Sang","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2308.15126v3.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06549v1","updated":"2023-10-10T11:51:12Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v1.pdf","comment":"23 pages, 8 tables, 8 figures"},{"id":"http://arxiv.org/abs/2310.05056v2","updated":"2023-10-10T11:18:28Z","published":"2023-10-08T07:42:41Z","title":"Language-driven Open-Vocabulary Keypoint Detection for Animal Body and\n  Face","summary":"  Current approaches for image-based keypoint detection on animal (including\nhuman) body and face are limited to specific keypoints and species. We address\nthe limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.\nIt aims to use text prompts to localize arbitrary keypoints of any species. To\naccomplish this objective, we propose Open-Vocabulary Keypoint Detection with\nSemantic-feature Matching (KDSM), which utilizes both vision and language\nmodels to harness the relationship between text and vision and thus achieve\nkeypoint detection through associating text prompt with relevant keypoint\nfeatures. Additionally, KDSM integrates domain distribution matrix matching and\nsome special designs to reinforce the relationship between language and vision,\nthereby improving the model's generalizability and performance. Extensive\nexperiments show that our proposed components bring significant performance\nimprovements, and our overall method achieves impressive results in OVKD.\nRemarkably, our method outperforms the state-of-the-art few-shot keypoint\ndetection methods using a zero-shot fashion. We will make the source code\npublicly accessible.\n","authors":["Hao Zhang","Kaipeng Zhang","Lumin Xu","Shenqi Lai","Wenqi Shao","Nanning Zheng","Ping Luo","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.05056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06525v1","updated":"2023-10-10T11:14:29Z","published":"2023-10-10T11:14:29Z","title":"Perceptual MAE for Image Manipulation Localization: A High-level Vision\n  Learner Focusing on Low-level Features","summary":"  Nowadays, multimedia forensics faces unprecedented challenges due to the\nrapid advancement of multimedia generation technology thereby making Image\nManipulation Localization (IML) crucial in the pursuit of truth. The key to IML\nlies in revealing the artifacts or inconsistencies between the tampered and\nauthentic areas, which are evident under pixel-level features. Consequently,\nexisting studies treat IML as a low-level vision task, focusing on allocating\ntampered masks by crafting pixel-level features such as image RGB noises, edge\nsignals, or high-frequency features. However, in practice, tampering commonly\noccurs at the object level, and different classes of objects have varying\nlikelihoods of becoming targets of tampering. Therefore, object semantics are\nalso vital in identifying the tampered areas in addition to pixel-level\nfeatures. This necessitates IML models to carry out a semantic understanding of\nthe entire image. In this paper, we reformulate the IML task as a high-level\nvision task that greatly benefits from low-level features. Based on such an\ninterpretation, we propose a method to enhance the Masked Autoencoder (MAE) by\nincorporating high-resolution inputs and a perceptual loss supervision module,\nwhich is termed Perceptual MAE (PMAE). While MAE has demonstrated an impressive\nunderstanding of object semantics, PMAE can also compensate for low-level\nsemantics with our proposed enhancements. Evidenced by extensive experiments,\nthis paradigm effectively unites the low-level and high-level features of the\nIML task and outperforms state-of-the-art tampering localization methods on all\nfive publicly available datasets.\n","authors":["Xiaochen Ma","Jizhe Zhou","Xiong Xu","Zhuohang Jiang","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2310.06525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04494v3","updated":"2023-10-10T11:10:59Z","published":"2023-01-11T14:42:47Z","title":"Multi-label Image Classification using Adaptive Graph Convolutional\n  Networks: from a Single Domain to Multiple Domains","summary":"  This paper proposes an adaptive graph-based approach for multi-label image\nclassification. Graph-based methods have been largely exploited in the field of\nmulti-label classification, given their ability to model label correlations.\nSpecifically, their effectiveness has been proven not only when considering a\nsingle domain but also when taking into account multiple domains. However, the\ntopology of the used graph is not optimal as it is pre-defined heuristically.\nIn addition, consecutive Graph Convolutional Network (GCN) aggregations tend to\ndestroy the feature similarity. To overcome these issues, an architecture for\nlearning the graph connectivity in an end-to-end fashion is introduced. This is\ndone by integrating an attention-based mechanism and a similarity-preserving\nstrategy. The proposed framework is then extended to multiple domains using an\nadversarial training scheme. Numerous experiments are reported on well-known\nsingle-domain and multi-domain benchmarks. The results demonstrate that our\napproach achieves competitive results in terms of mean Average Precision (mAP)\nand model size as compared to the state-of-the-art. The code will be made\npublicly available.\n","authors":["Indel Pal Singh","Enjie Ghorbel","Oyebade Oyedotun","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2301.04494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06522v1","updated":"2023-10-10T11:08:31Z","published":"2023-10-10T11:08:31Z","title":"Watt For What: Rethinking Deep Learning's Energy-Performance\n  Relationship","summary":"  Deep learning models have revolutionized various fields, from image\nrecognition to natural language processing, by achieving unprecedented levels\nof accuracy. However, their increasing energy consumption has raised concerns\nabout their environmental impact, disadvantaging smaller entities in research\nand exacerbating global energy consumption. In this paper, we explore the\ntrade-off between model accuracy and electricity consumption, proposing a\nmetric that penalizes large consumption of electricity. We conduct a\ncomprehensive study on the electricity consumption of various deep learning\nmodels across different GPUs, presenting a detailed analysis of their\naccuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity\nconsumed, we demonstrate how smaller, more energy-efficient models can\nsignificantly expedite research while mitigating environmental concerns. Our\nresults highlight the potential for a more sustainable approach to deep\nlearning, emphasizing the importance of optimizing models for efficiency. This\nresearch also contributes to a more equitable research landscape, where smaller\nentities can compete effectively with larger counterparts. This advocates for\nthe adoption of efficient deep learning practices to reduce electricity\nconsumption, safeguarding the environment for future generations whilst also\nhelping ensure a fairer competitive landscape.\n","authors":["Shreyank N Gowda","Xinyue Hao","Gen Li","Laura Sevilla-Lara","Shashank Narayana Gowda"],"pdf_url":"https://arxiv.org/pdf/2310.06522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05136v2","updated":"2023-10-10T11:04:26Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v2.pdf","comment":"27 pages (include appendix), technical report"},{"id":"http://arxiv.org/abs/2310.05371v2","updated":"2023-10-10T10:55:10Z","published":"2023-10-09T03:00:15Z","title":"Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using\n  mpMRI Segmentation and Classification","summary":"  Prostate cancer (PCa) is a severe disease among men globally. It is important\nto identify PCa early and make a precise diagnosis for effective treatment. For\nPCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as\nan invaluable imaging modality that offers a precise anatomical view of the\nprostate gland and its tissue structure. Deep learning (DL) models can enhance\nexisting clinical systems and improve patient care by locating regions of\ninterest for physicians. Recently, DL techniques have been employed to develop\na pipeline for segmenting and classifying different cancer types. These studies\nshow that DL can be used to increase diagnostic precision and give objective\nresults without variability. This work uses well-known DL models for the\nclassification and segmentation of mpMRI images to detect PCa. Our\nimplementation involves four pipelines; Semantic DeepSegNet with ResNet50,\nDeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with\na long short-term memory (LSTM). Each segmentation model is paired with a\ndifferent classifier to evaluate the performance using different metrics. The\nresults of our experiments show that the pipeline that uses the combination of\nU-Net and the LSTM model outperforms all other combinations, excelling in both\nsegmentation and classification tasks.\n","authors":["Anil B. Gavade","Neel Kanwal","Priyanka A. Gavade","Rajendra Nerli"],"pdf_url":"https://arxiv.org/pdf/2310.05371v2.pdf","comment":"Accepted at CISCON-2023"},{"id":"http://arxiv.org/abs/2304.08072v2","updated":"2023-10-10T10:53:29Z","published":"2023-04-17T08:34:41Z","title":"Two-stage MR Image Segmentation Method for Brain Tumors based on\n  Attention Mechanism","summary":"  Multimodal magnetic resonance imaging (MRI) can reveal different patterns of\nhuman tissue and is crucial for clinical diagnosis. However, limited by cost,\nnoise and manual labeling, obtaining diverse and reliable multimodal MR images\nremains a challenge. For the same lesion, different MRI manifestations have\ngreat differences in background information, coarse positioning and fine\nstructure. In order to obtain better generation and segmentation performance, a\ncoordination-spatial attention generation adversarial network (CASP-GAN) based\non the cycle-consistent generative adversarial network (CycleGAN) is proposed.\nThe performance of the generator is optimized by introducing the Coordinate\nAttention (CA) module and the Spatial Attention (SA) module. The two modules\ncan make full use of the captured location information, accurately locating the\ninterested region, and enhancing the generator model network structure. The\nability to extract the structure information and the detailed information of\nthe original medical image can help generate the desired image with higher\nquality. There exist some problems in the original CycleGAN that the training\ntime is long, the parameter amount is too large, and it is difficult to\nconverge. In response to this problem, we introduce the Coordinate Attention\n(CA) module to replace the Res Block to reduce the number of parameters, and\ncooperate with the spatial information extraction network above to strengthen\nthe information extraction ability. On the basis of CASP-GAN, an attentional\ngenerative cross-modality segmentation (AGCMS) method is further proposed. This\nmethod inputs the modalities generated by CASP-GAN and the real modalities into\nthe segmentation network for brain tumor segmentation. Experimental results\nshow that CASP-GAN outperforms CycleGAN and some state-of-the-art methods in\nPSNR, SSMI and RMSE in most tasks.\n","authors":["Li Zhu","Jiawei Jiang","Lin Lu","Jin Li"],"pdf_url":"https://arxiv.org/pdf/2304.08072v2.pdf","comment":"Some contributing authors are not signed"},{"id":"http://arxiv.org/abs/2309.12559v2","updated":"2023-10-10T10:14:28Z","published":"2023-09-22T01:06:16Z","title":"Invariant Learning via Probability of Sufficient and Necessary Causes","summary":"  Out-of-distribution (OOD) generalization is indispensable for learning models\nin the wild, where testing distribution typically unknown and different from\nthe training. Recent methods derived from causality have shown great potential\nin achieving OOD generalization. However, existing methods mainly focus on the\ninvariance property of causes, while largely overlooking the property of\n\\textit{sufficiency} and \\textit{necessity} conditions. Namely, a necessary but\ninsufficient cause (feature) is invariant to distribution shift, yet it may not\nhave required accuracy. By contrast, a sufficient yet unnecessary cause\n(feature) tends to fit specific data well but may have a risk of adapting to a\nnew domain. To capture the information of sufficient and necessary causes, we\nemploy a classical concept, the probability of sufficiency and necessary causes\n(PNS), which indicates the probability of whether one is the necessary and\nsufficient cause. To associate PNS with OOD generalization, we propose PNS risk\nand formulate an algorithm to learn representation with a high PNS value. We\ntheoretically analyze and prove the generalizability of the PNS risk.\nExperiments on both synthetic and real-world benchmarks demonstrate the\neffectiveness of the proposed method. The details of the implementation can be\nfound at the GitHub repository: https://github.com/ymy4323460/CaSN.\n","authors":["Mengyue Yang","Zhen Fang","Yonggang Zhang","Yali Du","Furui Liu","Jean-Francois Ton","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2309.12559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05314v2","updated":"2023-10-10T10:09:27Z","published":"2023-05-09T10:06:37Z","title":"CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and\n  Subtyping in Whole Slide Images","summary":"  The visual examination of tissue biopsy sections is fundamental for cancer\ndiagnosis, with pathologists analyzing sections at multiple magnifications to\ndiscern tumor cells and their subtypes. However, existing attention-based\nmultiple instance learning (MIL) models, used for analyzing Whole Slide Images\n(WSIs) in cancer diagnostics, often overlook the contextual information of\ntumor and neighboring tiles, leading to misclassifications. To address this, we\npropose the Context-Aware Multiple Instance Learning (CAMIL) architecture.\nCAMIL incorporates neighbor-constrained attention to consider dependencies\namong tiles within a WSI and integrates contextual constraints as prior\nknowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell\nlung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16) metastasis,\nachieving test AUCs of 0.959\\% and 0.975\\%, respectively, outperforming other\nstate-of-the-art methods. Additionally, CAMIL enhances model interpretability\nby identifying regions of high diagnostic value.\n","authors":["Olga Fourkioti","Matt De Vries","Chris Bakal"],"pdf_url":"https://arxiv.org/pdf/2305.05314v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.02014v2","updated":"2023-10-10T10:03:24Z","published":"2022-12-05T04:04:21Z","title":"Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding","summary":"  Automatic parsing of human anatomies at instance-level from 3D computed\ntomography (CT) scans is a prerequisite step for many clinical applications.\nThe presence of pathologies, broken structures or limited field-of-view (FOV)\nall can make anatomy parsing algorithms vulnerable. In this work, we explore\nhow to exploit and conduct the prosperous detection-then-segmentation paradigm\nin 3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering complicated shapes, sizes and orientations of anatomies,\nwithout lose of generality, we present the nine degrees-of-freedom (9-DoF) pose\nestimation solution in full 3D space using a novel single-stage,\nnon-hierarchical forward representation. Our whole framework is executed in a\nsteerable manner where any anatomy of interest can be directly retrieved to\nfurther boost the inference efficiency. We have validated the proposed method\non three medical imaging parsing tasks of ribs, spine, and abdominal organs.\nFor rib parsing, CT scans have been annotated at the rib instance-level for\nquantitative evaluation, similarly for spine vertebrae and abdominal organs.\nExtensive experiments on 9-DoF box detection and rib instance segmentation\ndemonstrate the effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%) in high efficiency, compared\nfavorably against several strong baselines (e.g., CenterNet, FCOS, and\nnnU-Net). For spine identification and segmentation, our method achieves a new\nstate-of-the-art result on the public CTSpine1K dataset. Last, we report highly\ncompetitive results in multi-organ segmentation at FLARE22 competition. Our\nannotations, code and models will be made publicly available at:\nhttps://github.com/alibaba-damo-academy/Med_Query.\n","authors":["Heng Guo","Jianfeng Zhang","Ke Yan","Le Lu","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2212.02014v2.pdf","comment":"updated version"},{"id":"http://arxiv.org/abs/2310.06489v1","updated":"2023-10-10T09:57:19Z","published":"2023-10-10T09:57:19Z","title":"Deep Learning for Automatic Detection and Facial Recognition in Japanese\n  Macaques: Illuminating Social Networks","summary":"  Individual identification plays a pivotal role in ecology and ethology,\nnotably as a tool for complex social structures understanding. However,\ntraditional identification methods often involve invasive physical tags and can\nprove both disruptive for animals and time-intensive for researchers. In recent\nyears, the integration of deep learning in research offered new methodological\nperspectives through automatization of complex tasks. Harnessing object\ndetection and recognition technologies is increasingly used by researchers to\nachieve identification on video footage. This study represents a preliminary\nexploration into the development of a non-invasive tool for face detection and\nindividual identification of Japanese macaques (Macaca fuscata) through deep\nlearning. The ultimate goal of this research is, using identifications done on\nthe dataset, to automatically generate a social network representation of the\nstudied population. The current main results are promising: (i) the creation of\na Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%\naccuracy and (ii) the creation of an individual recognizer for K{\\=o}jima\nisland macaques population (YOLOv8n model), reaching a 83% accuracy. We also\ncreated a K{\\=o}jima population social network by traditional methods, based on\nco-occurrences on videos. Thus, we provide a benchmark against which the\nautomatically generated network will be assessed for reliability. These\npreliminary results are a testament to the potential of this innovative\napproach to provide the scientific community with a tool for tracking\nindividuals and social network studies in Japanese macaques.\n","authors":["Julien Paulet","Axel Molina","Benjamin Beltzung","Takafumi Suzumura","Shinya Yamamoto","Cdric Sueur"],"pdf_url":"https://arxiv.org/pdf/2310.06489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v1","updated":"2023-10-10T09:57:17Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06486v1","updated":"2023-10-10T09:53:59Z","published":"2023-10-10T09:53:59Z","title":"Topological RANSAC for instance verification and retrieval without\n  fine-tuning","summary":"  This paper presents an innovative approach to enhancing explainable image\nretrieval, particularly in situations where a fine-tuning set is unavailable.\nThe widely-used SPatial verification (SP) method, despite its efficacy, relies\non a spatial model and the hypothesis-testing strategy for instance\nrecognition, leading to inherent limitations, including the assumption of\nplanar structures and neglect of topological relations among features. To\naddress these shortcomings, we introduce a pioneering technique that replaces\nthe spatial model with a topological one within the RANSAC process. We propose\nbio-inspired saccade and fovea functions to verify the topological consistency\namong features, effectively circumventing the issues associated with SP's\nspatial model. Our experimental results demonstrate that our method\nsignificantly outperforms SP, achieving state-of-the-art performance in\nnon-fine-tuning retrieval. Furthermore, our approach can enhance performance\nwhen used in conjunction with fine-tuned features. Importantly, our method\nretains high explainability and is lightweight, offering a practical and\nadaptable solution for a variety of real-world applications.\n","authors":["Guoyuan An","Juhyung Seon","Inkyu An","Yuchi Huo","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.06486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11643v3","updated":"2023-10-10T09:45:30Z","published":"2023-07-21T15:22:32Z","title":"Morphological Image Analysis and Feature Extraction for Reasoning with\n  AI-based Defect Detection and Classification Models","summary":"  As the use of artificial intelligent (AI) models becomes more prevalent in\nindustries such as engineering and manufacturing, it is essential that these\nmodels provide transparent reasoning behind their predictions. This paper\nproposes the AI-Reasoner, which extracts the morphological characteristics of\ndefects (DefChars) from images and utilises decision trees to reason with the\nDefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.\ncharts) and textual explanations to provide insights into outputs made by\nmasked-based defect detection and classification models. It also provides\neffective mitigation strategies to enhance data pre-processing and overall\nmodel performance. The AI-Reasoner was tested on explaining the outputs of an\nIE Mask R-CNN model using a set of 366 images containing defects. The results\ndemonstrated its effectiveness in explaining the IE Mask R-CNN model's\npredictions. Overall, the proposed AI-Reasoner provides a solution for\nimproving the performance of AI models in industrial applications that require\ndefect analysis.\n","authors":["Jiajun Zhang","Georgina Cosma","Sarah Bugby","Axel Finke","Jason Watkins"],"pdf_url":"https://arxiv.org/pdf/2307.11643v3.pdf","comment":"8 pages, 3 figures, 5 tables; accepted in 2023 IEEE symposium series\n  on computational intelligence (SSCI)"},{"id":"http://arxiv.org/abs/2308.08231v2","updated":"2023-10-10T09:41:24Z","published":"2023-08-16T09:06:32Z","title":"DDF-HO: Hand-Held Object Reconstruction via Conditional Directed\n  Distance Field","summary":"  Reconstructing hand-held objects from a single RGB image is an important and\nchallenging problem. Existing works utilizing Signed Distance Fields (SDF)\nreveal limitations in comprehensively capturing the complex hand-object\ninteractions, since SDF is only reliable within the proximity of the target,\nand hence, infeasible to simultaneously encode local hand and object cues. To\naddress this issue, we propose DDF-HO, a novel approach leveraging Directed\nDistance Field (DDF) as the shape representation. Unlike SDF, DDF maps a ray in\n3D space, consisting of an origin and a direction, to corresponding DDF values,\nincluding a binary visibility signal determining whether the ray intersects the\nobjects and a distance value measuring the distance from origin to target in\nthe given direction. We randomly sample multiple rays and collect local to\nglobal geometric features for them by introducing a novel 2D ray-based feature\naggregation scheme and a 3D intersection-aware hand pose embedding, combining\n2D-3D features to model hand-object interactions. Extensive experiments on\nsynthetic and real-world datasets demonstrate that DDF-HO consistently\noutperforms all baseline methods by a large margin, especially under Chamfer\nDistance, with about $80\\%$ leap forward. Codes are available at\n\\url{https://github.com/ZhangCYG/DDFHO}.\n","authors":["Chenyangguang Zhang","Yan Di","Ruida Zhang","Guangyao Zhai","Fabian Manhardt","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2308.08231v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06470v1","updated":"2023-10-10T09:41:13Z","published":"2023-10-10T09:41:13Z","title":"Focus on Local Regions for Query-based Object Detection","summary":"  Query-based methods have garnered significant attention in object detection\nsince the advent of DETR, the pioneering end-to-end query-based detector.\nHowever, these methods face challenges like slow convergence and suboptimal\nperformance. Notably, self-attention in object detection often hampers\nconvergence due to its global focus. To address these issues, we propose FoLR,\na transformer-like architecture with only decoders. We enhance the\nself-attention mechanism by isolating connections between irrelevant objects\nthat makes it focus on local regions but not global regions. We also design the\nadaptive sampling method to extract effective features based on queries' local\nregions from feature maps. Additionally, we employ a look-back strategy for\ndecoders to retain prior information, followed by the Feature Mixer module to\nfuse features and queries. Experimental results demonstrate FoLR's\nstate-of-the-art performance in query-based detectors, excelling in convergence\nspeed and computational efficiency.\n","authors":["Hongbin Xu","Yamei Xia","Shuai Zhao","Bo Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.06470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06468v1","updated":"2023-10-10T09:39:38Z","published":"2023-10-10T09:39:38Z","title":"A Geometrical Approach to Evaluate the Adversarial Robustness of Deep\n  Neural Networks","summary":"  Deep Neural Networks (DNNs) are widely used for computer vision tasks.\nHowever, it has been shown that deep models are vulnerable to adversarial\nattacks, i.e., their performances drop when imperceptible perturbations are\nmade to the original inputs, which may further degrade the following visual\ntasks or introduce new problems such as data and privacy security. Hence,\nmetrics for evaluating the robustness of deep models against adversarial\nattacks are desired. However, previous metrics are mainly proposed for\nevaluating the adversarial robustness of shallow networks on the small-scale\ndatasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness\n(CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet\ndataset), it is computationally expensive and its performance relies on a\ntractable number of samples. In this paper, we propose the Adversarial\nConverging Time Score (ACTS), an attack-dependent metric that quantifies the\nadversarial robustness of a DNN on a specific input. Our key observation is\nthat local neighborhoods on a DNN's output surface would have different shapes\ngiven different inputs. Hence, given different inputs, it requires different\ntime for converging to an adversarial sample. Based on this geometry meaning,\nACTS measures the converging time as an adversarial robustness metric. We\nvalidate the effectiveness and generalization of the proposed ACTS metric\nagainst different adversarial attacks on the large-scale ImageNet dataset using\nstate-of-the-art deep networks. Extensive experiments show that our ACTS metric\nis an efficient and effective adversarial metric over the previous CLEVER\nmetric.\n","authors":["Yang Wang","Bo Dong","Ke Xu","Haiyin Piao","Yufei Ding","Baocai Yin","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06468v1.pdf","comment":"ACM Transactions on Multimedia Computing, Communications, and\n  Applications (ACM TOMM)"},{"id":"http://arxiv.org/abs/2305.07336v3","updated":"2023-10-10T09:25:33Z","published":"2023-05-12T09:28:09Z","title":"MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with\n  Bird's Eye View based Appearance and Motion Features","summary":"  Identifying moving objects is an essential capability for autonomous systems,\nas it provides critical information for pose estimation, navigation, collision\navoidance, and static map construction. In this paper, we present MotionBEV, a\nfast and accurate framework for LiDAR moving object segmentation, which\nsegments moving objects with appearance and motion features in the bird's eye\nview (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV\nrepresentation to improve computational efficiency. Specifically, we learn\nappearance features with a simplified PointNet and compute motion features\nthrough the height differences of consecutive frames of point clouds projected\nonto vertical columns in the polar BEV coordinate system. We employ a\ndual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)\nto adaptively fuse the spatio-temporal information from appearance and motion\nfeatures. Our approach achieves state-of-the-art performance on the\nSemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical\neffectiveness of our method, we provide a LiDAR-MOS dataset recorded by a\nsolid-state LiDAR, which features non-repetitive scanning patterns and a small\nfield of view.\n","authors":["Bo Zhou","Jiapeng Xie","Yan Pan","Jiajie Wu","Chuanzhao Lu"],"pdf_url":"https://arxiv.org/pdf/2305.07336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13150v3","updated":"2023-10-10T09:17:14Z","published":"2023-08-25T03:08:41Z","title":"Enhancing Breast Cancer Classification Using Transfer ResNet with\n  Lightweight Attention Mechanism","summary":"  Despite the remarkable results of deep learning in breast cancer image\nclassification, challenges such as data imbalance and interpretability still\nexist and require cross-domain knowledge and collaboration among medical\nexperts. In this study, we propose a dual-activated lightweight attention\nResNet50 module method-based breast cancer classification method that\neffectively addresses challenges such as data imbalance and interpretability.\nOur model fuses a pre-trained deep ResNet50 and a lightweight attention\nmechanism to accomplish classification by embedding an attention module in\nlayer 4 of ResNet50 and adding two fully connected layers. For the fully\nconnected network design, we employ both Leaky ReLU and ReLU activation\nfunctions. On medical histopathology datasets, our model outperforms\nconventional models, visual transformers, and large models in terms of\nprecision, accuracy, recall, F1 score, and GMean. In particular, the model\ndemonstrates significant robustness and broad applicability when dealing with\nthe unbalanced breast cancer dataset. Our model is tested on 40X, 100X, 200X,\nand 400X images and achieves accuracies of 98.5%, 98.7%, 97.9%, and 94.3%,\nrespectively. Through an in-depth analysis of loss and accuracy, as well as\nGrad-CAM analysis, we comprehensively assessed the model performance and gained\nperspective on its training process. In the later stages of training, the\nvalidated losses and accuracies change minimally, showing that the model avoids\noverfitting and exhibits good generalization ability. Overall, this study\nprovides an effective solution for breast cancer image classification with\npractical applica\n","authors":["Suxing Liu"],"pdf_url":"https://arxiv.org/pdf/2308.13150v3.pdf","comment":"13 pages, 8 figures,6 tables"},{"id":"http://arxiv.org/abs/2310.00369v2","updated":"2023-10-10T09:12:37Z","published":"2023-09-30T13:21:29Z","title":"Distilling Inductive Bias: Knowledge Distillation Beyond Model\n  Compression","summary":"  With the rapid development of computer vision, Vision Transformers (ViTs)\noffer the tantalizing prospect of unified information processing across visual\nand textual domains. But due to the lack of inherent inductive biases in ViTs,\nthey require enormous amount of data for training. To make their applications\npractical, we introduce an innovative ensemble-based distillation approach\ndistilling inductive bias from complementary lightweight teacher models. Prior\nsystems relied solely on convolution-based teaching. However, this method\nincorporates an ensemble of light teachers with different architectural\ntendencies, such as convolution and involution, to instruct the student\ntransformer jointly. Because of these unique inductive biases, instructors can\naccumulate a wide range of knowledge, even from readily identifiable stored\ndatasets, which leads to enhanced student performance. Our proposed framework\nalso involves precomputing and storing logits in advance, essentially the\nunnormalized predictions of the model. This optimization can accelerate the\ndistillation process by eliminating the need for repeated forward passes during\nknowledge distillation, significantly reducing the computational burden and\nenhancing efficiency.\n","authors":["Gousia Habib","Tausifa Jan Saleem","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2310.00369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06440v1","updated":"2023-10-10T09:12:27Z","published":"2023-10-10T09:12:27Z","title":"Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic\n  Reasoning Task 2023","summary":"  In this paper, we present our solution to a Multi-modal Algorithmic Reasoning\nTask: SMART-101 Challenge. Different from the traditional visual\nquestion-answering datasets, this challenge evaluates the abstraction,\ndeduction, and generalization abilities of neural networks in solving\nvisuolinguistic puzzles designed specifically for children in the 6-8 age\ngroup. We employed a divide-and-conquer approach. At the data level, inspired\nby the challenge paper, we categorized the whole questions into eight types and\nutilized the llama-2-chat model to directly generate the type for each question\nin a zero-shot manner. Additionally, we trained a yolov7 model on the icon45\ndataset for object detection and combined it with the OCR method to recognize\nand locate objects and text within the images. At the model level, we utilized\nthe BLIP-2 model and added eight adapters to the image encoder VIT-G to\nadaptively extract visual features for different question types. We fed the\npre-constructed question templates as input and generated answers using the\nflan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an\naccuracy score of 26.5 on the validation set and 24.30 on the private test set.\n","authors":["Xiangyu Wu","Yang Yang","Shengdong Xu","Yifeng Wu","Qingguo Chen","Jianfeng Lu"],"pdf_url":"https://arxiv.org/pdf/2310.06440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12301v2","updated":"2023-10-10T09:08:21Z","published":"2023-09-21T17:58:26Z","title":"Environment-biased Feature Ranking for Novelty Detection Robustness","summary":"  We tackle the problem of robust novelty detection, where we aim to detect\nnovelties in terms of semantic content while being invariant to changes in\nother, irrelevant factors. Specifically, we operate in a setup with multiple\nenvironments, where we determine the set of features that are associated more\nwith the environments, rather than to the content relevant for the task. Thus,\nwe propose a method that starts with a pretrained embedding and a multi-env\nsetup and manages to rank the features based on their environment-focus. First,\nwe compute a per-feature score based on the feature distribution variance\nbetween envs. Next, we show that by dropping the highly scored ones, we manage\nto remove spurious correlations and improve the overall performance by up to\n6%, both in covariance and sub-population shift cases, both for a real and a\nsynthetic benchmark, that we introduce for this task.\n","authors":["Stefan Smeu","Elena Burceanu","Emanuela Haller","Andrei Liviu Nicolicioiu"],"pdf_url":"https://arxiv.org/pdf/2309.12301v2.pdf","comment":"The updated, long version of the paper is available at\n  arXiv:2310.03738"},{"id":"http://arxiv.org/abs/2310.06437v1","updated":"2023-10-10T09:06:39Z","published":"2023-10-10T09:06:39Z","title":"Skeleton Ground Truth Extraction: Methodology, Annotation Tool and\n  Benchmarks","summary":"  Skeleton Ground Truth (GT) is critical to the success of supervised skeleton\nextraction methods, especially with the popularity of deep learning techniques.\nFurthermore, we see skeleton GTs used not only for training skeleton detectors\nwith Convolutional Neural Networks (CNN) but also for evaluating\nskeleton-related pruning and matching algorithms. However, most existing shape\nand image datasets suffer from the lack of skeleton GT and inconsistency of GT\nstandards. As a result, it is difficult to evaluate and reproduce CNN-based\nskeleton detectors and algorithms on a fair basis. In this paper, we present a\nheuristic strategy for object skeleton GT extraction in binary shapes and\nnatural images. Our strategy is built on an extended theory of diagnosticity\nhypothesis, which enables encoding human-in-the-loop GT extraction based on\nclues from the target's context, simplicity, and completeness. Using this\nstrategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing\nshape and image datasets. The GTs are then structurally evaluated with\nrepresentative methods to build viable baselines for fair comparisons.\nExperiments demonstrate that GTs generated by our strategy yield promising\nquality with respect to standard consistency, and also provide a balance\nbetween simplicity and completeness.\n","authors":["Cong Yang","Bipin Indurkhya","John See","Bo Gao","Yan Ke","Zeyd Boukhers","Zhenyu Yang","Marcin Grzegorzek"],"pdf_url":"https://arxiv.org/pdf/2310.06437v1.pdf","comment":"Accepted for publication in the International Journal of Computer\n  Vision (IJCV)"},{"id":"http://arxiv.org/abs/2310.06433v1","updated":"2023-10-10T09:03:01Z","published":"2023-10-10T09:03:01Z","title":"Retromorphic Testing: A New Approach to the Test Oracle Problem","summary":"  A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.\n","authors":["Boxi Yu","Qiuyang Mang","Qingshuo Guo","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2310.06433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03504v2","updated":"2023-10-10T09:01:57Z","published":"2023-09-07T06:27:39Z","title":"Stroke-based Neural Painting and Stylization with Dynamically Predicted\n  Painting Region","summary":"  Stroke-based rendering aims to recreate an image with a set of strokes. Most\nexisting methods render complex images using an uniform-block-dividing\nstrategy, which leads to boundary inconsistency artifacts. To solve the\nproblem, we propose Compositional Neural Painter, a novel stroke-based\nrendering framework which dynamically predicts the next painting region based\non the current canvas, instead of dividing the image plane uniformly into\npainting regions. We start from an empty canvas and divide the painting process\ninto several steps. At each step, a compositor network trained with a phasic RL\nstrategy first predicts the next painting region, then a painter network\ntrained with a WGAN discriminator predicts stroke parameters, and a stroke\nrenderer paints the strokes onto the painting region of the current canvas.\nMoreover, we extend our method to stroke-based style transfer with a novel\ndifferentiable distance transform loss, which helps preserve the structure of\nthe input image during stroke-based stylization. Extensive experiments show our\nmodel outperforms the existing models in both stroke-based neural painting and\nstroke-based stylization. Code is available at\nhttps://github.com/sjtuplayer/Compositional_Neural_Painter\n","authors":["Teng Hu","Ran Yi","Haokun Zhu","Liang Liu","Jinlong Peng","Yabiao Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2309.03504v2.pdf","comment":"ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.06430v1","updated":"2023-10-10T08:54:14Z","published":"2023-10-10T08:54:14Z","title":"Conformal Prediction for Deep Classifier via Label Ranking","summary":"  Conformal prediction is a statistical framework that generates prediction\nsets containing ground-truth labels with a desired coverage guarantee. The\npredicted probabilities produced by machine learning models are generally\nmiscalibrated, leading to large prediction sets in conformal prediction. In\nthis paper, we empirically and theoretically show that disregarding the\nprobabilities' value will mitigate the undesirable effect of miscalibrated\nprobability values. Then, we propose a novel algorithm named $\\textit{Sorted\nAdaptive prediction sets}$ (SAPS), which discards all the probability values\nexcept for the maximum softmax probability. The key idea behind SAPS is to\nminimize the dependence of the non-conformity score on the probability values\nwhile retaining the uncertainty information. In this manner, SAPS can produce\nsets of small size and communicate instance-wise uncertainty. Theoretically, we\nprovide a finite-sample coverage guarantee of SAPS and show that the expected\nvalue of set size from SAPS is always smaller than APS. Extensive experiments\nvalidate that SAPS not only lessens the prediction sets but also broadly\nenhances the conditional coverage rate and adaptation of prediction sets.\n","authors":["Jianguo Huang","Huajun Xi","Linjun Zhang","Huaxiu Yao","Yue Qiu","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2310.06430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06420v1","updated":"2023-10-10T08:44:47Z","published":"2023-10-10T08:44:47Z","title":"AnoDODE: Anomaly Detection with Diffusion ODE","summary":"  Anomaly detection is the process of identifying atypical data samples that\nsignificantly deviate from the majority of the dataset. In the realm of\nclinical screening and diagnosis, detecting abnormalities in medical images\nholds great importance. Typically, clinical practice provides access to a vast\ncollection of normal images, while abnormal images are relatively scarce. We\nhypothesize that abnormal images and their associated features tend to manifest\nin low-density regions of the data distribution. Following this assumption, we\nturn to diffusion ODEs for unsupervised anomaly detection, given their\ntractability and superior performance in density estimation tasks. More\nprecisely, we propose a new anomaly detection method based on diffusion ODEs by\nestimating the density of features extracted from multi-scale medical images.\nOur anomaly scoring mechanism depends on computing the negative log-likelihood\nof features extracted from medical images at different scales, quantified in\nbits per dimension. Furthermore, we propose a reconstruction-based anomaly\nlocalization suitable for our method. Our proposed method not only identifie\nanomalies but also provides interpretability at both the image and pixel\nlevels. Through experiments on the BraTS2021 medical dataset, our proposed\nmethod outperforms existing methods. These results confirm the effectiveness\nand robustness of our method.\n","authors":["Xianyao Hu","Congming Jin"],"pdf_url":"https://arxiv.org/pdf/2310.06420v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.04441v2","updated":"2023-10-10T08:33:24Z","published":"2023-04-10T07:57:24Z","title":"Self-training with dual uncertainty for semi-supervised medical image\n  segmentation","summary":"  In the field of semi-supervised medical image segmentation, the shortage of\nlabeled data is the fundamental problem. How to effectively learn image\nfeatures from unlabeled images to improve segmentation accuracy is the main\nresearch direction in this field. Traditional self-training methods can\npartially solve the problem of insufficient labeled data by generating pseudo\nlabels for iterative training. However, noise generated due to the model's\nuncertainty during training directly affects the segmentation results.\nTherefore, we added sample-level and pixel-level uncertainty to stabilize the\ntraining process based on the self-training framework. Specifically, we saved\nseveral moments of the model during pre-training, and used the difference\nbetween their predictions on unlabeled samples as the sample-level uncertainty\nestimate for that sample. Then, we gradually add unlabeled samples from easy to\nhard during training. At the same time, we added a decoder with different\nupsampling methods to the segmentation network and used the difference between\nthe outputs of the two decoders as pixel-level uncertainty. In short, we\nselectively retrained unlabeled samples and assigned pixel-level uncertainty to\npseudo labels to optimize the self-training process. We compared the\nsegmentation results of our model with five semi-supervised approaches on the\npublic 2017 ACDC dataset and 2018 Prostate dataset. Our proposed method\nachieves better segmentation performance on both datasets under the same\nsettings, demonstrating its effectiveness, robustness, and potential\ntransferability to other medical image segmentation tasks. Keywords: Medical\nimage segmentation, semi-supervised learning, self-training, uncertainty\nestimation\n","authors":["Zhanhong Qiu","Haitao Gan","Ming Shi","Zhongwei Huang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.04441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03063v2","updated":"2023-10-10T08:30:08Z","published":"2023-09-06T15:05:04Z","title":"Prompt-based Ingredient-Oriented All-in-One Image Restoration","summary":"  Image restoration aims to recover the high-quality images from their degraded\nobservations. Since most existing methods have been dedicated into single\ndegradation removal, they may not yield optimal results on other types of\ndegradations, which do not satisfy the applications in real world scenarios. In\nthis paper, we propose a novel data ingredient-oriented approach that leverages\nprompt-based learning to enable a single model to efficiently tackle multiple\nimage degradation tasks. Specifically, we utilize a encoder to capture features\nand introduce prompts with degradation-specific information to guide the\ndecoder in adaptively recovering images affected by various degradations. In\norder to model the local invariant properties and non-local information for\nhigh-quality image restoration, we combined CNNs operations and Transformers.\nSimultaneously, we made several key designs in the Transformer blocks\n(multi-head rearranged attention with prompts and simple-gate feed-forward\nnetwork) to reduce computational requirements and selectively determines what\ninformation should be persevered to facilitate efficient recovery of\npotentially sharp images. Furthermore, we incorporate a feature fusion\nmechanism further explores the multi-scale information to improve the\naggregated features. The resulting tightly interlinked hierarchy architecture,\nnamed as CAPTNet, extensive experiments demonstrate that our method performs\ncompetitively to the state-of-the-art.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2309.03063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06403v1","updated":"2023-10-10T08:14:24Z","published":"2023-10-10T08:14:24Z","title":"Boundary Discretization and Reliable Classification Network for Temporal\n  Action Detection","summary":"  Temporal action detection aims to recognize the action category and determine\nthe starting and ending time of each action instance in untrimmed videos. The\nmixed methods have achieved remarkable performance by simply merging\nanchor-based and anchor-free approaches. However, there are still two crucial\nissues in the mixed framework: (1) Brute-force merging and handcrafted anchors\ndesign affect the performance and practical application of the mixed methods.\n(2) A large number of false positives in action category predictions further\nimpact the detection performance. In this paper, we propose a novel Boundary\nDiscretization and Reliable Classification Network (BDRC-Net) that addresses\nthe above issues by introducing boundary discretization and reliable\nclassification modules. Specifically, the boundary discretization module (BDM)\nelegantly merges anchor-based and anchor-free approaches in the form of\nboundary discretization, avoiding the handcrafted anchors design required by\ntraditional mixed methods. Furthermore, the reliable classification module\n(RCM) predicts reliable action categories to reduce false positives in action\ncategory predictions. Extensive experiments conducted on different benchmarks\ndemonstrate that our proposed method achieves favorable performance compared\nwith the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%\non THUMOS'14, outperforming the previous best by 1.5%. The code will be\nreleased at https://github.com/zhenyingfang/BDRC-Net.\n","authors":["Zhenying Fang"],"pdf_url":"https://arxiv.org/pdf/2310.06403v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.01405v3","updated":"2023-10-10T08:00:53Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v3.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2310.06389v1","updated":"2023-10-10T07:52:30Z","published":"2023-10-10T07:52:30Z","title":"Learning Stackable and Skippable LEGO Bricks for Efficient,\n  Reconfigurable, and Variable-Resolution Diffusion Modeling","summary":"  Diffusion models excel at generating photo-realistic images but come with\nsignificant computational costs in both training and sampling. While various\ntechniques address these computational challenges, a less-explored issue is\ndesigning an efficient and adaptable network backbone for iterative refinement.\nCurrent options like U-Net and Vision Transformer often rely on\nresource-intensive deep networks and lack the flexibility needed for generating\nimages at variable resolutions or with a smaller network than used in training.\nThis study introduces LEGO bricks, which seamlessly integrate Local-feature\nEnrichment and Global-content Orchestration. These bricks can be stacked to\ncreate a test-time reconfigurable diffusion backbone, allowing selective\nskipping of bricks to reduce sampling costs and generate higher-resolution\nimages than the training data. LEGO bricks enrich local regions with an MLP and\ntransform them using a Transformer block while maintaining a consistent\nfull-resolution image across all bricks. Experimental results demonstrate that\nLEGO bricks enhance training efficiency, expedite convergence, and facilitate\nvariable-resolution image generation while maintaining strong generative\nperformance. Moreover, LEGO significantly reduces sampling time compared to\nother methods, establishing it as a valuable enhancement for diffusion models.\n","authors":["Huangjie Zheng","Zhendong Wang","Jianbo Yuan","Guanghan Ning","Pengcheng He","Quanzeng You","Hongxia Yang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06385v1","updated":"2023-10-10T07:48:40Z","published":"2023-10-10T07:48:40Z","title":"3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic\n  Indoor Environments","summary":"  The existence of variable factors within the environment can cause a decline\nin camera localization accuracy, as it violates the fundamental assumption of a\nstatic environment in Simultaneous Localization and Mapping (SLAM) algorithms.\nRecent semantic SLAM systems towards dynamic environments either rely solely on\n2D semantic information, or solely on geometric information, or combine their\nresults in a loosely integrated manner. In this research paper, we introduce\n3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object\ndetection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic\nand geometric constraints sequentially. We designed a 3D part-aware hybrid\ntransformer for point cloud-based object detection to identify dynamic objects.\nSubsequently, we propose a dynamic feature filter based on HDBSCAN clustering\nto extract objects with significant absolute depth differences. When compared\nagainst ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across\nthe dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the\nperformance of the other four leading SLAM systems designed for dynamic\nenvironments.\n","authors":["Ghanta Sai Krishna","Kundrapu Supriya","Sabur Baidya"],"pdf_url":"https://arxiv.org/pdf/2310.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04086v2","updated":"2023-10-10T07:47:36Z","published":"2023-10-06T08:30:20Z","title":"End-to-End Chess Recognition","summary":"  Chess recognition refers to the task of identifying the chess pieces\nconfiguration from a chessboard image. Contrary to the predominant approach\nthat aims to solve this task through the pipeline of chessboard detection,\nsquare localization, and piece classification, we rely on the power of deep\nlearning models and introduce two novel methodologies to circumvent this\npipeline and directly predict the chessboard configuration from the entire\nimage. In doing so, we avoid the inherent error accumulation of the sequential\napproaches and the need for intermediate annotations. Furthermore, we introduce\na new dataset, Chess Recognition Dataset (ChessReD), specifically designed for\nchess recognition that consists of 10,800 images and their corresponding\nannotations. In contrast to existing synthetic datasets with limited angles,\nthis dataset comprises a diverse collection of real images of chess formations\ncaptured from various angles using smartphone cameras; a sensor choice made to\nensure real-world applicability. We use this dataset to both train our model\nand evaluate and compare its performance to that of the current\nstate-of-the-art. Our approach in chess recognition on this new benchmark\ndataset outperforms related approaches, achieving a board recognition accuracy\nof 15.26% ($\\approx$7x better than the current state-of-the-art).\n","authors":["Athanasios Masouris","Jan van Gemert"],"pdf_url":"https://arxiv.org/pdf/2310.04086v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2302.08861v2","updated":"2023-10-10T07:38:02Z","published":"2023-02-17T13:16:17Z","title":"AliasNet: Alias Artefact Suppression Network for Accelerated\n  Phase-Encode MRI","summary":"  Sparse reconstruction is an important aspect of MRI, helping to reduce\nacquisition time and improve spatial-temporal resolution. Popular methods are\nbased mostly on compressed sensing (CS), which relies on the random sampling of\nk-space to produce incoherent (noise-like) artefacts. Due to hardware\nconstraints, 1D Cartesian phase-encode under-sampling schemes are popular for\n2D CS-MRI. However, 1D under-sampling limits 2D incoherence between\nmeasurements, yielding structured aliasing artefacts (ghosts) that may be\ndifficult to remove assuming a 2D sparsity model. Reconstruction algorithms\ntypically deploy direction-insensitive 2D regularisation for these\ndirection-associated artefacts. Recognising that phase-encode artefacts can be\nseparated into contiguous 1D signals, we develop two decoupling techniques that\nenable explicit 1D regularisation and leverage the excellent 1D incoherence\ncharacteristics. We also derive a combined 1D + 2D reconstruction technique\nthat takes advantage of spatial relationships within the image. Experiments\nconducted on retrospectively under-sampled brain and knee data demonstrate that\ncombination of the proposed 1D AliasNet modules with existing 2D deep learned\n(DL) recovery techniques leads to an improvement in image quality. We also find\nAliasNet enables a superior scaling of performance compared to increasing the\nsize of the original 2D network layers. AliasNet therefore improves the\nregularisation of aliasing artefacts arising from phase-encode under-sampling,\nby tailoring the network architecture to account for their expected appearance.\nThe proposed 1D + 2D approach is compatible with any existing 2D DL recovery\ntechnique deployed for this application.\n","authors":["Marlon E. Bran Lorenzana","Shekhar S. Chandra","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2302.08861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06372v1","updated":"2023-10-10T07:25:06Z","published":"2023-10-10T07:25:06Z","title":"Leveraging Diffusion-Based Image Variations for Robust Training on\n  Poisoned Data","summary":"  Backdoor attacks pose a serious security threat for training neural networks\nas they surreptitiously introduce hidden functionalities into a model. Such\nbackdoors remain silent during inference on clean inputs, evading detection due\nto inconspicuous behavior. However, once a specific trigger pattern appears in\nthe input data, the backdoor activates, causing the model to execute its\nconcealed function. Detecting such poisoned samples within vast datasets is\nvirtually impossible through manual inspection. To address this challenge, we\npropose a novel approach that enables model training on potentially poisoned\ndatasets by utilizing the power of recent diffusion models. Specifically, we\ncreate synthetic variations of all training samples, leveraging the inherent\nresilience of diffusion models to potential trigger patterns in the data. By\ncombining this generative approach with knowledge distillation, we produce\nstudent models that maintain their general performance on the task while\nexhibiting robust resistance to backdoor triggers.\n","authors":["Lukas Struppek","Martin B. Hentschel","Clifton Poth","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06372v1.pdf","comment":"11 pages, 3 tables, 2 figures"},{"id":"http://arxiv.org/abs/2310.06370v1","updated":"2023-10-10T07:20:37Z","published":"2023-10-10T07:20:37Z","title":"Advanced Efficient Strategy for Detection of Dark Objects Based on\n  Spiking Network with Multi-Box Detection","summary":"  Several deep learning algorithms have shown amazing performance for existing\nobject detection tasks, but recognizing darker objects is the largest\nchallenge. Moreover, those techniques struggled to detect or had a slow\nrecognition rate, resulting in significant performance losses. As a result, an\nimproved and accurate detection approach is required to address the above\ndifficulty. The whole study proposes a combination of spiked and normal\nconvolution layers as an energy-efficient and reliable object detector model.\nThe proposed model is split into two sections. The first section is developed\nas a feature extractor, which utilizes pre-trained VGG16, and the second\nsection of the proposal structure is the combination of spiked and normal\nConvolutional layers to detect the bounding boxes of images. We drew a\npre-trained model for classifying detected objects. With state of the art\nPython libraries, spike layers can be trained efficiently. The proposed spike\nconvolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark\ndatasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects\nin the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS\nfor its forward path calculations. Experimental results indicated superior\nperformance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and\nCenter of loc+Xception based on mAP for the VOC dataset.\n","authors":["Munawar Ali","Baoqun Yin","Hazrat Bilal","Aakash Kumar","Ali Muhammad","Avinash Rohra"],"pdf_url":"https://arxiv.org/pdf/2310.06370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13976v6","updated":"2023-10-10T07:18:46Z","published":"2022-11-25T09:38:22Z","title":"Expanding Small-Scale Datasets with Guided Imagination","summary":"  The power of DNNs relies heavily on the quantity and quality of training\ndata. However, collecting and annotating data on a large scale is often\nexpensive and time-consuming. To address this issue, we explore a new task,\ntermed dataset expansion, aimed at expanding a ready-to-use small dataset by\nautomatically creating new labeled samples. To this end, we present a Guided\nImagination Framework (GIF) that leverages cutting-edge generative models like\nDALL-E2 and Stable Diffusion (SD) to \"imagine\" and create informative new data\nfrom the input seed data. Specifically, GIF conducts data imagination by\noptimizing the latent features of the seed data in the semantically meaningful\nspace of the prior model, resulting in the creation of photo-realistic images\nwith new content. To guide the imagination towards creating informative samples\nfor model training, we introduce two key criteria, i.e., class-maintained\ninformation boosting and sample diversity promotion. These criteria are\nverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%\nhigher model accuracy on natural image datasets than unguided expansion with\nSD. With these essential criteria, GIF successfully expands small datasets in\nvarious scenarios, boosting model accuracy by 36.9% on average over six natural\nimage datasets and by 13.5% on average over three medical datasets. The source\ncode is available at https://github.com/Vanint/DatasetExpansion.\n","authors":["Yifan Zhang","Daquan Zhou","Bryan Hooi","Kai Wang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2211.13976v6.pdf","comment":"NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion"},{"id":"http://arxiv.org/abs/2310.06368v1","updated":"2023-10-10T07:08:49Z","published":"2023-10-10T07:08:49Z","title":"CoinSeg: Contrast Inter- and Intra- Class Representations for\n  Incremental Segmentation","summary":"  Class incremental semantic segmentation aims to strike a balance between the\nmodel's stability and plasticity by maintaining old knowledge while adapting to\nnew concepts. However, most state-of-the-art methods use the freeze strategy\nfor stability, which compromises the model's plasticity.In contrast, releasing\nparameter training for plasticity could lead to the best performance for all\ncategories, but this requires discriminative feature representation.Therefore,\nwe prioritize the model's plasticity and propose the Contrast inter- and\nintra-class representations for Incremental Segmentation (CoinSeg), which\npursues discriminative representations for flexible parameter tuning. Inspired\nby the Gaussian mixture model that samples from a mixture of Gaussian\ndistributions, CoinSeg emphasizes intra-class diversity with multiple\ncontrastive representation centroids. Specifically, we use mask proposals to\nidentify regions with strong objectness that are likely to be diverse\ninstances/centroids of a category. These mask proposals are then used for\ncontrastive representations to reinforce intra-class diversity. Meanwhile, to\navoid bias from intra-class diversity, we also apply category-level\npseudo-labels to enhance category-level consistency and inter-category\ndiversity. Additionally, CoinSeg ensures the model's stability and alleviates\nforgetting through a specific flexible tuning strategy. We validate CoinSeg on\nPascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and\nachieve superior results compared to previous state-of-the-art methods,\nespecially in more challenging and realistic long-term scenarios. Code is\navailable at https://github.com/zkzhang98/CoinSeg.\n","authors":["Zekang Zhang","Guangyu Gao","Jianbo Jiao","Chi Harold Liu","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2310.06368v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2305.14829v3","updated":"2023-10-10T06:38:00Z","published":"2023-05-24T07:34:49Z","title":"Deakin RF-Sensing: Experiments on Correlated Knowledge Distillation for\n  Monitoring Human Postures with Radios","summary":"  In this work, we propose and develop a simple experimental testbed to study\nthe feasibility of a novel idea by coupling radio frequency (RF) sensing\ntechnology with Correlated Knowledge Distillation (CKD) theory towards\ndesigning lightweight, near real-time and precise human pose monitoring\nsystems. The proposed CKD framework transfers and fuses pose knowledge from a\nrobust \"Teacher\" model to a parameterized \"Student\" model, which can be a\npromising technique for obtaining accurate yet lightweight pose estimates. To\nassure its efficacy, we implemented CKD for distilling logits in our integrated\nSoftware Defined Radio (SDR)-based experimental setup and investigated the\nRF-visual signal correlation. Our CKD-RF sensing technique is characterized by\ntwo modes - a camera-fed Teacher Class Network (e.g., images, videos) with an\nSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD model\ntrains a dual multi-branch teacher and student network by distilling and fusing\nknowledge bases. The resulting CKD models are then subsequently used to\nidentify the multimodal correlation and teach the student branch in reverse.\nInstead of simply aggregating their learnings, CKD training comprised multiple\nparallel transformations with the two domains, i.e., visual images and RF\nsignals. Once trained, our CKD model can efficiently preserve privacy and\nutilize the multimodal correlated logits from the two different neural networks\nfor estimating poses without using visual signals/video frames (by using only\nthe RF signals).\n","authors":["Shiva Raj Pokhrel","Jonathan Kua","Deol Satish","Philip Williams","Arkady Zaslavsky","Seng W. Loke","Jinho Choi"],"pdf_url":"https://arxiv.org/pdf/2305.14829v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06351v1","updated":"2023-10-10T06:37:03Z","published":"2023-10-10T06:37:03Z","title":"Fire Detection From Image and Video Using YOLOv5","summary":"  For the detection of fire-like targets in indoor, outdoor and forest fire\nimages, as well as fire detection under different natural lights, an improved\nYOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection\nmodel expands the feature extraction network from three dimensions, which\nenhances feature propagation of fire small targets identification, improves\nnetwork performance, and reduces model parameters. Furthermore, through the\npromotion of the feature pyramid, the top-performing prediction box is\nobtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art\nobject detection networks, notably in the detection of small targets of fire\nand smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection\nmodel can effectively deal with the inspection of small fire targets, as well\nas fire-like and smoke-like objects with F1 score 0.88. When the input image\nsize is 416 x 416 resolution, the average detection time is 0.12 s per frame,\nwhich can provide real-time forest fire detection. Moreover, the algorithm\nproposed in this paper can also be applied to small target detection under\nother complicated situations. The proposed system shows an improved approach in\nall fire detection metrics such as precision, recall, and mean average\nprecision.\n","authors":["Arafat Islam","Md. Imtiaz Habib"],"pdf_url":"https://arxiv.org/pdf/2310.06351v1.pdf","comment":"6 pages, 6 sections, unpublished paper"},{"id":"http://arxiv.org/abs/2310.06347v1","updated":"2023-10-10T06:32:24Z","published":"2023-10-10T06:32:24Z","title":"JointNet: Extending Text-to-Image Diffusion for Dense Distribution\n  Modeling","summary":"  We introduce JointNet, a novel neural network architecture for modeling the\njoint distribution of images and an additional dense modality (e.g., depth\nmaps). JointNet is extended from a pre-trained text-to-image diffusion model,\nwhere a copy of the original network is created for the new dense modality\nbranch and is densely connected with the RGB branch. The RGB branch is locked\nduring network fine-tuning, which enables efficient learning of the new\nmodality distribution while maintaining the strong generalization ability of\nthe large-scale pre-trained diffusion model. We demonstrate the effectiveness\nof JointNet by using RGBD diffusion as an example and through extensive\nexperiments, showcasing its applicability in a variety of applications,\nincluding joint RGBD generation, dense depth prediction, depth-conditioned\nimage generation, and coherent tile-based 3D panorama generation.\n","authors":["Jingyang Zhang","Shiwei Li","Yuanxun Lu","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2310.06347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06344v1","updated":"2023-10-10T06:27:30Z","published":"2023-10-10T06:27:30Z","title":"Filter Pruning For CNN With Enhanced Linear Representation Redundancy","summary":"  Structured network pruning excels non-structured methods because they can\ntake advantage of the thriving developed parallel computing techniques. In this\npaper, we propose a new structured pruning method. Firstly, to create more\nstructured redundancy, we present a data-driven loss function term calculated\nfrom the correlation coefficient matrix of different feature maps in the same\nlayer, named CCM-loss. This loss term can encourage the neural network to learn\nstronger linear representation relations between feature maps during the\ntraining from the scratch so that more homogenous parts can be removed later in\npruning. CCM-loss provides us with another universal transcendental\nmathematical tool besides L*-norm regularization, which concentrates on\ngenerating zeros, to generate more redundancy but for the different genres.\nFurthermore, we design a matching channel selection strategy based on principal\ncomponents analysis to exploit the maximum potential ability of CCM-loss. In\nour new strategy, we mainly focus on the consistency and integrality of the\ninformation flow in the network. Instead of empirically hard-code the retain\nratio for each layer, our channel selection strategy can dynamically adjust\neach layer's retain ratio according to the specific circumstance of a\nper-trained model to push the prune ratio to the limit. Notably, on the\nCifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only\n1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs\nare 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet\ndataset, our approach achieves 42.8% and 47.3% storage and computation\nreductions, respectively, with an accuracy of 76.23%. Our code is available at\nhttps://github.com/Bojue-Wang/CCM-LRR.\n","authors":["Bojue Wang","Chunmei Ma","Bin Liu","Nianbo Liu","Jinqi Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.06344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04741v2","updated":"2023-10-10T06:22:45Z","published":"2023-10-07T08:54:43Z","title":"Balancing stability and plasticity in continual learning: the\n  readout-decomposition of activation change (RDAC) framework","summary":"  Continual learning (CL) algorithms strive to acquire new knowledge while\npreserving prior information. However, this stability-plasticity trade-off\nremains a central challenge. This paper introduces a framework that dissects\nthis trade-off, offering valuable insights into CL algorithms. The\nReadout-Decomposition of Activation Change (RDAC) framework first addresses the\nstability-plasticity dilemma and its relation to catastrophic forgetting. It\nrelates learning-induced activation changes in the range of prior readouts to\nthe degree of stability and changes in the null space to the degree of\nplasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the\nframework clarifies the stability-plasticity trade-offs of the popular\nregularization algorithms Synaptic intelligence (SI), Elastic-weight\nconsolidation (EWC), and learning without Forgetting (LwF), and replay-based\nalgorithms Gradient episodic memory (GEM), and data replay. GEM and data replay\npreserved stability and plasticity, while SI, EWC, and LwF traded off\nplasticity for stability. The inability of the regularization algorithms to\nmaintain plasticity was linked to them restricting the change of activations in\nthe null space of the prior readout. Additionally, for one-hidden-layer linear\nneural networks, we derived a gradient decomposition algorithm to restrict\nactivation change only in the range of the prior readouts, to maintain high\nstability while not further sacrificing plasticity. Results demonstrate that\nthe algorithm maintained stability without significant plasticity loss. The\nRDAC framework informs the behavior of existing CL algorithms and paves the way\nfor novel CL approaches. Finally, it sheds light on the connection between\nlearning-induced activation/representation changes and the stability-plasticity\ndilemma, also offering insights into representational drift in biological\nsystems.\n","authors":["Daniel Anthes","Sushrut Thorat","Peter Knig","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2310.04741v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06337v1","updated":"2023-10-10T06:13:09Z","published":"2023-10-10T06:13:09Z","title":"Local Style Awareness of Font Images","summary":"  When we compare fonts, we often pay attention to styles of local parts, such\nas serifs and curvatures. This paper proposes an attention mechanism to find\nimportant local parts. The local parts with larger attention are then\nconsidered important. The proposed mechanism can be trained in a\nquasi-self-supervised manner that requires no manual annotation other than\nknowing that a set of character images is from the same font, such as\nHelvetica. After confirming that the trained attention mechanism can find\nstyle-relevant local parts, we utilize the resulting attention for local\nstyle-aware font generation. Specifically, we design a new reconstruction loss\nfunction to put more weight on the local parts with larger attention for\ngenerating character images with more accurate style realization. This loss\nfunction has the merit of applicability to various font generation models. Our\nexperimental results show that the proposed loss function improves the quality\nof generated character images by several few-shot font generation models.\n","authors":["Daichi Haraguchi","Seiichi Uchida"],"pdf_url":"https://arxiv.org/pdf/2310.06337v1.pdf","comment":"Accepted at ICDAR WML 2023"},{"id":"http://arxiv.org/abs/2310.06332v1","updated":"2023-10-10T06:03:39Z","published":"2023-10-10T06:03:39Z","title":"CrowdRec: 3D Crowd Reconstruction from Single Color Images","summary":"  This is a technical report for the GigaCrowd challenge. Reconstructing 3D\ncrowds from monocular images is a challenging problem due to mutual occlusions,\nserver depth ambiguity, and complex spatial distribution. Since no large-scale\n3D crowd dataset can be used to train a robust model, the current multi-person\nmesh recovery methods can hardly achieve satisfactory performance in crowded\nscenes. In this paper, we exploit the crowd features and propose a\ncrowd-constrained optimization to improve the common single-person method on\ncrowd images. To avoid scale variations, we first detect human bounding-boxes\nand 2D poses from the original images with off-the-shelf detectors. Then, we\ntrain a single-person mesh recovery network using existing in-the-wild image\ndatasets. To promote a more reasonable spatial distribution, we further propose\na crowd constraint to refine the single-person network parameters. With the\noptimization, we can obtain accurate body poses and shapes with reasonable\nabsolute positions from a large-scale crowd image using a single-person\nbackbone. The code will be publicly available\nat~\\url{https://github.com/boycehbz/CrowdRec}.\n","authors":["Buzhen Huang","Jingyi Ju","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06332v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2309.09464v3","updated":"2023-10-10T05:59:43Z","published":"2023-09-18T03:55:41Z","title":"Reducing Adversarial Training Cost with Gradient Approximation","summary":"  Deep learning models have achieved state-of-the-art performances in various\ndomains, while they are vulnerable to the inputs with well-crafted but small\nperturbations, which are named after adversarial examples (AEs). Among many\nstrategies to improve the model robustness against AEs, Projected Gradient\nDescent (PGD) based adversarial training is one of the most effective methods.\nUnfortunately, the prohibitive computational overhead of generating strong\nenough AEs, due to the maximization of the loss function, sometimes makes the\nregular PGD adversarial training impractical when using larger and more\ncomplicated models. In this paper, we propose that the adversarial loss can be\napproximated by the partial sum of Taylor series. Furthermore, we approximate\nthe gradient of adversarial loss and propose a new and efficient adversarial\ntraining method, adversarial training with gradient approximation (GAAT), to\nreduce the cost of building up robust models. Additionally, extensive\nexperiments demonstrate that this efficiency improvement can be achieved\nwithout any or with very little loss in accuracy on natural and adversarial\nexamples, which show that our proposed method saves up to 60\\% of the training\ntime with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100\ndatasets.\n","authors":["Huihui Gong"],"pdf_url":"https://arxiv.org/pdf/2309.09464v3.pdf","comment":"The experiments are insufficient, later will be updated. Withraw this\n  manuscript"},{"id":"http://arxiv.org/abs/2310.06329v1","updated":"2023-10-10T05:54:04Z","published":"2023-10-10T05:54:04Z","title":"Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using\n  Object Detection Algorithms","summary":"  Recent years have seen tremendous advancements in the area of autonomous\npayload delivery via unmanned aerial vehicles, or drones. However, most of\nthese works involve delivering the payload at a predetermined location using\nits GPS coordinates. By relying on GPS coordinates for navigation, the\nprecision of payload delivery is restricted to the accuracy of the GPS network\nand the availability and strength of the GPS connection, which may be severely\nrestricted by the weather condition at the time and place of operation. In this\nwork we describe the development of a micro-class UAV and propose a novel\nnavigation method that improves the accuracy of conventional navigation methods\nby incorporating a deep-learning-based computer vision approach to identify and\nprecisely align the UAV with a target marked at the payload delivery position.\nThis proposed method achieves a 500% increase in average horizontal precision\nover conventional GPS-based approaches.\n","authors":["Aditya Vadduri","Anagh Benjwal","Abhishek Pai","Elkan Quadros","Aniruddh Kammar","Prajwal Uday"],"pdf_url":"https://arxiv.org/pdf/2310.06329v1.pdf","comment":"Second International Conference on Artificial Intelligence,\n  Computational Electronics and Communication System (AICECS 2023)"},{"id":"http://arxiv.org/abs/2310.05255v2","updated":"2023-10-10T05:48:25Z","published":"2023-10-08T18:07:15Z","title":"Persis: A Persian Font Recognition Pipeline Using Convolutional Neural\n  Networks","summary":"  What happens if we encounter a suitable font for our design work but do not\nknow its name? Visual Font Recognition (VFR) systems are used to identify the\nfont typeface in an image. These systems can assist graphic designers in\nidentifying fonts used in images. A VFR system also aids in improving the speed\nand accuracy of Optical Character Recognition (OCR) systems. In this paper, we\nintroduce the first publicly available datasets in the field of Persian font\nrecognition and employ Convolutional Neural Networks (CNN) to address this\nproblem. The results show that the proposed pipeline obtained 78.0% top-1\naccuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the\nKAFD dataset. Furthermore, the average time spent in the entire pipeline for\none sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,\nrespectively. We conclude that CNN methods can be used to recognize Persian\nfonts without the need for additional pre-processing steps such as feature\nextraction, binarization, normalization, etc.\n","authors":["Mehrdad Mohammadian","Neda Maleki","Tobias Olsson","Fredrik Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2310.05255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09359v2","updated":"2023-10-10T05:31:01Z","published":"2022-11-17T06:18:45Z","title":"How to Fine-Tune Vision Models with SGD","summary":"  SGD and AdamW are the two most used optimizers for fine-tuning large neural\nnetworks in computer vision. When the two methods perform the same, SGD is\npreferable because it uses less memory (12 bytes/parameter with momentum and 8\nbytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite\nof downstream tasks, especially those with distribution shifts, we find that\nfine-tuning with AdamW performs substantially better than SGD on modern Vision\nTransformer and ConvNeXt models. We find that large gaps in performance between\nSGD and AdamW occur when the fine-tuning gradients in the first \"embedding\"\nlayer are much larger than in the rest of the model. Our analysis suggests an\neasy fix that works consistently across datasets and models: freezing the\nembedding layer (less than 1% of the parameters) leads to SGD with or without\nmomentum performing slightly better than AdamW while using less memory (e.g.,\non ViT-L, SGD uses 33% less GPU memory). Our insights result in\nstate-of-the-art accuracies on five popular distribution shift benchmarks:\nWILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.\n","authors":["Ananya Kumar","Ruoqi Shen","Sebastien Bubeck","Suriya Gunasekar"],"pdf_url":"https://arxiv.org/pdf/2211.09359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05863v2","updated":"2023-10-10T05:30:49Z","published":"2023-10-09T17:00:20Z","title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models","summary":"  Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/BriansIDP/AudioVisualLLM.git, and the training code and\nmodel checkpoints will be released soon.\n","authors":["Guangzhi Sun","Wenyi Yu","Changli Tang","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06318v1","updated":"2023-10-10T05:28:02Z","published":"2023-10-10T05:28:02Z","title":"Adversarial Masked Image Inpainting for Robust Detection of Mpox and\n  Non-Mpox","summary":"  Due to the lack of efficient mpox diagnostic technology, mpox cases continue\nto increase. Recently, the great potential of deep learning models in detecting\nmpox and non-mpox has been proven. However, existing models learn image\nrepresentations via image classification, which results in they may be easily\nsusceptible to interference from real-world noise, require diverse non-mpox\nimages, and fail to detect abnormal input. These drawbacks make classification\nmodels inapplicable in real-world settings. To address these challenges, we\npropose \"Mask, Inpainting, and Measure\" (MIM). In MIM's pipeline, a generative\nadversarial network only learns mpox image representations by inpainting the\nmasked mpox images. Then, MIM determines whether the input belongs to mpox by\nmeasuring the similarity between the inpainted image and the original image.\nThe underlying intuition is that since MIM solely models mpox images, it\nstruggles to accurately inpaint non-mpox images in real-world settings. Without\nutilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can\nhandle abnormal inputs. We used the recognized mpox dataset (MSLD) and images\nof eighteen non-mpox skin diseases to verify the effectiveness and robustness\nof MIM. Experimental results show that the average AUROC of MIM achieves\n0.8237. In addition, we demonstrated the drawbacks of classification models and\nbuttressed the potential of MIM through clinical validation. Finally, we\ndeveloped an online smartphone app to provide free testing to the public in\naffected areas. This work first employs generative models to improve mpox\ndetection and provides new insights into binary decision-making tasks in\nmedical images.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2310.06318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06313v1","updated":"2023-10-10T05:13:17Z","published":"2023-10-10T05:13:17Z","title":"Advancing Pose-Guided Image Synthesis with Progressive Conditional\n  Diffusion Models","summary":"  Recent work has showcased the significant potential of diffusion models in\npose-guided person image synthesis. However, owing to the inconsistency in pose\nbetween the source and target images, synthesizing an image with a distinct\npose, relying exclusively on the source image and target pose information,\nremains a formidable challenge. This paper presents Progressive Conditional\nDiffusion Models (PCDMs) that incrementally bridge the gap between person\nimages under the target and source poses through three stages. Specifically, in\nthe first stage, we design a simple prior conditional diffusion model that\npredicts the global features of the target image by mining the global alignment\nrelationship between pose coordinates and image appearance. Then, the second\nstage establishes a dense correspondence between the source and target images\nusing the global features from the previous stage, and an inpainting\nconditional diffusion model is proposed to further align and enhance the\ncontextual features, generating a coarse-grained person image. In the third\nstage, we propose a refining conditional diffusion model to utilize the\ncoarsely generated image from the previous stage as a condition, achieving\ntexture restoration and enhancing fine-detail consistency. The three-stage\nPCDMs work progressively to generate the final high-quality and high-fidelity\nsynthesized image. Both qualitative and quantitative results demonstrate the\nconsistency and photorealism of our proposed PCDMs under challenging\nscenarios.The code and model will be available at\nhttps://github.com/muzishen/PCDMs.\n","authors":["Fei Shen","Hu Ye","Jun Zhang","Cong Wang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06311v1","updated":"2023-10-10T05:09:05Z","published":"2023-10-10T05:09:05Z","title":"Improving Compositional Text-to-image Generation with Large\n  Vision-Language Models","summary":"  Recent advancements in text-to-image models, particularly diffusion models,\nhave shown significant promise. However, compositional text-to-image models\nfrequently encounter difficulties in generating high-quality images that\naccurately align with input texts describing multiple objects, variable\nattributes, and intricate spatial relationships. To address this limitation, we\nemploy large vision-language models (LVLMs) for multi-dimensional assessment of\nthe alignment between generated images and their corresponding input texts.\nUtilizing this assessment, we fine-tune the diffusion model to enhance its\nalignment capabilities. During the inference phase, an initial image is\nproduced using the fine-tuned diffusion model. The LVLM is then employed to\npinpoint areas of misalignment in the initial image, which are subsequently\ncorrected using the image editing algorithm until no further misalignments are\ndetected by the LVLM. The resultant image is consequently more closely aligned\nwith the input text. Our experimental results validate that the proposed\nmethodology significantly improves text-image alignment in compositional image\ngeneration, particularly with respect to object number, attribute binding,\nspatial relationships, and aesthetic quality.\n","authors":["Song Wen","Guian Fang","Renrui Zhang","Peng Gao","Hao Dong","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2310.06311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05720v2","updated":"2023-10-10T05:00:48Z","published":"2023-10-09T13:45:21Z","title":"HyperLips: Hyper Control Lips with High Resolution Decoder for Talking\n  Face Generation","summary":"  Talking face generation has a wide range of potential applications in the\nfield of virtual digital humans. However, rendering high-fidelity facial video\nwhile ensuring lip synchronization is still a challenge for existing\naudio-driven talking face generation approaches. To address this issue, we\npropose HyperLips, a two-stage framework consisting of a hypernetwork for\ncontrolling lips and a high-resolution decoder for rendering high-fidelity\nfaces. In the first stage, we construct a base face generation network that\nuses the hypernetwork to control the encoding latent code of the visual face\ninformation over audio. First, FaceEncoder is used to obtain latent code by\nextracting features from the visual face information taken from the video\nsource containing the face frame.Then, HyperConv, which weighting parameters\nare updated by HyperNet with the audio features as input, will modify the\nlatent code to synchronize the lip movement with the audio. Finally,\nFaceDecoder will decode the modified and synchronized latent code into visual\nface content. In the second stage, we obtain higher quality face videos through\na high-resolution decoder. To further improve the quality of face generation,\nwe trained a high-resolution decoder, HRDecoder, using face images and detected\nsketches generated from the first stage as input.Extensive quantitative and\nqualitative experiments show that our method outperforms state-of-the-art work\nwith more realistic, high-fidelity, and lip synchronization. Project page:\nhttps://semchan.github.io/HyperLips/\n","authors":["Yaosen Chen","Yu Yao","Zhiqiang Li","Wei Wang","Yanru Zhang","Han Yang","Xuming Wen"],"pdf_url":"https://arxiv.org/pdf/2310.05720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12493v4","updated":"2023-10-10T04:23:11Z","published":"2023-07-24T02:50:44Z","title":"TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition","summary":"  Text-driven diffusion models have exhibited impressive generative\ncapabilities, enabling various image editing tasks. In this paper, we propose\nTF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the\npower of text-driven diffusion models for cross-domain image-guided\ncomposition. This task aims to seamlessly integrate user-provided objects into\na specific visual context. Current diffusion-based methods often involve costly\ninstance-based optimization or finetuning of pretrained models on customized\ndatasets, which can potentially undermine their rich prior. In contrast,\nTF-ICON can leverage off-the-shelf diffusion models to perform cross-domain\nimage-guided composition without requiring additional training, finetuning, or\noptimization. Moreover, we introduce the exceptional prompt, which contains no\ninformation, to facilitate text-driven diffusion models in accurately inverting\nreal images into latent representations, forming the basis for compositing. Our\nexperiments show that equipping Stable Diffusion with the exceptional prompt\noutperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,\nCOCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile\nvisual domains. Code is available at https://github.com/Shilin-LU/TF-ICON\n","authors":["Shilin Lu","Yanzhu Liu","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2307.12493v4.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.04991v2","updated":"2023-10-10T04:20:57Z","published":"2023-10-08T03:35:27Z","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and\n  Decoupling","summary":"  This paper proposes Video-Teller, a video-language foundation model that\nleverages multi-modal fusion and fine-grained modality alignment to\nsignificantly enhance the video-to-text generation task. Video-Teller boosts\nthe training efficiency by utilizing frozen pretrained vision and language\nmodules. It capitalizes on the robust linguistic capabilities of large language\nmodels, enabling the generation of both concise and elaborate video\ndescriptions. To effectively integrate visual and auditory information,\nVideo-Teller builds upon the image-based BLIP-2 model and introduces a cascaded\nQ-Former which fuses information across frames and ASR texts. To better guide\nvideo summarization, we introduce a fine-grained modality alignment objective,\nwhere the cascaded Q-Former's output embedding is trained to align with the\ncaption/summary embedding created by a pretrained text auto-encoder.\nExperimental results demonstrate the efficacy of our proposed video-language\nfoundation model in accurately comprehending videos and generating coherent and\nprecise language descriptions. It is worth noting that the fine-grained\nalignment enhances the model's capabilities (4% improvement of CIDEr score on\nMSR-VTT) with only 13% extra parameters in training and zero additional cost in\ninference.\n","authors":["Haogeng Liu","Qihang Fan","Tingkai Liu","Linjie Yang","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06291v1","updated":"2023-10-10T04:10:56Z","published":"2023-10-10T04:10:56Z","title":"Three-Dimensional Medical Image Fusion with Deformable Cross-Attention","summary":"  Multimodal medical image fusion plays an instrumental role in several areas\nof medical image processing, particularly in disease recognition and tumor\ndetection. Traditional fusion methods tend to process each modality\nindependently before combining the features and reconstructing the fusion\nimage. However, this approach often neglects the fundamental commonalities and\ndisparities between multimodal information. Furthermore, the prevailing\nmethodologies are largely confined to fusing two-dimensional (2D) medical image\nslices, leading to a lack of contextual supervision in the fusion images and\nsubsequently, a decreased information yield for physicians relative to\nthree-dimensional (3D) images. In this study, we introduce an innovative\nunsupervised feature mutual learning fusion network designed to rectify these\nlimitations. Our approach incorporates a Deformable Cross Feature Blend (DCFB)\nmodule that facilitates the dual modalities in discerning their respective\nsimilarities and differences. We have applied our model to the fusion of 3D MRI\nand PET images obtained from 660 patients in the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset. Through the application of the DCFB\nmodule, our network generates high-quality MRI-PET fusion images. Experimental\nresults demonstrate that our method surpasses traditional 2D image fusion\nmethods in performance metrics such as Peak Signal to Noise Ratio (PSNR) and\nStructural Similarity Index Measure (SSIM). Importantly, the capacity of our\nmethod to fuse 3D images enhances the information available to physicians and\nresearchers, thus marking a significant step forward in the field. The code\nwill soon be available online.\n","authors":["Lin Liu","Xinxin Fan","Chulong Zhang","Jingjing Dai","Yaoqin Xie","Xiaokun Liang"],"pdf_url":"https://arxiv.org/pdf/2310.06291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00067v2","updated":"2023-10-10T04:01:38Z","published":"2023-04-28T19:37:17Z","title":"Unsupervised Discovery of 3D Hierarchical Structure with Generative\n  Diffusion Features","summary":"  Inspired by recent findings that generative diffusion models learn\nsemantically meaningful representations, we use them to discover the intrinsic\nhierarchical structure in biomedical 3D images using unsupervised segmentation.\nWe show that features of diffusion models from different stages of a\nU-Net-based ladder-like architecture capture different hierarchy levels in 3D\nbiomedical images. We design three losses to train a predictive unsupervised\nsegmentation network that encourages the decomposition of 3D volumes into\nmeaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D\ndiffusion models and use the consistency of their features across subvolumes.\nSecond, we use the visual consistency between subvolumes. Third, we use the\ninvariance to photometric augmentations as a regularizer. Our models achieve\nbetter performance than prior unsupervised structure discovery approaches on\nchallenging biologically-inspired synthetic datasets and on a real-world brain\ntumor MRI dataset.\n","authors":["Nurislam Tursynbek","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2305.00067v2.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2308.06160v2","updated":"2023-10-10T03:59:41Z","published":"2023-08-11T14:38:11Z","title":"DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion\n  Models","summary":"  Current deep networks are very data-hungry and benefit from training on\nlargescale datasets, which are often time-consuming to collect and annotate. By\ncontrast, synthetic data can be generated infinitely using generative models\nsuch as DALL-E and diffusion models, with minimal effort and cost. In this\npaper, we present DatasetDM, a generic dataset generation model that can\nproduce diverse synthetic images and the corresponding high-quality perception\nannotations (e.g., segmentation masks, and depth). Our method builds upon the\npre-trained diffusion model and extends text-guided image synthesis to\nperception data generation. We show that the rich latent code of the diffusion\nmodel can be effectively decoded as accurate perception annotations using a\ndecoder module. Training the decoder only needs less than 1% (around 100\nimages) manually labeled images, enabling the generation of an infinitely large\nannotated dataset. Then these synthetic data can be used for training various\nperception models for downstream tasks. To showcase the power of the proposed\napproach, we generate datasets with rich dense pixel-wise labels for a wide\nrange of downstream tasks, including semantic segmentation, instance\nsegmentation, and depth estimation. Notably, it achieves 1) state-of-the-art\nresults on semantic segmentation and instance segmentation; 2) significantly\nmore robust on domain generalization than using the real data alone; and\nstate-of-the-art results in zero-shot segmentation setting; and 3) flexibility\nfor efficient application and novel task composition (e.g., image editing). The\nproject website and code can be found at\nhttps://weijiawu.github.io/DatasetDM_page/ and\nhttps://github.com/showlab/DatasetDM, respectively\n","authors":["Weijia Wu","Yuzhong Zhao","Hao Chen","Yuchao Gu","Rui Zhao","Yefei He","Hong Zhou","Mike Zheng Shou","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2308.06160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09907v3","updated":"2023-10-10T03:55:14Z","published":"2023-02-20T11:08:07Z","title":"General Rotation Invariance Learning for Point Clouds via Weight-Feature\n  Alignment","summary":"  Compared to 2D images, 3D point clouds are much more sensitive to rotations.\nWe expect the point features describing certain patterns to keep invariant to\nthe rotation transformation. There are many recent SOTA works dedicated to\nrotation-invariant learning for 3D point clouds. However, current\nrotation-invariant methods lack generalizability on the point clouds in the\nopen scenes due to the reliance on the global distribution, \\ie the global\nscene and backgrounds. Considering that the output activation is a function of\nthe pattern and its orientation, we need to eliminate the effect of the\norientation.In this paper, inspired by the idea that the network weights can be\nconsidered a set of points distributed in the same 3D space as the input\npoints, we propose Weight-Feature Alignment (WFA) to construct a local\nInvariant Reference Frame (IRF) via aligning the features with the principal\naxes of the network weights. Our WFA algorithm provides a general solution for\nthe point clouds of all scenes. WFA ensures the model achieves the target that\nthe response activity is a necessary and sufficient condition of the pattern\nmatching degree. Practically, we perform experiments on the point clouds of\nboth single objects and open large-range scenes. The results suggest that our\nmethod almost bridges the gap between rotation invariance learning and normal\nmethods.\n","authors":["Liang Xie","Yibo Yang","Wenxiao Wang","Binbin Lin","Deng Cai","Xiaofei He","Ronghua Liang"],"pdf_url":"https://arxiv.org/pdf/2302.09907v3.pdf","comment":"4 figures"},{"id":"http://arxiv.org/abs/2310.01641v2","updated":"2023-10-10T03:50:28Z","published":"2023-10-02T21:09:43Z","title":"You Only Look at Once for Real-time and Generic Multi-Task","summary":"  High precision, lightweight, and real-time responsiveness are three essential\nrequirements for implementing autonomous driving. In this study, we present an\nadaptive, real-time, and lightweight multi-task model designed to concurrently\naddress object detection, drivable area segmentation, and lane line\nsegmentation tasks. Specifically, we developed an end-to-end multi-task model\nwith a unified and streamlined segmentation structure. We introduced a\nlearnable parameter that adaptively concatenate features in segmentation necks,\nusing the same loss function for all segmentation tasks. This eliminates the\nneed for customizations and enhances the model's generalization capabilities.\nWe also introduced a segmentation head composed only of a series of\nconvolutional layers, which reduces the inference time. We achieved competitive\nresults on the BDD100k dataset, particularly in visualization outcomes. The\nperformance results show a mAP50 of 81.1% for object detection, a mIoU of 91.0%\nfor drivable area segmentation, and an IoU of 28.8% for lane line segmentation.\nAdditionally, we introduced real-world scenarios to evaluate our model's\nperformance in a real scene, which significantly outperforms competitors. This\ndemonstrates that our model not only exhibits competitive performance but is\nalso more flexible and faster than existing multi-task models. The source codes\nand pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/YOLOv8-multi-task\n","authors":["Jiayuan Wang","Q. M. Jonathan Wu","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.01641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05718v3","updated":"2023-10-10T03:49:32Z","published":"2023-06-09T07:30:10Z","title":"Learning Domain-Aware Detection Head with Prompt Tuning","summary":"  Domain adaptive object detection (DAOD) aims to generalize detectors trained\non an annotated source domain to an unlabelled target domain. However, existing\nmethods focus on reducing the domain bias of the detection backbone by\ninferring a discriminative visual encoder, while ignoring the domain bias in\nthe detection head. Inspired by the high generalization of vision-language\nmodels (VLMs), applying a VLM as the robust detection backbone following a\ndomain-aware detection head is a reasonable way to learn the discriminative\ndetector for each domain, rather than reducing the domain bias in traditional\nmethods. To achieve the above issue, we thus propose a novel DAOD framework\nnamed Domain-Aware detection head with Prompt tuning (DA-Pro), which applies\nthe learnable domain-adaptive prompt to generate the dynamic detection head for\neach domain. Formally, the domain-adaptive prompt consists of the\ndomain-invariant tokens, domain-specific tokens, and the domain-related textual\ndescription along with the class label. Furthermore, two constraints between\nthe source and target domains are applied to ensure that the domain-adaptive\nprompt can capture the domains-shared and domain-specific knowledge. A prompt\nensemble strategy is also proposed to reduce the effect of prompt disturbance.\nComprehensive experiments over multiple cross-domain adaptation tasks\ndemonstrate that using the domain-adaptive prompt can produce an effectively\ndomain-related detection head for boosting domain-adaptive object detection.\nOur code is available at https://github.com/Therock90421/DA-Pro.\n","authors":["Haochen Li","Rui Zhang","Hantao Yao","Xinkai Song","Yifan Hao","Yongwei Zhao","Ling Li","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2306.05718v3.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.04769v2","updated":"2023-10-10T03:48:32Z","published":"2023-10-07T10:25:50Z","title":"1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023\n  Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction","summary":"  This report introduce our work on Egocentric 3D Hand Pose Estimation\nworkshop. Using AssemblyHands, this challenge focuses on egocentric 3D hand\npose estimation from a single-view image. In the competition, we adopt ViT\nbased backbones and a simple regressor for 3D keypoints prediction, which\nprovides strong model baselines. We noticed that Hand-objects occlusions and\nself-occlusions lead to performance degradation, thus proposed a non-model\nmethod to merge multi-view results in the post-process stage. Moreover, We\nutilized test time augmentation and model ensemble to make further improvement.\nWe also found that public dataset and rational preprocess are beneficial. Our\nmethod achieved 12.21mm MPJPE on test dataset, achieve the first place in\nEgocentric 3D Hand Pose Estimation challenge.\n","authors":["Zhishan Zhou","Zhi Lv","Shihao Zhou","Minqiang Zou","Tong Wu","Mochen Yu","Yao Tang","Jiajun Liang"],"pdf_url":"https://arxiv.org/pdf/2310.04769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05897v4","updated":"2023-10-10T03:42:57Z","published":"2022-06-13T04:03:49Z","title":"$\\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse\n  Consistency","summary":"  We present an approach to learning regular spatial transformations between\nimage pairs in the context of medical image registration. Contrary to\noptimization-based registration techniques and many modern learning-based\nmethods, we do not directly penalize transformation irregularities but instead\npromote transformation regularity via an inverse consistency penalty. We use a\nneural network to predict a map between a source and a target image as well as\nthe map when swapping the source and target images. Different from existing\napproaches, we compose these two resulting maps and regularize deviations of\nthe $\\bf{Jacobian}$ of this composition from the identity matrix. This\nregularizer -- $\\texttt{GradICON}$ -- results in much better convergence when\ntraining registration models compared to promoting inverse consistency of the\ncomposition of maps directly while retaining the desirable implicit\nregularization effects of the latter. We achieve state-of-the-art registration\nperformance on a variety of real-world medical image datasets using a single\nset of hyperparameters and a single non-dataset-specific training protocol.\n","authors":["Lin Tian","Hastings Greer","Franois-Xavier Vialard","Roland Kwitt","Ral San Jos Estpar","Richard Jarrett Rushmore","Nikolaos Makris","Sylvain Bouix","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2206.05897v4.pdf","comment":"29 pages, 16 figures, CVPR 2023"},{"id":"http://arxiv.org/abs/2310.05538v2","updated":"2023-10-10T03:41:51Z","published":"2023-10-09T09:01:53Z","title":"M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion\n  for Polyp Localization in Colonoscopy Images","summary":"  Polyp segmentation is crucial for preventing colorectal cancer a common type\nof cancer. Deep learning has been used to segment polyps automatically, which\nreduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images\nis challenging because of its complex characteristics, such as color,\nocclusion, and various shapes of polyps. To address this challenge, a novel\nfrequency-based fully convolutional neural network, Multi-Frequency Feature\nFusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose\nthe input image into low/high/full-frequency components to use the\ncharacteristics of each component. We used three independent multi-frequency\nencoders to map multiple input images into a high-dimensional feature space. In\nthe Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied\nbetween each frequency component to preserve scale information. Subsequently,\nscalable attention was applied to emphasize polyp regions in a high-dimensional\nfeature space. Finally, we designed three multi-task learning (i.e., region,\nedge, and distance) in four decoder blocks to learn the structural\ncharacteristics of the region. The proposed model outperformed various\nsegmentation models with performance gains of 6.92% and 7.52% on average for\nall metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyeong Park","Nur Suriza Syazwany","Yerim Jung","Yu-Han Im","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05538v2.pdf","comment":"5pages. 2023 IEEE International Conference on Image Processing\n  (ICIP). IEEE, 2023"},{"id":"http://arxiv.org/abs/2310.06283v1","updated":"2023-10-10T03:34:31Z","published":"2023-10-10T03:34:31Z","title":"Towards More Efficient Depression Risk Recognition via Gait","summary":"  Depression, a highly prevalent mental illness, affects over 280 million\nindividuals worldwide. Early detection and timely intervention are crucial for\npromoting remission, preventing relapse, and alleviating the emotional and\nfinancial burdens associated with depression. However, patients with depression\noften go undiagnosed in the primary care setting. Unlike many physiological\nillnesses, depression lacks objective indicators for recognizing depression\nrisk, and existing methods for depression risk recognition are time-consuming\nand often encounter a shortage of trained medical professionals. The\ncorrelation between gait and depression risk has been empirically established.\nGait can serve as a promising objective biomarker, offering the advantage of\nefficient and convenient data collection. However, current methods for\nrecognizing depression risk based on gait have only been validated on small,\nprivate datasets, lacking large-scale publicly available datasets for research\npurposes. Additionally, these methods are primarily limited to hand-crafted\napproaches. Gait is a complex form of motion, and hand-crafted gait features\noften only capture a fraction of the intricate associations between gait and\ndepression risk. Therefore, this study first constructs a large-scale gait\ndatabase, encompassing over 1,200 individuals, 40,000 gait sequences, and\ncovering six perspectives and three types of attire. Two commonly used\npsychological scales are provided as depression risk annotations. Subsequently,\na deep learning-based depression risk recognition model is proposed, overcoming\nthe limitations of hand-crafted approaches. Through experiments conducted on\nthe constructed large-scale database, the effectiveness of the proposed method\nis validated, and numerous instructive insights are presented in the paper,\nhighlighting the significant potential of gait-based depression risk\nrecognition.\n","authors":["Min Ren","Muchan Tao","Xuecai Hu","Xiaotong Liu","Qiong Li","Yongzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04999v2","updated":"2023-10-10T03:32:58Z","published":"2023-10-08T04:00:20Z","title":"Symmetrical Linguistic Feature Distillation with CLIP for Scene Text\n  Recognition","summary":"  In this paper, we explore the potential of the Contrastive Language-Image\nPretraining (CLIP) model in scene text recognition (STR), and establish a novel\nSymmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to\nleverage both visual and linguistic knowledge in CLIP. Different from previous\nCLIP-based methods mainly considering feature generalization on visual\nencoding, we propose a symmetrical distillation strategy (SDS) that further\ncaptures the linguistic knowledge in the CLIP text encoder. By cascading the\nCLIP image encoder with the reversed CLIP text encoder, a symmetrical structure\nis built with an image-to-text feature flow that covers not only visual but\nalso linguistic information for distillation.Benefiting from the natural\nalignment in CLIP, such guidance flow provides a progressive optimization\nobjective from vision to language, which can supervise the STR feature\nforwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss\n(LCL) is proposed to enhance the linguistic capability by considering\nsecond-order statistics during the optimization. Overall, CLIP-OCR is the first\nto design a smooth transition between image and text for the STR task.Extensive\nexperiments demonstrate the effectiveness of CLIP-OCR with 93.8% average\naccuracy on six popular STR benchmarks.Code will be available at\nhttps://github.com/wzx99/CLIPOCR.\n","authors":["Zixiao Wang","Hongtao Xie","Yuxin Wang","Jianjun Xu","Boqiang Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04999v2.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.06282v1","updated":"2023-10-10T03:32:33Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04750v2","updated":"2023-10-10T03:22:31Z","published":"2023-10-07T09:10:28Z","title":"DiffNAS: Bootstrapping Diffusion Models by Prompting for Better\n  Architectures","summary":"  Diffusion models have recently exhibited remarkable performance on synthetic\ndata. After a diffusion path is selected, a base model, such as UNet, operates\nas a denoising autoencoder, primarily predicting noises that need to be\neliminated step by step. Consequently, it is crucial to employ a model that\naligns with the expected budgets to facilitate superior synthetic performance.\nIn this paper, we meticulously analyze the diffusion model and engineer a base\nmodel search approach, denoted \"DiffNAS\". Specifically, we leverage GPT-4 as a\nsupernet to expedite the search, supplemented with a search memory to enhance\nthe results. Moreover, we employ RFID as a proxy to promptly rank the\nexperimental outcomes produced by GPT-4. We also adopt a rapid-convergence\ntraining strategy to boost search efficiency. Rigorous experimentation\ncorroborates that our algorithm can augment the search efficiency by 2 times\nunder GPT-based scenarios, while also attaining a performance of 2.82 with 0.37\nimprovement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.\n","authors":["Wenhao Li","Xiu Su","Shan You","Fei Wang","Chen Qian","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02736v3","updated":"2023-10-10T03:17:37Z","published":"2023-07-06T02:44:32Z","title":"An Uncertainty Aided Framework for Learning based Liver $T_1$\n  Mapping and Analysis","summary":"  Objective: Quantitative $T_1\\rho$ imaging has potential for assessment of\nbiochemical alterations of liver pathologies. Deep learning methods have been\nemployed to accelerate quantitative $T_1\\rho$ imaging. To employ artificial\nintelligence-based quantitative imaging methods in complicated clinical\nenvironment, it is valuable to estimate the uncertainty of the predicated\n$T_1\\rho$ values to provide the confidence level of the quantification results.\nThe uncertainty should also be utilized to aid the post-hoc quantitative\nanalysis and model learning tasks. Approach: To address this need, we propose a\nparametric map refinement approach for learning-based $T_1\\rho$ mapping and\ntrain the model in a probabilistic way to model the uncertainty. We also\npropose to utilize the uncertainty map to spatially weight the training of an\nimproved $T_1\\rho$ mapping network to further improve the mapping performance\nand to remove pixels with unreliable $T_1\\rho$ values in the region of\ninterest. The framework was tested on a dataset of 51 patients with different\nliver fibrosis stages. Main results: Our results indicate that the\nlearning-based map refinement method leads to a relative mapping error of less\nthan 3% and provides uncertainty estimation simultaneously. The estimated\nuncertainty reflects the actual error level, and it can be used to further\nreduce relative $T_1\\rho$ mapping error to 2.60% as well as removing unreliable\npixels in the region of interest effectively. Significance: Our studies\ndemonstrate the proposed approach has potential to provide a learning-based\nquantitative MRI system for trustworthy $T_1\\rho$ mapping of the liver.\n","authors":["Chaoxing Huang","Vincent Wai Sun Wong","Queenie Chan","Winnie Chiu Wing Chu","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2307.02736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05916v2","updated":"2023-10-10T03:14:37Z","published":"2023-10-09T17:59:04Z","title":"Interpreting CLIP's Image Representation via Text-Based Decomposition","summary":"  We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2310.05916v2.pdf","comment":"Project page and code:\n  https://yossigandelsman.github.io/clip_decomposition/"},{"id":"http://arxiv.org/abs/2310.06275v1","updated":"2023-10-10T03:13:33Z","published":"2023-10-10T03:13:33Z","title":"High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying\n  Expression Conditioned Neural Radiance Field","summary":"  One crucial aspect of 3D head avatar reconstruction lies in the details of\nfacial expressions. Although recent NeRF-based photo-realistic 3D head avatar\nmethods achieve high-quality avatar rendering, they still encounter challenges\nretaining intricate facial expression details because they overlook the\npotential of specific expression variations at different spatial positions when\nconditioning the radiance field. Motivated by this observation, we introduce a\nnovel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained\nby a simple MLP-based generation network, encompassing both spatial positional\nfeatures and global expression information. Benefiting from rich and diverse\ninformation of the SVE at different positions, the proposed SVE-conditioned\nneural radiance field can deal with intricate facial expressions and achieve\nrealistic rendering and geometry details of high-fidelity 3D head avatars.\nAdditionally, to further elevate the geometric and rendering quality, we\nintroduce a new coarse-to-fine training strategy, including a geometry\ninitialization strategy at the coarse stage and an adaptive importance sampling\nstrategy at the fine stage. Extensive experiments indicate that our method\noutperforms other state-of-the-art (SOTA) methods in rendering and geometry\nquality on mobile phone-collected and public datasets.\n","authors":["Minghan Qin","Yifan Liu","Yuelang Xu","Xiaochen Zhao","Yebin Liu","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06275v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2206.00311v3","updated":"2023-10-10T03:06:45Z","published":"2022-06-01T08:27:19Z","title":"MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining","summary":"  Text images contain both visual and linguistic information. However, existing\npre-training techniques for text recognition mainly focus on either visual\nrepresentation learning or linguistic knowledge learning. In this paper, we\npropose a novel approach MaskOCR to unify vision and language pre-training in\nthe classical encoder-decoder recognition framework. We adopt the masked image\nmodeling approach to pre-train the feature encoder using a large set of\nunlabeled real text images, which allows us to learn strong visual\nrepresentations. In contrast to introducing linguistic knowledge with an\nadditional language model, we directly pre-train the sequence decoder.\nSpecifically, we transform text data into synthesized text images to unify the\ndata modalities of vision and language, and enhance the language modeling\ncapability of the sequence decoder using a proposed masked image-language\nmodeling scheme. Significantly, the encoder is frozen during the pre-training\nphase of the sequence decoder. Experimental results demonstrate that our\nproposed method achieves superior performance on benchmark datasets, including\nChinese and English text images.\n","authors":["Pengyuan Lyu","Chengquan Zhang","Shanshan Liu","Meina Qiao","Yangliu Xu","Liang Wu","Kun Yao","Junyu Han","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2206.00311v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13570v3","updated":"2023-10-10T03:01:10Z","published":"2023-09-24T07:06:45Z","title":"Robust Digital-Twin Localization via An RGBD-based Transformer Network\n  and A Comprehensive Evaluation on a Mobile Dataset","summary":"  The potential of digital-twin technology, involving the creation of precise\ndigital replicas of physical objects, to reshape AR experiences in 3D object\ntracking and localization scenarios is significant. However, enabling robust 3D\nobject tracking in dynamic mobile AR environments remains a formidable\nchallenge. These scenarios often require a more robust pose estimator capable\nof handling the inherent sensor-level measurement noise. In this paper,\nrecognizing the challenges of comprehensive solutions in existing literature,\nwe propose a transformer-based 6DoF pose estimator designed to achieve\nstate-of-the-art accuracy under real-world noisy data. To systematically\nvalidate the new solution's performance against the prior art, we also\nintroduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2),\nwhich is focused on digital-twin object tracking scenarios. Expanded from an\nexisting DTTD v1 (DTTD1), the new dataset adds digital-twin data captured using\na cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the\napplicability of our approach to iPhone sensor data. Through extensive\nexperimentation and in-depth analysis, we illustrate the effectiveness of our\nmethods under significant depth data errors, surpassing the performance of\nexisting baselines. Code and dataset are made publicly available at:\nhttps://github.com/augcog/DTTD2\n","authors":["Zixun Huang","Keling Yao","Seth Z. Zhao","Chuanyu Pan","Tianjian Xu","Weiyu Feng","Allen Y. Yang"],"pdf_url":"https://arxiv.org/pdf/2309.13570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13808v3","updated":"2023-10-10T03:00:48Z","published":"2022-09-28T03:27:37Z","title":"Streaming Video Temporal Action Segmentation In Real Time","summary":"  Temporal action segmentation (TAS) is a critical step toward long-term video\nunderstanding. Recent studies follow a pattern that builds models based on\nfeatures instead of raw video picture information. However, we claim those\nmodels are trained complicatedly and limit application scenarios. It is hard\nfor them to segment human actions of video in real time because they must work\nafter the full video features are extracted. As the real-time action\nsegmentation task is different from TAS task, we define it as streaming video\nreal-time temporal action segmentation (SVTAS) task. In this paper, we propose\na real-time end-to-end multi-modality model for SVTAS task. More specifically,\nunder the circumstances that we cannot get any future information, we segment\nthe current human action of streaming video chunk in real time. Furthermore,\nthe model we propose combines the last steaming video chunk feature extracted\nby language model with the current image feature extracted by image model to\nimprove the quantity of real-time temporal action segmentation. To the best of\nour knowledge, it is the first multi-modality real-time temporal action\nsegmentation model. Under the same evaluation criteria as full video temporal\naction segmentation, our model segments human action in real time with less\nthan 40% of state-of-the-art model computation and achieves 90% of the accuracy\nof the full video state-of-the-art model.\n","authors":["Wujun Wen","Yunheng Li","Zhuben Dong","Lin Feng","Wanxiao Yang","Shenlan Liu"],"pdf_url":"https://arxiv.org/pdf/2209.13808v3.pdf","comment":"accepted by ISKE2023"},{"id":"http://arxiv.org/abs/2310.05202v2","updated":"2023-10-10T02:52:09Z","published":"2023-10-08T15:28:01Z","title":"Enhancing Cross-Dataset Performance of Distracted Driving Detection With\n  Score-Softmax Classifier","summary":"  Deep neural networks enable real-time monitoring of in-vehicle driver,\nfacilitating the timely prediction of distractions, fatigue, and potential\nhazards. This technology is now integral to intelligent transportation systems.\nRecent research has exposed unreliable cross-dataset end-to-end driver behavior\nrecognition due to overfitting, often referred to as ``shortcut learning\",\nresulting from limited data samples. In this paper, we introduce the\nScore-Softmax classifier, which addresses this issue by enhancing inter-class\nindependence and Intra-class uncertainty. Motivated by human rating patterns,\nwe designed a two-dimensional supervisory matrix based on marginal Gaussian\ndistributions to train the classifier. Gaussian distributions help amplify\nintra-class uncertainty while ensuring the Score-Softmax classifier learns\naccurate knowledge. Furthermore, leveraging the summation of independent\nGaussian distributed random variables, we introduced a multi-channel\ninformation fusion method. This strategy effectively resolves the\nmulti-information fusion challenge for the Score-Softmax classifier.\nConcurrently, we substantiate the necessity of transfer learning and\nmulti-dataset combination. We conducted cross-dataset experiments using the\nSFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax\nimproves cross-dataset performance without modifying the model architecture.\nThis provides a new approach for enhancing neural network generalization.\nAdditionally, our information fusion approach outperforms traditional methods.\n","authors":["Cong Duan","Zixuan Liu","Jiahao Xia","Minghai Zhang","Jiacai Liao","Libo Cao"],"pdf_url":"https://arxiv.org/pdf/2310.05202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13869v5","updated":"2023-10-10T02:49:40Z","published":"2022-10-25T09:45:49Z","title":"Jet tagging algorithm of graph network with HaarPooling message passing","summary":"  Recently methods of graph neural networks (GNNs) have been applied to solving\nthe problems in high energy physics (HEP) and have shown its great potential\nfor quark-gluon tagging with graph representation of jet events. In this paper,\nwe introduce an approach of GNNs combined with a HaarPooling operation to\nanalyze the events, called HaarPooling Message Passing neural network (HMPNet).\nIn HMPNet, HaarPooling not only extracts the features of graph, but embeds\nadditional information obtained by clustering of k-means of different particle\nfeatures. We construct Haarpooling from five different features: absolute\nenergy $\\log E$, transverse momentum $\\log p_T$, relative coordinates\n$(\\Delta\\eta,\\Delta\\phi)$, the mixed ones $(\\log E, \\log p_T)$ and $(\\log E,\n\\log p_T, \\Delta\\eta,\\Delta\\phi)$. The results show that an appropriate\nselection of information for HaarPooling enhances the accuracy of quark-gluon\ntagging, as adding extra information of $\\log P_T$ to the HMPNet outperforms\nall the others, whereas adding relative coordinates information\n$(\\Delta\\eta,\\Delta\\phi)$ is not very effective. This implies that by adding\neffective particle features from HaarPooling can achieve much better results\nthan solely pure message passing neutral network (MPNN) can do, which\ndemonstrates significant improvement of feature extraction via the pooling\nprocess. Finally we compare the HMPNet study, ordering by $p_T$, with other\nstudies and prove that the HMPNet is also a good choice of GNN algorithms for\njet tagging.\n","authors":["Fei Ma","Feiyi Liu","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2210.13869v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10502v2","updated":"2023-10-10T02:46:01Z","published":"2023-06-18T08:51:14Z","title":"Online Map Vectorization for Autonomous Driving: A Rasterization\n  Perspective","summary":"  Vectorized high-definition (HD) map is essential for autonomous driving,\nproviding detailed and precise environmental information for advanced\nperception and planning. However, current map vectorization methods often\nexhibit deviations, and the existing evaluation metric for map vectorization\nlacks sufficient sensitivity to detect these deviations. To address these\nlimitations, we propose integrating the philosophy of rasterization into map\nvectorization. Specifically, we introduce a new rasterization-based evaluation\nmetric, which has superior sensitivity and is better suited to real-world\nautonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization\nvia Rasterization), a novel framework that applies differentiable rasterization\nto vectorized outputs and then performs precise and geometry-aware supervision\non rasterized HD maps. Notably, MapVR designs tailored rasterization strategies\nfor various geometric shapes, enabling effective adaptation to a wide range of\nmap elements. Experiments show that incorporating rasterization into map\nvectorization greatly enhances performance with no extra computational cost\nduring inference, leading to more accurate map perception and ultimately\npromoting safer autonomous driving.\n","authors":["Gongjie Zhang","Jiahao Lin","Shuang Wu","Yilin Song","Zhipeng Luo","Yang Xue","Shijian Lu","Zuoguan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.10502v2.pdf","comment":"[NeurIPS 2023]"},{"id":"http://arxiv.org/abs/2306.17560v2","updated":"2023-10-10T02:33:18Z","published":"2023-06-30T11:23:49Z","title":"Class-Incremental Learning using Diffusion Model for Distillation and\n  Replay","summary":"  Class-incremental learning aims to learn new classes in an incremental\nfashion without forgetting the previously learned ones. Several research works\nhave shown how additional data can be used by incremental models to help\nmitigate catastrophic forgetting. In this work, following the recent\nbreakthrough in text-to-image generative models and their wide distribution, we\npropose the use of a pretrained Stable Diffusion model as a source of\nadditional data for class-incremental learning. Compared to competitive methods\nthat rely on external, often unlabeled, datasets of real images, our approach\ncan generate synthetic samples belonging to the same classes as the previously\nencountered images. This allows us to use those additional data samples not\nonly in the distillation loss but also for replay in the classification loss.\nExperiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and\nImageNet demonstrate how this new approach can be used to further improve the\nperformance of state-of-the-art methods for class-incremental learning on large\nscale datasets.\n","authors":["Quentin Jodelet","Xin Liu","Yin Jun Phua","Tsuyoshi Murata"],"pdf_url":"https://arxiv.org/pdf/2306.17560v2.pdf","comment":"Best paper award at 1st Workshop on Visual Continual Learning, ICCV\n  2023"},{"id":"http://arxiv.org/abs/2310.04671v2","updated":"2023-10-10T02:31:24Z","published":"2023-10-07T03:16:30Z","title":"Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem\n  Formulation and Dataset","summary":"  This paper addresses the problem of predicting hazards that drivers may\nencounter while driving a car. We formulate it as a task of anticipating\nimpending accidents using a single input image captured by car dashcams. Unlike\nexisting approaches to driving hazard prediction that rely on computational\nsimulations or anomaly detection from videos, this study focuses on high-level\ninference from static images. The problem needs predicting and reasoning about\nfuture events based on uncertain observations, which falls under visual\nabductive reasoning. To enable research in this understudied area, a new\ndataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is\ncreated. The dataset consists of 15K dashcam images of street scenes, and each\nimage is associated with a tuple containing car speed, a hypothesized hazard\ndescription, and visual entities present in the scene. These are annotated by\nhuman annotators, who identify risky scenes and provide descriptions of\npotential accidents that could occur a few seconds later. We present several\nbaseline methods and evaluate their performance on our dataset, identifying\nremaining issues and discussing future directions. This study contributes to\nthe field by introducing a novel problem formulation and dataset, enabling\nresearchers to explore the potential of multi-modal AI for driving hazard\nprediction.\n","authors":["Korawat Charoenpitaks","Van-Quang Nguyen","Masanori Suganuma","Masahiro Takahashi","Ryoma Niihara","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2310.04671v2.pdf","comment":"Main Paper: 10 pages, Supplementary Materials: 25 pages"},{"id":"http://arxiv.org/abs/2210.01189v2","updated":"2023-10-10T02:03:13Z","published":"2022-10-03T19:00:38Z","title":"Rank-N-Contrast: Learning Continuous Representations for Regression","summary":"  Deep regression models typically learn in an end-to-end fashion without\nexplicitly emphasizing a regression-aware representation. Consequently, the\nlearned representations exhibit fragmentation and fail to capture the\ncontinuous nature of sample orders, inducing suboptimal results across a wide\nrange of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a\nframework that learns continuous representations for regression by contrasting\nsamples against each other based on their rankings in the target space. We\ndemonstrate, theoretically and empirically, that RNC guarantees the desired\norder of learned representations in accordance with the target orders, enjoying\nnot only better performance but also significantly improved robustness,\nefficiency, and generalization. Extensive experiments using five real-world\nregression datasets that span computer vision, human-computer interaction, and\nhealthcare verify that RNC achieves state-of-the-art performance, highlighting\nits intriguing properties including better data efficiency, robustness to\nspurious targets and data corruptions, and generalization to distribution\nshifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.\n","authors":["Kaiwen Zha","Peng Cao","Jeany Son","Yuzhe Yang","Dina Katabi"],"pdf_url":"https://arxiv.org/pdf/2210.01189v2.pdf","comment":"NeurIPS 2023 Spotlight. The first two authors contributed equally to\n  this paper"},{"id":"http://arxiv.org/abs/2310.05446v2","updated":"2023-10-10T02:00:14Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v2.pdf","comment":"Updated version with a PDF"},{"id":"http://arxiv.org/abs/2310.05393v2","updated":"2023-10-10T01:59:36Z","published":"2023-10-09T04:16:35Z","title":"Hierarchical Side-Tuning for Vision Transformers","summary":"  Fine-tuning pre-trained Vision Transformers (ViT) has consistently\ndemonstrated promising performance in the realm of visual recognition. However,\nadapting large pre-trained models to various tasks poses a significant\nchallenge. This challenge arises from the need for each model to undergo an\nindependent and comprehensive fine-tuning process, leading to substantial\ncomputational and memory demands. While recent advancements in\nParameter-efficient Transfer Learning (PETL) have demonstrated their ability to\nachieve superior performance compared to full fine-tuning with a smaller subset\nof parameter updates, they tend to overlook dense prediction tasks such as\nobject detection and segmentation. In this paper, we introduce Hierarchical\nSide-Tuning (HST), a novel PETL approach that enables ViT transfer to various\ndownstream tasks effectively. Diverging from existing methods that exclusively\nfine-tune parameters within input spaces or certain modules connected to the\nbackbone, we tune a lightweight and hierarchical side network (HSN) that\nleverages intermediate activations extracted from the backbone and generates\nmulti-scale features to make predictions. To validate HST, we conducted\nextensive experiments encompassing diverse visual tasks, including\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, our method achieves state-of-the-art average Top-1\naccuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.\nWhen applied to object detection tasks on COCO testdev benchmark, HST even\nsurpasses full fine-tuning and obtains better performance with 49.7 box AP and\n43.2 mask AP using Cascade Mask R-CNN.\n","authors":["Weifeng Lin","Ziheng Wu","Jiayu Chen","Wentao Yang","Mingxin Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2310.05393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v2","updated":"2023-10-10T01:31:06Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high. In light of these observations, we conclude that TTA for\nsegmentation presents significant challenges, and simply using classic TTA\nmethods cannot address this problem well.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04893v3","updated":"2023-10-10T01:24:45Z","published":"2023-06-08T02:45:15Z","title":"Coping with Change: Learning Invariant and Minimum Sufficient\n  Representations for Fine-Grained Visual Categorization","summary":"  Fine-grained visual categorization (FGVC) is a challenging task due to\nsimilar visual appearances between various species. Previous studies always\nimplicitly assume that the training and test data have the same underlying\ndistributions, and that features extracted by modern backbone architectures\nremain discriminative and generalize well to unseen test data. However, we\nempirically justify that these conditions are not always true on benchmark\ndatasets. To this end, we combine the merits of invariant risk minimization\n(IRM) and information bottleneck (IB) principle to learn invariant and minimum\nsufficient (IMS) representations for FGVC, such that the overall model can\nalways discover the most succinct and consistent fine-grained features. We\napply the matrix-based R{\\'e}nyi's $\\alpha$-order entropy to simplify and\nstabilize the training of IB; we also design a ``soft\" environment partition\nscheme to make IRM applicable to FGVC task. To the best of our knowledge, we\nare the first to address the problem of FGVC from a generalization perspective\nand develop a new information-theoretic solution accordingly. Extensive\nexperiments demonstrate the consistent performance gain offered by our IMS.\n","authors":["Shuo Ye","Shujian Yu","Wenjin Hou","Yu Wang","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2306.04893v3.pdf","comment":"Manuscript accepted by CVIU, code is available at Github"},{"id":"http://arxiv.org/abs/2310.06238v1","updated":"2023-10-10T01:22:41Z","published":"2023-10-10T01:22:41Z","title":"Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for\n  Unbiased Question-Answering","summary":"  In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.\n","authors":["Xiulong Liu","Zhikang Dong","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06234v1","updated":"2023-10-10T01:04:15Z","published":"2023-10-10T01:04:15Z","title":"Efficient Adaptation of Large Vision Transformer via Adapter\n  Re-Composing","summary":"  The advent of high-capacity pre-trained models has revolutionized\nproblem-solving in computer vision, shifting the focus from training\ntask-specific models to adapting pre-trained models. Consequently, effectively\nadapting large pre-trained models to downstream tasks in an efficient manner\nhas become a prominent research area. Existing solutions primarily concentrate\non designing lightweight adapters and their interaction with pre-trained\nmodels, with the goal of minimizing the number of parameters requiring updates.\nIn this study, we propose a novel Adapter Re-Composing (ARC) strategy that\naddresses efficient pre-trained model adaptation from a fresh perspective. Our\napproach considers the reusability of adaptation parameters and introduces a\nparameter-sharing scheme. Specifically, we leverage symmetric\ndown-/up-projections to construct bottleneck operations, which are shared\nacross layers. By learning low-dimensional re-scaling coefficients, we can\neffectively re-compose layer-adaptive adapters. This parameter-sharing strategy\nin adapter design allows us to significantly reduce the number of new\nparameters while maintaining satisfactory performance, thereby offering a\npromising approach to compress the adaptation cost. We conduct experiments on\n24 downstream image classification tasks using various Vision Transformer\nvariants to evaluate our method. The results demonstrate that our approach\nachieves compelling transfer learning performance with a reduced parameter\ncount. Our code is available at\n\\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.\n","authors":["Wei Dong","Dawei Yan","Zhijun Lin","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06234v1.pdf","comment":"Paper is accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06232v1","updated":"2023-10-10T00:59:26Z","published":"2023-10-10T00:59:26Z","title":"Spiking PointNet: Spiking Neural Networks for Point Clouds","summary":"  Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency,\nhave drawn much research attention on 2D visual recognition and shown gradually\nincreasing application potential. However, it still remains underexplored\nwhether SNNs can be generalized to 3D recognition. To this end, we present\nSpiking PointNet in the paper, the first spiking neural model for efficient\ndeep learning on point clouds. We discover that the two huge obstacles limiting\nthe application of SNNs in point clouds are: the intrinsic optimization\nobstacle of SNNs that impedes the training of a big spiking model with large\ntime steps, and the expensive memory and computation cost of PointNet that\nmakes training a big spiking point model unrealistic. To solve the problems\nsimultaneously, we present a trained-less but learning-more paradigm for\nSpiking PointNet with theoretical justifications and in-depth experimental\nanalysis. In specific, our Spiking PointNet is trained with only a single time\nstep but can obtain better performance with multiple time steps inference,\ncompared to the one trained directly with multiple time steps. We conduct\nvarious experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness\nof Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN\ncounterpart, which is rare in the SNN field thus providing a potential research\ndirection for the following work. Moreover, Spiking PointNet shows impressive\nspeedup and storage saving in the training phase.\n","authors":["Dayong Ren","Zhe Ma","Yuanpei Chen","Weihang Peng","Xiaode Liu","Yuhan Zhang","Yufei Guo"],"pdf_url":"https://arxiv.org/pdf/2310.06232v1.pdf","comment":"Accepted by NeurIPS"},{"id":"http://arxiv.org/abs/2310.06214v1","updated":"2023-10-10T00:07:25Z","published":"2023-10-10T00:07:25Z","title":"CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding","summary":"  3D visual grounding is the ability to localize objects in 3D scenes\nconditioned by utterances. Most existing methods devote the referring head to\nlocalize the referred object directly, causing failure in complex scenarios. In\naddition, it does not illustrate how and why the network reaches the final\ndecision. In this paper, we address this question Can we design an\ninterpretable 3D visual grounding framework that has the potential to mimic the\nhuman perception system?. To this end, we formulate the 3D visual grounding\nproblem as a sequence-to-sequence task by first predicting a chain of anchors\nand then the final target. Interpretability not only improves the overall\nperformance but also helps us identify failure cases. Following the chain of\nthoughts approach enables us to decompose the referring task into interpretable\nintermediate steps, boosting the performance and making our framework extremely\ndata-efficient. Moreover, our proposed framework can be easily integrated into\nany existing architecture. We validate our approach through comprehensive\nexperiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent\nperformance gains compared to existing methods without requiring manually\nannotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is\nsignificantly data-efficient, whereas on the Sr3D dataset, when trained only on\n10% of the data, we match the SOTA performance that trained on the entire data.\n","authors":["Eslam Mohamed Bakr","Mohamed Ayman","Mahmoud Ahmed","Habib Slim","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2310.06214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1903.06262v8","updated":"2023-10-10T23:31:32Z","published":"2019-03-08T18:20:36Z","title":"A Grid-based Method for Removing Overlaps of Dimensionality Reduction\n  Scatterplot Layouts","summary":"  Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous\nvisualization tool for analyzing multidimensional datasets. Despite their\npopularity, such scatterplots suffer from occlusion, especially when\ninformative glyphs are used to represent data instances, potentially\nobfuscating critical information for the analysis under execution. Different\nstrategies have been devised to address this issue, either producing\noverlap-free layouts that lack the powerful capabilities of contemporary DR\ntechniques in uncovering interesting data patterns or eliminating overlaps as a\npost-processing strategy. Despite the good results of post-processing\ntechniques, most of the best methods typically expand or distort the\nscatterplot area, thus reducing glyphs' size (sometimes) to unreadable\ndimensions, defeating the purpose of removing overlaps. This paper presents\nDistance Grid (DGrid), a novel post-processing strategy to remove overlaps from\nDR layouts that faithfully preserves the original layout's characteristics and\nbounds the minimum glyph sizes. We show that DGrid surpasses the\nstate-of-the-art in overlap removal (through an extensive comparative\nevaluation considering multiple different metrics) while also being one of the\nfastest techniques, especially for large datasets. A user study with 51\nparticipants also shows that DGrid is consistently ranked among the top\ntechniques for preserving the original scatterplots' visual characteristics and\nthe aesthetics of the final results.\n","authors":["Gladys M. Hilasaca","Wilson E. Marclio-Jr","Danilo M. Eler","Rafael M. Martins","Fernando V. Paulovich"],"pdf_url":"https://arxiv.org/pdf/1903.06262v8.pdf","comment":"14 pages, 10 figures. A preprint version of a publication at IEEE\n  Transactions on Visualization and Computer Graphics (TVCG), 2023"},{"id":"http://arxiv.org/abs/2301.05174v2","updated":"2023-10-10T22:58:45Z","published":"2023-01-12T18:00:00Z","title":"Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A\n  Reproducibility Study","summary":"  Most approaches to cross-modal retrieval (CMR) focus either on object-centric\ndatasets, meaning that each document depicts or describes a single object, or\non scene-centric datasets, meaning that each image depicts or describes a\ncomplex scene that involves multiple objects and relations between them. We\nposit that a robust CMR model should generalize well across both dataset types.\nDespite recent advances in CMR, the reproducibility of the results and their\ngeneralizability across different dataset types has not been studied before. We\naddress this gap and focus on the reproducibility of the state-of-the-art CMR\nresults when evaluated on object-centric and scene-centric datasets. We select\ntwo state-of-the-art CMR models with different architectures: (i) CLIP; and\n(ii) X-VLM. Additionally, we select two scene-centric datasets, and three\nobject-centric datasets, and determine the relative performance of the selected\nmodels on these datasets. We focus on reproducibility, replicability, and\ngeneralizability of the outcomes of previously published CMR experiments. We\ndiscover that the experiments are not fully reproducible and replicable.\nBesides, the relative performance results partially generalize across\nobject-centric and scene-centric datasets. On top of that, the scores obtained\non object-centric datasets are much lower than the scores obtained on\nscene-centric datasets. For reproducibility and transparency we make our source\ncode and the trained models publicly available.\n","authors":["Mariya Hendriksen","Svitlana Vakulenko","Ernst Kuiper","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2301.05174v2.pdf","comment":"18 pages, accepted as a reproducibility paper at ECIR 2023"},{"id":"http://arxiv.org/abs/2306.04865v3","updated":"2023-10-10T22:56:41Z","published":"2023-06-08T01:35:43Z","title":"MyStyle++: A Controllable Personalized Generative Prior","summary":"  In this paper, we propose an approach to obtain a personalized generative\nprior with explicit control over a set of attributes. We build upon MyStyle, a\nrecently introduced method, that tunes the weights of a pre-trained StyleGAN\nface generator on a few images of an individual. This system allows\nsynthesizing, editing, and enhancing images of the target individual with high\nfidelity to their facial features. However, MyStyle does not demonstrate\nprecise control over the attributes of the generated images. We propose to\naddress this problem through a novel optimization system that organizes the\nlatent space in addition to tuning the generator. Our key contribution is to\nformulate a loss that arranges the latent codes, corresponding to the input\nimages, along a set of specific directions according to their attributes. We\ndemonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,\nand enhance images of an individual with great control over the attributes,\nwhile preserving the unique facial characteristics of that individual.\n","authors":["Libing Zeng","Lele Chen","Yi Xu","Nima Kalantari"],"pdf_url":"https://arxiv.org/pdf/2306.04865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07060v1","updated":"2023-10-10T22:54:01Z","published":"2023-10-10T22:54:01Z","title":"BeSt-LeS: Benchmarking Stroke Lesion Segmentation using Deep Supervision","summary":"  Brain stroke has become a significant burden on global health and thus we\nneed remedies and prevention strategies to overcome this challenge. For this,\nthe immediate identification of stroke and risk stratification is the primary\ntask for clinicians. To aid expert clinicians, automated segmentation models\nare crucial. In this work, we consider the publicly available dataset ATLAS\n$v2.0$ to benchmark various end-to-end supervised U-Net style models.\nSpecifically, we have benchmarked models on both 2D and 3D brain images and\nevaluated them using standard metrics. We have achieved the highest Dice score\nof 0.583 on the 2D transformer-based model and 0.504 on the 3D residual U-Net\nrespectively. We have conducted the Wilcoxon test for 3D models to correlate\nthe relationship between predicted and actual stroke volume. For\nreproducibility, the code and model weights are made publicly available:\nhttps://github.com/prantik-pdeb/BeSt-LeS.\n","authors":["Prantik Deb","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju S"],"pdf_url":"https://arxiv.org/pdf/2310.07060v1.pdf","comment":"Accepted to MICCAI BrainLes 2023 (oral)"},{"id":"http://arxiv.org/abs/2310.07056v1","updated":"2023-10-10T22:36:15Z","published":"2023-10-10T22:36:15Z","title":"TextPSG: Panoptic Scene Graph Generation from Textual Descriptions","summary":"  Panoptic Scene Graph has recently been proposed for comprehensive scene\nunderstanding. However, previous works adopt a fully-supervised learning\nmanner, requiring large amounts of pixel-wise densely-annotated data, which is\nalways tedious and expensive to obtain. To address this limitation, we study a\nnew problem of Panoptic Scene Graph Generation from Purely Textual Descriptions\n(Caption-to-PSG). The key idea is to leverage the large collection of free\nimage-caption data on the Web alone to generate panoptic scene graphs. The\nproblem is very challenging for three constraints: 1) no location priors; 2) no\nexplicit links between visual regions and textual entities; and 3) no\npre-defined concept sets. To tackle this problem, we propose a new framework\nTextPSG consisting of four modules, i.e., a region grouper, an entity grounder,\na segment merger, and a label generator, with several novel techniques. The\nregion grouper first groups image pixels into different segments and the entity\ngrounder then aligns visual segments with language entities based on the\ntextual description of the segment being referred to. The grounding results can\nthus serve as pseudo labels enabling the segment merger to learn the segment\nsimilarity as well as guiding the label generator to learn object semantics and\nrelation predicates, resulting in a fine-grained structured scene\nunderstanding. Our framework is effective, significantly outperforming the\nbaselines and achieving strong out-of-distribution robustness. We perform\ncomprehensive ablation studies to corroborate the effectiveness of our design\nchoices and provide an in-depth analysis to highlight future directions. Our\ncode, data, and results are available on our project page:\nhttps://vis-www.cs.umass.edu/TextPSG.\n","authors":["Chengyang Zhao","Yikang Shen","Zhenfang Chen","Mingyu Ding","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2310.07056v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.00772v2","updated":"2023-10-10T21:42:59Z","published":"2023-10-01T19:41:49Z","title":"SMOOT: Saliency Guided Mask Optimized Online Training","summary":"  Deep Neural Networks are powerful tools for understanding complex patterns\nand making decisions. However, their black-box nature impedes a complete\nunderstanding of their inner workings. Saliency-Guided Training (SGT) methods\ntry to highlight the prominent features in the model's training based on the\noutput to alleviate this problem. These methods use back-propagation and\nmodified gradients to guide the model toward the most relevant features while\nkeeping the impact on the prediction accuracy negligible. SGT makes the model's\nfinal result more interpretable by masking input partially. In this way,\nconsidering the model's output, we can infer how each segment of the input\naffects the output. In the particular case of image as the input, masking is\napplied to the input pixels. However, the masking strategy and number of pixels\nwhich we mask, are considered as a hyperparameter. Appropriate setting of\nmasking strategy can directly affect the model's training. In this paper, we\nfocus on this issue and present our contribution. We propose a novel method to\ndetermine the optimal number of masked images based on input, accuracy, and\nmodel loss during the training. The strategy prevents information loss which\nleads to better accuracy values. Also, by integrating the model's performance\nin the strategy formula, we show that our model represents the salient features\nmore meaningful. Our experimental results demonstrate a substantial improvement\nin both model accuracy and the prominence of saliency, thereby affirming the\neffectiveness of our proposed solution.\n","authors":["Ali Karkehabadi","Houman Homayoun","Avesta Sasan"],"pdf_url":"https://arxiv.org/pdf/2310.00772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07033v1","updated":"2023-10-10T21:40:19Z","published":"2023-10-10T21:40:19Z","title":"Computational Pathology at Health System Scale -- Self-Supervised\n  Foundation Models from Three Billion Images","summary":"  Recent breakthroughs in self-supervised learning have enabled the use of\nlarge unlabeled datasets to train visual foundation models that can generalize\nto a variety of downstream tasks. While this training paradigm is well suited\nfor the medical domain where annotations are scarce, large-scale pre-training\nin the medical domain, and in particular pathology, has not been extensively\nstudied. Previous work in self-supervised learning in pathology has leveraged\nsmaller datasets for both pre-training and evaluating downstream performance.\nThe aim of this project is to train the largest academic foundation model and\nbenchmark the most prominent self-supervised learning algorithms by\npre-training and evaluating downstream performance on large clinical pathology\ndatasets. We collected the largest pathology dataset to date, consisting of\nover 3 billion images from over 423 thousand microscopy slides. We compared\npre-training of visual transformer models using the masked autoencoder (MAE)\nand DINO algorithms. We evaluated performance on six clinically relevant tasks\nfrom three anatomic sites and two institutions: breast cancer detection,\ninflammatory bowel disease detection, breast cancer estrogen receptor\nprediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer\nimmunotherapy response prediction. Our results demonstrate that pre-training on\npathology data is beneficial for downstream performance compared to\npre-training on natural images. Additionally, the DINO algorithm achieved\nbetter generalization performance across all tasks tested. The presented\nresults signify a phase change in computational pathology research, paving the\nway into a new era of more performant models based on large-scale, parallel\npre-training at the billion-image scale.\n","authors":["Gabriele Campanella","Ricky Kwan","Eugene Fluder","Jennifer Zeng","Aryeh Stock","Brandon Veremis","Alexandros D. Polydorides","Cyrus Hedvat","Adam Schoenfeld","Chad Vanderbilt","Patricia Kovatch","Carlos Cordon-Cardo","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2310.07033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07028v1","updated":"2023-10-10T21:30:05Z","published":"2023-10-10T21:30:05Z","title":"Facial Forgery-based Deepfake Detection using Fine-Grained Features","summary":"  Facial forgery by deepfakes has caused major security risks and raised severe\nsocietal concerns. As a countermeasure, a number of deepfake detection methods\nhave been proposed. Most of them model deepfake detection as a binary\nclassification problem using a backbone convolutional neural network (CNN)\narchitecture pretrained for the task. These CNN-based methods have demonstrated\nvery high efficacy in deepfake detection with the Area under the Curve (AUC) as\nhigh as $0.99$. However, the performance of these methods degrades\nsignificantly when evaluated across datasets and deepfake manipulation\ntechniques. This draws our attention towards learning more subtle, local, and\ndiscriminative features for deepfake detection. In this paper, we formulate\ndeepfake detection as a fine-grained classification problem and propose a new\nfine-grained solution to it. Specifically, our method is based on learning\nsubtle and generalizable features by effectively suppressing background noise\nand learning discriminative features at various scales for deepfake detection.\nThrough extensive experimental validation, we demonstrate the superiority of\nour method over the published research in cross-dataset and cross-manipulation\ngeneralization of deepfake detectors for the majority of the experimental\nscenarios.\n","authors":["Aakash Varma Nadimpalli","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2310.07028v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.07027v1","updated":"2023-10-10T21:29:41Z","published":"2023-10-10T21:29:41Z","title":"Utilizing Synthetic Data for Medical Vision-Language Pre-training:\n  Bypassing the Need for Real Images","summary":"  Medical Vision-Language Pre-training (VLP) learns representations jointly\nfrom medical images and paired radiology reports. It typically requires\nlarge-scale paired image-text datasets to achieve effective pre-training for\nboth the image encoder and text encoder. The advent of text-guided generative\nmodels raises a compelling question: Can VLP be implemented solely with\nsynthetic images generated from genuine radiology reports, thereby mitigating\nthe need for extensively pairing and curating image-text datasets? In this\nwork, we scrutinize this very question by examining the feasibility and\neffectiveness of employing synthetic images for medical VLP. We replace real\nmedical images with their synthetic equivalents, generated from authentic\nmedical reports. Utilizing three state-of-the-art VLP algorithms, we\nexclusively train on these synthetic samples. Our empirical evaluation across\nthree subsequent tasks, namely image classification, semantic segmentation and\nobject detection, reveals that the performance achieved through synthetic data\nis on par with or even exceeds that obtained with real images. As a pioneering\ncontribution to this domain, we introduce a large-scale synthetic medical image\ndataset, paired with anonymized real radiology reports. This alleviates the\nneed of sharing medical images, which are not easy to curate and share in\npractice. The code and the dataset will be made publicly available upon paper\nacceptance.\n","authors":["Che Liu","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2310.07027v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.07021v1","updated":"2023-10-10T21:16:29Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08123v2","updated":"2023-10-10T20:28:15Z","published":"2023-07-16T18:42:01Z","title":"Solving Inverse Problems with Latent Diffusion Models via Hard Data\n  Consistency","summary":"  Diffusion models have recently emerged as powerful generative priors for\nsolving inverse problems. However, training diffusion models in the pixel space\nare both data-intensive and computationally demanding, which restricts their\napplicability as priors for high-dimensional real-world data such as medical\nimages. Latent diffusion models, which operate in a much lower-dimensional\nspace, offer a solution to these challenges. However, incorporating latent\ndiffusion models to solve inverse problems remains a challenging problem due to\nthe nonlinearity of the encoder and decoder. To address these issues, we\npropose \\textit{ReSample}, an algorithm that can solve general inverse problems\nwith pre-trained latent diffusion models. Our algorithm incorporates data\nconsistency by solving an optimization problem during the reverse sampling\nprocess, a concept that we term as hard data consistency. Upon solving this\noptimization problem, we propose a novel resampling scheme to map the\nmeasurement-consistent sample back onto the noisy data manifold and\ntheoretically demonstrate its benefits. Lastly, we apply our algorithm to solve\na wide range of linear and nonlinear inverse problems in both natural and\nmedical images, demonstrating that our approach outperforms existing\nstate-of-the-art approaches, including those based on pixel-space diffusion\nmodels.\n","authors":["Bowen Song","Soo Min Kwon","Zecheng Zhang","Xinyu Hu","Qing Qu","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2307.08123v2.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2306.12511v3","updated":"2023-10-10T20:27:25Z","published":"2023-06-21T18:49:22Z","title":"Semi-Implicit Denoising Diffusion Models (SIDDMs)","summary":"  Despite the proliferation of generative models, achieving fast sampling\nduring inference without compromising sample diversity and quality remains\nchallenging. Existing models such as Denoising Diffusion Probabilistic Models\n(DDPM) deliver high-quality, diverse samples but are slowed by an inherently\nhigh number of iterative steps. The Denoising Diffusion Generative Adversarial\nNetworks (DDGAN) attempted to circumvent this limitation by integrating a GAN\nmodel for larger jumps in the diffusion process. However, DDGAN encountered\nscalability limitations when applied to large datasets. To address these\nlimitations, we introduce a novel approach that tackles the problem by matching\nimplicit and explicit factors. More specifically, our approach involves\nutilizing an implicit model to match the marginal distributions of noisy data\nand the explicit conditional distribution of the forward diffusion. This\ncombination allows us to effectively match the joint denoising distributions.\nUnlike DDPM but similar to DDGAN, we do not enforce a parametric distribution\nfor the reverse step, enabling us to take large steps during inference. Similar\nto the DDPM but unlike DDGAN, we take advantage of the exact form of the\ndiffusion process. We demonstrate that our proposed method obtains comparable\ngenerative performance to diffusion-based models and vastly superior results to\nmodels with a small number of sampling steps.\n","authors":["Yanwu Xu","Mingming Gong","Shaoan Xie","Wei Wei","Matthias Grundmann","Kayhan Batmanghelich","Tingbo Hou"],"pdf_url":"https://arxiv.org/pdf/2306.12511v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06992v1","updated":"2023-10-10T20:25:30Z","published":"2023-10-10T20:25:30Z","title":"Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models","summary":"  Object tracking is central to robot perception and scene understanding.\nTracking-by-detection has long been a dominant paradigm for object tracking of\nspecific object categories. Recently, large-scale pre-trained models have shown\npromising advances in detecting and segmenting objects and parts in 2D static\nimages in the wild. This begs the question: can we re-purpose these large-scale\npre-trained static image models for open-vocabulary video tracking? In this\npaper, we re-purpose an open-vocabulary detector, segmenter, and dense optical\nflow estimator, into a model that tracks and segments objects of any category\nin 2D videos. Our method predicts object and part tracks with associated\nlanguage descriptions in monocular videos, rebuilding the pipeline of Tractor\nwith modern large pre-trained models for static image detection and\nsegmentation: we detect open-vocabulary object instances and propagate their\nboxes from frame to frame using a flow-based motion model, refine the\npropagated boxes with the box regression module of the visual detector, and\nprompt an open-world segmenter with the refined box to segment the objects. We\ndecide the termination of an object track based on the objectness score of the\npropagated boxes, as well as forward-backward optical flow consistency. We\nre-identify objects across occlusions using deep feature matching. We show that\nour model achieves strong performance on multiple established video object\nsegmentation and tracking benchmarks, and can produce reasonable tracks in\nmanipulation data. In particular, our model outperforms previous\nstate-of-the-art in UVO and BURST, benchmarks for open-world object tracking\nand segmentation, despite never being explicitly trained for tracking. We hope\nthat our approach can serve as a simple and extensible framework for future\nresearch.\n","authors":["Wen-Hsuan Chu","Adam W. Harley","Pavel Tokmakov","Achal Dave","Leonidas Guibas","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.06992v1.pdf","comment":"Project page available at https://wenhsuanchu.github.io/ovtracktor/"},{"id":"http://arxiv.org/abs/2310.06984v1","updated":"2023-10-10T20:11:13Z","published":"2023-10-10T20:11:13Z","title":"Leveraging Neural Radiance Fields for Uncertainty-Aware Visual\n  Localization","summary":"  As a promising fashion for visual localization, scene coordinate regression\n(SCR) has seen tremendous progress in the past decade. Most recent methods\nusually adopt neural networks to learn the mapping from image pixels to 3D\nscene coordinates, which requires a vast amount of annotated training data. We\npropose to leverage Neural Radiance Fields (NeRF) to generate training samples\nfor SCR. Despite NeRF's efficiency in rendering, many of the rendered data are\npolluted by artifacts or only contain minimal information gain, which can\nhinder the regression accuracy or bring unnecessary computational costs with\nredundant data. These challenges are addressed in three folds in this paper:\n(1) A NeRF is designed to separately predict uncertainties for the rendered\ncolor and depth images, which reveal data reliability at the pixel level. (2)\nSCR is formulated as deep evidential learning with epistemic uncertainty, which\nis used to evaluate information gain and scene coordinate quality. (3) Based on\nthe three arts of uncertainties, a novel view selection policy is formed that\nsignificantly improves data efficiency. Experiments on public datasets\ndemonstrate that our method could select the samples that bring the most\ninformation gain and promote the performance with the highest efficiency.\n","authors":["Le Chen","Weirong Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2310.06984v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06982v1","updated":"2023-10-10T20:04:44Z","published":"2023-10-10T20:04:44Z","title":"Data Distillation Can Be Like Vodka: Distilling More Times For Better\n  Quality","summary":"  Dataset distillation aims to minimize the time and memory needed for training\ndeep networks on large datasets, by creating a small set of synthetic images\nthat has a similar generalization performance to that of the full dataset.\nHowever, current dataset distillation techniques fall short, showing a notable\nperformance gap when compared to training on the original data. In this work,\nwe are the first to argue that using just one synthetic subset for distillation\nwill not yield optimal generalization performance. This is because the training\ndynamics of deep networks drastically change during the training. Hence,\nmultiple synthetic subsets are required to capture the training dynamics at\ndifferent phases of training. To address this issue, we propose Progressive\nDataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic\nimages, each conditioned on the previous sets, and trains the model on the\ncumulative union of these subsets without requiring additional training time.\nOur extensive experiments show that PDD can effectively improve the performance\nof existing dataset distillation methods by up to 4.3%. In addition, our method\nfor the first time enable generating considerably larger synthetic datasets.\n","authors":["Xuxi Chen","Yu Yang","Zhangyang Wang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2310.06982v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.06968v1","updated":"2023-10-10T19:46:58Z","published":"2023-10-10T19:46:58Z","title":"ObjectComposer: Consistent Generation of Multiple Objects Without\n  Fine-tuning","summary":"  Recent text-to-image generative models can generate high-fidelity images from\ntext prompts. However, these models struggle to consistently generate the same\nobjects in different contexts with the same appearance. Consistent object\ngeneration is important to many downstream tasks like generating comic book\nillustrations with consistent characters and setting. Numerous approaches\nattempt to solve this problem by extending the vocabulary of diffusion models\nthrough fine-tuning. However, even lightweight fine-tuning approaches can be\nprohibitively expensive to run at scale and in real-time. We introduce a method\ncalled ObjectComposer for generating compositions of multiple objects that\nresemble user-specified images. Our approach is training-free, leveraging the\nabilities of preexisting models. We build upon the recent BLIP-Diffusion model,\nwhich can generate images of single objects specified by reference images.\nObjectComposer enables the consistent generation of compositions containing\nmultiple specific objects simultaneously, all without modifying the weights of\nthe underlying models.\n","authors":["Alec Helbling","Evan Montoya","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2310.06968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15321v5","updated":"2023-10-10T19:44:36Z","published":"2023-08-29T14:16:09Z","title":"Elucidating the Exposure Bias in Diffusion Models","summary":"  Diffusion models have demonstrated impressive generative capabilities, but\ntheir \\textit{exposure bias} problem, described as the input mismatch between\ntraining and sampling, lacks in-depth exploration. In this paper, we\nsystematically investigate the exposure bias problem in diffusion models by\nfirst analytically modelling the sampling distribution, based on which we then\nattribute the prediction error at each sampling step as the root cause of the\nexposure bias issue. Furthermore, we discuss potential solutions to this issue\nand propose an intuitive metric for it. Along with the elucidation of exposure\nbias, we propose a simple, yet effective, training-free method called Epsilon\nScaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly\nmoves the sampling trajectory closer to the vector field learned in the\ntraining phase by scaling down the network output (Epsilon), mitigating the\ninput mismatch between training and sampling. Experiments on various diffusion\nframeworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings,\nand deterministic vs. stochastic sampling verify the effectiveness of our\nmethod. Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID\non CIFAR-10 under 100-step unconditional generation. The code is available at\n\\url{https://github.com/forever208/ADM-ES} and\n\\url{https://github.com/forever208/EDM-ES}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2308.15321v5.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2310.06966v1","updated":"2023-10-10T19:32:59Z","published":"2023-10-10T19:32:59Z","title":"On the Interpretability of Part-Prototype Based Classifiers: A Human\n  Centric Analysis","summary":"  Part-prototype networks have recently become methods of interest as an\ninterpretable alternative to many of the current black-box image classifiers.\nHowever, the interpretability of these methods from the perspective of human\nusers has not been sufficiently explored. In this work, we have devised a\nframework for evaluating the interpretability of part-prototype-based models\nfrom a human perspective. The proposed framework consists of three actionable\nmetrics and experiments. To demonstrate the usefulness of our framework, we\nperformed an extensive set of experiments using Amazon Mechanical Turk. They\nnot only show the capability of our framework in assessing the interpretability\nof various part-prototype-based models, but they also are, to the best of our\nknowledge, the most comprehensive work on evaluating such methods in a unified\nframework.\n","authors":["Omid Davoodi","Shayan Mohammadizadehsamakosh","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2310.06966v1.pdf","comment":"Intended for submission to Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2310.06958v1","updated":"2023-10-10T19:21:41Z","published":"2023-10-10T19:21:41Z","title":"Comparing the robustness of modern no-reference image- and video-quality\n  metrics to adversarial attacks","summary":"  Nowadays neural-network-based image- and video-quality metrics show better\nperformance compared to traditional methods. However, they also became more\nvulnerable to adversarial attacks that increase metrics' scores without\nimproving visual quality. The existing benchmarks of quality metrics compare\ntheir performance in terms of correlation with subjective quality and\ncalculation time. However, the adversarial robustness of image-quality metrics\nis also an area worth researching. In this paper, we analyse modern metrics'\nrobustness to different adversarial attacks. We adopted adversarial attacks\nfrom computer vision tasks and compared attacks' efficiency against 15\nno-reference image/video-quality metrics. Some metrics showed high resistance\nto adversarial attacks which makes their usage in benchmarks safer than\nvulnerable metrics. The benchmark accepts new metrics submissions for\nresearchers who want to make their metrics more robust to attacks or to find\nsuch metrics for their needs. Try our benchmark using pip install\nrobustness-benchmark.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Sergey Lavrushkin","Ekaterina Shumitskaya","Maksim Velikanov","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06945v1","updated":"2023-10-10T19:06:10Z","published":"2023-10-10T19:06:10Z","title":"End-to-end Evaluation of Practical Video Analytics Systems for Face\n  Detection and Recognition","summary":"  Practical video analytics systems that are deployed in bandwidth constrained\nenvironments like autonomous vehicles perform computer vision tasks such as\nface detection and recognition. In an end-to-end face analytics system, inputs\nare first compressed using popular video codecs like HEVC and then passed onto\nmodules that perform face detection, alignment, and recognition sequentially.\nTypically, the modules of these systems are evaluated independently using\ntask-specific imbalanced datasets that can misconstrue performance estimates.\nIn this paper, we perform a thorough end-to-end evaluation of a face analytics\nsystem using a driving-specific dataset, which enables meaningful\ninterpretations. We demonstrate how independent task evaluations, dataset\nimbalances, and inconsistent annotations can lead to incorrect system\nperformance estimates. We propose strategies to create balanced evaluation\nsubsets of our dataset and to make its annotations consistent across multiple\nanalytics tasks and scenarios. We then evaluate the end-to-end system\nperformance sequentially to account for task interdependencies. Our experiments\nshow that our approach provides consistent, accurate, and interpretable\nestimates of the system's performance which is critical for real-world\napplications.\n","authors":["Praneet Singh","Edward J. Delp","Amy R. Reibman"],"pdf_url":"https://arxiv.org/pdf/2310.06945v1.pdf","comment":"Accepted to Autonomous Vehicles and Machines 2023 Conference, IS&T\n  Electronic Imaging (EI) Symposium"},{"id":"http://arxiv.org/abs/2305.09900v2","updated":"2023-10-10T19:01:41Z","published":"2023-05-17T02:20:34Z","title":"Efficient Equivariant Transfer Learning from Pretrained Models","summary":"  Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of Basu\net al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and\noptimization-based methods, respectively, over features from group-transformed\ninputs to obtain equivariant outputs from non-equivariant neural networks.\nWhile Kaba et al. (2022) are only concerned with training from scratch, we find\nthat equitune performs poorly on equivariant zero-shot tasks despite good\nfinetuning results. We hypothesize that this is because pretrained models\nprovide better quality features for certain transformations than others and\nsimply averaging them is deleterious. Hence, we propose {\\lambda}-equitune that\naverages the features using importance weights, {\\lambda}s. These weights are\nlearned directly from the data using a small neural network, leading to\nexcellent zero-shot and finetuned results that outperform equitune. Further, we\nprove that {\\lambda}-equitune is equivariant and a universal approximator of\nequivariant functions. Additionally, we show that the method of Kaba et al.\n(2022) used with appropriate loss functions, which we call equizero, also gives\nexcellent zero-shot and finetuned performance. Both equitune and equizero are\nspecial cases of {\\lambda}-equitune. To show the simplicity and generality of\nour method, we validate on a wide range of diverse applications and models such\nas 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.\n","authors":["Sourya Basu","Pulkit Katdare","Prasanna Sattigeri","Vijil Chenthamarakshan","Katherine Driggs-Campbell","Payel Das","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2305.09900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06916v1","updated":"2023-10-10T18:12:46Z","published":"2023-10-10T18:12:46Z","title":"Distributed Transfer Learning with 4th Gen Intel Xeon Processors","summary":"  In this paper, we explore how transfer learning, coupled with Intel Xeon,\nspecifically 4th Gen Intel Xeon scalable processor, defies the conventional\nbelief that training is primarily GPU-dependent. We present a case study where\nwe achieved near state-of-the-art accuracy for image classification on a\npublicly available Image Classification TensorFlow dataset using Intel Advanced\nMatrix Extensions(AMX) and distributed training with Horovod.\n","authors":["Lakshmi Arunachalam","Fahim Mohammad","Vrushabh H. Sanghavi"],"pdf_url":"https://arxiv.org/pdf/2310.06916v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.06907v1","updated":"2023-10-10T18:03:41Z","published":"2023-10-10T18:03:41Z","title":"Self-supervised Object-Centric Learning for Videos","summary":"  Unsupervised multi-object segmentation has shown impressive results on images\nby utilizing powerful semantics learned from self-supervised pretraining. An\nadditional modality such as depth or motion is often used to facilitate the\nsegmentation in video sequences. However, the performance improvements observed\nin synthetic sequences, which rely on the robustness of an additional cue, do\nnot translate to more challenging real-world scenarios. In this paper, we\npropose the first fully unsupervised method for segmenting multiple objects in\nreal-world sequences. Our object-centric learning framework spatially binds\nobjects to slots on each frame and then relates these slots across frames. From\nthese temporally-aware slots, the training objective is to reconstruct the\nmiddle frame in a high-level semantic feature space. We propose a masking\nstrategy by dropping a significant portion of tokens in the feature space for\nefficiency and regularization. Additionally, we address over-clustering by\nmerging slots based on similarity. Our method can successfully segment multiple\ninstances of complex and high-variety classes in YouTube videos.\n","authors":["Grkay Aydemir","Weidi Xie","Fatma Gney"],"pdf_url":"https://arxiv.org/pdf/2310.06907v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06906v1","updated":"2023-10-10T18:03:29Z","published":"2023-10-10T18:03:29Z","title":"Distillation Improves Visual Place Recognition for Low-Quality Queries","summary":"  The shift to online computing for real-time visual localization often\nrequires streaming query images/videos to a server for visual place recognition\n(VPR), where fast video transmission may result in reduced resolution or\nincreased quantization. This compromises the quality of global image\ndescriptors, leading to decreased VPR performance. To improve the low recall\nrate for low-quality query images, we present a simple yet effective method\nthat uses high-quality queries only during training to distill better feature\nrepresentations for deep-learning-based VPR, such as NetVLAD. Specifically, we\nuse mean squared error (MSE) loss between the global descriptors of queries\nwith different qualities, and inter-channel correlation knowledge distillation\n(ICKD) loss over their corresponding intermediate features. We validate our\napproach using the both Pittsburgh 250k dataset and our own indoor dataset with\nvarying quantization levels. By fine-tuning NetVLAD parameters with our\ndistillation-augmented losses, we achieve notable VPR recall-rate improvements\nover low-quality queries, as demonstrated in our extensive experimental\nresults. We believe this work not only pushes forward the VPR research but also\nprovides valuable insights for applications needing dependable place\nrecognition under resource-limited conditions.\n","authors":["Anbang Yang","Yao Wang","John-Ross Rizzo","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2310.06906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06904v1","updated":"2023-10-10T18:01:52Z","published":"2023-10-10T18:01:52Z","title":"Mitigating stereotypical biases in text to image generative systems","summary":"  State-of-the-art generative text-to-image models are known to exhibit social\nbiases and over-represent certain groups like people of perceived lighter skin\ntones and men in their outcomes. In this work, we propose a method to mitigate\nsuch biases and ensure that the outcomes are fair across different groups of\npeople. We do this by finetuning text-to-image models on synthetic data that\nvaries in perceived skin tones and genders constructed from diverse text\nprompts. These text prompts are constructed from multiplicative combinations of\nethnicities, genders, professions, age groups, and so on, resulting in diverse\nsynthetic data. Our diversity finetuned (DFT) model improves the group fairness\nmetric by 150% for perceived skin tone and 97.7% for perceived gender. Compared\nto baselines, DFT models generate more people with perceived darker skin tone\nand more women. To foster open research, we will release all text prompts and\ncode to generate training images.\n","authors":["Piero Esposito","Parmida Atighehchian","Anastasis Germanidis","Deepti Ghadiyaram"],"pdf_url":"https://arxiv.org/pdf/2310.06904v1.pdf","comment":"4 figures, 8 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2306.08937v2","updated":"2023-10-10T13:48:12Z","published":"2023-06-15T08:21:15Z","title":"DocumentNet: Bridging the Data Gap in Document Pre-Training","summary":"  Document understanding tasks, in particular, Visually-rich Document Entity\nRetrieval (VDER), have gained significant attention in recent years thanks to\ntheir broad applications in enterprise AI. However, publicly available data\nhave been scarce for these tasks due to strict privacy constraints and high\nannotation costs. To make things worse, the non-overlapping entity spaces from\ndifferent datasets hinder the knowledge transfer between document types. In\nthis paper, we propose a method to collect massive-scale and weakly labeled\ndata from the web to benefit the training of VDER models. The collected\ndataset, named DocumentNet, does not depend on specific document types or\nentity sets, making it universally applicable to all VDER tasks. The current\nDocumentNet consists of 30M documents spanning nearly 400 document types\norganized in a four-level ontology. Experiments on a set of broadly adopted\nVDER tasks show significant improvements when DocumentNet is incorporated into\nthe pre-training for both classic and few-shot learning settings. With the\nrecent emergence of large language models (LLMs), DocumentNet provides a large\ndata source to extend their multi-modal capabilities for VDER.\n","authors":["Lijun Yu","Jin Miao","Xiaoyu Sun","Jiayi Chen","Alexander G. Hauptmann","Hanjun Dai","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2306.08937v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06566v1","updated":"2023-10-10T12:25:52Z","published":"2023-10-10T12:25:52Z","title":"Efficient Retrieval of Images with Irregular Patterns using\n  Morphological Image Analysis: Applications to Industrial and Healthcare\n  datasets","summary":"  Image retrieval is the process of searching and retrieving images from a\ndatabase based on their visual content and features. Recently, much attention\nhas been directed towards the retrieval of irregular patterns within industrial\nor medical images by extracting features from the images, such as deep\nfeatures, colour-based features, shape-based features and local features. This\nhas applications across a spectrum of industries, including fault inspection,\ndisease diagnosis, and maintenance prediction. This paper proposes an image\nretrieval framework to search for images containing similar irregular patterns\nby extracting a set of morphological features (DefChars) from images; the\ndatasets employed in this paper contain wind turbine blade images with defects,\nchest computerised tomography scans with COVID-19 infection, heatsink images\nwith defects, and lake ice images. The proposed framework was evaluated with\ndifferent feature extraction methods (DefChars, resized raw image, local binary\npattern, and scale-invariant feature transforms) and distance metrics to\ndetermine the most efficient parameters in terms of retrieval performance\nacross datasets. The retrieval results show that the proposed framework using\nthe DefChars and the Manhattan distance metric achieves a mean average\nprecision of 80% and a low standard deviation of 0.09 across classes of\nirregular patterns, outperforming alternative feature-metric combinations\nacross all datasets. Furthermore, the low standard deviation between each class\nhighlights DefChars' capability for a reliable image retrieval task, even in\nthe presence of class imbalances or small-sized datasets.\n","authors":["Jiajun Zhang","Georgina Cosma","Sarah Bugby","Jason Watkins"],"pdf_url":"https://arxiv.org/pdf/2310.06566v1.pdf","comment":"35 pages, 5 figures, 19 tables (17 tables in appendix), submitted to\n  Special Issue: Advances and Challenges in Multimodal Machine Learning 2nd\n  Edition, Journal of Imaging, MDPI"},{"id":"http://arxiv.org/abs/2310.06491v1","updated":"2023-10-10T09:59:08Z","published":"2023-10-10T09:59:08Z","title":"A Multi-facet Paradigm to Bridge Large Language Model and Recommendation","summary":"  Large Language Models (LLMs) have garnered considerable attention in\nrecommender systems. To achieve LLM-based recommendation, item indexing and\ngeneration grounding are two essential steps, bridging between recommendation\nitems and natural language. Item indexing assigns a unique identifier to\nrepresent each item in natural language, and generation grounding grounds the\ngenerated token sequences to in-corpus items. However, previous works suffer\nfrom inherent limitations in the two steps. For item indexing, existing\nID-based identifiers (e.g., numeric IDs) and description-based identifiers\n(e.g., titles) often compromise semantic richness or uniqueness. Moreover,\ngeneration grounding might inadvertently produce out-of-corpus identifiers.\nWorse still, autoregressive generation heavily relies on the initial token's\nquality. To combat these issues, we propose a novel multi-facet paradigm,\nnamely TransRec, to bridge the LLMs to recommendation. Specifically, TransRec\nemploys multi-facet identifiers that incorporate ID, title, and attribute,\nachieving both distinctiveness and semantics. Additionally, we introduce a\nspecialized data structure for TransRec to guarantee the in-corpus identifier\ngeneration and adopt substring indexing to encourage LLMs to generate from any\nposition. We implement TransRec on two backbone LLMs, i.e., BART-large and\nLLaMA-7B. Empirical results on three real-world datasets under diverse settings\n(e.g., full training and few-shot training with warm- and cold-start testings)\nattest to the superiority of TransRec.\n","authors":["Xinyu Lin","Wenjie Wang","Yongqi Li","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2310.06491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06486v1","updated":"2023-10-10T09:53:59Z","published":"2023-10-10T09:53:59Z","title":"Topological RANSAC for instance verification and retrieval without\n  fine-tuning","summary":"  This paper presents an innovative approach to enhancing explainable image\nretrieval, particularly in situations where a fine-tuning set is unavailable.\nThe widely-used SPatial verification (SP) method, despite its efficacy, relies\non a spatial model and the hypothesis-testing strategy for instance\nrecognition, leading to inherent limitations, including the assumption of\nplanar structures and neglect of topological relations among features. To\naddress these shortcomings, we introduce a pioneering technique that replaces\nthe spatial model with a topological one within the RANSAC process. We propose\nbio-inspired saccade and fovea functions to verify the topological consistency\namong features, effectively circumventing the issues associated with SP's\nspatial model. Our experimental results demonstrate that our method\nsignificantly outperforms SP, achieving state-of-the-art performance in\nnon-fine-tuning retrieval. Furthermore, our approach can enhance performance\nwhen used in conjunction with fine-tuned features. Importantly, our method\nretains high explainability and is lightweight, offering a practical and\nadaptable solution for a variety of real-world applications.\n","authors":["Guoyuan An","Juhyung Seon","Inkyu An","Yuchi Huo","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.06486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06444v1","updated":"2023-10-10T09:16:15Z","published":"2023-10-10T09:16:15Z","title":"Query-dominant User Interest Network for Large-Scale Search Ranking","summary":"  Historical behaviors have shown great effect and potential in various\nprediction tasks, including recommendation and information retrieval. The\noverall historical behaviors are various but noisy while search behaviors are\nalways sparse. Most existing approaches in personalized search ranking adopt\nthe sparse search behaviors to learn representation with bottleneck, which do\nnot sufficiently exploit the crucial long-term interest. In fact, there is no\ndoubt that user long-term interest is various but noisy for instant search, and\nhow to exploit it well still remains an open problem.\n  To tackle this problem, in this work, we propose a novel model named\nQuery-dominant user Interest Network (QIN), including two cascade units to\nfilter the raw user behaviors and reweigh the behavior subsequences.\nSpecifically, we propose a relevance search unit (RSU), which aims to search a\nsubsequence relevant to the query first and then search the sub-subsequences\nrelevant to the target item. These items are then fed into an attention unit\ncalled Fused Attention Unit (FAU). It should be able to calculate attention\nscores from the ID field and attribute field separately, and then adaptively\nfuse the item embedding and content embedding based on the user engagement of\npast period. Extensive experiments and ablation studies on real-world datasets\ndemonstrate the superiority of our model over state-of-the-art methods. The QIN\nnow has been successfully deployed on Kuaishou search, an online video search\nplatform, and obtained 7.6% improvement on CTR.\n","authors":["Tong Guo","Xuanping Li","Haitao Yang","Xiao Liang","Yong Yuan","Jingyou Hou","Bingqing Ke","Chao Zhang","junlin He","Shunyu Zhang","Enyun Yu"," Wenwu"],"pdf_url":"https://arxiv.org/pdf/2310.06444v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2309.00976v2","updated":"2023-10-10T08:56:16Z","published":"2023-09-02T16:20:41Z","title":"Pure Message Passing Can Estimate Common Neighbor for Link Prediction","summary":"  Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto}\nstandard in graph representation learning. However, when it comes to link\nprediction, they often struggle, surpassed by simple heuristics such as Common\nNeighbor (CN). This discrepancy stems from a fundamental limitation: while\nMPNNs excel in node-level representation, they stumble with encoding the joint\nstructural features essential to link prediction, like CN. To bridge this gap,\nwe posit that, by harnessing the orthogonality of input vectors, pure\nmessage-passing can indeed capture joint structural features. Specifically, we\nstudy the proficiency of MPNNs in approximating CN heuristics. Based on our\nfindings, we introduce the Message Passing Link Predictor (MPLP), a novel link\nprediction model. MPLP taps into quasi-orthogonal vectors to estimate\nlink-level structural features, all while preserving the node-level\ncomplexities. Moreover, our approach demonstrates that leveraging\nmessage-passing to capture structural features could offset MPNNs'\nexpressiveness limitations at the expense of estimation variance. We conduct\nexperiments on benchmark datasets from various domains, where our method\nconsistently outperforms the baseline methods.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2309.00976v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2310.06393v1","updated":"2023-10-10T07:57:00Z","published":"2023-10-10T07:57:00Z","title":"Harnessing Administrative Data Inventories to Create a Reliable\n  Transnational Reference Database for Crop Type Monitoring","summary":"  With leaps in machine learning techniques and their applicationon Earth\nobservation challenges has unlocked unprecedented performance across the\ndomain. While the further development of these methods was previously limited\nby the availability and volume of sensor data and computing resources, the lack\nof adequate reference data is now constituting new bottlenecks. Since creating\nsuch ground-truth information is an expensive and error-prone task, new ways\nmust be devised to source reliable, high-quality reference data on large\nscales. As an example, we showcase E URO C ROPS, a reference dataset for crop\ntype classification that aggregates and harmonizes administrative data surveyed\nin different countries with the goal of transnational interoperability.\n","authors":["Maja Schneider","Marco Krner"],"pdf_url":"https://arxiv.org/pdf/2310.06393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06390v1","updated":"2023-10-10T07:53:36Z","published":"2023-10-10T07:53:36Z","title":"P5: Plug-and-Play Persona Prompting for Personalized Response Selection","summary":"  The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.\n","authors":["Joosung Lee","Minsik Oh","Donghun Lee"],"pdf_url":"https://arxiv.org/pdf/2310.06390v1.pdf","comment":"EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.01271v2","updated":"2023-10-10T04:34:56Z","published":"2023-10-02T15:16:31Z","title":"LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System","summary":"  As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nfirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .\n","authors":["Xue Zongyue","Liu Huanghai","Hu Yiran","Kong Kangle","Wang Chenlu","Liu Yun","Shen Weixing"],"pdf_url":"https://arxiv.org/pdf/2310.01271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04678v2","updated":"2023-10-10T04:13:36Z","published":"2023-10-07T03:25:06Z","title":"DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based\n  Queries","summary":"  In scientific research, the ability to effectively retrieve relevant\ndocuments based on complex, multifaceted queries is critical. Existing\nevaluation datasets for this task are limited, primarily due to the high cost\nand effort required to annotate resources that effectively represent complex\nqueries. To address this, we propose a novel task, Scientific DOcument\nRetrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed\nto handle the complex nature of user queries in scientific research. We\ndeveloped a benchmark dataset within the field of computer science, consisting\nof 100 human-authored complex query cases. For each complex query, we assembled\na collection of 100 relevant documents and produced annotated relevance scores\nfor ranking them. Recognizing the significant labor of expert annotation, we\nalso introduce Anno-GPT, a scalable framework for validating the performance of\nLarge Language Models (LLMs) on expert-level dataset annotation tasks. LLM\nannotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,\nwithout compromising quality. Furthermore, due to the multi-tiered structure of\nthese complex queries, the DORIS-MAE dataset can be extended to over 4,000\nsub-query test cases without requiring additional annotation. We evaluated 17\nrecent retrieval methods on DORIS-MAE, observing notable performance drops\ncompared to traditional datasets. This highlights the need for better\napproaches to handle complex, multifaceted queries in scientific research. Our\ndataset and codebase are available at\nhttps://github.com/Real-Doris-Mae/Doris-Mae-Dataset.\n","authors":["Jianyou Wang","Kaicheng Wang","Xiaoyue Wang","Prudhviraj Naidu","Leon Bergen","Ramamohan Paturi"],"pdf_url":"https://arxiv.org/pdf/2310.04678v2.pdf","comment":"To appear in NeurIPS 2023 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2310.06282v1","updated":"2023-10-10T03:32:33Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04633v2","updated":"2023-10-10T02:58:14Z","published":"2023-10-07T01:00:40Z","title":"Unbiased and Robust: External Attention-enhanced Graph Contrastive\n  Learning for Cross-domain Sequential Recommendation","summary":"  Cross-domain sequential recommenders (CSRs) are gaining considerable research\nattention as they can capture user sequential preference by leveraging side\ninformation from multiple domains. However, these works typically follow an\nideal setup, i.e., different domains obey similar data distribution, which\nignores the bias brought by asymmetric interaction densities (a.k.a. the\ninter-domain density bias). Besides, the frequently adopted mechanism (e.g.,\nthe self-attention network) in sequence encoder only focuses on the\ninteractions within a local view, which overlooks the global correlations\nbetween different training batches. To this end, we propose an External\nAttention-enhanced Graph Contrastive Learning framework, namely EA-GCL.\nSpecifically, to remove the impact of the inter-domain density bias, an\nauxiliary Self-Supervised Learning (SSL) task is attached to the traditional\ngraph encoder under a multi-task learning manner. To robustly capture users'\nbehavioral patterns, we develop an external attention-based sequence encoder\nthat contains an MLP-based memory-sharing structure. Unlike the self-attention\nmechanism, such a structure can effectively alleviate the bias interference\nfrom the batch-based training scheme. Extensive experiments on two real-world\ndatasets demonstrate that EA-GCL outperforms several state-of-the-art baselines\non CSR tasks. The source codes and relevant datasets are available at\nhttps://github.com/HoupingY/EA-GCL.\n","authors":["Xinhua Wang","Houping Yue","Zizheng Wang","Liancheng Xu","Jinyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04633v2.pdf","comment":"9 pages, 4 figures, accepted by ICDM 2023 (workshop-GML4Rec)"},{"id":"http://arxiv.org/abs/2301.05174v2","updated":"2023-10-10T22:58:45Z","published":"2023-01-12T18:00:00Z","title":"Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A\n  Reproducibility Study","summary":"  Most approaches to cross-modal retrieval (CMR) focus either on object-centric\ndatasets, meaning that each document depicts or describes a single object, or\non scene-centric datasets, meaning that each image depicts or describes a\ncomplex scene that involves multiple objects and relations between them. We\nposit that a robust CMR model should generalize well across both dataset types.\nDespite recent advances in CMR, the reproducibility of the results and their\ngeneralizability across different dataset types has not been studied before. We\naddress this gap and focus on the reproducibility of the state-of-the-art CMR\nresults when evaluated on object-centric and scene-centric datasets. We select\ntwo state-of-the-art CMR models with different architectures: (i) CLIP; and\n(ii) X-VLM. Additionally, we select two scene-centric datasets, and three\nobject-centric datasets, and determine the relative performance of the selected\nmodels on these datasets. We focus on reproducibility, replicability, and\ngeneralizability of the outcomes of previously published CMR experiments. We\ndiscover that the experiments are not fully reproducible and replicable.\nBesides, the relative performance results partially generalize across\nobject-centric and scene-centric datasets. On top of that, the scores obtained\non object-centric datasets are much lower than the scores obtained on\nscene-centric datasets. For reproducibility and transparency we make our source\ncode and the trained models publicly available.\n","authors":["Mariya Hendriksen","Svitlana Vakulenko","Ernst Kuiper","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2301.05174v2.pdf","comment":"18 pages, accepted as a reproducibility paper at ECIR 2023"},{"id":"http://arxiv.org/abs/2309.01808v2","updated":"2023-10-10T22:30:42Z","published":"2023-09-04T20:52:33Z","title":"DiscoverPath: A Knowledge Refinement and Retrieval System for\n  Interdisciplinarity on Biomedical Research","summary":"  The exponential growth in scholarly publications necessitates advanced tools\nfor efficient article retrieval, especially in interdisciplinary fields where\ndiverse terminologies are used to describe similar research. Traditional\nkeyword-based search engines often fall short in assisting users who may not be\nfamiliar with specific terminologies. To address this, we present a knowledge\ngraph-based paper search engine for biomedical research to enhance the user\nexperience in discovering relevant queries and articles. The system, dubbed\nDiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS)\ntagging to extract terminologies and relationships from article abstracts to\ncreate a KG. To reduce information overload, DiscoverPath presents users with a\nfocused subgraph containing the queried entity and its neighboring nodes and\nincorporates a query recommendation system, enabling users to iteratively\nrefine their queries. The system is equipped with an accessible Graphical User\nInterface that provides an intuitive visualization of the KG, query\nrecommendations, and detailed article information, enabling efficient article\nretrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath\nis open-sourced at https://github.com/ynchuang/DiscoverPath.\n","authors":["Yu-Neng Chuang","Guanchu Wang","Chia-Yuan Chang","Kwei-Herng Lai","Daochen Zha","Ruixiang Tang","Fan Yang","Alfredo Costilla Reyes","Kaixiong Zhou","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2309.01808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07008v1","updated":"2023-10-10T20:49:43Z","published":"2023-10-10T20:49:43Z","title":"Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs","summary":"  Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.\n","authors":["Mikhail Salnikov","Maria Lysyuk","Pavel Braslavski","Anton Razzhigaev","Valentin Malykh","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2310.07008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14367v2","updated":"2023-10-10T19:08:10Z","published":"2022-07-28T19:59:41Z","title":"An Equity-Aware Recommender System for Curating Art Exhibits Based on\n  Locally-Constrained Graph Matching","summary":"  Public art shapes our shared spaces. Public art should speak to community and\ncontext, and yet, recent work has demonstrated numerous instances of art in\nprominent institutions favoring outdated cultural norms and legacy communities.\nMotivated by this, we develop a novel recommender system to curate public art\nexhibits with built-in equity objectives and a local value-based allocation of\nconstrained resources. We develop a cost matrix by drawing on Schelling's model\nof segregation. Using the cost matrix as an input, the scoring function is\noptimized via a projected gradient descent to obtain a soft assignment matrix.\nOur optimization program allocates artwork to public spaces in a way that\nde-prioritizes \"in-group\" preferences, by satisfying minimum representation and\nexposure criteria. We draw on existing literature to develop a fairness metric\nfor our algorithmic output, and we assess the effectiveness of our approach and\ndiscuss its potential pitfalls from both a curatorial and equity standpoint.\n","authors":["Anna Haensch","Dina Deitsch"],"pdf_url":"https://arxiv.org/pdf/2207.14367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08614v8","updated":"2023-10-10T19:06:20Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over RDF data like knowledge graphs has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, the IR and NLP\ncommunities have addressed QA over text, but such systems barely utilize\nsemantic data and knowledge. This paper presents a method for complex questions\nthat can seamlessly operate over a mixture of RDF datasets and text corpora, or\nindividual sources, in a unified framework. Our method, called UNIQORN, builds\na context graph on-the-fly, by retrieving question-relevant evidences from the\nRDF data and/or a text corpus, using fine-tuned BERT models. The resulting\ngraph typically contains all question-relevant evidences but also a lot of\nnoise. UNIQORN copes with this input by a graph algorithm for Group Steiner\nTrees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA -- in a full training mode, as\nwell as in zero-shot settings. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v8.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2310.06913v1","updated":"2023-10-10T18:09:32Z","published":"2023-10-10T18:09:32Z","title":"A Comparative Study of Transformer-based Neural Text Representation\n  Techniques on Bug Triaging","summary":"  Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.\n","authors":["Atish Kumar Dipongkor","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2310.06913v1.pdf","comment":"12 pages, to appear in the Proceedings of 38th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE'23)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2310.06839v1","updated":"2023-10-10T17:59:58Z","published":"2023-10-10T17:59:58Z","title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression","summary":"  In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational/financial cost, longer latency, and inferior\nperformance. Some studies reveal that the performance of LLMs depends on both\nthe density and the position of the key information (question relevant) in the\ninput prompt. Inspired by these findings, we propose LongLLMLingua for prompt\ncompression towards improving LLMs' perception of the key information to\nsimultaneously address the three challenges. We conduct evaluation on a wide\nrange of long context scenarios including single-/multi-document QA, few-shot\nlearning, summarization, synthetic tasks, and code completion. The experimental\nresults show that LongLLMLingua compressed prompt can derive higher performance\nwith much less cost. The latency of the end-to-end system is also reduced. For\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost\nof up to 17.1% over the original prompt with ~4x fewer tokens as input to\nGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000\nsamples from the LongBench and ZeroScrolls benchmark, respectively.\nAdditionally, when compressing prompts of ~10k tokens at a compression rate of\n2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our\ncode is available at https://aka.ms/LLMLingua.\n","authors":["Huiqiang Jiang","Qianhui Wu","Xufang Luo","Dongsheng Li","Chin-Yew Lin","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.06839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06837v1","updated":"2023-10-10T17:59:51Z","published":"2023-10-10T17:59:51Z","title":"Generating and Evaluating Tests for K-12 Students with Language Model\n  Simulations: A Case Study on Sentence Reading Efficiency","summary":"  Developing an educational test can be expensive and time-consuming, as each\nitem must be written by experts and then evaluated by collecting hundreds of\nstudent responses. Moreover, many tests require multiple distinct sets of\nquestions administered throughout the school year to closely monitor students'\nprogress, known as parallel tests. In this study, we focus on tests of silent\nsentence reading efficiency, used to assess students' reading ability over\ntime. To generate high-quality parallel tests, we propose to fine-tune large\nlanguage models (LLMs) to simulate how previous students would have responded\nto unseen items. With these simulated responses, we can estimate each item's\ndifficulty and ambiguity. We first use GPT-4 to generate new test items\nfollowing a list of expert-developed rules and then apply a fine-tuned LLM to\nfilter the items based on criteria from psychological measurements. We also\npropose an optimal-transport-inspired technique for generating parallel tests\nand show the generated tests closely correspond to the original test's\ndifficulty and reliability based on crowdworker responses. Our evaluation of a\ngenerated test with 234 students from grades 2 to 8 produces test scores highly\ncorrelated (r=0.93) to those of a standard test form written by human experts\nand evaluated across thousands of K-12 students.\n","authors":["Eric Zelikman","Wanjing Anya Ma","Jasmine E. Tran","Diyi Yang","Jason D. Yeatman","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2310.06837v1.pdf","comment":"Accepted to EMNLP 2023 (Main)"},{"id":"http://arxiv.org/abs/2310.06835v1","updated":"2023-10-10T17:59:26Z","published":"2023-10-10T17:59:26Z","title":"Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement\n  Learning","summary":"  Recent advances in reinforcement learning (RL) have shown much promise across\na variety of applications. However, issues such as scalability, explainability,\nand Markovian assumptions limit its applicability in certain domains. We\nobserve that many of these shortcomings emanate from the simulator as opposed\nto the RL training algorithms themselves. As such, we propose a semantic proxy\nfor simulation based on a temporal extension to annotated logic. In comparison\nwith two high-fidelity simulators, we show up to three orders of magnitude\nspeed-up while preserving the quality of policy learned in addition to showing\nthe ability to model and leverage non-Markovian dynamics and instantaneous\nactions while providing an explainable trace describing the outcomes of the\nagent actions.\n","authors":["Kaustuv Mukherji","Devendra Parkar","Lahari Pokala","Dyuman Aditya","Paulo Shakarian","Clark Dorman"],"pdf_url":"https://arxiv.org/pdf/2310.06835v1.pdf","comment":"Submitted to IEEE International Conference on Semantic Computing"},{"id":"http://arxiv.org/abs/2310.06827v1","updated":"2023-10-10T17:57:00Z","published":"2023-10-10T17:57:00Z","title":"Teaching Language Models to Hallucinate Less with Synthetic Tasks","summary":"  Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.\n","authors":["Erik Jones","Hamid Palangi","Clarisse Simes","Varun Chandrasekaran","Subhabrata Mukherjee","Arindam Mitra","Ahmed Awadallah","Ece Kamar"],"pdf_url":"https://arxiv.org/pdf/2310.06827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06825v1","updated":"2023-10-10T17:54:58Z","published":"2023-10-10T17:54:58Z","title":"Mistral 7B","summary":"  We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.\n","authors":["Albert Q. Jiang","Alexandre Sablayrolles","Arthur Mensch","Chris Bamford","Devendra Singh Chaplot","Diego de las Casas","Florian Bressand","Gianna Lengyel","Guillaume Lample","Lucile Saulnier","Llio Renard Lavaud","Marie-Anne Lachaux","Pierre Stock","Teven Le Scao","Thibaut Lavril","Thomas Wang","Timothe Lacroix","William El Sayed"],"pdf_url":"https://arxiv.org/pdf/2310.06825v1.pdf","comment":"Models and code are available at\n  https://mistral.ai/news/announcing-mistral-7b/"},{"id":"http://arxiv.org/abs/2310.06823v1","updated":"2023-10-10T17:53:36Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Moun Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.06816v1","updated":"2023-10-10T17:39:03Z","published":"2023-10-10T17:39:03Z","title":"Text Embeddings Reveal (Almost) As Much As Text","summary":"  How much private information do text embeddings reveal about the original\ntext? We investigate the problem of embedding \\textit{inversion},\nreconstructing the full text represented in dense text embeddings. We frame the\nproblem as controlled generation: generating text that, when reembedded, is\nclose to a fixed point in latent space. We find that although a na\\\"ive model\nconditioned on the embedding performs poorly, a multi-step method that\niteratively corrects and re-embeds text is able to recover $92\\%$ of\n$32\\text{-token}$ text inputs exactly. We train our model to decode text\nembeddings from two state-of-the-art embedding models, and also show that our\nmodel can recover important personal information (full names) from a dataset of\nclinical notes. Our code is available on Github:\n\\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.\n","authors":["John X. Morris","Volodymyr Kuleshov","Vitaly Shmatikov","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2310.06816v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05914v2","updated":"2023-10-10T17:31:00Z","published":"2023-10-09T17:58:34Z","title":"NEFTune: Noisy Embeddings Improve Instruction Finetuning","summary":"  We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.\n","authors":["Neel Jain","Ping-yeh Chiang","Yuxin Wen","John Kirchenbauer","Hong-Min Chu","Gowthami Somepalli","Brian R. Bartoldson","Bhavya Kailkhura","Avi Schwarzschild","Aniruddha Saha","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2310.05914v2.pdf","comment":"25 pages, Code is available on Github:\n  https://github.com/neelsjain/NEFTune"},{"id":"http://arxiv.org/abs/2310.03030v3","updated":"2023-10-10T17:30:04Z","published":"2023-09-20T17:21:43Z","title":"GPT-MolBERTa: GPT Molecular Features Language Model for molecular\n  property prediction","summary":"  With the emergence of Transformer architectures and their powerful\nunderstanding of textual data, a new horizon has opened up to predict the\nmolecular properties based on text description. While SMILES are the most\ncommon form of representation, they are lacking robustness, rich information\nand canonicity, which limit their effectiveness in becoming generalizable\nrepresentations. Here, we present GPT-MolBERTa, a self-supervised large\nlanguage model (LLM) which uses detailed textual descriptions of molecules to\npredict their properties. A text based description of 326000 molecules were\ncollected using ChatGPT and used to train LLM to learn the representation of\nmolecules. To predict the properties for the downstream tasks, both BERT and\nRoBERTa models were used in the finetuning stage. Experiments show that\nGPT-MolBERTa performs well on various molecule property benchmarks, and\napproaching state of the art performance in regression tasks. Additionally,\nfurther analysis of the attention mechanisms show that GPT-MolBERTa is able to\npick up important information from the input textual data, displaying the\ninterpretability of the model.\n","authors":["Suryanarayanan Balaji","Rishikesh Magar","Yayati Jadhav","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2310.03030v3.pdf","comment":"Paper has 17 pages, 4 figures and 4 tables, along with 71 references"},{"id":"http://arxiv.org/abs/2306.08749v2","updated":"2023-10-10T17:29:52Z","published":"2023-06-14T21:17:31Z","title":"Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology\n  Reports","summary":"  Despite the reduction in turn-around times in radiology reports with the use\nof speech recognition software, persistent communication errors can\nsignificantly impact the interpretation of the radiology report. Pre-filling a\nradiology report holds promise in mitigating reporting errors, and despite\nefforts in the literature to generate medical reports, there exists a lack of\napproaches that exploit the longitudinal nature of patient visit records in the\nMIMIC-CXR dataset. To address this gap, we propose to use longitudinal\nmulti-modal data, i.e., previous patient visit CXR, current visit CXR, and\nprevious visit report, to pre-fill the 'findings' section of a current patient\nvisit report. We first gathered the longitudinal visit information for 26,625\npatients from the MIMIC-CXR dataset and created a new dataset called\nLongitudinal-MIMIC. With this new dataset, a transformer-based model was\ntrained to capture the information from longitudinal patient visit records\ncontaining multi-modal data (CXR images + reports) via a cross-attention-based\nmulti-modal fusion module and a hierarchical memory-driven decoder. In contrast\nto previous work that only uses current visit data as input to train a model,\nour work exploits the longitudinal information available to pre-fill the\n'findings' section of radiology reports. Experiments show that our approach\noutperforms several recent approaches. Code will be published at\nhttps://github.com/CelestialShine/Longitudinal-Chest-X-Ray.\n","authors":["Qingqing Zhu","Tejas Sudharshan Mathai","Pritam Mukherjee","Yifan Peng","Ronald M. Summers","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2306.08749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06803v1","updated":"2023-10-10T17:21:03Z","published":"2023-10-10T17:21:03Z","title":"Advancing Transformer's Capabilities in Commonsense Reasoning","summary":"  Recent advances in general purpose pre-trained language models have shown\ngreat potential in commonsense reasoning. However, current works still perform\npoorly on standard commonsense reasoning benchmarks including the Com2Sense\nDataset. We argue that this is due to a disconnect with current cutting-edge\nmachine learning methods. In this work, we aim to bridge the gap by introducing\ncurrent ML-based methods to improve general purpose pre-trained language models\nin the task of commonsense reasoning. Specifically, we experiment with and\nsystematically evaluate methods including knowledge transfer, model ensemble,\nand introducing an additional pairwise contrastive objective. Our best model\noutperforms the strongest previous works by ~15\\% absolute gains in Pairwise\nAccuracy and ~8.7\\% absolute gains in Standard Accuracy.\n","authors":["Yu Zhou","Yunqiu Han","Hanyu Zhou","Yulun Wu"],"pdf_url":"https://arxiv.org/pdf/2310.06803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05803v2","updated":"2023-10-10T17:13:03Z","published":"2023-05-09T23:24:09Z","title":"Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly\n  Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) aims to bypass the need for\nlaborious pixel-level annotation by using only image-level annotation. Most\nexisting methods rely on Class Activation Maps (CAM) to derive pixel-level\npseudo-labels and use them to train a fully supervised semantic segmentation\nmodel. Although these pseudo-labels are class-aware, indicating the coarse\nregions for particular classes, they are not object-aware and fail to delineate\naccurate object boundaries. To address this, we introduce a simple yet\neffective method harnessing the Segment Anything Model (SAM), a class-agnostic\nfoundation model capable of producing fine-grained instance masks of objects,\nparts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM\nmasks, resulting in high-quality pseudo-labels that are both class-aware and\nobject-aware. Our approach is highly versatile and can be easily integrated\ninto existing WSSS methods without any modification. Despite its simplicity,\nour approach shows consistent gain over the state-of-the-art WSSS methods on\nboth PASCAL VOC and MS-COCO datasets.\n","authors":["Tianle Chen","Zheda Mai","Ruiwen Li","Wei-lun Chao"],"pdf_url":"https://arxiv.org/pdf/2305.05803v2.pdf","comment":"Tianle Chen and Zheda Mai contributed equally to this work. Our code\n  is available at \\url{https://github.com/cskyl/SAM_WSSS}"},{"id":"http://arxiv.org/abs/2310.06801v1","updated":"2023-10-10T17:11:20Z","published":"2023-10-10T17:11:20Z","title":"Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation\n  Learning","summary":"  This paper concerns imitation learning (IL) (i.e, the problem of learning to\nmimic expert behaviors from demonstrations) in cooperative multi-agent systems.\nThe learning problem under consideration poses several challenges,\ncharacterized by high-dimensional state and action spaces and intricate\ninter-agent dependencies. In a single-agent setting, IL has proven to be done\nefficiently through an inverse soft-Q learning process given expert\ndemonstrations. However, extending this framework to a multi-agent context\nintroduces the need to simultaneously learn both local value functions to\ncapture local observations and individual actions, and a joint value function\nfor exploiting centralized learning. In this work, we introduce a novel\nmulti-agent IL algorithm designed to address these challenges. Our approach\nenables the centralized learning by leveraging mixing networks to aggregate\ndecentralized Q functions. A main advantage of this approach is that the\nweights of the mixing networks can be trained using information derived from\nglobal states. We further establish conditions for the mixing networks under\nwhich the multi-agent objective function exhibits convexity within the Q\nfunction space. We present extensive experiments conducted on some challenging\ncompetitive and cooperative multi-agent game environments, including an\nadvanced version of the Star-Craft multi-agent challenge (i.e., SMACv2), which\ndemonstrates the effectiveness of our proposed algorithm compared to existing\nstate-of-the-art multi-agent IL algorithms.\n","authors":["The Viet Bui","Tien Mai","Thanh Hong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.06801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06800v1","updated":"2023-10-10T17:11:14Z","published":"2023-10-10T17:11:14Z","title":"Test & Evaluation Best Practices for Machine Learning-Enabled Systems","summary":"  Machine learning (ML) - based software systems are rapidly gaining adoption\nacross various domains, making it increasingly essential to ensure they perform\nas intended. This report presents best practices for the Test and Evaluation\n(T&E) of ML-enabled software systems across its lifecycle. We categorize the\nlifecycle of ML-enabled software systems into three stages: component,\nintegration and deployment, and post-deployment. At the component level, the\nprimary objective is to test and evaluate the ML model as a standalone\ncomponent. Next, in the integration and deployment stage, the goal is to\nevaluate an integrated ML-enabled system consisting of both ML and non-ML\ncomponents. Finally, once the ML-enabled software system is deployed and\noperationalized, the T&E objective is to ensure the system performs as\nintended. Maintenance activities for ML-enabled software systems span the\nlifecycle and involve maintaining various assets of ML-enabled software\nsystems.\n  Given its unique characteristics, the T&E of ML-enabled software systems is\nchallenging. While significant research has been reported on T&E at the\ncomponent level, limited work is reported on T&E in the remaining two stages.\nFurthermore, in many cases, there is a lack of systematic T&E strategies\nthroughout the ML-enabled system's lifecycle. This leads practitioners to\nresort to ad-hoc T&E practices, which can undermine user confidence in the\nreliability of ML-enabled software systems. New systematic testing approaches,\nadequacy measurements, and metrics are required to address the T&E challenges\nacross all stages of the ML-enabled system lifecycle.\n","authors":["Jaganmohan Chandrasekaran","Tyler Cody","Nicola McCarthy","Erin Lanus","Laura Freeman"],"pdf_url":"https://arxiv.org/pdf/2310.06800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06794v1","updated":"2023-10-10T17:07:05Z","published":"2023-10-10T17:07:05Z","title":"$f$-Policy Gradients: A General Framework for Goal Conditioned RL using\n  $f$-Divergences","summary":"  Goal-Conditioned Reinforcement Learning (RL) problems often have access to\nsparse rewards where the agent receives a reward signal only when it has\nachieved the goal, making policy optimization a difficult problem. Several\nworks augment this sparse reward with a learned dense reward function, but this\ncan lead to sub-optimal policies if the reward is misaligned. Moreover, recent\nworks have demonstrated that effective shaping rewards for a particular problem\ncan depend on the underlying learning algorithm. This paper introduces a novel\nway to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG\nminimizes the f-divergence between the agent's state visitation distribution\nand the goal, which we show can lead to an optimal policy. We derive gradients\nfor various f-divergences to optimize this objective. Our learning paradigm\nprovides dense learning signals for exploration in sparse reward settings. We\nfurther introduce an entropy-regularized policy optimization objective, that we\ncall $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective.\nWe show that several metric-based shaping rewards like L2 can be used with\n$s$-MaxEnt RL, providing a common ground to study such metric-based shaping\nrewards with efficient exploration. We find that $f$-PG has better performance\ncompared to standard policy gradient methods on a challenging gridworld as well\nas the Point Maze and FetchReach environments. More information on our website\nhttps://agarwalsiddhant10.github.io/projects/fpg.html.\n","authors":["Siddhant Agarwal","Ishan Durugkar","Peter Stone","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06794v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06793v1","updated":"2023-10-10T17:06:41Z","published":"2023-10-10T17:06:41Z","title":"Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement\n  Learning","summary":"  We study matrix estimation problems arising in reinforcement learning (RL)\nwith low-rank structure. In low-rank bandits, the matrix to be recovered\nspecifies the expected arm rewards, and for low-rank Markov Decision Processes\n(MDPs), it may for example characterize the transition kernel of the MDP. In\nboth cases, each entry of the matrix carries important information, and we seek\nestimation methods with low entry-wise error. Importantly, these methods\nfurther need to accommodate for inherent correlations in the available data\n(e.g. for MDPs, the data consists of system trajectories). We investigate the\nperformance of simple spectral-based matrix estimation approaches: we show that\nthey efficiently recover the singular subspaces of the matrix and exhibit\nnearly-minimal entry-wise error. These new results on low-rank matrix\nestimation make it possible to devise reinforcement learning algorithms that\nfully exploit the underlying low-rank structure. We provide two examples of\nsuch algorithms: a regret minimization algorithm for low-rank bandit problems,\nand a best policy identification algorithm for reward-free RL in low-rank MDPs.\nBoth algorithms yield state-of-the-art performance guarantees.\n","authors":["Stefan Stojanovic","Yassir Jedra","Alexandre Proutiere"],"pdf_url":"https://arxiv.org/pdf/2310.06793v1.pdf","comment":"To appear in NeurIPS 2023"},{"id":"http://arxiv.org/abs/1910.09143v4","updated":"2023-10-10T17:06:28Z","published":"2019-10-21T04:24:29Z","title":"Dynamic Subgoal-based Exploration via Bayesian Optimization","summary":"  Reinforcement learning in sparse-reward navigation environments with\nexpensive and limited interactions is challenging and poses a need for\neffective exploration. Motivated by complex navigation tasks that require\nreal-world training (when cheap simulators are not available), we consider an\nagent that faces an unknown distribution of environments and must decide on an\nexploration strategy. It may leverage a series of training environments to\nimprove its policy before it is evaluated in a test environment drawn from the\nsame environment distribution. Most existing approaches focus on fixed\nexploration strategies, while the few that view exploration as a\nmeta-optimization problem tend to ignore the need for cost-efficient\nexploration. We propose a cost-aware Bayesian optimization approach that\nefficiently searches over a class of dynamic subgoal-based exploration\nstrategies. The algorithm adjusts a variety of levers -- the locations of the\nsubgoals, the length of each episode, and the number of replications per trial\n-- in order to overcome the challenges of sparse rewards, expensive\ninteractions, and noise. An experimental evaluation demonstrates that the new\napproach outperforms existing baselines across a number of problem domains. We\nalso provide a theoretical foundation and prove that the method asymptotically\nidentifies a near-optimal subgoal design.\n","authors":["Yijia Wang","Matthias Poloczek","Daniel R. Jiang"],"pdf_url":"https://arxiv.org/pdf/1910.09143v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06790v1","updated":"2023-10-10T17:04:21Z","published":"2023-10-10T17:04:21Z","title":"Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with\n  Automatic Differentiation: Koopman and Neural ODE Approaches","summary":"  Data-driven approximations of the Koopman operator are promising for\npredicting the time evolution of systems characterized by complex dynamics.\nAmong these methods, the approach known as extended dynamic mode decomposition\nwith dictionary learning (EDMD-DL) has garnered significant attention. Here we\npresent a modification of EDMD-DL that concurrently determines both the\ndictionary of observables and the corresponding approximation of the Koopman\noperator. This innovation leverages automatic differentiation to facilitate\ngradient descent computations through the pseudoinverse. We also address the\nperformance of several alternative methodologies. We assess a 'pure' Koopman\napproach, which involves the direct time-integration of a linear,\nhigh-dimensional system governing the dynamics within the space of observables.\nAdditionally, we explore a modified approach where the system alternates\nbetween spaces of states and observables at each time step -- this approach no\nlonger satisfies the linearity of the true Koopman operator representation. For\nfurther comparisons, we also apply a state space approach (neural ODEs). We\nconsider systems encompassing two and three-dimensional ordinary differential\nequation systems featuring steady, oscillatory, and chaotic attractors, as well\nas partial differential equations exhibiting increasingly complex and intricate\nbehaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the\nstate space approach offers superior performance compared to the 'pure' Koopman\napproach where the entire time evolution occurs in the space of observables.\nWhen the temporal evolution of the Koopman approach alternates between states\nand observables at each time step, however, its predictions become comparable\nto those of the state space approach.\n","authors":["C. Ricardo Constante-Amores","Alec J. Linot","Michael D. Graham"],"pdf_url":"https://arxiv.org/pdf/2310.06790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06786v1","updated":"2023-10-10T16:57:28Z","published":"2023-10-10T16:57:28Z","title":"OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text","summary":"  There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.\n","authors":["Keiran Paster","Marco Dos Santos","Zhangir Azerbayev","Jimmy Ba"],"pdf_url":"https://arxiv.org/pdf/2310.06786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06779v1","updated":"2023-10-10T16:54:25Z","published":"2023-10-10T16:54:25Z","title":"A Supervised Embedding and Clustering Anomaly Detection method for\n  classification of Mobile Network Faults","summary":"  The paper introduces Supervised Embedding and Clustering Anomaly Detection\n(SEMC-AD), a method designed to efficiently identify faulty alarm logs in a\nmobile network and alleviate the challenges of manual monitoring caused by the\ngrowing volume of alarm logs. SEMC-AD employs a supervised embedding approach\nbased on deep neural networks, utilizing historical alarm logs and their labels\nto extract numerical representations for each log, effectively addressing the\nissue of imbalanced classification due to a small proportion of anomalies in\nthe dataset without employing one-hot encoding. The robustness of the embedding\nis evaluated by plotting the two most significant principle components of the\nembedded alarm logs, revealing that anomalies form distinct clusters with\nsimilar embeddings. Multivariate normal Gaussian clustering is then applied to\nthese components, identifying clusters with a high ratio of anomalies to normal\nalarms (above 90%) and labeling them as the anomaly group. To classify new\nalarm logs, we check if their embedded vectors' two most significant principle\ncomponents fall within the anomaly-labeled clusters. If so, the log is\nclassified as an anomaly. Performance evaluation demonstrates that SEMC-AD\noutperforms conventional random forest and gradient boosting methods without\nembedding. SEMC-AD achieves 99% anomaly detection, whereas random forest and\nXGBoost only detect 86% and 81% of anomalies, respectively. While supervised\nclassification methods may excel in labeled datasets, the results demonstrate\nthat SEMC-AD is more efficient in classifying anomalies in datasets with\nnumerous categorical features, significantly enhancing anomaly detection,\nreducing operator burden, and improving network maintenance.\n","authors":["R. Mosayebi","H. Kia","A. Kianpour Raki"],"pdf_url":"https://arxiv.org/pdf/2310.06779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06777v1","updated":"2023-10-10T16:51:32Z","published":"2023-10-10T16:51:32Z","title":"Information Content Exploration","summary":"  Sparse reward environments are known to be challenging for reinforcement\nlearning agents. In such environments, efficient and scalable exploration is\ncrucial. Exploration is a means by which an agent gains information about the\nenvironment. We expand on this topic and propose a new intrinsic reward that\nsystemically quantifies exploratory behavior and promotes state coverage by\nmaximizing the information content of a trajectory taken by an agent. We\ncompare our method to alternative exploration based intrinsic reward\ntechniques, namely Curiosity Driven Learning and Random Network Distillation.\nWe show that our information theoretic reward induces efficient exploration and\noutperforms in various games, including Montezuma Revenge, a known difficult\ntask for reinforcement learning. Finally, we propose an extension that\nmaximizes information content in a discretely compressed latent space which\nboosts sample efficiency and generalizes to continuous state spaces.\n","authors":["Jacob Chmura","Hasham Burhani","Xiao Qi Shi"],"pdf_url":"https://arxiv.org/pdf/2310.06777v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.06771v1","updated":"2023-10-10T16:48:18Z","published":"2023-10-10T16:48:18Z","title":"Correlated Noise Provably Beats Independent Noise for Differentially\n  Private Learning","summary":"  Differentially private learning algorithms inject noise into the learning\nprocess. While the most common private learning algorithm, DP-SGD, adds\nindependent Gaussian noise in each iteration, recent work on matrix\nfactorization mechanisms has shown empirically that introducing correlations in\nthe noise can greatly improve their utility. We characterize the asymptotic\nlearning utility for any choice of the correlation function, giving precise\nanalytical bounds for linear regression and as the solution to a convex program\nfor general convex functions. We show, using these bounds, how correlated noise\nprovably improves upon vanilla DP-SGD as a function of problem parameters such\nas the effective dimension and condition number. Moreover, our analytical\nexpression for the near-optimal correlation function circumvents the cubic\ncomplexity of the semi-definite program used to optimize the noise correlation\nmatrix in previous work. We validate our theory with experiments on private\ndeep learning. Our work matches or outperforms prior work while being efficient\nboth in terms of compute and memory.\n","authors":["Christopher A. Choquette-Choo","Krishnamurthy Dvijotham","Krishna Pillutla","Arun Ganesh","Thomas Steinke","Abhradeep Thakurta"],"pdf_url":"https://arxiv.org/pdf/2310.06771v1.pdf","comment":"Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, and Krishna\n  Pillutla contributed equally"},{"id":"http://arxiv.org/abs/2304.05292v3","updated":"2023-10-10T16:40:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v3.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2310.06763v1","updated":"2023-10-10T16:39:47Z","published":"2023-10-10T16:39:47Z","title":"FABind: Fast and Accurate Protein-Ligand Binding","summary":"  Modeling the interaction between proteins and ligands and accurately\npredicting their binding structures is a critical yet challenging task in drug\ndiscovery. Recent advancements in deep learning have shown promise in\naddressing this challenge, with sampling-based and regression-based methods\nemerging as two prominent approaches. However, these methods have notable\nlimitations. Sampling-based methods often suffer from low efficiency due to the\nneed for generating multiple candidate structures for selection. On the other\nhand, regression-based methods offer fast predictions but may experience\ndecreased accuracy. Additionally, the variation in protein sizes often requires\nexternal modules for selecting suitable binding pockets, further impacting\nefficiency. In this work, we propose $\\mathbf{FABind}$, an end-to-end model\nthat combines pocket prediction and docking to achieve accurate and fast\nprotein-ligand binding. $\\mathbf{FABind}$ incorporates a unique ligand-informed\npocket prediction module, which is also leveraged for docking pose estimation.\nThe model further enhances the docking process by incrementally integrating the\npredicted pocket to optimize protein-ligand binding, reducing discrepancies\nbetween training and inference. Through extensive experiments on benchmark\ndatasets, our proposed $\\mathbf{FABind}$ demonstrates strong advantages in\nterms of effectiveness and efficiency compared to existing methods. Our code is\navailable at $\\href{https://github.com/QizhiPei/FABind}{Github}$.\n","authors":["Qizhi Pei","Kaiyuan Gao","Lijun Wu","Jinhua Zhu","Yingce Xia","Shufang Xie","Tao Qin","Kun He","Tie-Yan Liu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06763v1.pdf","comment":"Neural Information Processing Systems (NIPS 2023)"},{"id":"http://arxiv.org/abs/2310.06756v1","updated":"2023-10-10T16:27:12Z","published":"2023-10-10T16:27:12Z","title":"Going Beyond Neural Network Feature Similarity: The Network Feature\n  Complexity and Its Interpretation Using Category Theory","summary":"  The behavior of neural networks still remains opaque, and a recently widely\nnoted phenomenon is that networks often achieve similar performance when\ninitialized with different random parameters. This phenomenon has attracted\nsignificant attention in measuring the similarity between features learned by\ndistinct networks. However, feature similarity could be vague in describing the\nsame feature since equivalent features hardly exist. In this paper, we expand\nthe concept of equivalent feature and provide the definition of what we call\nfunctionally equivalent features. These features produce equivalent output\nunder certain transformations. Using this definition, we aim to derive a more\nintrinsic metric for the so-called feature complexity regarding the redundancy\nof features learned by a neural network at each layer. We offer a formal\ninterpretation of our approach through the lens of category theory, a\nwell-developed area in mathematics. To quantify the feature complexity, we\nfurther propose an efficient algorithm named Iterative Feature Merging. Our\nexperimental results validate our ideas and theories from various perspectives.\nWe empirically demonstrate that the functionally equivalence widely exists\namong different features learned by the same neural network and we could reduce\nthe number of parameters of the network without affecting the performance.The\nIFM shows great potential as a data-agnostic model prune method. We have also\ndrawn several interesting empirical findings regarding the defined feature\ncomplexity.\n","authors":["Yiting Chen","Zhanpeng Zhou","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03853v4","updated":"2023-10-10T16:22:46Z","published":"2023-04-07T22:27:18Z","title":"StepMix: A Python Package for Pseudo-Likelihood Estimation of\n  Generalized Mixture Models with External Variables","summary":"  StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper.\n","authors":["Sacha Morin","Robin Legault","Flix Lalibert","Zsuzsa Bakk","Charles-douard Gigure","Roxane de la Sablonnire","ric Lacourse"],"pdf_url":"https://arxiv.org/pdf/2304.03853v4.pdf","comment":"Sacha Morin and Robin Legault contributed equally"},{"id":"http://arxiv.org/abs/2310.06746v1","updated":"2023-10-10T16:19:20Z","published":"2023-10-10T16:19:20Z","title":"Causal Rule Learning: Enhancing the Understanding of Heterogeneous\n  Treatment Effect via Weighted Causal Rules","summary":"  Interpretability is a key concern in estimating heterogeneous treatment\neffects using machine learning methods, especially for healthcare applications\nwhere high-stake decisions are often made. Inspired by the Predictive,\nDescriptive, Relevant framework of interpretability, we propose causal rule\nlearning which finds a refined set of causal rules characterizing potential\nsubgroups to estimate and enhance our understanding of heterogeneous treatment\neffects. Causal rule learning involves three phases: rule discovery, rule\nselection, and rule analysis. In the rule discovery phase, we utilize a causal\nforest to generate a pool of causal rules with corresponding subgroup average\ntreatment effects. The selection phase then employs a D-learning method to\nselect a subset of these rules to deconstruct individual-level treatment\neffects as a linear combination of the subgroup-level effects. This helps to\nanswer an ignored question by previous literature: what if an individual\nsimultaneously belongs to multiple groups with different average treatment\neffects? The rule analysis phase outlines a detailed procedure to further\nanalyze each rule in the subset from multiple perspectives, revealing the most\npromising rules for further validation. The rules themselves, their\ncorresponding subgroup treatment effects, and their weights in the linear\ncombination give us more insights into heterogeneous treatment effects.\nSimulation and real-world data analysis demonstrate the superior performance of\ncausal rule learning on the interpretable estimation of heterogeneous treatment\neffect when the ground truth is complex and the sample size is sufficient.\n","authors":["Ying Wu","Hanzhong Liu","Kai Ren","Xiangyu Chang"],"pdf_url":"https://arxiv.org/pdf/2310.06746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06743v1","updated":"2023-10-10T16:12:17Z","published":"2023-10-10T16:12:17Z","title":"Geographic Location Encoding with Spherical Harmonics and Sinusoidal\n  Representation Networks","summary":"  Learning feature representations of geographical space is vital for any\nmachine learning model that integrates geolocated data, spanning application\ndomains such as remote sensing, ecology, or epidemiology. Recent work mostly\nembeds coordinates using sine and cosine projections based on Double Fourier\nSphere (DFS) features -- these embeddings assume a rectangular data domain even\non global data, which can lead to artifacts, especially at the poles. At the\nsame time, relatively little attention has been paid to the exact design of the\nneural network architectures these functional embeddings are combined with.\nThis work proposes a novel location encoder for globally distributed geographic\ndata that combines spherical harmonic basis functions, natively defined on\nspherical surfaces, with sinusoidal representation networks (SirenNets) that\ncan be interpreted as learned Double Fourier Sphere embedding. We\nsystematically evaluate the cross-product of positional embeddings and neural\nnetwork architectures across various classification and regression benchmarks\nand synthetic evaluation datasets. In contrast to previous approaches that\nrequire the combination of both positional encoding and neural networks to\nlearn meaningful representations, we show that both spherical harmonics and\nsinusoidal representation networks are competitive on their own but set\nstate-of-the-art performances across tasks when combined. We provide source\ncode at www.github.com/marccoru/locationencoder\n","authors":["Marc Ruwurm","Konstantin Klemmer","Esther Rolf","Robin Zbinden","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2310.06743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06737v1","updated":"2023-10-10T16:07:23Z","published":"2023-10-10T16:07:23Z","title":"Multi-domain improves out-of-distribution and data-limited scenarios for\n  medical image analysis","summary":"  Current machine learning methods for medical image analysis primarily focus\non developing models tailored for their specific tasks, utilizing data within\ntheir target domain. These specialized models tend to be data-hungry and often\nexhibit limitations in generalizing to out-of-distribution samples. Recently,\nfoundation models have been proposed, which combine data from various domains\nand demonstrate excellent generalization capabilities. Building upon this, this\nwork introduces the incorporation of diverse medical image domains, including\ndifferent imaging modalities like X-ray, MRI, CT, and ultrasound images, as\nwell as various viewpoints such as axial, coronal, and sagittal views. We refer\nto this approach as multi-domain model and compare its performance to that of\nspecialized models. Our findings underscore the superior generalization\ncapabilities of multi-domain models, particularly in scenarios characterized by\nlimited data availability and out-of-distribution, frequently encountered in\nhealthcare applications. The integration of diverse data allows multi-domain\nmodels to utilize shared information across domains, enhancing the overall\noutcomes significantly. To illustrate, for organ recognition, multi-domain\nmodel can enhance accuracy by up to 10% compared to conventional specialized\nmodels.\n","authors":["Ece Ozkan","Xavier Boix"],"pdf_url":"https://arxiv.org/pdf/2310.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06725v1","updated":"2023-10-10T15:53:27Z","published":"2023-10-10T15:53:27Z","title":"Growing ecosystem of deep learning methods for modeling\n  protein$\\unicode{x2013}$protein interactions","summary":"  Numerous cellular functions rely on protein$\\unicode{x2013}$protein\ninteractions. Efforts to comprehensively characterize them remain challenged\nhowever by the diversity of molecular recognition mechanisms employed within\nthe proteome. Deep learning has emerged as a promising approach for tackling\nthis problem by exploiting both experimental data and basic biophysical\nknowledge about protein interactions. Here, we review the growing ecosystem of\ndeep learning methods for modeling protein interactions, highlighting the\ndiversity of these biophysically-informed models and their respective\ntrade-offs. We discuss recent successes in using representation learning to\ncapture complex features pertinent to predicting protein interactions and\ninteraction sites, geometric deep learning to reason over protein structures\nand predict complex structures, and generative modeling to design de novo\nprotein assemblies. We also outline some of the outstanding challenges and\npromising new directions. Opportunities abound to discover novel interactions,\nelucidate their physical mechanisms, and engineer binders to modulate their\nfunctions using deep learning and, ultimately, unravel how protein interactions\norchestrate complex cellular behaviors.\n","authors":["Julia R. Rogers","Gerg Nikolnyi","Mohammed AlQuraishi"],"pdf_url":"https://arxiv.org/pdf/2310.06725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06717v1","updated":"2023-10-10T15:45:19Z","published":"2023-10-10T15:45:19Z","title":"Improving Pseudo-Time Stepping Convergence for CFD Simulations With\n  Neural Networks","summary":"  Computational fluid dynamics (CFD) simulations of viscous fluids described by\nthe Navier-Stokes equations are considered. Depending on the Reynolds number of\nthe flow, the Navier-Stokes equations may exhibit a highly nonlinear behavior.\nThe system of nonlinear equations resulting from the discretization of the\nNavier-Stokes equations can be solved using nonlinear iteration methods, such\nas Newton's method. However, fast quadratic convergence is typically only\nobtained in a local neighborhood of the solution, and for many configurations,\nthe classical Newton iteration does not converge at all. In such cases,\nso-called globalization techniques may help to improve convergence.\n  In this paper, pseudo-transient continuation is employed in order to improve\nnonlinear convergence. The classical algorithm is enhanced by a neural network\nmodel that is trained to predict a local pseudo-time step. Generalization of\nthe novel approach is facilitated by predicting the local pseudo-time step\nseparately on each element using only local information on a patch of adjacent\nelements as input. Numerical results for standard benchmark problems, including\nflow through a backward facing step geometry and Couette flow, show the\nperformance of the machine learning-enhanced globalization approach; as the\nsoftware for the simulations, the CFD module of COMSOL Multiphysics is\nemployed.\n","authors":["Anouk Zandbergen","Tycho van Noorden","Alexander Heinlein"],"pdf_url":"https://arxiv.org/pdf/2310.06717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06715v1","updated":"2023-10-10T15:42:14Z","published":"2023-10-10T15:42:14Z","title":"S4Sleep: Elucidating the design space of deep-learning-based sleep stage\n  classification models","summary":"  Scoring sleep stages in polysomnography recordings is a time-consuming task\nplagued by significant inter-rater variability. Therefore, it stands to benefit\nfrom the application of machine learning algorithms. While many algorithms have\nbeen proposed for this purpose, certain critical architectural decisions have\nnot received systematic exploration. In this study, we meticulously investigate\nthese design choices within the broad category of encoder-predictor\narchitectures. We identify robust architectures applicable to both time series\nand spectrogram input representations. These architectures incorporate\nstructured state space models as integral components, leading to statistically\nsignificant advancements in performance on the extensive SHHS dataset. These\nimprovements are assessed through both statistical and systematic error\nestimations. We anticipate that the architectural insights gained from this\nstudy will not only prove valuable for future research in sleep staging but\nalso hold relevance for other time series annotation tasks.\n","authors":["Tiezhi Wang","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2310.06715v1.pdf","comment":"11 pages, 1 figure, code available at\n  https://github.com/AI4HealthUOL/s4sleep"},{"id":"http://arxiv.org/abs/2310.06714v1","updated":"2023-10-10T15:41:26Z","published":"2023-10-10T15:41:26Z","title":"Exploring Memorization in Fine-tuned Language Models","summary":"  LLMs have shown great capabilities in various tasks but also exhibited\nmemorization of training data, thus raising tremendous privacy and copyright\nconcerns. While prior work has studied memorization during pre-training, the\nexploration of memorization during fine-tuning is rather limited. Compared with\npre-training, fine-tuning typically involves sensitive data and diverse\nobjectives, thus may bring unique memorization behaviors and distinct privacy\nrisks. In this work, we conduct the first comprehensive analysis to explore\nLMs' memorization during fine-tuning across tasks. Our studies with\nopen-sourced and our own fine-tuned LMs across various tasks indicate that\nfine-tuned memorization presents a strong disparity among tasks. We provide an\nunderstanding of this task disparity via sparse coding theory and unveil a\nstrong correlation between memorization and attention score distribution. By\ninvestigating its memorization behavior, multi-task fine-tuning paves a\npotential strategy to mitigate fine-tuned memorization.\n","authors":["Shenglai Zeng","Yaxin Li","Jie Ren","Yiding Liu","Han Xu","Pengfei He","Yue Xing","Shuaiqiang Wang","Jiliang Tang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.06714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06713v1","updated":"2023-10-10T15:38:30Z","published":"2023-10-10T15:38:30Z","title":"Interpretable Traffic Event Analysis with Bayesian Networks","summary":"  Although existing machine learning-based methods for traffic accident\nanalysis can provide good quality results to downstream tasks, they lack\ninterpretability which is crucial for this critical problem. This paper\nproposes an interpretable framework based on Bayesian Networks for traffic\naccident prediction. To enable the ease of interpretability, we design a\ndataset construction pipeline to feed the traffic data into the framework while\nretaining the essential traffic data information. With a concrete case study,\nour framework can derive a Bayesian Network from a dataset based on the causal\nrelationships between weather and traffic events across the United States.\nConsequently, our framework enables the prediction of traffic accidents with\ncompetitive accuracy while examining how the probability of these events\nchanges under different conditions, thus illustrating transparent relationships\nbetween traffic and weather events. Additionally, the visualization of the\nnetwork simplifies the analysis of relationships between different variables,\nrevealing the primary causes of traffic accidents and ultimately providing a\nvaluable reference for reducing traffic accidents.\n","authors":["Tong Yuan","Jian Yang","Zeyi Wen"],"pdf_url":"https://arxiv.org/pdf/2310.06713v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.06710v1","updated":"2023-10-10T15:36:58Z","published":"2023-10-10T15:36:58Z","title":"Zero-Shot Transfer in Imitation Learning","summary":"  We present an algorithm that learns to imitate expert behavior and can\ntransfer to previously unseen domains without retraining. Such an algorithm is\nextremely relevant in real-world applications such as robotic learning because\n1) reward functions are difficult to design, 2) learned policies from one\ndomain are difficult to deploy in another domain and 3) learning directly in\nthe real world is either expensive or unfeasible due to security concerns. To\novercome these constraints, we combine recent advances in Deep RL by using an\nAnnealedVAE to learn a disentangled state representation and imitate an expert\nby learning a single Q-function which avoids adversarial training. We\ndemonstrate the effectiveness of our method in 3 environments ranging in\ndifficulty and the type of transfer knowledge required.\n","authors":["Alvaro Cauderan","Gauthier Boeshertz","Florian Schwarb","Calvin Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15909v2","updated":"2023-10-10T15:26:47Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, these RL algorithms struggle with\nlong-horizon tasks and out-of-distribution tasks since they rely on recurrent\nneural networks to process the sequence of experiences instead of summarizing\nthem into general RL components such as value functions. Moreover, even\ntransformers have a practical limit to the length of histories they can\nefficiently reason about before training and inference costs become\nprohibitive. In contrast, traditional RL algorithms are data-inefficient since\nthey do not leverage domain knowledge, but they do converge to an optimal\npolicy as more data becomes available. In this paper, we propose RL$^3$, a\nprincipled hybrid approach that combines traditional RL and meta-RL by\nincorporating task-specific action-values learned through traditional RL as an\ninput to the meta-RL neural network. We show that RL$^3$ earns greater\ncumulative reward on long-horizon and out-of-distribution tasks compared to\nRL$^2$, while maintaining the efficiency of the latter in the short term.\nExperiments are conducted on both custom and benchmark discrete domains from\nthe meta-RL literature that exhibit a range of short-term, long-term, and\ncomplex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06702v1","updated":"2023-10-10T15:25:33Z","published":"2023-10-10T15:25:33Z","title":"Temporally Aligning Long Audio Interviews with Questions: A Case Study\n  in Multimodal Data Integration","summary":"  The problem of audio-to-text alignment has seen significant amount of\nresearch using complete supervision during training. However, this is typically\nnot in the context of long audio recordings wherein the text being queried does\nnot appear verbatim within the audio file. This work is a collaboration with a\nnon-governmental organization called CARE India that collects long audio health\nsurveys from young mothers residing in rural parts of Bihar, India. Given a\nquestion drawn from a questionnaire that is used to guide these surveys, we aim\nto locate where the question is asked within a long audio recording. This is of\ngreat value to African and Asian organizations that would otherwise have to\npainstakingly go through long and noisy audio recordings to locate questions\n(and answers) of interest. Our proposed framework, INDENT, uses a\ncross-attention-based model and prior information on the temporal ordering of\nsentences to learn speech embeddings that capture the semantics of the\nunderlying spoken text. These learnt embeddings are used to retrieve the\ncorresponding audio segment based on text queries at inference time. We\nempirically demonstrate the significant effectiveness (improvement in R-avg of\nabout 3%) of our model over those obtained using text-based heuristics. We also\nshow how noisy ASR, generated using state-of-the-art ASR models for Indian\nlanguages, yields better results when used in place of speech. INDENT, trained\nonly on Hindi data is able to cater to all languages supported by the\n(semantically) shared text space. We illustrate this empirically on 11 Indic\nlanguages.\n","authors":["Piyush Singh Pasi","Karthikeya Battepati","Preethi Jyothi","Ganesh Ramakrishnan","Tanmay Mahapatra","Manoj Singh"],"pdf_url":"https://arxiv.org/pdf/2310.06702v1.pdf","comment":"Work Accepted in IJCAI-23- AI and Social Good Track"},{"id":"http://arxiv.org/abs/2005.00797v2","updated":"2023-10-10T15:20:14Z","published":"2020-05-02T11:10:32Z","title":"Multi-consensus Decentralized Accelerated Gradient Descent","summary":"  This paper considers the decentralized convex optimization problem, which has\na wide range of applications in large-scale machine learning, sensor networks,\nand control theory. We propose novel algorithms that achieve optimal\ncomputation complexity and near optimal communication complexity. Our\ntheoretical results give affirmative answers to the open problem on whether\nthere exists an algorithm that can achieve a communication complexity (nearly)\nmatching the lower bound depending on the global condition number instead of\nthe local one. Furthermore, the linear convergence of our algorithms only\ndepends on the strong convexity of global objective and it does \\emph{not}\nrequire the local functions to be convex. The design of our methods relies on a\nnovel integration of well-known techniques including Nesterov's acceleration,\nmulti-consensus and gradient-tracking. Empirical studies show the\noutperformance of our methods for machine learning applications.\n","authors":["Haishan Ye","Luo Luo","Ziang Zhou","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2005.00797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v2","updated":"2023-10-10T15:16:08Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI\n  Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art large language models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new versions\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration of writing variations in human-AI co-writing tasks.\nWith ABScribe, users can swiftly produce multiple variations using LLM prompts,\nwhich are auto-converted into reusable buttons. Variations are stored\nadjacently within text segments for rapid in-place comparisons using mouse-over\ninteractions on a context toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06694v1","updated":"2023-10-10T15:13:30Z","published":"2023-10-10T15:13:30Z","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n  Pruning","summary":"  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA\nmodels, on a wide range of downstream and instruction tuning evaluations, while\nrequiring only 3% of compute compared to training such models from scratch.\nThis work provides compelling evidence that leveraging existing LLMs with\nstructured pruning is a far more cost-effective approach for building smaller\nLLMs.\n","authors":["Mengzhou Xia","Tianyu Gao","Zhiyuan Zeng","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06694v1.pdf","comment":"The code and models are available at\n  https://github.com/princeton-nlp/LLM-Shearing"},{"id":"http://arxiv.org/abs/2306.07220v3","updated":"2023-10-10T15:10:04Z","published":"2023-06-12T16:26:38Z","title":"Strokes2Surface: Recovering Curve Networks From 4D Architectural Design\n  Sketches","summary":"  We present Strokes2Surface, an offline geometry reconstruction pipeline that\nrecovers well-connected curve networks from imprecise 4D sketches to bridge\nconcept design and digital modeling stages in architectural design. The input\nto our pipeline consists of 3D strokes' polyline vertices and their timestamps\nas the 4th dimension, along with additional metadata recorded throughout\nsketching. Inspired by architectural sketching practices, our pipeline combines\na classifier and two clustering models to achieve its goal. First, with a set\nof extracted hand-engineered features from the sketch, the classifier\nrecognizes the type of individual strokes between those depicting boundaries\n(Shape strokes) and those depicting enclosed areas (Scribble strokes). Next,\nthe two clustering models parse strokes of each type into distinct groups, each\nrepresenting an individual edge or face of the intended architectural object.\nCurve networks are then formed through topology recovery of consolidated Shape\nclusters and surfaced using Scribble clusters guiding the cycle discovery. Our\nevaluation is threefold: We confirm the usability of the Strokes2Surface\npipeline in architectural design use cases via a user study, we validate our\nchoice of features via statistical analysis and ablation studies on our\ncollected dataset, and we compare our outputs against a range of\nreconstructions computed using alternative methods.\n","authors":["S. Rasoulzadeh","M. Wimmer","I. Kovacic"],"pdf_url":"https://arxiv.org/pdf/2306.07220v3.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2310.04373v2","updated":"2023-10-10T15:01:11Z","published":"2023-10-06T16:59:17Z","title":"Confronting Reward Model Overoptimization with Constrained RLHF","summary":"  Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.\n","authors":["Ted Moskovitz","Aaditya K. Singh","DJ Strouse","Tuomas Sandholm","Ruslan Salakhutdinov","Anca D. Dragan","Stephen McAleer"],"pdf_url":"https://arxiv.org/pdf/2310.04373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06686v1","updated":"2023-10-10T15:00:27Z","published":"2023-10-10T15:00:27Z","title":"Generalized Wick Decompositions","summary":"  We review the cumulant decomposition (a way of decomposing the expectation of\na product of random variables (e.g. $\\mathbb{E}[XYZ]$) into a sum of terms\ncorresponding to partitions of these variables.) and the Wick decomposition (a\nway of decomposing a product of (not necessarily random) variables into a sum\nof terms corresponding to subsets of the variables). Then we generalize each\none to a new decomposition where the product function is generalized to an\narbitrary function.\n","authors":["Chris MacLeod","Evgenia Nitishinskaya","Buck Shlegeris"],"pdf_url":"https://arxiv.org/pdf/2310.06686v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2310.06684v1","updated":"2023-10-10T14:59:22Z","published":"2023-10-10T14:59:22Z","title":"Learning Multiplex Embeddings on Text-rich Networks with One Text\n  Encoder","summary":"  In real-world scenarios, texts in a network are often linked by multiple\nsemantic relations (e.g., papers in an academic network are referenced by other\npublications, written by the same author, or published in the same venue),\nwhere text documents and their relations form a multiplex text-rich network.\nMainstream text representation learning methods use pretrained language models\n(PLMs) to generate one embedding for each text unit, expecting that all types\nof relations between texts can be captured by these single-view embeddings.\nHowever, this presumption does not hold particularly in multiplex text-rich\nnetworks. Along another line of work, multiplex graph neural networks (GNNs)\ndirectly initialize node attributes as a feature vector for node representation\nlearning, but they cannot fully capture the semantics of the nodes' associated\ntexts. To bridge these gaps, we propose METERN, a new framework for learning\nMultiplex Embeddings on TExt-Rich Networks. In contrast to existing methods,\nMETERN uses one text encoder to model the shared knowledge across relations and\nleverages a small number of parameters per relation to derive relation-specific\nrepresentations. This allows the encoder to effectively capture the multiplex\nstructures in the network while also preserving parameter efficiency. We\nconduct experiments on nine downstream tasks in five networks from both\nacademic and e-commerce domains, where METERN outperforms baselines\nsignificantly and consistently. The code is available at\nhttps://github.com/PeterGriffinJin/METERN-submit.\n","authors":["Bowen Jin","Wentao Zhang","Yu Zhang","Yu Meng","Han Zhao","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.06684v1.pdf","comment":"9 pages, 11 appendix pages"},{"id":"http://arxiv.org/abs/2310.06682v1","updated":"2023-10-10T14:57:04Z","published":"2023-10-10T14:57:04Z","title":"On the importance of catalyst-adsorbate 3D interactions for relaxed\n  energy predictions","summary":"  The use of machine learning for material property prediction and discovery\nhas traditionally centered on graph neural networks that incorporate the\ngeometric configuration of all atoms. However, in practice not all this\ninformation may be readily available, e.g.~when evaluating the potentially\nunknown binding of adsorbates to catalyst. In this paper, we investigate\nwhether it is possible to predict a system's relaxed energy in the OC20 dataset\nwhile ignoring the relative position of the adsorbate with respect to the\nelectro-catalyst. We consider SchNet, DimeNet++ and FAENet as base\narchitectures and measure the impact of four modifications on model\nperformance: removing edges in the input graph, pooling independent\nrepresentations, not sharing the backbone weights and using an attention\nmechanism to propagate non-geometric relative information. We find that while\nremoving binding site information impairs accuracy as expected, modified models\nare able to predict relaxed energies with remarkably decent MAE. Our work\nsuggests future research directions in accelerated materials discovery where\ninformation on reactant configurations can be reduced or altogether omitted.\n","authors":["Alvaro Carbonero","Alexandre Duval","Victor Schmidt","Santiago Miret","Alex Hernandez-Garcia","Yoshua Bengio","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2310.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11005v2","updated":"2023-10-10T14:56:45Z","published":"2023-04-21T14:50:53Z","title":"Self-Correcting Bayesian Optimization through Bayesian Active Learning","summary":"  Gaussian processes are the model of choice in Bayesian optimization and\nactive learning. Yet, they are highly dependent on cleverly chosen\nhyperparameters to reach their full potential, and little effort is devoted to\nfinding good hyperparameters in the literature. We demonstrate the impact of\nselecting good hyperparameters for GPs and present two acquisition functions\nthat explicitly prioritize hyperparameter learning. Statistical distance-based\nActive Learning (SAL) considers the average disagreement between samples from\nthe posterior, as measured by a statistical distance. SAL outperforms the\nstate-of-the-art in Bayesian active learning on several test functions. We then\nintroduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to\nperform Bayesian optimization and active learning simultaneously. SCoreBO\nlearns the model hyperparameters at improved rates compared to vanilla BO,\nwhile outperforming the latest Bayesian optimization methods on traditional\nbenchmarks. Moreover, we demonstrate the importance of self-correction on\natypical Bayesian optimization tasks.\n","authors":["Carl Hvarfner","Erik Hellsten","Frank Hutter","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2304.11005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06679v1","updated":"2023-10-10T14:54:57Z","published":"2023-10-10T14:54:57Z","title":"Machine Learning Quantum Systems with Magnetic p-bits","summary":"  The slowing down of Moore's Law has led to a crisis as the computing\nworkloads of Artificial Intelligence (AI) algorithms continue skyrocketing.\nThere is an urgent need for scalable and energy-efficient hardware catering to\nthe unique requirements of AI algorithms and applications. In this environment,\nprobabilistic computing with p-bits emerged as a scalable, domain-specific, and\nenergy-efficient computing paradigm, particularly useful for probabilistic\napplications and algorithms. In particular, spintronic devices such as\nstochastic magnetic tunnel junctions (sMTJ) show great promise in designing\nintegrated p-computers. Here, we examine how a scalable probabilistic computer\nwith such magnetic p-bits can be useful for an emerging field combining machine\nlearning and quantum physics.\n","authors":["Shuvro Chowdhury","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2310.06679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00042v2","updated":"2023-10-10T14:48:59Z","published":"2023-05-31T13:21:54Z","title":"Graph-based methods coupled with specific distributional distances for\n  adversarial attack detection","summary":"  Artificial neural networks are prone to being fooled by carefully perturbed\ninputs which cause an egregious misclassification. These \\textit{adversarial}\nattacks have been the focus of extensive research. Likewise, there has been an\nabundance of research in ways to detect and defend against them. We introduce a\nnovel approach of detection and interpretation of adversarial attacks from a\ngraph perspective. For an input image, we compute an associated sparse graph\nusing the layer-wise relevance propagation algorithm \\cite{bach15}.\nSpecifically, we only keep edges of the neural network with the highest\nrelevance values. Three quantities are then computed from the graph which are\nthen compared against those computed from the training set. The result of the\ncomparison is a classification of the image as benign or adversarial. To make\nthe comparison, two classification methods are introduced: 1) an explicit\nformula based on Wasserstein distance applied to the degree of node and 2) a\nlogistic regression. Both classification methods produce strong results which\nlead us to believe that a graph-based interpretation of adversarial attacks is\nvaluable.\n","authors":["Dwight Nwaigwe","Lucrezia Carboni","Martial Mermillod","Sophie Achard","Michel Dojat"],"pdf_url":"https://arxiv.org/pdf/2306.00042v2.pdf","comment":"published in Neural Networks"},{"id":"http://arxiv.org/abs/2310.06670v1","updated":"2023-10-10T14:46:22Z","published":"2023-10-10T14:46:22Z","title":"Domain Generalization by Rejecting Extreme Augmentations","summary":"  Data augmentation is one of the most effective techniques for regularizing\ndeep learning models and improving their recognition performance in a variety\nof tasks and domains. However, this holds for standard in-domain settings, in\nwhich the training and test data follow the same distribution. For the\nout-of-domain case, where the test data follow a different and unknown\ndistribution, the best recipe for data augmentation is unclear. In this paper,\nwe show that for out-of-domain and domain generalization settings, data\naugmentation can provide a conspicuous and robust improvement in performance.\nTo do that, we propose a simple training procedure: (i) use uniform sampling on\nstandard data augmentation transformations; (ii) increase the strength\ntransformations to account for the higher data variance expected when working\nout-of-domain, and (iii) devise a new reward function to reject extreme\ntransformations that can harm the training. With this procedure, our data\naugmentation scheme achieves a level of accuracy that is comparable to or\nbetter than state-of-the-art methods on benchmark domain generalization\ndatasets. Code: \\url{https://github.com/Masseeh/DCAug}\n","authors":["Masih Aminbeidokhti","Fidel A. Guerrero Pea","Heitor Rapela Medeiros","Thomas Dubail","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2310.06670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06668v1","updated":"2023-10-10T14:42:34Z","published":"2023-10-10T14:42:34Z","title":"Latent Diffusion Counterfactual Explanations","summary":"  Counterfactual explanations have emerged as a promising method for\nelucidating the behavior of opaque black-box models. Recently, several works\nleveraged pixel-space diffusion models for counterfactual generation. To handle\nnoisy, adversarial gradients during counterfactual generation -- causing\nunrealistic artifacts or mere adversarial perturbations -- they required either\nauxiliary adversarially robust models or computationally intensive guidance\nschemes. However, such requirements limit their applicability, e.g., in\nscenarios with restricted access to the model's training data. To address these\nlimitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).\nLDCE harnesses the capabilities of recent class- or text-conditional foundation\nlatent diffusion models to expedite counterfactual generation and focus on the\nimportant, semantic parts of the data. Furthermore, we propose a novel\nconsensus guidance mechanism to filter out noisy, adversarial gradients that\nare misaligned with the diffusion model's implicit classifier. We demonstrate\nthe versatility of LDCE across a wide spectrum of models trained on diverse\ndatasets with different learning paradigms. Finally, we showcase how LDCE can\nprovide insights into model errors, enhancing our understanding of black-box\nmodel behavior.\n","authors":["Karim Farid","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.06668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06667v1","updated":"2023-10-10T14:42:32Z","published":"2023-10-10T14:42:32Z","title":"SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space","summary":"  Generative Adversarial Networks (GANs) can synthesize realistic images, with\nthe learned latent space shown to encode rich semantic information with various\ninterpretable directions. However, due to the unstructured nature of the\nlearned latent space, it inherits the bias from the training data where\nspecific groups of visual attributes that are not causally related tend to\nappear together, a phenomenon also known as spurious correlations, e.g., age\nand eyeglasses or women and lipsticks. Consequently, the learned distribution\noften lacks the proper modelling of the missing examples. The interpolation\nfollowing editing directions for one attribute could result in entangled\nchanges with other attributes. To address this problem, previous works\ntypically adjust the learned directions to minimize the changes in other\nattributes, yet they still fail on strongly correlated features. In this work,\nwe study the entanglement issue in both the training data and the learned\nlatent space for the StyleGAN2-FFHQ model. We propose a novel framework\nSC$^2$GAN that achieves disentanglement by re-projecting low-density latent\ncode samples in the original latent space and correcting the editing directions\nbased on both the high-density and low-density regions. By leveraging the\noriginal meaningful directions and semantic region-specific layers, our\nframework interpolates the original latent codes to generate images with\nattribute combination that appears infrequently, then inverts these samples\nback to the original latent space. We apply our framework to pre-existing\nmethods that learn meaningful latent directions and showcase its strong\ncapability to disentangle the attributes with small amounts of low-density\nregion samples added.\n","authors":["Zikun Chen","Han Zhao","Parham Aarabi","Ruowei Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.06667v1.pdf","comment":"Accepted to the Out Of Distribution Generalization in Computer Vision\n  workshop at ICCV2023"},{"id":"http://arxiv.org/abs/2310.06666v1","updated":"2023-10-10T14:41:38Z","published":"2023-10-10T14:41:38Z","title":"Unlock the Potential of Counterfactually-Augmented Data in\n  Out-Of-Distribution Generalization","summary":"  Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip\nthe corresponding labels -- has the potential to improve the\nOut-Of-Distribution (OOD) generalization capability of language models, as CAD\ninduces language models to exploit domain-independent causal features and\nexclude spurious correlations. However, the empirical results of CAD's OOD\ngeneralization are not as efficient as anticipated. In this study, we attribute\nthe inefficiency to the myopia phenomenon caused by CAD: language models only\nfocus on causal features that are edited in the augmentation operation and\nexclude other non-edited causal features. Therefore, the potential of CAD is\nnot fully exploited. To address this issue, we analyze the myopia phenomenon in\nfeature space from the perspective of Fisher's Linear Discriminant, then we\nintroduce two additional constraints based on CAD's structural properties\n(dataset-level and sentence-level) to help language models extract more\ncomplete causal features in CAD, thereby mitigating the myopia phenomenon and\nimproving OOD generalization capability. We evaluate our method on two tasks:\nSentiment Analysis and Natural Language Inference, and the experimental results\ndemonstrate that our method could unlock the potential of CAD and improve the\nOOD generalization performance of language models by 1.0% to 5.9%.\n","authors":["Caoyun Fan","Wenqing Chen","Jidong Tian","Yitian Li","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2310.06666v1.pdf","comment":"Expert Systems With Applications 2023. arXiv admin note: text overlap\n  with arXiv:2302.09345"},{"id":"http://arxiv.org/abs/2306.11526v2","updated":"2023-10-10T14:37:45Z","published":"2023-06-20T13:28:27Z","title":"Understanding Contrastive Learning Through the Lens of Margins","summary":"  Contrastive learning, along with its variations, has been a highly effective\nself-supervised learning method across diverse domains. Contrastive learning\nmeasures the distance between representations using cosine similarity and uses\ncross-entropy for representation learning. Within the same framework of\ncosine-similarity-based representation learning, margins have played a\nsignificant role in enhancing face and speaker recognition tasks.\nInterestingly, despite the shared reliance on the same similarity metrics and\nobjective functions, contrastive learning has not actively adopted margins.\nFurthermore, decision-boundary-based explanations are the only ones that have\nbeen used to explain the effect of margins in contrastive learning. In this\nwork, we propose a new perspective to understand the role of margins based on\ngradient analysis. Based on the new perspective, we analyze how margins affect\ngradients of contrastive learning and separate the effect into more elemental\nlevels. We separately analyze each and provide possible directions for\nimproving contrastive learning. Our experimental results demonstrate that\nemphasizing positive samples and scaling gradients depending on positive sample\nangles and logits are the keys to improving the generalization performance of\ncontrastive learning in both seen and unseen datasets, and other factors can\nonly marginally improve performance.\n","authors":["Daniel Rho","TaeSoo Kim","Sooill Park","Jaehyun Park","JaeHan Park"],"pdf_url":"https://arxiv.org/pdf/2306.11526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06661v1","updated":"2023-10-10T14:37:17Z","published":"2023-10-10T14:37:17Z","title":"Tertiary Lymphoid Structures Generation through Graph-based Diffusion","summary":"  Graph-based representation approaches have been proven to be successful in\nthe analysis of biomedical data, due to their capability of capturing intricate\ndependencies between biological entities, such as the spatial organization of\ndifferent cell types in a tumor tissue. However, to further enhance our\nunderstanding of the underlying governing biological mechanisms, it is\nimportant to accurately capture the actual distributions of such complex data.\nGraph-based deep generative models are specifically tailored to accomplish\nthat. In this work, we leverage state-of-the-art graph-based diffusion models\nto generate biologically meaningful cell-graphs. In particular, we show that\nthe adopted graph diffusion model is able to accurately learn the distribution\nof cells in terms of their tertiary lymphoid structures (TLS) content, a\nwell-established biomarker for evaluating the cancer progression in oncology\nresearch. Additionally, we further illustrate the utility of the learned\ngenerative models for data augmentation in a TLS classification task. To the\nbest of our knowledge, this is the first work that leverages the power of graph\ndiffusion models in generating meaningful biological cell structures.\n","authors":["Manuel Madeira","Dorina Thanou","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2310.06661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02599v3","updated":"2023-10-10T14:22:03Z","published":"2023-08-04T04:04:58Z","title":"Branched Latent Neural Maps","summary":"  We introduce Branched Latent Neural Maps (BLNMs) to learn finite dimensional\ninput-output maps encoding complex physical processes. A BLNM is defined by a\nsimple and compact feedforward partially-connected neural network that\nstructurally disentangles inputs with different intrinsic roles, such as the\ntime variable from model parameters of a differential equation, while\ntransferring them into a generic field of interest. BLNMs leverage latent\noutputs to enhance the learned dynamics and break the curse of dimensionality\nby showing excellent generalization properties with small training datasets and\nshort training times on a single processor. Indeed, their generalization error\nremains comparable regardless of the adopted discretization during the testing\nphase. Moreover, the partial connections significantly reduce the number of\ntunable parameters. We show the capabilities of BLNMs in a challenging test\ncase involving electrophysiology simulations in a biventricular cardiac model\nof a pediatric patient with hypoplastic left heart syndrome. The model includes\na 1D Purkinje network for fast conduction and a 3D heart-torso geometry.\nSpecifically, we trained BLNMs on 150 in silico generated 12-lead\nelectrocardiograms (ECGs) while spanning 7 model parameters, covering\ncell-scale and organ-level. Although the 12-lead ECGs manifest very fast\ndynamics with sharp gradients, after automatic hyperparameter tuning the\noptimal BLNM, trained in less than 3 hours on a single CPU, retains just 7\nhidden layers and 19 neurons per layer. The resulting mean square error is on\nthe order of $10^{-4}$ on a test dataset comprised of 50 electrophysiology\nsimulations. In the online phase, the BLNM allows for 5000x faster real-time\nsimulations of cardiac electrophysiology on a single core standard computer and\ncan be used to solve inverse problems via global optimization in a few seconds\nof computational time.\n","authors":["Matteo Salvador","Alison Lesley Marsden"],"pdf_url":"https://arxiv.org/pdf/2308.02599v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06648v1","updated":"2023-10-10T14:13:59Z","published":"2023-10-10T14:13:59Z","title":"Diversity from Human Feedback","summary":"  Diversity plays a significant role in many problems, such as ensemble\nlearning, reinforcement learning, and combinatorial optimization. How to define\nthe diversity measure is a longstanding problem. Many methods rely on expert\nexperience to define a proper behavior space and then obtain the diversity\nmeasure, which is, however, challenging in many scenarios. In this paper, we\npropose the problem of learning a behavior space from human feedback and\npresent a general method called Diversity from Human Feedback (DivHF) to solve\nit. DivHF learns a behavior descriptor consistent with human preference by\nquerying human feedback. The learned behavior descriptor can be combined with\nany distance measure to define a diversity measure. We demonstrate the\neffectiveness of DivHF by integrating it with the Quality-Diversity\noptimization algorithm MAP-Elites and conducting experiments on the QDax suite.\nThe results show that DivHF learns a behavior space that aligns better with\nhuman requirements compared to direct data-driven approaches and leads to more\ndiverse solutions under human preference. Our contributions include formulating\nthe problem, proposing the DivHF method, and demonstrating its effectiveness\nthrough experiments.\n","authors":["Ren-Jian Wang","Ke Xue","Yutong Wang","Peng Yang","Haobo Fu","Qiang Fu","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2310.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08066v2","updated":"2023-10-10T14:10:05Z","published":"2023-09-14T23:28:58Z","title":"Morphologically-Aware Consensus Computation via Heuristics-based\n  IterATive Optimization (MACCHIatO)","summary":"  The extraction of consensus segmentations from several binary or\nprobabilistic masks is important to solve various tasks such as the analysis of\ninter-rater variability or the fusion of several neural network outputs. One of\nthe most widely used methods to obtain such a consensus segmentation is the\nSTAPLE algorithm. In this paper, we first demonstrate that the output of that\nalgorithm is heavily impacted by the background size of images and the choice\nof the prior. We then propose a new method to construct a binary or a\nprobabilistic consensus segmentation based on the Fr\\'{e}chet means of\ncarefully chosen distances which makes it totally independent of the image\nbackground size. We provide a heuristic approach to optimize this criterion\nsuch that a voxel's class is fully determined by its voxel-wise distance to the\ndifferent masks, the connected component it belongs to and the group of raters\nwho segmented it. We compared extensively our method on several datasets with\nthe STAPLE method and the naive segmentation averaging method, showing that it\nleads to binary consensus masks of intermediate size between Majority Voting\nand STAPLE and to different posterior probabilities than Mask Averaging and\nSTAPLE methods. Our code is available at\nhttps://gitlab.inria.fr/dhamzaou/jaccardmap .\n","authors":["Dimitri Hamzaoui","Sarah Montagne","Raphale Renard-Penna","Nicholas Ayache","Herv Delingette"],"pdf_url":"https://arxiv.org/pdf/2309.08066v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2023:013"},{"id":"http://arxiv.org/abs/2310.06645v1","updated":"2023-10-10T14:07:49Z","published":"2023-10-10T14:07:49Z","title":"Self-Supervised Representation Learning for Online Handwriting Text\n  Classification","summary":"  Self-supervised learning offers an efficient way of extracting rich\nrepresentations from various types of unlabeled data while avoiding the cost of\nannotating large-scale datasets. This is achievable by designing a pretext task\nto form pseudo labels with respect to the modality and domain of the data.\nGiven the evolving applications of online handwritten texts, in this study, we\npropose the novel Part of Stroke Masking (POSM) as a pretext task for\npretraining models to extract informative representations from the online\nhandwriting of individuals in English and Chinese languages, along with two\nsuggested pipelines for fine-tuning the pretrained models. To evaluate the\nquality of the extracted representations, we use both intrinsic and extrinsic\nevaluation methods. The pretrained models are fine-tuned to achieve\nstate-of-the-art results in tasks such as writer identification, gender\nclassification, and handedness classification, also highlighting the\nsuperiority of utilizing the pretrained models over the models trained from\nscratch.\n","authors":["Pouya Mehralian","Bagher BabaAli","Ashena Gorgan Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2310.06645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06644v1","updated":"2023-10-10T14:07:37Z","published":"2023-10-10T14:07:37Z","title":"Zero-Level-Set Encoder for Neural Distance Fields","summary":"  Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., to compute a signed distance or occupancy value at\na specific spatial position. Previous methods tend to rely on the auto-decoder\nparadigm, which often requires densely-sampled and accurate signed distances to\nbe known during training and testing, as well as an additional optimization\nloop during inference. This introduces a lot of computational overhead, in\naddition to having to compute signed distances analytically, even during\ntesting. In this paper, we present a novel encoder-decoder neural network for\nembedding 3D shapes in a single forward pass. Our architecture is based on a\nmulti-scale hybrid system incorporating graph-based and voxel-based components,\nas well as a continuously differentiable decoder. Furthermore, the network is\ntrained to solve the Eikonal equation and only requires knowledge of the\nzero-level set for training and inference. Additional volumetric samples can be\ngenerated on-the-fly, and incorporated in an unsupervised manner. This means\nthat in contrast to most previous work, our network is able to output valid\nsigned distance fields without explicit prior knowledge of non-zero distance\nvalues or shape occupancy. In other words, our network computes approximate\nsolutions to the boundary-valued Eikonal equation. It also requires only a\nsingle forward pass during inference, instead of the common latent code\noptimization. We further propose a modification of the loss function in case\nthat surface normals are not well defined, e.g., in the context of\nnon-watertight surface-meshes and non-manifold geometry. We finally demonstrate\nthe efficacy, generalizability and scalability of our method on datasets\nconsisting of deforming 3D shapes, single class encoding and multiclass\nencoding, showcasing a wide range of possible applications.\n","authors":["Stefan Rhys Jeske","Jonathan Klein","Dominik L. Michels","Jan Bender"],"pdf_url":"https://arxiv.org/pdf/2310.06644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06643v1","updated":"2023-10-10T14:06:56Z","published":"2023-10-10T14:06:56Z","title":"Implicit Variational Inference for High-Dimensional Posteriors","summary":"  In variational inference, the benefits of Bayesian models rely on accurately\ncapturing the true posterior distribution. We propose using neural samplers\nthat specify implicit distributions, which are well-suited for approximating\ncomplex multimodal and correlated posteriors in high-dimensional spaces. Our\napproach advances inference using implicit distributions by introducing novel\nbounds that come about by locally linearising the neural sampler. This is\ndistinct from existing methods that rely on additional discriminator networks\nand unstable adversarial objectives. Furthermore, we present a new sampler\narchitecture that, for the first time, enables implicit distributions over\nmillions of latent variables, addressing computational concerns by using\ndifferentiable numerical approximations. Our empirical analysis indicates our\nmethod is capable of recovering correlations across layers in large Bayesian\nneural networks, a property that is crucial for a network's performance but\nnotoriously challenging to achieve. To the best of our knowledge, no other\nmethod has been shown to accomplish this task for such large models. Through\nexperiments in downstream tasks, we demonstrate that our expressive posteriors\noutperform state-of-the-art uncertainty quantification methods, validating the\neffectiveness of our training algorithm and the quality of the learned implicit\napproximation.\n","authors":["Anshuk Uppal","Kristoffer Stensbo-Smidt","Wouter K. Boomsma","Jes Frellsen"],"pdf_url":"https://arxiv.org/pdf/2310.06643v1.pdf","comment":"9 pages, and supplementary"},{"id":"http://arxiv.org/abs/2310.06639v1","updated":"2023-10-10T14:00:03Z","published":"2023-10-10T14:00:03Z","title":"The Lattice Overparametrization Paradigm for the Machine Learning of\n  Lattice Operators","summary":"  The machine learning of lattice operators has three possible bottlenecks.\nFrom a statistical standpoint, it is necessary to design a constrained class of\noperators based on prior information with low bias, and low complexity relative\nto the sample size. From a computational perspective, there should be an\nefficient algorithm to minimize an empirical error over the class. From an\nunderstanding point of view, the properties of the learned operator need to be\nderived, so its behavior can be theoretically understood. The statistical\nbottleneck can be overcome due to the rich literature about the representation\nof lattice operators, but there is no general learning algorithm for them. In\nthis paper, we discuss a learning paradigm in which, by overparametrizing a\nclass via elements in a lattice, an algorithm for minimizing functions in a\nlattice is applied to learn. We present the stochastic lattice gradient descent\nalgorithm as a general algorithm to learn on constrained classes of operators\nas long as a lattice overparametrization of it is fixed, and we discuss\nprevious works which are proves of concept. Moreover, if there are algorithms\nto compute the basis of an operator from its overparametrization, then its\nproperties can be deduced and the understanding bottleneck is also overcome.\nThis learning paradigm has three properties that modern methods based on neural\nnetworks lack: control, transparency and interpretability. Nowadays, there is\nan increasing demand for methods with these characteristics, and we believe\nthat mathematical morphology is in a unique position to supply them. The\nlattice overparametrization paradigm could be a missing piece for it to achieve\nits full potential within modern machine learning.\n","authors":["Diego Marcondes","Junior Barrera"],"pdf_url":"https://arxiv.org/pdf/2310.06639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06627v1","updated":"2023-10-10T13:45:59Z","published":"2023-10-10T13:45:59Z","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models","summary":"  Counterfactual reasoning ability is one of the core abilities of human\nintelligence. This reasoning process involves the processing of alternatives to\nobserved states or past events, and this process can improve our ability for\nplanning and decision-making. In this work, we focus on benchmarking the\ncounterfactual reasoning ability of multi-modal large language models. We take\nthe question and answer pairs from the VQAv2 dataset and add one counterfactual\npresupposition to the questions, with the answer being modified accordingly.\nAfter generating counterfactual questions and answers using ChatGPT, we\nmanually examine all generated questions and answers to ensure correctness.\nOver 2k counterfactual question and answer pairs are collected this way. We\nevaluate recent vision language models on our newly collected test dataset and\nfound that all models exhibit a large performance drop compared to the results\ntested on questions without the counterfactual presupposition. This result\nindicates that there still exists space for developing vision language models.\nApart from the vision language models, our proposed dataset can also serves as\na benchmark for evaluating the ability of code generation LLMs, results\ndemonstrate a large gap between GPT-4 and current open-source models. Our code\nand dataset are available at \\url{https://github.com/Letian2003/C-VQA}.\n","authors":["Letian Zhang","Xiaotong Zhai","Zhongkai Zhao","Xin Wen","Yongshuo Zong","Bingchen Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06627v1.pdf","comment":"Short paper accepted at ICCV 2023 VLAR workshop"},{"id":"http://arxiv.org/abs/2310.06625v1","updated":"2023-10-10T13:44:09Z","published":"2023-10-10T13:44:09Z","title":"iTransformer: Inverted Transformers Are Effective for Time Series\n  Forecasting","summary":"  The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformer is challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the unified embedding for each temporal token fuses\nmultiple variates with potentially unaligned timestamps and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any adaptation on the basic components. We propose iTransformer that\nsimply inverts the duties of the attention mechanism and the feed-forward\nnetwork. Specifically, the time points of individual series are embedded into\nvariate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves consistent state-of-the-art on several real-world datasets, which\nfurther empowers the Transformer family with promoted performance,\ngeneralization ability across different variates, and better utilization of\narbitrary lookback windows, making it a nice alternative as the fundamental\nbackbone of time series forecasting.\n","authors":["Yong Liu","Tengge Hu","Haoran Zhang","Haixu Wu","Shiyu Wang","Lintao Ma","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2310.06625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14439v3","updated":"2023-10-10T13:41:57Z","published":"2023-07-26T18:16:43Z","title":"Fixed Integral Neural Networks","summary":"  It is often useful to perform integration over learned functions represented\nby neural networks. However, this integration is usually performed numerically,\nas analytical integration over learned functions (especially neural networks)\nis generally viewed as intractable. In this work, we present a method for\nrepresenting the analytical integral of a learned function $f$. This allows the\nexact integral of a neural network to be computed, and enables constrained\nneural networks to be parametrised by applying constraints directly to the\nintegral. Crucially, we also introduce a method to constrain $f$ to be\npositive, a necessary condition for many applications (e.g. probability\ndistributions, distance metrics, etc). Finally, we introduce several\napplications where our fixed-integral neural network (FINN) can be utilised.\n","authors":["Ryan Kortvelesy"],"pdf_url":"https://arxiv.org/pdf/2307.14439v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06622v1","updated":"2023-10-10T13:39:18Z","published":"2023-10-10T13:39:18Z","title":"Robustness May be More Brittle than We Think under Different Degrees of\n  Distribution Shifts","summary":"  Out-of-distribution (OOD) generalization is a complicated problem due to the\nidiosyncrasies of possible distribution shifts between training and test\ndomains. Most benchmarks employ diverse datasets to address this issue;\nhowever, the degree of the distribution shift between the training domains and\nthe test domains of each dataset remains largely fixed. This may lead to biased\nconclusions that either underestimate or overestimate the actual OOD\nperformance of a model. Our study delves into a more nuanced evaluation setting\nthat covers a broad range of shift degrees. We show that the robustness of\nmodels can be quite brittle and inconsistent under different degrees of\ndistribution shifts, and therefore one should be more cautious when drawing\nconclusions from evaluations under a limited range of degrees. In addition, we\nobserve that large-scale pre-trained models, such as CLIP, are sensitive to\neven minute distribution shifts of novel downstream tasks. This indicates that\nwhile pre-trained representations may help improve downstream in-distribution\nperformance, they could have minimal or even adverse effects on generalization\nin certain OOD scenarios of the downstream task if not used properly. In light\nof these findings, we encourage future research to conduct evaluations across a\nbroader range of shift degrees whenever possible.\n","authors":["Kaican Li","Yifan Zhang","Lanqing Hong","Zhenguo Li","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13826v2","updated":"2023-10-10T13:36:47Z","published":"2023-06-24T00:39:12Z","title":"Generalised f-Mean Aggregation for Graph Neural Networks","summary":"  Graph Neural Network (GNN) architectures are defined by their implementations\nof update and aggregation modules. While many works focus on new ways to\nparametrise the update modules, the aggregation modules receive comparatively\nlittle attention. Because it is difficult to parametrise aggregation functions,\ncurrently most methods select a ``standard aggregator'' such as\n$\\mathrm{mean}$, $\\mathrm{sum}$, or $\\mathrm{max}$. While this selection is\noften made without any reasoning, it has been shown that the choice in\naggregator has a significant impact on performance, and the best choice in\naggregator is problem-dependent. Since aggregation is a lossy operation, it is\ncrucial to select the most appropriate aggregator in order to minimise\ninformation loss. In this paper, we present GenAgg, a generalised aggregation\noperator, which parametrises a function space that includes all standard\naggregators. In our experiments, we show that GenAgg is able to represent the\nstandard aggregators with much higher accuracy than baseline methods. We also\nshow that using GenAgg as a drop-in replacement for an existing aggregator in a\nGNN often leads to a significant boost in performance across various tasks.\n","authors":["Ryan Kortvelesy","Steven Morad","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2306.13826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05365v2","updated":"2023-10-10T13:34:54Z","published":"2023-10-09T02:51:01Z","title":"Molecular De Novo Design through Transformer-based Reinforcement\n  Learning","summary":"  In this work, we introduce a method to fine-tune a Transformer-based\ngenerative model for molecular de novo design. Leveraging the superior sequence\nlearning capacity of Transformers over Recurrent Neural Networks (RNNs), our\nmodel can generate molecular structures with desired properties effectively. In\ncontrast to the traditional RNN-based models, our proposed method exhibits\nsuperior performance in generating compounds predicted to be active against\nvarious biological targets, capturing long-term dependencies in the molecular\nstructure sequence. The model's efficacy is demonstrated across numerous tasks,\nincluding generating analogues to a query structure and producing compounds\nwith particular attributes, outperforming the baseline RNN-based methods. Our\napproach can be used for scaffold hopping, library expansion starting from a\nsingle molecule, and generating compounds with high predicted activity against\nbiological targets.\n","authors":["Tao Feng","Pengcheng Xu","Tianfan Fu","Siddhartha Laghuvarapu","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2310.05365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04570v2","updated":"2023-10-10T13:32:55Z","published":"2023-10-06T20:17:40Z","title":"Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction\n  from Variable-Sized Maps","summary":"  Estimating path loss for a transmitter-receiver location is key to many\nuse-cases including network planning and handover. Machine learning has become\na popular tool to predict wireless channel properties based on map data. In\nthis work, we present a transformer-based neural network architecture that\nenables predicting link-level properties from maps of various dimensions and\nfrom sparse measurements. The map contains information about buildings and\nfoliage. The transformer model attends to the regions that are relevant for\npath loss prediction and, therefore, scales efficiently to maps of different\nsize. Further, our approach works with continuous transmitter and receiver\ncoordinates without relying on discretization. In experiments, we show that the\nproposed model is able to efficiently learn dominant path losses from sparse\ntraining data and generalizes well when tested on novel maps.\n","authors":["Thomas M. Hehn","Tribhuvanesh Orekondy","Ori Shental","Arash Behboodi","Juan Bucheli","Akash Doshi","June Namgoong","Taesang Yoo","Ashwin Sampath","Joseph B. Soriaga"],"pdf_url":"https://arxiv.org/pdf/2310.04570v2.pdf","comment":"Accepted at IEEE GLOBECOM 2023, v2: Changed license on arxiv"},{"id":"http://arxiv.org/abs/2310.06609v1","updated":"2023-10-10T13:23:05Z","published":"2023-10-10T13:23:05Z","title":"Discovering Interpretable Physical Models Using Symbolic Regression and\n  Discrete Exterior Calculus","summary":"  Computational modeling is a key resource to gather insight into physical\nsystems in modern scientific research and engineering. While access to large\namount of data has fueled the use of Machine Learning (ML) to recover physical\nmodels from experiments and increase the accuracy of physical simulations,\npurely data-driven models have limited generalization and interpretability. To\novercome these limitations, we propose a framework that combines Symbolic\nRegression (SR) and Discrete Exterior Calculus (DEC) for the automated\ndiscovery of physical models starting from experimental data. Since these\nmodels consist of mathematical expressions, they are interpretable and amenable\nto analysis, and the use of a natural, general-purpose discrete mathematical\nlanguage for physics favors generalization with limited input data.\nImportantly, DEC provides building blocks for the discrete analogue of field\ntheories, which are beyond the state-of-the-art applications of SR to physical\nproblems. Further, we show that DEC allows to implement a strongly-typed SR\nprocedure that guarantees the mathematical consistency of the recovered models\nand reduces the search space of symbolic expressions. Finally, we prove the\neffectiveness of our methodology by re-discovering three models of Continuum\nPhysics from synthetic experimental data: Poisson equation, the Euler's\nElastica and the equations of Linear Elasticity. Thanks to their\ngeneral-purpose nature, the methods developed in this paper may be applied to\ndiverse contexts of physical modeling.\n","authors":["Simone Manti","Alessandro Lucantonio"],"pdf_url":"https://arxiv.org/pdf/2310.06609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06600v1","updated":"2023-10-10T13:08:50Z","published":"2023-10-10T13:08:50Z","title":"Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy\n  Labels","summary":"  Label noise is a pervasive problem in deep learning that often compromises\nthe generalization performance of trained models. Recently, leveraging\nprivileged information (PI) -- information available only during training but\nnot at test time -- has emerged as an effective approach to mitigate this\nissue. Yet, existing PI-based methods have failed to consistently outperform\ntheir no-PI counterparts in terms of preventing overfitting to label noise. To\naddress this deficiency, we introduce Pi-DUAL, an architecture designed to\nharness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the\noutput logits into a prediction term, based on conventional input features, and\na noise-fitting term influenced solely by PI. A gating mechanism steered by PI\nadaptively shifts focus between these terms, allowing the model to implicitly\nseparate the learning paths of clean and wrong labels. Empirically, Pi-DUAL\nachieves significant performance improvements on key PI benchmarks (e.g., +6.8%\non ImageNet-PI), establishing a new state-of-the-art test set accuracy.\nAdditionally, Pi-DUAL is a potent method for identifying noisy samples\npost-training, outperforming other strong methods at this task. Overall,\nPi-DUAL is a simple, scalable and practical approach for mitigating the effects\nof label noise in a variety of real-world scenarios with PI.\n","authors":["Ke Wang","Guillermo Ortiz-Jimenez","Rodolphe Jenatton","Mark Collier","Efi Kokiopoulou","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2310.06600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18459v2","updated":"2023-10-10T13:01:41Z","published":"2023-05-29T05:20:38Z","title":"Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning","summary":"  Diffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.\n","authors":["Haoran He","Chenjia Bai","Kang Xu","Zhuoran Yang","Weinan Zhang","Dong Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2305.18459v2.pdf","comment":"Accepted by NeurIPS 2023. 22 pages"},{"id":"http://arxiv.org/abs/2306.16922v2","updated":"2023-10-10T12:57:10Z","published":"2023-06-14T13:34:13Z","title":"The Expressive Leaky Memory Neuron: an Efficient and Expressive\n  Phenomenological Neuron Model Can Solve Long-Horizon Tasks","summary":"  Biological cortical neurons are remarkably sophisticated computational\ndevices, temporally integrating their vast synaptic input over an intricate\ndendritic tree, subject to complex, nonlinearly interacting internal biological\nprocesses. A recent study proposed to characterize this complexity by fitting\naccurate surrogate models to replicate the input-output relationship of a\ndetailed biophysical cortical pyramidal neuron model and discovered it needed\ntemporal convolutional networks (TCN) with millions of parameters. Requiring\nthese many parameters, however, could be the result of a misalignment between\nthe inductive biases of the TCN and cortical neuron's computations. In light of\nthis, and with the aim to explore the computational implications of leaky\nmemory units and nonlinear dendritic processing, we introduce the Expressive\nLeaky Memory (ELM) neuron model, a biologically inspired phenomenological model\nof a cortical neuron. Remarkably, by exploiting a few such slowly decaying\nmemory-like hidden states and two-layered nonlinear integration of synaptic\ninput, our ELM neuron can accurately match the aforementioned input-output\nrelationship with under ten-thousand trainable parameters. To further assess\nthe computational ramifications of our neuron design, we evaluate on various\ntasks with demanding temporal structures, including the Long Range Arena (LRA)\ndatasets, as well as a novel neuromorphic dataset based on the Spiking\nHeidelberg Digits dataset (SHD-Adding). Leveraging a larger number of memory\nunits with sufficiently long timescales, and correspondingly sophisticated\nsynaptic integration, the ELM neuron proves to be competitive on both datasets,\nreliably outperforming the classic Transformer or Chrono-LSTM architectures on\nlatter, even solving the Pathfinder-X task with over $70\\%$ accuracy (16k\ncontext length).\n","authors":["Aaron Spieler","Nasim Rahaman","Georg Martius","Bernhard Schlkopf","Anna Levina"],"pdf_url":"https://arxiv.org/pdf/2306.16922v2.pdf","comment":"24 pages, 12 figures, 10 tables, additional experiments and\n  clarifications"},{"id":"http://arxiv.org/abs/2108.04623v6","updated":"2023-10-10T12:54:03Z","published":"2021-08-10T12:08:15Z","title":"Maximizing Influence with Graph Neural Networks","summary":"  Finding the seed set that maximizes the influence spread over a network is a\nwell-known NP-hard problem. Though a greedy algorithm can provide near-optimal\nsolutions, the subproblem of influence estimation renders the solutions\ninefficient. In this work, we propose \\textsc{Glie}, a graph neural network\nthat learns how to estimate the influence spread of the independent cascade.\nGLIE relies on a theoretical upper bound that is tightened through supervised\ntraining.Experiments indicate that it provides accurate influence estimation\nfor real graphs up to 10 times larger than the train set.Subsequently, we\nincorporate it into three influence maximization techniques.We first utilize\nCost Effective Lazy Forward optimization substituting Monte Carlo simulations\nwith GLIE, surpassing the benchmarks albeit with a computational overhead. To\nimprove computational efficiency we first devise a Q-learning method that\nlearns to choose seeds sequentially using GLIE's predictions. Finally, we\narrive at the most efficient approach by developing a provably submodular\ninfluence spread based on GLIE's representations, to rank nodes while building\nthe seed set adaptively. The proposed algorithms are inductive, meaning they\nare trained on graphs with less than 300 nodes and up to 5 seeds, and tested on\ngraphs with millions of nodes and up to 200 seeds. The final method exhibits\nthe most promising combination of time efficiency and influence quality,\noutperforming several baselines.\n","authors":["George Panagopoulos","Nikolaos Tziortziotis","Fragkiskos D. Malliaros","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2108.04623v6.pdf","comment":"16, IEEE/ACM ASONAM 2023"},{"id":"http://arxiv.org/abs/2310.06588v1","updated":"2023-10-10T12:53:48Z","published":"2023-10-10T12:53:48Z","title":"FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics","summary":"  Despite the massive success of fine-tuning large Pre-trained Language Models\n(PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain\nsusceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM)\nis a simple yet effective dual-model approach that enhances the robustness of\nfine-tuned PLMs, which involves fine-tuning a model on the original training\nset (i.e. reference model), selecting a specified fraction of important\ntraining examples according to the training dynamics of the reference model,\nand fine-tuning the same model on these selected examples (i.e. main model).\nHowever, it suffers from the drawback of requiring fine-tuning the same model\ntwice, which is computationally expensive for large models. In this paper, we\nfirst show that 1) training dynamics are highly transferable across different\nmodel sizes and different pre-training methods, and that 2) main models\nfine-tuned using DM learn faster than when using conventional Empirical Risk\nMinimization (ERM). Building on these observations, we propose a novel\nfine-tuning approach based on the DM method: Fine-Tuning by transFerring\nTraining dynamics (FTFT). Compared with DM, FTFT uses more efficient reference\nmodels and then fine-tunes more capable main models for fewer steps. Our\nexperiments show that FTFT achieves better generalization robustness than ERM\nwhile spending less than half of the training cost.\n","authors":["Yupei Du","Albert Gatt","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.06588v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.06585v1","updated":"2023-10-10T12:52:42Z","published":"2023-10-10T12:52:42Z","title":"A Black-Box Physics-Informed Estimator based on Gaussian Process\n  Regression for Robot Inverse Dynamics Identification","summary":"  In this paper, we propose a black-box model based on Gaussian process\nregression for the identification of the inverse dynamics of robotic\nmanipulators. The proposed model relies on a novel multidimensional kernel,\ncalled \\textit{Lagrangian Inspired Polynomial} (\\kernelInitials{}) kernel. The\n\\kernelInitials{} kernel is based on two main ideas. First, instead of directly\nmodeling the inverse dynamics components, we model as GPs the kinetic and\npotential energy of the system. The GP prior on the inverse dynamics components\nis derived from those on the energies by applying the properties of GPs under\nlinear operators. Second, as regards the energy prior definition, we prove a\npolynomial structure of the kinetic and potential energy, and we derive a\npolynomial kernel that encodes this property. As a consequence, the proposed\nmodel allows also to estimate the kinetic and potential energy without\nrequiring any label on these quantities. Results on simulation and on two real\nrobotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA\nRV4FL, show that the proposed model outperforms state-of-the-art black-box\nestimators based both on Gaussian Processes and Neural Networks in terms of\naccuracy, generality and data efficiency. The experiments on the MELFA robot\nalso demonstrate that our approach achieves performance comparable to\nfine-tuned model-based estimators, despite requiring less prior information.\n","authors":["Giulio Giacomuzzo","Alberto Dalla Libera","Diego Romeres","Ruggero Carli"],"pdf_url":"https://arxiv.org/pdf/2310.06585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05534v2","updated":"2023-10-10T12:45:07Z","published":"2023-02-10T22:25:42Z","title":"Robust Knowledge Transfer in Tiered Reinforcement Learning","summary":"  In this paper, we study the Tiered Reinforcement Learning setting, a parallel\ntransfer learning framework, where the goal is to transfer knowledge from the\nlow-tier (source) task to the high-tier (target) task to reduce the exploration\nrisk of the latter while solving the two tasks in parallel. Unlike previous\nwork, we do not assume the low-tier and high-tier tasks share the same dynamics\nor reward functions, and focus on robust knowledge transfer without prior\nknowledge on the task similarity. We identify a natural and necessary condition\ncalled the ``Optimal Value Dominance'' for our objective. Under this condition,\nwe propose novel online learning algorithms such that, for the high-tier task,\nit can achieve constant regret on partial states depending on the task\nsimilarity and retain near-optimal regret when the two tasks are dissimilar,\nwhile for the low-tier task, it can keep near-optimal without making sacrifice.\nMoreover, we further study the setting with multiple low-tier tasks, and\npropose a novel transfer source selection mechanism, which can ensemble the\ninformation from all low-tier tasks and allow provable benefits on a much\nlarger state-action space.\n","authors":["Jiawei Huang","Niao He"],"pdf_url":"https://arxiv.org/pdf/2302.05534v2.pdf","comment":"46 Pages; 1 Figure; NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06574v1","updated":"2023-10-10T12:35:20Z","published":"2023-10-10T12:35:20Z","title":"XAI for Early Crop Classification","summary":"  We propose an approach for early crop classification through identifying\nimportant timesteps with eXplainable AI (XAI) methods. Our approach consists of\ntraining a baseline crop classification model to carry out layer-wise relevance\npropagation (LRP) so that the salient time step can be identified. We chose a\nselected number of such important time indices to create the bounding region of\nthe shortest possible classification timeframe. We identified the period 21st\nApril 2019 to 9th August 2019 as having the best trade-off in terms of accuracy\nand earliness. This timeframe only suffers a 0.75% loss in accuracy as compared\nto using the full timeseries. We observed that the LRP-derived important\ntimesteps also highlight small details in input values that differentiates\nbetween different classes and\n","authors":["Ayshah Chan","Maja Schneider","Marco Krner"],"pdf_url":"https://arxiv.org/pdf/2310.06574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06572v1","updated":"2023-10-10T12:31:29Z","published":"2023-10-10T12:31:29Z","title":"Deep Learning reconstruction with uncertainty estimation for $$\n  photon interaction in fast scintillator detectors","summary":"  This article presents a physics-informed deep learning method for the\nquantitative estimation of the spatial coordinates of gamma interactions within\na monolithic scintillator, with a focus on Positron Emission Tomography (PET)\nimaging. A Density Neural Network approach is designed to estimate the\n2-dimensional gamma photon interaction coordinates in a fast lead tungstate\n(PbWO4) monolithic scintillator detector. We introduce a custom loss function\nto estimate the inherent uncertainties associated with the reconstruction\nprocess and to incorporate the physical constraints of the detector.\n  This unique combination allows for more robust and reliable position\nestimations and the obtained results demonstrate the effectiveness of the\nproposed approach and highlights the significant benefits of the uncertainties\nestimation. We discuss its potential impact on improving PET imaging quality\nand show how the results can be used to improve the exploitation of the model,\nto bring benefits to the application and how to evaluate the validity of the\ngiven prediction and the associated uncertainties. Importantly, our proposed\nmethodology extends beyond this specific use case, as it can be generalized to\nother applications beyond PET imaging.\n","authors":["Geoffrey Daniel","Mohamed Bahi Yahiaoui","Claude Comtat","Sebastien Jan","Olga Kochebina","Jean-Marc Martinez","Viktoriya Sergeyeva","Viatcheslav Sharyy","Chi-Hsun Sung","Dominique Yvon"],"pdf_url":"https://arxiv.org/pdf/2310.06572v1.pdf","comment":"Submitted to Artificial Intelligence"},{"id":"http://arxiv.org/abs/2310.06571v1","updated":"2023-10-10T12:29:57Z","published":"2023-10-10T12:29:57Z","title":"Statistical properties and privacy guarantees of an original\n  distance-based fully synthetic data generation method","summary":"  Introduction: The amount of data generated by original research is growing\nexponentially. Publicly releasing them is recommended to comply with the Open\nScience principles. However, data collected from human participants cannot be\nreleased as-is without raising privacy concerns. Fully synthetic data represent\na promising answer to this challenge. This approach is explored by the French\nCentre de Recherche en {\\'E}pid{\\'e}miologie et Sant{\\'e} des Populations in\nthe form of a synthetic data generation framework based on Classification and\nRegression Trees and an original distance-based filtering. The goal of this\nwork was to develop a refined version of this framework and to assess its\nrisk-utility profile with empirical and formal tools, including novel ones\ndeveloped for the purpose of this evaluation.Materials and Methods: Our\nsynthesis framework consists of four successive steps, each of which is\ndesigned to prevent specific risks of disclosure. We assessed its performance\nby applying two or more of these steps to a rich epidemiological dataset.\nPrivacy and utility metrics were computed for each of the resulting synthetic\ndatasets, which were further assessed using machine learning\napproaches.Results: Computed metrics showed a satisfactory level of protection\nagainst attribute disclosure attacks for each synthetic dataset, especially\nwhen the full framework was used. Membership disclosure attacks were formally\nprevented without significantly altering the data. Machine learning approaches\nshowed a low risk of success for simulated singling out and linkability\nattacks. Distributional and inferential similarity with the original data were\nhigh with all datasets.Discussion: This work showed the technical feasibility\nof generating publicly releasable synthetic data using a multi-step framework.\nFormal and empirical tools specifically developed for this demonstration are a\nvaluable contribution to this field. Further research should focus on the\nextension and validation of these tools, in an effort to specify the intrinsic\nqualities of alternative data synthesis methods.Conclusion: By successfully\nassessing the quality of data produced using a novel multi-step synthetic data\ngeneration framework, we showed the technical and conceptual soundness of the\nOpen-CESP initiative, which seems ripe for full-scale implementation.\n","authors":["Rmy Chapelle","Bruno Falissard"],"pdf_url":"https://arxiv.org/pdf/2310.06571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06557v1","updated":"2023-10-10T12:13:38Z","published":"2023-10-10T12:13:38Z","title":"Data efficient deep learning for medical image analysis: A survey","summary":"  The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.\n","authors":["Suruchi Kumari","Pravendra Singh"],"pdf_url":"https://arxiv.org/pdf/2310.06557v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.06555v1","updated":"2023-10-10T12:10:40Z","published":"2023-10-10T12:10:40Z","title":"On Temporal References in Emergent Communication","summary":"  As humans, we use linguistic elements referencing time, such as before or\ntomorrow, to easily share past experiences and future predictions. While\ntemporal aspects of the language have been considered in computational\nlinguistics, no such exploration has been done within the field of emergent\ncommunication. We research this gap, providing the first reported temporal\nvocabulary within emergent communication literature. Our experimental analysis\nshows that a different agent architecture is sufficient for the natural\nemergence of temporal references, and that no additional losses are necessary.\nOur readily transferable architectural insights provide the basis for the\nincorporation of temporal referencing into other emergent communication\nenvironments.\n","authors":["Olaf Lipinski","Adam J. Sobey","Federico Cerutti","Timothy J. Norman"],"pdf_url":"https://arxiv.org/pdf/2310.06555v1.pdf","comment":"26 pages, 13 figures. Code available at\n  https://anonymous.4open.science/r/TRG-E137/README.md"},{"id":"http://arxiv.org/abs/2308.15126v3","updated":"2023-10-10T11:57:26Z","published":"2023-08-29T08:51:24Z","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n","authors":["Junyang Wang","Yiyang Zhou","Guohai Xu","Pengcheng Shi","Chenlin Zhao","Haiyang Xu","Qinghao Ye","Ming Yan","Ji Zhang","Jihua Zhu","Jitao Sang","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2308.15126v3.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.14970v3","updated":"2023-10-10T11:54:37Z","published":"2023-09-26T14:42:28Z","title":"Recurrent Hypernetworks are Surprisingly Strong in Meta-RL","summary":"  Deep reinforcement learning (RL) is notoriously impractical to deploy due to\nsample inefficiency. Meta-RL directly addresses this sample inefficiency by\nlearning to perform few-shot learning when a distribution of related tasks is\navailable for meta-training. While many specialized meta-RL methods have been\nproposed, recent work suggests that end-to-end learning in conjunction with an\noff-the-shelf sequential model, such as a recurrent network, is a surprisingly\nstrong baseline. However, such claims have been controversial due to limited\nsupporting evidence, particularly in the face of prior work establishing\nprecisely the opposite. In this paper, we conduct an empirical investigation.\nWhile we likewise find that a recurrent network can achieve strong performance,\nwe demonstrate that the use of hypernetworks is crucial to maximizing their\npotential. Surprisingly, when combined with hypernetworks, the recurrent\nbaselines that are far simpler than existing specialized methods actually\nachieve the strongest performance of all methods evaluated.\n","authors":["Jacob Beck","Risto Vuorio","Zheng Xiong","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2309.14970v3.pdf","comment":"Published at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06549v1","updated":"2023-10-10T11:51:12Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v1.pdf","comment":"23 pages, 8 tables, 8 figures"},{"id":"http://arxiv.org/abs/2310.06543v1","updated":"2023-10-10T11:42:49Z","published":"2023-10-10T11:42:49Z","title":"An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for\n  Travelling Salesman Problems","summary":"  Recent years have witnessed a surge in research on machine learning for\ncombinatorial optimization since learning-based approaches can outperform\ntraditional heuristics and approximate exact solvers at a lower computation\ncost. However, most existing work on supervised neural combinatorial\noptimization focuses on TSP instances with a fixed number of cities and\nrequires large amounts of training samples to achieve a good performance,\nmaking them less practical to be applied to realistic optimization scenarios.\nThis work aims to develop a data-driven graph representation learning method\nfor solving travelling salesman problems (TSPs) with various numbers of cities.\nTo this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that\ncan learn to solve TSPs after being trained on solution data of various sizes\nwith an imbalanced distribution. We formulate the TSP as a link prediction task\non sparse connected graphs. A residual gated encoder is trained to learn latent\nedge embeddings, followed by an edge-centered decoder to output link\npredictions in an end-to-end manner. To improve the model's generalization\ncapability of solving large-scale problems, we introduce an active sampling\nstrategy into the training process. In addition, we generate a benchmark\ndataset containing 50,000 TSP instances with a size from 50 to 500 cities,\nfollowing an extremely scale-imbalanced distribution, making it ideal for\ninvestigating the model's performance for practical applications. We conduct\nexperiments using different amounts of training data with various scales, and\nthe experimental results demonstrate that the proposed data-driven approach\nachieves a highly competitive performance among state-of-the-art learning-based\nmethods for solving TSPs.\n","authors":["Shiqing Liu","Xueming Yan","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2310.06543v1.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.06540v1","updated":"2023-10-10T11:38:16Z","published":"2023-10-10T11:38:16Z","title":"A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo:\n  A Romanian Clickbait Corpus of News Articles","summary":"  To increase revenue, news websites often resort to using deceptive news\ntitles, luring users into clicking on the title and reading the full news.\nClickbait detection is the task that aims to automatically detect this form of\nfalse advertisement and avoid wasting the precious time of online users.\nDespite the importance of the task, to the best of our knowledge, there is no\npublicly available clickbait corpus for the Romanian language. To this end, we\nintroduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news\nsamples which are manually annotated with clickbait and non-clickbait labels.\nFurthermore, we conduct experiments with four machine learning methods, ranging\nfrom handcrafted models to recurrent and transformer-based neural networks, to\nestablish a line-up of competitive baselines. We also carry out experiments\nwith a weighted voting ensemble. Among the considered baselines, we propose a\nnovel BERT-based contrastive learning model that learns to encode news titles\nand contents into a deep metric space such that titles and contents of\nnon-clickbait news have high cosine similarity, while titles and contents of\nclickbait news have low cosine similarity. Our data set and code to reproduce\nthe baselines are publicly available for download at\nhttps://github.com/dariabroscoteanu/RoCliCo.\n","authors":["Daria-Mihaela Broscoteanu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2310.06540v1.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06537v1","updated":"2023-10-10T11:34:53Z","published":"2023-10-10T11:34:53Z","title":"Data-level hybrid strategy selection for disk fault prediction model\n  based on multivariate GAN","summary":"  Data class imbalance is a common problem in classification problems, where\nminority class samples are often more important and more costly to misclassify\nin a classification task. Therefore, it is very important to solve the data\nclass imbalance classification problem. The SMART dataset exhibits an evident\nclass imbalance, comprising a substantial quantity of healthy samples and a\ncomparatively limited number of defective samples. This dataset serves as a\nreliable indicator of the disc's health status. In this paper, we obtain the\nbest balanced disk SMART dataset for a specific classification model by mixing\nand integrating the data synthesised by multivariate generative adversarial\nnetworks (GAN) to balance the disk SMART dataset at the data level; and combine\nit with genetic algorithms to obtain higher disk fault classification\nprediction accuracy on a specific classification model.\n","authors":["Shuangshuang Yuan","Peng Wu","Yuehui Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06534v1","updated":"2023-10-10T11:28:40Z","published":"2023-10-10T11:28:40Z","title":"Disk failure prediction based on multi-layer domain adaptive learning","summary":"  Large scale data storage is susceptible to failure. As disks are damaged and\nreplaced, traditional machine learning models, which rely on historical data to\nmake predictions, struggle to accurately predict disk failures. This paper\npresents a novel method for predicting disk failures by leveraging multi-layer\ndomain adaptive learning techniques. First, disk data with numerous faults is\nselected as the source domain, and disk data with fewer faults is selected as\nthe target domain. A training of the feature extraction network is performed\nwith the selected origin and destination domains. The contrast between the two\ndomains facilitates the transfer of diagnostic knowledge from the domain of\nsource and target. According to the experimental findings, it has been\ndemonstrated that the proposed technique can generate a reliable prediction\nmodel and improve the ability to predict failures on disk data with few failure\nsamples.\n","authors":["Guangfu Gao","Peng Wu","Hussain Dawood"],"pdf_url":"https://arxiv.org/pdf/2310.06534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14189v2","updated":"2023-10-10T11:27:49Z","published":"2023-05-23T16:11:00Z","title":"Beyond Shared Vocabulary: Increasing Representational Word Similarities\n  across Languages for Multilingual Machine Translation","summary":"  Using a vocabulary that is shared across languages is common practice in\nMultilingual Neural Machine Translation (MNMT). In addition to its simple\ndesign, shared tokens play an important role in positive knowledge transfer,\nassuming that shared tokens refer to similar meanings across languages.\nHowever, when word overlap is small, especially due to different writing\nsystems, transfer is inhibited. In this paper, we define word-level information\ntransfer pathways via word equivalence classes and rely on graph networks to\nfuse word embeddings across languages. Our experiments demonstrate the\nadvantages of our approach: 1) embeddings of words with similar meanings are\nbetter aligned across languages, 2) our method achieves consistent BLEU\nimprovements of up to 2.3 points for high- and low-resource MNMT, and 3) less\nthan 1.0\\% additional trainable parameters are required with a limited increase\nin computational costs, while inference time remains identical to the baseline.\nWe release the codebase to the community.\n","authors":["Di Wu","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2305.14189v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.05537v2","updated":"2023-10-10T11:12:13Z","published":"2023-10-09T09:01:25Z","title":"ParFam -- Symbolic Regression Based on Continuous Global Optimization","summary":"  The problem of symbolic regression (SR) arises in many different\napplications, such as identifying physical laws or deriving mathematical\nequations describing the behavior of financial markets from given data. Various\nmethods exist to address the problem of SR, often based on genetic programming.\nHowever, these methods are usually quite complicated and require a lot of\nhyperparameter tuning and computational resources. In this paper, we present\nour new method ParFam that utilizes parametric families of suitable symbolic\nfunctions to translate the discrete symbolic regression problem into a\ncontinuous one, resulting in a more straightforward setup compared to current\nstate-of-the-art methods. In combination with a powerful global optimizer, this\napproach results in an effective method to tackle the problem of SR.\nFurthermore, it can be easily extended to more advanced algorithms, e.g., by\nadding a deep neural network to find good-fitting parametric families. We prove\nthe performance of ParFam with extensive numerical experiments based on the\ncommon SR benchmark suit SRBench, showing that we achieve state-of-the-art\nresults. Our code and results can be found at\nhttps://github.com/Philipp238/parfam .\n","authors":["Philipp Scholl","Katharina Bieker","Hillary Hauger","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2310.05537v2.pdf","comment":"Code: https://github.com/Philipp238/parfam"},{"id":"http://arxiv.org/abs/2310.06522v1","updated":"2023-10-10T11:08:31Z","published":"2023-10-10T11:08:31Z","title":"Watt For What: Rethinking Deep Learning's Energy-Performance\n  Relationship","summary":"  Deep learning models have revolutionized various fields, from image\nrecognition to natural language processing, by achieving unprecedented levels\nof accuracy. However, their increasing energy consumption has raised concerns\nabout their environmental impact, disadvantaging smaller entities in research\nand exacerbating global energy consumption. In this paper, we explore the\ntrade-off between model accuracy and electricity consumption, proposing a\nmetric that penalizes large consumption of electricity. We conduct a\ncomprehensive study on the electricity consumption of various deep learning\nmodels across different GPUs, presenting a detailed analysis of their\naccuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity\nconsumed, we demonstrate how smaller, more energy-efficient models can\nsignificantly expedite research while mitigating environmental concerns. Our\nresults highlight the potential for a more sustainable approach to deep\nlearning, emphasizing the importance of optimizing models for efficiency. This\nresearch also contributes to a more equitable research landscape, where smaller\nentities can compete effectively with larger counterparts. This advocates for\nthe adoption of efficient deep learning practices to reduce electricity\nconsumption, safeguarding the environment for future generations whilst also\nhelping ensure a fairer competitive landscape.\n","authors":["Shreyank N Gowda","Xinyue Hao","Gen Li","Laura Sevilla-Lara","Shashank Narayana Gowda"],"pdf_url":"https://arxiv.org/pdf/2310.06522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06961v3","updated":"2023-10-10T10:56:22Z","published":"2023-08-14T06:32:52Z","title":"Graph Structural Residuals: A Learning Approach to Diagnosis","summary":"  Traditional model-based diagnosis relies on constructing explicit system\nmodels, a process that can be laborious and expertise-demanding. In this paper,\nwe propose a novel framework that combines concepts of model-based diagnosis\nwith deep graph structure learning. This data-driven approach leverages data to\nlearn the system's underlying structure and provide dynamic observations,\nrepresented by two distinct graph adjacency matrices. Our work facilitates a\nseamless integration of graph structure learning with model-based diagnosis by\nmaking three main contributions: (i) redefining the constructs of system\nrepresentation, observations, and faults (ii) introducing two distinct versions\nof a self-supervised graph structure learning model architecture and (iii)\ndemonstrating the potential of our data-driven diagnostic method through\nexperiments on a system of coupled oscillators.\n","authors":["Jan Lukas Augustin","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2308.06961v3.pdf","comment":"11 pages, added missing section heading"},{"id":"http://arxiv.org/abs/2310.06514v1","updated":"2023-10-10T10:55:49Z","published":"2023-10-10T10:55:49Z","title":"AttributionLab: Faithfulness of Feature Attribution Under Controllable\n  Environments","summary":"  Feature attribution explains neural network outputs by identifying relevant\ninput features. How do we know if the identified features are indeed relevant\nto the network? This notion is referred to as faithfulness, an essential\nproperty that reflects the alignment between the identified (attributed)\nfeatures and the features used by the model. One recent trend to test\nfaithfulness is to design the data such that we know which input features are\nrelevant to the label and then train a model on the designed data.\nSubsequently, the identified features are evaluated by comparing them with\nthese designed ground truth features. However, this idea has the underlying\nassumption that the neural network learns to use all and only these designed\nfeatures, while there is no guarantee that the learning process trains the\nnetwork in this way. In this paper, we solve this missing link by explicitly\ndesigning the neural network by manually setting its weights, along with\ndesigning data, so we know precisely which input features in the dataset are\nrelevant to the designed network. Thus, we can test faithfulness in\nAttributionLab, our designed synthetic environment, which serves as a sanity\ncheck and is effective in filtering out attribution methods. If an attribution\nmethod is not faithful in a simple controlled environment, it can be unreliable\nin more complex scenarios. Furthermore, the AttributionLab environment serves\nas a laboratory for controlled experiments through which we can study feature\nattribution methods, identify issues, and suggest potential improvements.\n","authors":["Yang Zhang","Yawei Li","Hannah Brown","Mina Rezaei","Bernd Bischl","Philip Torr","Ashkan Khakzar","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2310.06514v1.pdf","comment":"32 pages including Appendix"},{"id":"http://arxiv.org/abs/2310.05371v2","updated":"2023-10-10T10:55:10Z","published":"2023-10-09T03:00:15Z","title":"Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using\n  mpMRI Segmentation and Classification","summary":"  Prostate cancer (PCa) is a severe disease among men globally. It is important\nto identify PCa early and make a precise diagnosis for effective treatment. For\nPCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as\nan invaluable imaging modality that offers a precise anatomical view of the\nprostate gland and its tissue structure. Deep learning (DL) models can enhance\nexisting clinical systems and improve patient care by locating regions of\ninterest for physicians. Recently, DL techniques have been employed to develop\na pipeline for segmenting and classifying different cancer types. These studies\nshow that DL can be used to increase diagnostic precision and give objective\nresults without variability. This work uses well-known DL models for the\nclassification and segmentation of mpMRI images to detect PCa. Our\nimplementation involves four pipelines; Semantic DeepSegNet with ResNet50,\nDeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with\na long short-term memory (LSTM). Each segmentation model is paired with a\ndifferent classifier to evaluate the performance using different metrics. The\nresults of our experiments show that the pipeline that uses the combination of\nU-Net and the LSTM model outperforms all other combinations, excelling in both\nsegmentation and classification tasks.\n","authors":["Anil B. Gavade","Neel Kanwal","Priyanka A. Gavade","Rajendra Nerli"],"pdf_url":"https://arxiv.org/pdf/2310.05371v2.pdf","comment":"Accepted at CISCON-2023"},{"id":"http://arxiv.org/abs/2304.08072v2","updated":"2023-10-10T10:53:29Z","published":"2023-04-17T08:34:41Z","title":"Two-stage MR Image Segmentation Method for Brain Tumors based on\n  Attention Mechanism","summary":"  Multimodal magnetic resonance imaging (MRI) can reveal different patterns of\nhuman tissue and is crucial for clinical diagnosis. However, limited by cost,\nnoise and manual labeling, obtaining diverse and reliable multimodal MR images\nremains a challenge. For the same lesion, different MRI manifestations have\ngreat differences in background information, coarse positioning and fine\nstructure. In order to obtain better generation and segmentation performance, a\ncoordination-spatial attention generation adversarial network (CASP-GAN) based\non the cycle-consistent generative adversarial network (CycleGAN) is proposed.\nThe performance of the generator is optimized by introducing the Coordinate\nAttention (CA) module and the Spatial Attention (SA) module. The two modules\ncan make full use of the captured location information, accurately locating the\ninterested region, and enhancing the generator model network structure. The\nability to extract the structure information and the detailed information of\nthe original medical image can help generate the desired image with higher\nquality. There exist some problems in the original CycleGAN that the training\ntime is long, the parameter amount is too large, and it is difficult to\nconverge. In response to this problem, we introduce the Coordinate Attention\n(CA) module to replace the Res Block to reduce the number of parameters, and\ncooperate with the spatial information extraction network above to strengthen\nthe information extraction ability. On the basis of CASP-GAN, an attentional\ngenerative cross-modality segmentation (AGCMS) method is further proposed. This\nmethod inputs the modalities generated by CASP-GAN and the real modalities into\nthe segmentation network for brain tumor segmentation. Experimental results\nshow that CASP-GAN outperforms CycleGAN and some state-of-the-art methods in\nPSNR, SSMI and RMSE in most tasks.\n","authors":["Li Zhu","Jiawei Jiang","Lin Lu","Jin Li"],"pdf_url":"https://arxiv.org/pdf/2304.08072v2.pdf","comment":"Some contributing authors are not signed"},{"id":"http://arxiv.org/abs/2310.06511v1","updated":"2023-10-10T10:48:52Z","published":"2023-10-10T10:48:52Z","title":"Self-Supervised Set Representation Learning for Unsupervised\n  Meta-Learning","summary":"  Dataset distillation methods have achieved remarkable success in distilling a\nlarge dataset into a small set of representative samples. However, they are not\ndesigned to produce a distilled dataset that can be effectively used for\nfacilitating self-supervised pre-training. To this end, we propose a novel\nproblem of distilling an unlabeled dataset into a set of small synthetic\nsamples for efficient self-supervised learning (SSL). We first prove that a\ngradient of synthetic samples with respect to a SSL objective in naive bilevel\noptimization is \\textit{biased} due to the randomness originating from data\naugmentations or masking. To address this issue, we propose to minimize the\nmean squared error (MSE) between a model's representations of the synthetic\nexamples and their corresponding learnable target feature representations for\nthe inner objective, which does not introduce any randomness. Our primary\nmotivation is that the model obtained by the proposed inner optimization can\nmimic the \\textit{self-supervised target model}. To achieve this, we also\nintroduce the MSE between representations of the inner model and the\nself-supervised target model on the original full dataset for outer\noptimization. Lastly, assuming that a feature extractor is fixed, we only\noptimize a linear head on top of the feature extractor, which allows us to\nreduce the computational cost and obtain a closed-form solution of the head\nwith kernel ridge regression. We empirically validate the effectiveness of our\nmethod on various applications involving transfer learning.\n","authors":["Dong Bok Lee","Seanie Lee","Joonho Ko","Kenji Kawaguchi","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.06511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14839v3","updated":"2023-10-10T10:28:41Z","published":"2023-07-27T13:18:52Z","title":"Kernelised Normalising Flows","summary":"  Normalising Flows are non-parametric statistical models characterised by\ntheir dual capabilities of density estimation and generation. This duality\nrequires an inherently invertible architecture. However, the requirement of\ninvertibility imposes constraints on their expressiveness, necessitating a\nlarge number of parameters and innovative architectural designs to achieve good\nresults. Whilst flow-based models predominantly rely on neural-network-based\ntransformations for expressive designs, alternative transformation methods have\nreceived limited attention. In this work, we present Ferumal flow, a novel\nkernelised normalising flow paradigm that integrates kernels into the\nframework. Our results demonstrate that a kernelised flow can yield competitive\nor superior results compared to neural network-based flows whilst maintaining\nparameter efficiency. Kernelised flows excel especially in the low-data regime,\nenabling flexible non-parametric density estimation in applications with sparse\ndata availability.\n","authors":["Eshant English","Matthias Kirchler","Christoph Lippert"],"pdf_url":"https://arxiv.org/pdf/2307.14839v3.pdf","comment":"Alternate title: Kernelized Normalizing Flows"},{"id":"http://arxiv.org/abs/2310.06506v1","updated":"2023-10-10T10:26:30Z","published":"2023-10-10T10:26:30Z","title":"Runway Sign Classifier: A DAL C Certifiable Machine Learning System","summary":"  In recent years, the remarkable progress of Machine Learning (ML)\ntechnologies within the domain of Artificial Intelligence (AI) systems has\npresented unprecedented opportunities for the aviation industry, paving the way\nfor further advancements in automation, including the potential for single\npilot or fully autonomous operation of large commercial airplanes. However, ML\ntechnology faces major incompatibilities with existing airborne certification\nstandards, such as ML model traceability and explainability issues or the\ninadequacy of traditional coverage metrics. Certification of ML-based airborne\nsystems using current standards is problematic due to these challenges. This\npaper presents a case study of an airborne system utilizing a Deep Neural\nNetwork (DNN) for airport sign detection and classification. Building upon our\nprevious work, which demonstrates compliance with Design Assurance Level (DAL)\nD, we upgrade the system to meet the more stringent requirements of Design\nAssurance Level C. To achieve DAL C, we employ an established architectural\nmitigation technique involving two redundant and dissimilar Deep Neural\nNetworks. The application of novel ML-specific data management techniques\nfurther enhances this approach. This work is intended to illustrate how the\ncertification challenges of ML-based systems can be addressed for medium\ncriticality airborne applications.\n","authors":["Konstantin Dmitriev","Johann Schumann","Islam Bostanov","Mostafa Abdelhamid","Florian Holzapfel"],"pdf_url":"https://arxiv.org/pdf/2310.06506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06504v1","updated":"2023-10-10T10:22:05Z","published":"2023-10-10T10:22:05Z","title":"Revisit Input Perturbation Problems for LLMs: A Unified Robustness\n  Evaluation Framework for Noisy Slot Filling Task","summary":"  With the increasing capabilities of large language models (LLMs), these\nhigh-performance models have achieved state-of-the-art results on a wide range\nof natural language processing (NLP) tasks. However, the models' performance on\ncommonly-used benchmark datasets often fails to accurately reflect their\nreliability and robustness when applied to real-world noisy data. To address\nthese challenges, we propose a unified robustness evaluation framework based on\nthe slot-filling task to systematically evaluate the dialogue understanding\ncapability of LLMs in diverse input perturbation scenarios. Specifically, we\nconstruct a input perturbation evaluation dataset, Noise-LLM, which contains\nfive types of single perturbation and four types of mixed perturbation data.\nFurthermore, we utilize a multi-level data augmentation method (character,\nword, and sentence levels) to construct a candidate data pool, and carefully\ndesign two ways of automatic task demonstration construction strategies\n(instance-level and entity-level) with various prompt templates. Our aim is to\nassess how well various robustness methods of LLMs perform in real-world noisy\nscenarios. The experiments have demonstrated that the current open-source LLMs\ngenerally achieve limited perturbation robustness performance. Based on these\nexperimental observations, we make some forward-looking suggestions to fuel the\nresearch in this direction.\n","authors":["Guanting Dong","Jinxu Zhao","Tingfeng Hui","Daichi Guo","Wenlong Wan","Boqi Feng","Yueyan Qiu","Zhuoma Gongque","Keqing He","Zechen Wang","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2310.06504v1.pdf","comment":"Accepted at NLPCC 2023 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2310.04444v2","updated":"2023-10-10T10:15:14Z","published":"2023-10-02T22:35:40Z","title":"What's the Magic Word? A Control Theory of LLM Prompting","summary":"  Prompt engineering is effective and important in the deployment of LLMs but\nis poorly understood mathematically. Here, we formalize prompt engineering as\nan optimal control problem on LLMs -- where the prompt is considered a control\nvariable for modulating the output distribution of the LLM. Within this\nframework, we ask a simple question: given a sequence of tokens, does there\nalways exist a prompt we can prepend that will steer the LLM toward accurately\npredicting the final token? We call such an optimal prompt the magic word since\nprepending the prompt causes the LLM to output the correct answer. If magic\nwords exist, can we find them? If so, what are their properties? We offer\nanalytic analysis on the controllability of the self-attention head where we\nprove a bound on controllability as a function of the singular values of its\nweight matrices. We take inspiration from control theory to propose a metric\ncalled $k-\\epsilon$ controllability to characterize LLM steerability. We\ncompute the $k-\\epsilon$ controllability of a panel of large language models,\nincluding Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language\nmodeling tasks. Remarkably, we find that magic words of 10 tokens or less exist\nfor over 97% of WikiText instances surveyed for each model.\n","authors":["Aman Bhargava","Cameron Witkowski","Manav Shah","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2310.04444v2.pdf","comment":"18 pages, 8 figures. Under review for ICLR 2024"},{"id":"http://arxiv.org/abs/2309.12559v2","updated":"2023-10-10T10:14:28Z","published":"2023-09-22T01:06:16Z","title":"Invariant Learning via Probability of Sufficient and Necessary Causes","summary":"  Out-of-distribution (OOD) generalization is indispensable for learning models\nin the wild, where testing distribution typically unknown and different from\nthe training. Recent methods derived from causality have shown great potential\nin achieving OOD generalization. However, existing methods mainly focus on the\ninvariance property of causes, while largely overlooking the property of\n\\textit{sufficiency} and \\textit{necessity} conditions. Namely, a necessary but\ninsufficient cause (feature) is invariant to distribution shift, yet it may not\nhave required accuracy. By contrast, a sufficient yet unnecessary cause\n(feature) tends to fit specific data well but may have a risk of adapting to a\nnew domain. To capture the information of sufficient and necessary causes, we\nemploy a classical concept, the probability of sufficiency and necessary causes\n(PNS), which indicates the probability of whether one is the necessary and\nsufficient cause. To associate PNS with OOD generalization, we propose PNS risk\nand formulate an algorithm to learn representation with a high PNS value. We\ntheoretically analyze and prove the generalizability of the PNS risk.\nExperiments on both synthetic and real-world benchmarks demonstrate the\neffectiveness of the proposed method. The details of the implementation can be\nfound at the GitHub repository: https://github.com/ymy4323460/CaSN.\n","authors":["Mengyue Yang","Zhen Fang","Yonggang Zhang","Yali Du","Furui Liu","Jean-Francois Ton","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2309.12559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04475v2","updated":"2023-10-10T10:06:05Z","published":"2023-03-08T09:47:00Z","title":"RACCER: Towards Reachable and Certain Counterfactual Explanations for\n  Reinforcement Learning","summary":"  While reinforcement learning (RL) algorithms have been successfully applied\nto numerous tasks, their reliance on neural networks makes their behavior\ndifficult to understand and trust. Counterfactual explanations are\nhuman-friendly explanations that offer users actionable advice on how to alter\nthe model inputs to achieve the desired output from a black-box system.\nHowever, current approaches to generating counterfactuals in RL ignore the\nstochastic and sequential nature of RL tasks and can produce counterfactuals\nthat are difficult to obtain or do not deliver the desired outcome. In this\nwork, we propose RACCER, the first RL-specific approach to generating\ncounterfactual explanations for the behavior of RL agents. We first propose and\nimplement a set of RL-specific counterfactual properties that ensure easily\nreachable counterfactuals with highly probable desired outcomes. We use a\nheuristic tree search of the agent's execution trajectories to find the most\nsuitable counterfactuals based on the defined properties. We evaluate RACCER in\ntwo tasks as well as conduct a user study to show that RL-specific\ncounterfactuals help users better understand agents' behavior compared to the\ncurrent state-of-the-art approaches.\n","authors":["Jasmina Gajcin","Ivana Dusparic"],"pdf_url":"https://arxiv.org/pdf/2303.04475v2.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.02402v2","updated":"2023-10-10T10:00:53Z","published":"2023-10-03T19:53:12Z","title":"On the Parallel Complexity of Multilevel Monte Carlo in Stochastic\n  Gradient Descent","summary":"  In the stochastic gradient descent (SGD) for sequential simulations such as\nthe neural stochastic differential equations, the Multilevel Monte Carlo (MLMC)\nmethod is known to offer better theoretical computational complexity compared\nto the naive Monte Carlo approach. However, in practice, MLMC scales poorly on\nmassively parallel computing platforms such as modern GPUs, because of its\nlarge parallel complexity which is equivalent to that of the naive Monte Carlo\nmethod. To cope with this issue, we propose the delayed MLMC gradient estimator\nthat drastically reduces the parallel complexity of MLMC by recycling\npreviously computed gradient components from earlier steps of SGD. The proposed\nestimator provably reduces the average parallel complexity per iteration at the\ncost of a slightly worse per-iteration convergence rate. In our numerical\nexperiments, we use an example of deep hedging to demonstrate the superior\nparallel complexity of our method compared to the standard MLMC in SGD.\n","authors":["Kei Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2310.02402v2.pdf","comment":"Fixed a typo in the title and added acknowledgement"},{"id":"http://arxiv.org/abs/2310.06489v1","updated":"2023-10-10T09:57:19Z","published":"2023-10-10T09:57:19Z","title":"Deep Learning for Automatic Detection and Facial Recognition in Japanese\n  Macaques: Illuminating Social Networks","summary":"  Individual identification plays a pivotal role in ecology and ethology,\nnotably as a tool for complex social structures understanding. However,\ntraditional identification methods often involve invasive physical tags and can\nprove both disruptive for animals and time-intensive for researchers. In recent\nyears, the integration of deep learning in research offered new methodological\nperspectives through automatization of complex tasks. Harnessing object\ndetection and recognition technologies is increasingly used by researchers to\nachieve identification on video footage. This study represents a preliminary\nexploration into the development of a non-invasive tool for face detection and\nindividual identification of Japanese macaques (Macaca fuscata) through deep\nlearning. The ultimate goal of this research is, using identifications done on\nthe dataset, to automatically generate a social network representation of the\nstudied population. The current main results are promising: (i) the creation of\na Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%\naccuracy and (ii) the creation of an individual recognizer for K{\\=o}jima\nisland macaques population (YOLOv8n model), reaching a 83% accuracy. We also\ncreated a K{\\=o}jima population social network by traditional methods, based on\nco-occurrences on videos. Thus, we provide a benchmark against which the\nautomatically generated network will be assessed for reliability. These\npreliminary results are a testament to the potential of this innovative\napproach to provide the scientific community with a tool for tracking\nindividuals and social network studies in Japanese macaques.\n","authors":["Julien Paulet","Axel Molina","Benjamin Beltzung","Takafumi Suzumura","Shinya Yamamoto","Cdric Sueur"],"pdf_url":"https://arxiv.org/pdf/2310.06489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v1","updated":"2023-10-10T09:57:17Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06483v1","updated":"2023-10-10T09:50:54Z","published":"2023-10-10T09:50:54Z","title":"Variance Reduced Online Gradient Descent for Kernelized Pairwise\n  Learning with Limited Memory","summary":"  Pairwise learning is essential in machine learning, especially for problems\ninvolving loss functions defined on pairs of training examples. Online gradient\ndescent (OGD) algorithms have been proposed to handle online pairwise learning,\nwhere data arrives sequentially. However, the pairwise nature of the problem\nmakes scalability challenging, as the gradient computation for a new sample\ninvolves all past samples. Recent advancements in OGD algorithms have aimed to\nreduce the complexity of calculating online gradients, achieving complexities\nless than $O(T)$ and even as low as $O(1)$. However, these approaches are\nprimarily limited to linear models and have induced variance. In this study, we\npropose a limited memory OGD algorithm that extends to kernel online pairwise\nlearning while improving the sublinear regret. Specifically, we establish a\nclear connection between the variance of online gradients and the regret, and\nconstruct online gradients using the most recent stratified samples with a\nlimited buffer of size of $s$ representing all past data, which have a\ncomplexity of $O(sT)$ and employs $O(\\sqrt{T}\\log{T})$ random Fourier features\nfor kernel approximation. Importantly, our theoretical results demonstrate that\nthe variance-reduced online gradients lead to an improved sublinear regret\nbound. The experiments on real-world datasets demonstrate the superiority of\nour algorithm over both kernelized and linear online pairwise learning\nalgorithms.\n","authors":["Hilal AlQuabeh","Bhaskar Mukhoty","Bin Gu"],"pdf_url":"https://arxiv.org/pdf/2310.06483v1.pdf","comment":"Accepted in ACML2023"},{"id":"http://arxiv.org/abs/2310.06481v1","updated":"2023-10-10T09:49:06Z","published":"2023-10-10T09:49:06Z","title":"An improved CTGAN for data processing method of imbalanced disk failure","summary":"  To address the problem of insufficient failure data generated by disks and\nthe imbalance between the number of normal and failure data. The existing\nConditional Tabular Generative Adversarial Networks (CTGAN) deep learning\nmethods have been proven to be effective in solving imbalance disk failure\ndata. But CTGAN cannot learn the internal information of disk failure data very\nwell. In this paper, a fault diagnosis method based on improved CTGAN, a\nclassifier for specific category discrimination is added and a discriminator\ngenerate adversarial network based on residual network is proposed. We named it\nResidual Conditional Tabular Generative Adversarial Networks (RCTGAN). Firstly,\nto enhance the stability of system a residual network is utilized. RCTGAN uses\na small amount of real failure data to synthesize fake fault data; Then, the\nsynthesized data is mixed with the real data to balance the amount of normal\nand failure data; Finally, four classifier (multilayer perceptron, support\nvector machine, decision tree, random forest) models are trained using the\nbalanced data set, and the performance of the models is evaluated using G-mean.\nThe experimental results show that the data synthesized by the RCTGAN can\nfurther improve the fault diagnosis accuracy of the classifier.\n","authors":["Jingbo Jia","Peng Wu","Hussain Dawood"],"pdf_url":"https://arxiv.org/pdf/2310.06481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05506v2","updated":"2023-10-10T09:35:54Z","published":"2023-05-09T14:54:59Z","title":"FedGT: Identification of Malicious Clients in Federated Learning with\n  Secure Aggregation","summary":"  We propose FedGT, a novel framework for identifying malicious clients in\nfederated learning with secure aggregation. Inspired by group testing, the\nframework leverages overlapping groups of clients to identify the presence of\nmalicious clients in the groups via a decoding operation. The clients\nidentified as malicious are then removed from the training of the model, which\nis performed over the remaining clients. By choosing the size, number, and\noverlap between groups, FedGT strikes a balance between privacy and security.\nSpecifically, the server learns the aggregated model of the clients in each\ngroup - vanilla federated learning and secure aggregation correspond to the\nextreme cases of FedGT with group size equal to one and the total number of\nclients, respectively. The effectiveness of FedGT is demonstrated through\nextensive experiments on the MNIST, CIFAR-10, and ISIC2019 datasets in a\ncross-silo setting under different data-poisoning attacks. These experiments\nshowcase FedGT's ability to identify malicious clients, resulting in high model\nutility. We further show that FedGT significantly outperforms the private\nrobust aggregation approach based on the geometric median recently proposed by\nPillutla et al. on heterogeneous client data (ISIC2019) and in the presence of\ntargeted attacks (CIFAR-10 and ISIC2019).\n","authors":["Marvin Xhemrishi","Johan stman","Antonia Wachter-Zeh","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2305.05506v2.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2310.06452v1","updated":"2023-10-10T09:25:44Z","published":"2023-10-10T09:25:44Z","title":"Understanding the Effects of RLHF on LLM Generalisation and Diversity","summary":"  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the trade-off between generalisation\nand diversity.\n","authors":["Robert Kirk","Ishita Mediratta","Christoforos Nalmpantis","Jelena Luketina","Eric Hambro","Edward Grefenstette","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2310.06452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06448v1","updated":"2023-10-10T09:17:17Z","published":"2023-10-10T09:17:17Z","title":"Asynchronous Federated Learning with Incentive Mechanism Based on\n  Contract Theory","summary":"  To address the challenges posed by the heterogeneity inherent in federated\nlearning (FL) and to attract high-quality clients, various incentive mechanisms\nhave been employed. However, existing incentive mechanisms are typically\nutilized in conventional synchronous aggregation, resulting in significant\nstraggler issues. In this study, we propose a novel asynchronous FL framework\nthat integrates an incentive mechanism based on contract theory. Within the\nincentive mechanism, we strive to maximize the utility of the task publisher by\nadaptively adjusting clients' local model training epochs, taking into account\nfactors such as time delay and test accuracy. In the asynchronous scheme,\nconsidering client quality, we devise aggregation weights and an access control\nalgorithm to facilitate asynchronous aggregation. Through experiments conducted\non the MNIST dataset, the simulation results demonstrate that the test accuracy\nachieved by our framework is 3.12% and 5.84% higher than that achieved by\nFedAvg and FedProx without any attacks, respectively. The framework exhibits a\n1.35% accuracy improvement over the ideal Local SGD under attacks. Furthermore,\naiming for the same target accuracy, our framework demands notably less\ncomputation time than both FedAvg and FedProx.\n","authors":["Danni Yang","Yun Ji","Zhoubin Kou","Xiaoxiong Zhong","Sheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13150v3","updated":"2023-10-10T09:17:14Z","published":"2023-08-25T03:08:41Z","title":"Enhancing Breast Cancer Classification Using Transfer ResNet with\n  Lightweight Attention Mechanism","summary":"  Despite the remarkable results of deep learning in breast cancer image\nclassification, challenges such as data imbalance and interpretability still\nexist and require cross-domain knowledge and collaboration among medical\nexperts. In this study, we propose a dual-activated lightweight attention\nResNet50 module method-based breast cancer classification method that\neffectively addresses challenges such as data imbalance and interpretability.\nOur model fuses a pre-trained deep ResNet50 and a lightweight attention\nmechanism to accomplish classification by embedding an attention module in\nlayer 4 of ResNet50 and adding two fully connected layers. For the fully\nconnected network design, we employ both Leaky ReLU and ReLU activation\nfunctions. On medical histopathology datasets, our model outperforms\nconventional models, visual transformers, and large models in terms of\nprecision, accuracy, recall, F1 score, and GMean. In particular, the model\ndemonstrates significant robustness and broad applicability when dealing with\nthe unbalanced breast cancer dataset. Our model is tested on 40X, 100X, 200X,\nand 400X images and achieves accuracies of 98.5%, 98.7%, 97.9%, and 94.3%,\nrespectively. Through an in-depth analysis of loss and accuracy, as well as\nGrad-CAM analysis, we comprehensively assessed the model performance and gained\nperspective on its training process. In the later stages of training, the\nvalidated losses and accuracies change minimally, showing that the model avoids\noverfitting and exhibits good generalization ability. Overall, this study\nprovides an effective solution for breast cancer image classification with\npractical applica\n","authors":["Suxing Liu"],"pdf_url":"https://arxiv.org/pdf/2308.13150v3.pdf","comment":"13 pages, 8 figures,6 tables"},{"id":"http://arxiv.org/abs/2310.06446v1","updated":"2023-10-10T09:17:12Z","published":"2023-10-10T09:17:12Z","title":"Rule Mining for Correcting Classification Models","summary":"  Machine learning models need to be continually updated or corrected to ensure\nthat the prediction accuracy remains consistently high. In this study, we\nconsider scenarios where developers should be careful to change the prediction\nresults by the model correction, such as when the model is part of a complex\nsystem or software. In such scenarios, the developers want to control the\nspecification of the corrections. To achieve this, the developers need to\nunderstand which subpopulations of the inputs get inaccurate predictions by the\nmodel. Therefore, we propose correction rule mining to acquire a comprehensive\nlist of rules that describe inaccurate subpopulations and how to correct them.\nWe also develop an efficient correction rule mining algorithm that is a\ncombination of frequent itemset mining and a unique pruning technique for\ncorrection rules. We observed that the proposed algorithm found various rules\nwhich help to collect data insufficiently learned, directly correct model\noutputs, and analyze concept drift.\n","authors":["Hirofumi Suzuki","Hiroaki Iwashita","Takuya Takagi","Yuta Fujishige","Satoshi Hara"],"pdf_url":"https://arxiv.org/pdf/2310.06446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00369v2","updated":"2023-10-10T09:12:37Z","published":"2023-09-30T13:21:29Z","title":"Distilling Inductive Bias: Knowledge Distillation Beyond Model\n  Compression","summary":"  With the rapid development of computer vision, Vision Transformers (ViTs)\noffer the tantalizing prospect of unified information processing across visual\nand textual domains. But due to the lack of inherent inductive biases in ViTs,\nthey require enormous amount of data for training. To make their applications\npractical, we introduce an innovative ensemble-based distillation approach\ndistilling inductive bias from complementary lightweight teacher models. Prior\nsystems relied solely on convolution-based teaching. However, this method\nincorporates an ensemble of light teachers with different architectural\ntendencies, such as convolution and involution, to instruct the student\ntransformer jointly. Because of these unique inductive biases, instructors can\naccumulate a wide range of knowledge, even from readily identifiable stored\ndatasets, which leads to enhanced student performance. Our proposed framework\nalso involves precomputing and storing logits in advance, essentially the\nunnormalized predictions of the model. This optimization can accelerate the\ndistillation process by eliminating the need for repeated forward passes during\nknowledge distillation, significantly reducing the computational burden and\nenhancing efficiency.\n","authors":["Gousia Habib","Tausifa Jan Saleem","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2310.00369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15649v2","updated":"2023-10-10T09:10:58Z","published":"2023-09-27T13:36:03Z","title":"Generative Speech Recognition Error Correction with Large Language\n  Models and Task-Activating Prompting","summary":"  We explore the ability of large language models (LLMs) to act as speech\nrecognition post-processors that perform rescoring and error correction. Our\nfirst focus is on instruction prompting to let LLMs perform these task without\nfine-tuning, for which we evaluate different prompting schemes, both zero- and\nfew-shot in-context learning, and a novel task activation prompting method that\ncombines causal instructions and demonstration to increase its context windows.\nNext, we show that rescoring only by in-context learning with frozen LLMs\nachieves results that are competitive with rescoring by domain-tuned LMs, using\na pretrained first-pass recognition system and rescoring output on two\nout-of-domain tasks (ATIS and WSJ). By combining prompting techniques with\nfine-tuning we achieve error rates below the N-best oracle level, showcasing\nthe generalization power of the LLMs.\n","authors":["Chao-Han Huck Yang","Yile Gu","Yi-Chieh Liu","Shalini Ghosh","Ivan Bulyko","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2309.15649v2.pdf","comment":"Accepted to IEEE Automatic Speech Recognition and Understanding\n  (ASRU) 2023. 8 pages. 2nd version revised from Sep 29th's version"},{"id":"http://arxiv.org/abs/2305.03308v3","updated":"2023-10-10T09:10:08Z","published":"2023-05-05T06:17:57Z","title":"Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of\n  Motion Artifacts in Photoplethysmogram Signals on Edge Devices","summary":"  Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts\nin real-world settings, despite their widespread use in Internet-of-Things\n(IoT) based wearable and smart health devices for cardiovascular health\nmonitoring. This study proposed a lightweight deep neural network, called\nTiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge\ndevices. The model was trained and tested on a public dataset, PPG DaLiA, which\nfeatured complex artifacts with diverse lengths and morphologies during various\ndaily activities of 15 subjects using a watch-type device (Empatica E4). The\nmodel structure, training method and loss function were specifically designed\nto balance detection accuracy and speed for real-time PPG artifact detection in\nresource-constrained embedded devices. To optimize the model size and\ncapability in multi-scale feature representation, the model employed depth-wise\nseparable convolution and atrous spatial pyramid pooling modules, respectively.\nAdditionally, the contrastive loss was also utilized to further optimize the\nfeature embeddings. With additional model pruning, Tiny-PPG achieved\nstate-of-the-art detection accuracy of 87.4% while only having 19,726 model\nparameters (0.15 megabytes), and was successfully deployed on an STM32 embedded\nsystem for real-time PPG artifact detection. Therefore, this study provides an\neffective solution for resource-constraint IoT smart health devices in PPG\nartifact detection.\n","authors":["Yali Zheng","Chen Wu","Peizheng Cai","Zhiqiang Zhong","Hongda Huang","Yuqi Jiang"],"pdf_url":"https://arxiv.org/pdf/2305.03308v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12301v2","updated":"2023-10-10T09:08:21Z","published":"2023-09-21T17:58:26Z","title":"Environment-biased Feature Ranking for Novelty Detection Robustness","summary":"  We tackle the problem of robust novelty detection, where we aim to detect\nnovelties in terms of semantic content while being invariant to changes in\nother, irrelevant factors. Specifically, we operate in a setup with multiple\nenvironments, where we determine the set of features that are associated more\nwith the environments, rather than to the content relevant for the task. Thus,\nwe propose a method that starts with a pretrained embedding and a multi-env\nsetup and manages to rank the features based on their environment-focus. First,\nwe compute a per-feature score based on the feature distribution variance\nbetween envs. Next, we show that by dropping the highly scored ones, we manage\nto remove spurious correlations and improve the overall performance by up to\n6%, both in covariance and sub-population shift cases, both for a real and a\nsynthetic benchmark, that we introduce for this task.\n","authors":["Stefan Smeu","Elena Burceanu","Emanuela Haller","Andrei Liviu Nicolicioiu"],"pdf_url":"https://arxiv.org/pdf/2309.12301v2.pdf","comment":"The updated, long version of the paper is available at\n  arXiv:2310.03738"},{"id":"http://arxiv.org/abs/2310.06437v1","updated":"2023-10-10T09:06:39Z","published":"2023-10-10T09:06:39Z","title":"Skeleton Ground Truth Extraction: Methodology, Annotation Tool and\n  Benchmarks","summary":"  Skeleton Ground Truth (GT) is critical to the success of supervised skeleton\nextraction methods, especially with the popularity of deep learning techniques.\nFurthermore, we see skeleton GTs used not only for training skeleton detectors\nwith Convolutional Neural Networks (CNN) but also for evaluating\nskeleton-related pruning and matching algorithms. However, most existing shape\nand image datasets suffer from the lack of skeleton GT and inconsistency of GT\nstandards. As a result, it is difficult to evaluate and reproduce CNN-based\nskeleton detectors and algorithms on a fair basis. In this paper, we present a\nheuristic strategy for object skeleton GT extraction in binary shapes and\nnatural images. Our strategy is built on an extended theory of diagnosticity\nhypothesis, which enables encoding human-in-the-loop GT extraction based on\nclues from the target's context, simplicity, and completeness. Using this\nstrategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing\nshape and image datasets. The GTs are then structurally evaluated with\nrepresentative methods to build viable baselines for fair comparisons.\nExperiments demonstrate that GTs generated by our strategy yield promising\nquality with respect to standard consistency, and also provide a balance\nbetween simplicity and completeness.\n","authors":["Cong Yang","Bipin Indurkhya","John See","Bo Gao","Yan Ke","Zeyd Boukhers","Zhenyu Yang","Marcin Grzegorzek"],"pdf_url":"https://arxiv.org/pdf/2310.06437v1.pdf","comment":"Accepted for publication in the International Journal of Computer\n  Vision (IJCV)"},{"id":"http://arxiv.org/abs/2310.02717v2","updated":"2023-10-10T08:59:25Z","published":"2023-10-04T10:40:50Z","title":"Online Clustering of Bandits with Misspecified User Models","summary":"  The contextual linear bandit is an important online learning problem where\ngiven arm features, a learning agent selects an arm at each round to maximize\nthe cumulative rewards in the long run. A line of works, called the clustering\nof bandits (CB), utilize the collaborative effect over user preferences and\nhave shown significant improvements over classic linear bandit algorithms.\nHowever, existing CB algorithms require well-specified linear user models and\ncan fail when this critical assumption does not hold. Whether robust CB\nalgorithms can be designed for more practical scenarios with misspecified user\nmodels remains an open problem. In this paper, we are the first to present the\nimportant problem of clustering of bandits with misspecified user models\n(CBMUM), where the expected rewards in user models can be perturbed away from\nperfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB\n(representing the learned clustering structure with dynamic graph and sets,\nrespectively), that can accommodate the inaccurate user preference estimations\nand erroneous clustering caused by model misspecifications. We prove regret\nupper bounds of $O(\\epsilon_*T\\sqrt{md\\log T} + d\\sqrt{mT}\\log T)$ for our\nalgorithms under milder assumptions than previous CB works (notably, we move\npast a restrictive technical assumption on the distribution of the arms), which\nmatch the lower bound asymptotically in $T$ up to logarithmic factors, and also\nmatch the state-of-the-art results in several degenerate cases. The techniques\nin proving the regret caused by misclustering users are quite general and may\nbe of independent interest. Experiments on both synthetic and real-world data\nshow our outperformance over previous algorithms.\n","authors":["Zhiyong Wang","Jize Xie","Xutong Liu","Shuai Li","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2310.02717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00976v2","updated":"2023-10-10T08:56:16Z","published":"2023-09-02T16:20:41Z","title":"Pure Message Passing Can Estimate Common Neighbor for Link Prediction","summary":"  Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto}\nstandard in graph representation learning. However, when it comes to link\nprediction, they often struggle, surpassed by simple heuristics such as Common\nNeighbor (CN). This discrepancy stems from a fundamental limitation: while\nMPNNs excel in node-level representation, they stumble with encoding the joint\nstructural features essential to link prediction, like CN. To bridge this gap,\nwe posit that, by harnessing the orthogonality of input vectors, pure\nmessage-passing can indeed capture joint structural features. Specifically, we\nstudy the proficiency of MPNNs in approximating CN heuristics. Based on our\nfindings, we introduce the Message Passing Link Predictor (MPLP), a novel link\nprediction model. MPLP taps into quasi-orthogonal vectors to estimate\nlink-level structural features, all while preserving the node-level\ncomplexities. Moreover, our approach demonstrates that leveraging\nmessage-passing to capture structural features could offset MPNNs'\nexpressiveness limitations at the expense of estimation variance. We conduct\nexperiments on benchmark datasets from various domains, where our method\nconsistently outperforms the baseline methods.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2309.00976v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2310.06430v1","updated":"2023-10-10T08:54:14Z","published":"2023-10-10T08:54:14Z","title":"Conformal Prediction for Deep Classifier via Label Ranking","summary":"  Conformal prediction is a statistical framework that generates prediction\nsets containing ground-truth labels with a desired coverage guarantee. The\npredicted probabilities produced by machine learning models are generally\nmiscalibrated, leading to large prediction sets in conformal prediction. In\nthis paper, we empirically and theoretically show that disregarding the\nprobabilities' value will mitigate the undesirable effect of miscalibrated\nprobability values. Then, we propose a novel algorithm named $\\textit{Sorted\nAdaptive prediction sets}$ (SAPS), which discards all the probability values\nexcept for the maximum softmax probability. The key idea behind SAPS is to\nminimize the dependence of the non-conformity score on the probability values\nwhile retaining the uncertainty information. In this manner, SAPS can produce\nsets of small size and communicate instance-wise uncertainty. Theoretically, we\nprovide a finite-sample coverage guarantee of SAPS and show that the expected\nvalue of set size from SAPS is always smaller than APS. Extensive experiments\nvalidate that SAPS not only lessens the prediction sets but also broadly\nenhances the conditional coverage rate and adaptation of prediction sets.\n","authors":["Jianguo Huang","Huajun Xi","Linjun Zhang","Huaxiu Yao","Yue Qiu","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2310.06430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06427v1","updated":"2023-10-10T08:52:16Z","published":"2023-10-10T08:52:16Z","title":"TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems","summary":"  Learning complex multi-agent system dynamics from data is crucial across many\ndomains, such as in physical simulations and material modeling. Extended from\npurely data-driven approaches, existing physics-informed approaches such as\nHamiltonian Neural Network strictly follow energy conservation law to introduce\ninductive bias, making their learning more sample efficiently. However, many\nreal-world systems do not strictly conserve energy, such as spring systems with\nfrictions. Recognizing this, we turn our attention to a broader physical\nprinciple: Time-Reversal Symmetry, which depicts that the dynamics of a system\nshall remain invariant when traversed back over time. It still helps to\npreserve energies for conservative systems and in the meanwhile, serves as a\nstrong inductive bias for non-conservative, reversible systems. To inject such\ninductive bias, in this paper, we propose a simple-yet-effective\nself-supervised regularization term as a soft constraint that aligns the\nforward and backward trajectories predicted by a continuous graph neural\nnetwork-based ordinary differential equation (GraphODE). It effectively imposes\ntime-reversal symmetry to enable more accurate model predictions across a wider\nrange of dynamical systems under classical mechanics. In addition, we further\nprovide theoretical analysis to show that our regularization essentially\nminimizes higher-order Taylor expansion terms during the ODE integration steps,\nwhich enables our model to be more noise-tolerant and even applicable to\nirreversible systems. Experimental results on a variety of physical systems\ndemonstrate the effectiveness of our proposed method. Particularly, it achieves\nan MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.\n","authors":["Zijie Huang","Wanjia Zhao","Jingdong Gao","Ziniu Hu","Xiao Luo","Yadi Cao","Yuanzhou Chen","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10916v3","updated":"2023-10-10T08:45:00Z","published":"2023-09-19T20:28:24Z","title":"What Learned Representations and Influence Functions Can Tell Us About\n  Adversarial Examples","summary":"  Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n  In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.\n","authors":["Shakila Mahjabin Tonni","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2309.10916v3.pdf","comment":"20 pages, Accepted in IJCNLP_AACL 2023"},{"id":"http://arxiv.org/abs/2310.06417v1","updated":"2023-10-10T08:40:47Z","published":"2023-10-10T08:40:47Z","title":"Advective Diffusion Transformers for Topological Generalization in Graph\n  Learning","summary":"  Graph diffusion equations are intimately related to graph neural networks\n(GNNs) and have recently attracted attention as a principled framework for\nanalyzing GNN dynamics, formalizing their expressive power, and justifying\narchitectural choices. One key open questions in graph learning is the\ngeneralization capabilities of GNNs. A major limitation of current approaches\nhinges on the assumption that the graph topologies in the training and test\nsets come from the same distribution. In this paper, we make steps towards\nunderstanding the generalization of GNNs by exploring how graph diffusion\nequations extrapolate and generalize in the presence of varying graph\ntopologies. We first show deficiencies in the generalization capability of\nexisting models built upon local diffusion on graphs, stemming from the\nexponential sensitivity to topology variation. Our subsequent analysis reveals\nthe promise of non-local diffusion, which advocates for feature propagation\nover fully-connected latent graphs, under the assumption of a specific\ndata-generating condition. In addition to these findings, we propose a novel\ngraph encoder backbone, Advective Diffusion Transformer (ADiT), inspired by\nadvective graph diffusion equations that have a closed-form solution backed up\nwith theoretical guarantees of desired generalization under topological\ndistribution shifts. The new model, functioning as a versatile graph\nTransformer, demonstrates superior performance across a wide range of graph\nlearning tasks.\n","authors":["Qitian Wu","Chenxiao Yang","Kaipeng Zeng","Fan Nie","Michael Bronstein","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06417v1.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2310.01366v2","updated":"2023-10-10T08:39:31Z","published":"2023-10-02T17:30:14Z","title":"Window-based Model Averaging Improves Generalization in Heterogeneous\n  Federated Learning","summary":"  Federated Learning (FL) aims to learn a global model from distributed users\nwhile protecting their privacy. However, when data are distributed\nheterogeneously the learning process becomes noisy, unstable, and biased\ntowards the last seen clients' data, slowing down convergence. To address these\nissues and improve the robustness and generalization capabilities of the global\nmodel, we propose WIMA (Window-based Model Averaging). WIMA aggregates global\nmodels from different rounds using a window-based approach, effectively\ncapturing knowledge from multiple users and reducing the bias from the last\nones. By adopting a windowed view on the rounds, WIMA can be applied from the\ninitial stages of training. Importantly, our method introduces no additional\ncommunication or client-side computation overhead. Our experiments demonstrate\nthe robustness of WIMA against distribution shifts and bad client sampling,\nresulting in smoother and more stable learning trends. Additionally, WIMA can\nbe easily integrated with state-of-the-art algorithms. We extensively evaluate\nour approach on standard FL benchmarks, demonstrating its effectiveness.\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2310.01366v2.pdf","comment":"International Conference on Computer Vision Workshop (ICCVW)"},{"id":"http://arxiv.org/abs/2310.06415v1","updated":"2023-10-10T08:36:21Z","published":"2023-10-10T08:36:21Z","title":"Deep reinforcement learning uncovers processes for separating azeotropic\n  mixtures without prior knowledge","summary":"  Process synthesis in chemical engineering is a complex planning problem due\nto vast search spaces, continuous parameters and the need for generalization.\nDeep reinforcement learning agents, trained without prior knowledge, have shown\nto outperform humans in various complex planning problems in recent years.\nExisting work on reinforcement learning for flowsheet synthesis shows promising\nconcepts, but focuses on narrow problems in a single chemical system, limiting\nits practicality. We present a general deep reinforcement learning approach for\nflowsheet synthesis. We demonstrate the adaptability of a single agent to the\ngeneral task of separating binary azeotropic mixtures. Without prior knowledge,\nit learns to craft near-optimal flowsheets for multiple chemical systems,\nconsidering different feed compositions and conceptual approaches. On average,\nthe agent can separate more than 99% of the involved materials into pure\ncomponents, while autonomously learning fundamental process engineering\nparadigms. This highlights the agent's planning flexibility, an encouraging\nstep toward true generality.\n","authors":["Quirin Gttl","Jonathan Pirnay","Jakob Burger","Dominik G. Grimm"],"pdf_url":"https://arxiv.org/pdf/2310.06415v1.pdf","comment":"36 pages, 7 figures, 4 tables. G\\\"ottl and Pirnay contributed equally\n  as joint first authors. Burger and Grimm contributed equally as joint last\n  authors"},{"id":"http://arxiv.org/abs/2304.04441v2","updated":"2023-10-10T08:33:24Z","published":"2023-04-10T07:57:24Z","title":"Self-training with dual uncertainty for semi-supervised medical image\n  segmentation","summary":"  In the field of semi-supervised medical image segmentation, the shortage of\nlabeled data is the fundamental problem. How to effectively learn image\nfeatures from unlabeled images to improve segmentation accuracy is the main\nresearch direction in this field. Traditional self-training methods can\npartially solve the problem of insufficient labeled data by generating pseudo\nlabels for iterative training. However, noise generated due to the model's\nuncertainty during training directly affects the segmentation results.\nTherefore, we added sample-level and pixel-level uncertainty to stabilize the\ntraining process based on the self-training framework. Specifically, we saved\nseveral moments of the model during pre-training, and used the difference\nbetween their predictions on unlabeled samples as the sample-level uncertainty\nestimate for that sample. Then, we gradually add unlabeled samples from easy to\nhard during training. At the same time, we added a decoder with different\nupsampling methods to the segmentation network and used the difference between\nthe outputs of the two decoders as pixel-level uncertainty. In short, we\nselectively retrained unlabeled samples and assigned pixel-level uncertainty to\npseudo labels to optimize the self-training process. We compared the\nsegmentation results of our model with five semi-supervised approaches on the\npublic 2017 ACDC dataset and 2018 Prostate dataset. Our proposed method\nachieves better segmentation performance on both datasets under the same\nsettings, demonstrating its effectiveness, robustness, and potential\ntransferability to other medical image segmentation tasks. Keywords: Medical\nimage segmentation, semi-supervised learning, self-training, uncertainty\nestimation\n","authors":["Zhanhong Qiu","Haitao Gan","Ming Shi","Zhongwei Huang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.04441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11375v4","updated":"2023-10-10T08:32:56Z","published":"2023-06-20T08:31:24Z","title":"Top-down machine learning of coarse-grained protein force-fields","summary":"  Developing accurate and efficient coarse-grained representations of proteins\nis crucial for understanding their folding, function, and interactions over\nextended timescales. Our methodology involves simulating proteins with\nmolecular dynamics and utilizing the resulting trajectories to train a neural\nnetwork potential through differentiable trajectory reweighting. Remarkably,\nthis method requires only the native conformation of proteins, eliminating the\nneed for labeled data derived from extensive simulations or memory-intensive\nend-to-end differentiable simulations. Once trained, the model can be employed\nto run parallel molecular dynamics simulations and sample folding events for\nproteins both within and beyond the training distribution, showcasing its\nextrapolation capabilities. By applying Markov State Models, native-like\nconformations of the simulated proteins can be predicted from the\ncoarse-grained simulations. Owing to its theoretical transferability and\nability to use solely experimental static structures as training data, we\nanticipate that this approach will prove advantageous for developing new\nprotein force fields and further advancing the study of protein dynamics,\nfolding, and interactions.\n","authors":["Carles Navarro","Maciej Majewski","Gianni de Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2306.11375v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09897v3","updated":"2023-10-10T08:15:32Z","published":"2022-08-21T14:53:15Z","title":"Multiple Descent in the Multiple Random Feature Model","summary":"  Recent works have demonstrated a double descent phenomenon in\nover-parameterized learning. Although this phenomenon has been investigated by\nrecent works, it has not been fully understood in theory. In this paper, we\ninvestigate the multiple descent phenomenon in a class of multi-component\nprediction models. We first consider a ''double random feature model'' (DRFM)\nconcatenating two types of random features, and study the excess risk achieved\nby the DRFM in ridge regression. We calculate the precise limit of the excess\nrisk under the high dimensional framework where the training sample size, the\ndimension of data, and the dimension of random features tend to infinity\nproportionally. Based on the calculation, we further theoretically demonstrate\nthat the risk curves of DRFMs can exhibit triple descent. We then provide a\nthorough experimental study to verify our theory. At last, we extend our study\nto the ''multiple random feature model'' (MRFM), and show that MRFMs ensembling\n$K$ types of random features may exhibit $(K+1)$-fold descent. Our analysis\npoints out that risk curves with a specific number of descent generally exist\nin learning multi-component prediction models.\n","authors":["Xuran Meng","Jianfeng Yao","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2208.09897v3.pdf","comment":"89 pages, 9 figures. Version 3 adds new description of triple descent\n  in certain double random feature model, deletes the discussion of NTK\n  regimes, and adds more literature references"},{"id":"http://arxiv.org/abs/2310.06404v1","updated":"2023-10-10T08:15:24Z","published":"2023-10-10T08:15:24Z","title":"Hexa: Self-Improving for Knowledge-Grounded Dialogue System","summary":"  A common practice in knowledge-grounded dialogue generation is to explicitly\nutilize intermediate steps (e.g., web-search, memory retrieval) with modular\napproaches. However, data for such steps are often inaccessible compared to\nthose of dialogue responses as they are unobservable in an ordinary dialogue.\nTo fill in the absence of these data, we develop a self-improving method to\nimprove the generative performances of intermediate steps without the ground\ntruth data. In particular, we propose a novel bootstrapping scheme with a\nguided prompt and a modified loss function to enhance the diversity of\nappropriate self-generated responses. Through experiments on various benchmark\ndatasets, we empirically demonstrate that our method successfully leverages a\nself-improving mechanism in generating intermediate and final responses and\nimproves the performances on the task of knowledge-grounded dialogue\ngeneration.\n","authors":["Daejin Jo","Daniel Wontae Nam","Gunsoo Han","Kyoung-Woon On","Taehwan Kwon","Seungeun Rho","Sungwoong Kim"],"pdf_url":"https://arxiv.org/pdf/2310.06404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06399v1","updated":"2023-10-10T08:06:32Z","published":"2023-10-10T08:06:32Z","title":"Lo-Hi: Practical ML Drug Discovery Benchmark","summary":"  Finding new drugs is getting harder and harder. One of the hopes of drug\ndiscovery is to use machine learning models to predict molecular properties.\nThat is why models for molecular property prediction are being developed and\ntested on benchmarks such as MoleculeNet. However, existing benchmarks are\nunrealistic and are too different from applying the models in practice. We have\ncreated a new practical \\emph{Lo-Hi} benchmark consisting of two tasks: Lead\nOptimization (Lo) and Hit Identification (Hi), corresponding to the real drug\ndiscovery process. For the Hi task, we designed a novel molecular splitting\nalgorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested\nstate-of-the-art and classic ML models, revealing which works better under\npractical settings. We analyzed modern benchmarks and showed that they are\nunrealistic and overoptimistic.\n  Review: https://openreview.net/forum?id=H2Yb28qGLV\n  Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023\n  Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter\n","authors":["Simon Steshin"],"pdf_url":"https://arxiv.org/pdf/2310.06399v1.pdf","comment":"29 pages, Advances in Neural Information Processing Systems, 2023"},{"id":"http://arxiv.org/abs/2310.04859v2","updated":"2023-10-10T08:03:47Z","published":"2023-10-07T15:47:31Z","title":"Universal Graph Random Features","summary":"  We propose a novel random walk-based algorithm for unbiased estimation of\narbitrary functions of a weighted adjacency matrix, coined universal graph\nrandom features (u-GRFs). This includes many of the most popular examples of\nkernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time\ncomplexity with respect to the number of nodes, overcoming the notoriously\nprohibitive cubic scaling of exact graph kernel evaluation. It can also be\ntrivially distributed across machines, permitting learning on much larger\nnetworks. At the heart of the algorithm is a modulation function which\nupweights or downweights the contribution from different random walks depending\non their lengths. We show that by parameterising it with a neural network we\ncan obtain u-GRFs that give higher-quality kernel estimates or perform\nefficient, scalable kernel learning. We provide robust theoretical analysis and\nsupport our findings with experiments including pointwise estimation of fixed\ngraph kernels, solving non-homogeneous graph ordinary differential equations,\nnode clustering and kernel regression on triangular meshes.\n","authors":["Isaac Reid","Krzysztof Choromanski","Eli Berger","Adrian Weller"],"pdf_url":"https://arxiv.org/pdf/2310.04859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01405v3","updated":"2023-10-10T08:00:53Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v3.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2310.06396v1","updated":"2023-10-10T07:59:23Z","published":"2023-10-10T07:59:23Z","title":"Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach","summary":"  Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nincluding those that affect both node features and graph topology. This paper\ninvestigates GNNs derived from diverse neural flows, concentrating on their\nconnection to various stability notions such as BIBO stability, Lyapunov\nstability, structural stability, and conservative stability. We argue that\nLyapunov stability, despite its common use, does not necessarily ensure\nadversarial robustness. Inspired by physics principles, we advocate for the use\nof conservative Hamiltonian neural flows to construct GNNs that are robust to\nadversarial attacks. The adversarial robustness of different neural flow GNNs\nis empirically compared on several benchmark datasets under a variety of\nadversarial attacks. Extensive numerical experiments demonstrate that GNNs\nleveraging conservative Hamiltonian flows with Lyapunov stability substantially\nimprove robustness against adversarial perturbations. The implementation code\nof experiments is available at\nhttps://github.com/zknus/NeurIPS-2023-HANG-Robustness.\n","authors":["Kai Zhao","Qiyu Kang","Yang Song","Rui She","Sijie Wang","Wee Peng Tay"],"pdf_url":"https://arxiv.org/pdf/2310.06396v1.pdf","comment":"Accepted by Advances in Neural Information Processing Systems\n  (NeurIPS), New Orleans, USA, Dec. 2023, spotlight"},{"id":"http://arxiv.org/abs/2310.06393v1","updated":"2023-10-10T07:57:00Z","published":"2023-10-10T07:57:00Z","title":"Harnessing Administrative Data Inventories to Create a Reliable\n  Transnational Reference Database for Crop Type Monitoring","summary":"  With leaps in machine learning techniques and their applicationon Earth\nobservation challenges has unlocked unprecedented performance across the\ndomain. While the further development of these methods was previously limited\nby the availability and volume of sensor data and computing resources, the lack\nof adequate reference data is now constituting new bottlenecks. Since creating\nsuch ground-truth information is an expensive and error-prone task, new ways\nmust be devised to source reliable, high-quality reference data on large\nscales. As an example, we showcase E URO C ROPS, a reference dataset for crop\ntype classification that aggregates and harmonizes administrative data surveyed\nin different countries with the goal of transnational interoperability.\n","authors":["Maja Schneider","Marco Krner"],"pdf_url":"https://arxiv.org/pdf/2310.06393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10400v2","updated":"2023-10-10T07:51:31Z","published":"2023-09-19T08:03:38Z","title":"PoSE: Efficient Context Window Extension of LLMs via Positional\n  Skip-wise Training","summary":"  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n","authors":["Dawei Zhu","Nan Yang","Liang Wang","Yifan Song","Wenhao Wu","Furu Wei","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2309.10400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06387v1","updated":"2023-10-10T07:50:29Z","published":"2023-10-10T07:50:29Z","title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context\n  Demonstrations","summary":"  Large Language Models (LLMs) have shown remarkable success in various tasks,\nbut concerns about their safety and the potential for generating malicious\ncontent have emerged. In this paper, we explore the power of In-Context\nLearning (ICL) in manipulating the alignment ability of LLMs. We find that by\nproviding just few in-context demonstrations without fine-tuning, LLMs can be\nmanipulated to increase or decrease the probability of jailbreaking, i.e.\nanswering malicious prompts. Based on these observations, we propose In-Context\nAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding\naligned language model purposes. ICA crafts malicious contexts to guide models\nin generating harmful outputs, while ICD enhances model robustness by\ndemonstrations of rejecting to answer harmful prompts. Our experiments show the\neffectiveness of ICA and ICD in increasing or reducing the success rate of\nadversarial jailbreaking attacks. Overall, we shed light on the potential of\nICL to influence LLM behavior and provide a new perspective for enhancing the\nsafety and alignment of LLMs.\n","authors":["Zeming Wei","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06380v1","updated":"2023-10-10T07:46:54Z","published":"2023-10-10T07:46:54Z","title":"CAST: Cluster-Aware Self-Training for Tabular Data","summary":"  Self-training has gained attraction because of its simplicity and\nversatility, yet it is vulnerable to noisy pseudo-labels. Several studies have\nproposed successful approaches to tackle this issue, but they have diminished\nthe advantages of self-training because they require specific modifications in\nself-training algorithms or model architectures. Furthermore, most of them are\nincompatible with gradient boosting decision trees, which dominate the tabular\ndomain. To address this, we revisit the cluster assumption, which states that\ndata samples that are close to each other tend to belong to the same class.\nInspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for\ntabular data. CAST is a simple and universally adaptable approach for enhancing\nexisting self-training algorithms without significant modifications.\nConcretely, our method regularizes the confidence of the classifier, which\nrepresents the value of the pseudo-label, forcing the pseudo-labels in\nlow-density regions to have lower confidence by leveraging prior knowledge for\neach class within the training data. Extensive empirical evaluations on up to\n20 real-world datasets confirm not only the superior performance of CAST but\nalso its robustness in various setups in self-training contexts.\n","authors":["Minwook Kim","Juseong Kim","Kibeom Kim","Donggil Kang","Giltae Song"],"pdf_url":"https://arxiv.org/pdf/2310.06380v1.pdf","comment":"17 pages with appendix"},{"id":"http://arxiv.org/abs/2310.06379v1","updated":"2023-10-10T07:43:41Z","published":"2023-10-10T07:43:41Z","title":"Initialization Bias of Fourier Neural Operator: Revisiting the Edge of\n  Chaos","summary":"  This paper investigates the initialization bias of the Fourier neural\noperator (FNO). A mean-field theory for FNO is established, analyzing the\nbehavior of the random FNO from an ``edge of chaos'' perspective. We uncover\nthat the forward and backward propagation behaviors exhibit characteristics\nunique to FNO, induced by mode truncation, while also showcasing similarities\nto those of densely connected networks. Building upon this observation, we also\npropose a FNO version of the He initialization scheme to mitigate the negative\ninitialization bias leading to training instability. Experimental results\ndemonstrate the effectiveness of our initialization scheme, enabling stable\ntraining of a 32-layer FNO without the need for additional techniques or\nsignificant performance degradation.\n","authors":["Takeshi Koshizuka","Masahiro Fujisawa","Yusuke Tanaka","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2310.06379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08861v2","updated":"2023-10-10T07:38:02Z","published":"2023-02-17T13:16:17Z","title":"AliasNet: Alias Artefact Suppression Network for Accelerated\n  Phase-Encode MRI","summary":"  Sparse reconstruction is an important aspect of MRI, helping to reduce\nacquisition time and improve spatial-temporal resolution. Popular methods are\nbased mostly on compressed sensing (CS), which relies on the random sampling of\nk-space to produce incoherent (noise-like) artefacts. Due to hardware\nconstraints, 1D Cartesian phase-encode under-sampling schemes are popular for\n2D CS-MRI. However, 1D under-sampling limits 2D incoherence between\nmeasurements, yielding structured aliasing artefacts (ghosts) that may be\ndifficult to remove assuming a 2D sparsity model. Reconstruction algorithms\ntypically deploy direction-insensitive 2D regularisation for these\ndirection-associated artefacts. Recognising that phase-encode artefacts can be\nseparated into contiguous 1D signals, we develop two decoupling techniques that\nenable explicit 1D regularisation and leverage the excellent 1D incoherence\ncharacteristics. We also derive a combined 1D + 2D reconstruction technique\nthat takes advantage of spatial relationships within the image. Experiments\nconducted on retrospectively under-sampled brain and knee data demonstrate that\ncombination of the proposed 1D AliasNet modules with existing 2D deep learned\n(DL) recovery techniques leads to an improvement in image quality. We also find\nAliasNet enables a superior scaling of performance compared to increasing the\nsize of the original 2D network layers. AliasNet therefore improves the\nregularisation of aliasing artefacts arising from phase-encode under-sampling,\nby tailoring the network architecture to account for their expected appearance.\nThe proposed 1D + 2D approach is compatible with any existing 2D DL recovery\ntechnique deployed for this application.\n","authors":["Marlon E. Bran Lorenzana","Shekhar S. Chandra","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2302.08861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00270v3","updated":"2023-10-10T07:30:39Z","published":"2023-09-30T06:20:21Z","title":"SpatialRank: Urban Event Ranking with NDCG Optimization on\n  Spatiotemporal Data","summary":"  The problem of urban event ranking aims at predicting the top-k most risky\nlocations of future events such as traffic accidents and crimes. This problem\nis of fundamental importance to public safety and urban administration\nespecially when limited resources are available. The problem is, however,\nchallenging due to complex and dynamic spatio-temporal correlations between\nlocations, uneven distribution of urban events in space, and the difficulty to\ncorrectly rank nearby locations with similar features. Prior works on event\nforecasting mostly aim at accurately predicting the actual risk score or counts\nof events for all the locations. Rankings obtained as such usually have low\nquality due to prediction errors. Learning-to-rank methods directly optimize\nmeasures such as Normalized Discounted Cumulative Gain (NDCG), but cannot\nhandle the spatiotemporal autocorrelation existing among locations. In this\npaper, we bridge the gap by proposing a novel spatial event ranking approach\nnamed SpatialRank. SpatialRank features adaptive graph convolution layers that\ndynamically learn the spatiotemporal dependencies across locations from data.\nIn addition, the model optimizes through surrogates a hybrid NDCG loss with a\nspatial component to better rank neighboring spatial locations. We design an\nimportance-sampling with a spatial filtering algorithm to effectively evaluate\nthe loss during training. Comprehensive experiments on three real-world\ndatasets demonstrate that SpatialRank can effectively identify the top riskiest\nlocations of crimes and traffic accidents and outperform state-of-art methods\nin terms of NDCG by up to 12.7%.\n","authors":["Bang An","Xun Zhou","Yongjian Zhong","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2310.00270v3.pdf","comment":"37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)"},{"id":"http://arxiv.org/abs/2310.06372v1","updated":"2023-10-10T07:25:06Z","published":"2023-10-10T07:25:06Z","title":"Leveraging Diffusion-Based Image Variations for Robust Training on\n  Poisoned Data","summary":"  Backdoor attacks pose a serious security threat for training neural networks\nas they surreptitiously introduce hidden functionalities into a model. Such\nbackdoors remain silent during inference on clean inputs, evading detection due\nto inconspicuous behavior. However, once a specific trigger pattern appears in\nthe input data, the backdoor activates, causing the model to execute its\nconcealed function. Detecting such poisoned samples within vast datasets is\nvirtually impossible through manual inspection. To address this challenge, we\npropose a novel approach that enables model training on potentially poisoned\ndatasets by utilizing the power of recent diffusion models. Specifically, we\ncreate synthetic variations of all training samples, leveraging the inherent\nresilience of diffusion models to potential trigger patterns in the data. By\ncombining this generative approach with knowledge distillation, we produce\nstudent models that maintain their general performance on the task while\nexhibiting robust resistance to backdoor triggers.\n","authors":["Lukas Struppek","Martin B. Hentschel","Clifton Poth","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06372v1.pdf","comment":"11 pages, 3 tables, 2 figures"},{"id":"http://arxiv.org/abs/2310.06371v1","updated":"2023-10-10T07:23:37Z","published":"2023-10-10T07:23:37Z","title":"Partition-based differentially private synthetic data generation","summary":"  Private synthetic data sharing is preferred as it keeps the distribution and\nnuances of original data compared to summary statistics. The state-of-the-art\nmethods adopt a select-measure-generate paradigm, but measuring large domain\nmarginals still results in much error and allocating privacy budget iteratively\nis still difficult. To address these issues, our method employs a\npartition-based approach that effectively reduces errors and improves the\nquality of synthetic data, even with a limited privacy budget. Results from our\nexperiments demonstrate the superiority of our method over existing approaches.\nThe synthetic data produced using our approach exhibits improved quality and\nutility, making it a preferable choice for private synthetic data sharing.\n","authors":["Meifan Zhang","Dihang Deng","Lihua Yin"],"pdf_url":"https://arxiv.org/pdf/2310.06371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13976v6","updated":"2023-10-10T07:18:46Z","published":"2022-11-25T09:38:22Z","title":"Expanding Small-Scale Datasets with Guided Imagination","summary":"  The power of DNNs relies heavily on the quantity and quality of training\ndata. However, collecting and annotating data on a large scale is often\nexpensive and time-consuming. To address this issue, we explore a new task,\ntermed dataset expansion, aimed at expanding a ready-to-use small dataset by\nautomatically creating new labeled samples. To this end, we present a Guided\nImagination Framework (GIF) that leverages cutting-edge generative models like\nDALL-E2 and Stable Diffusion (SD) to \"imagine\" and create informative new data\nfrom the input seed data. Specifically, GIF conducts data imagination by\noptimizing the latent features of the seed data in the semantically meaningful\nspace of the prior model, resulting in the creation of photo-realistic images\nwith new content. To guide the imagination towards creating informative samples\nfor model training, we introduce two key criteria, i.e., class-maintained\ninformation boosting and sample diversity promotion. These criteria are\nverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%\nhigher model accuracy on natural image datasets than unguided expansion with\nSD. With these essential criteria, GIF successfully expands small datasets in\nvarious scenarios, boosting model accuracy by 36.9% on average over six natural\nimage datasets and by 13.5% on average over three medical datasets. The source\ncode is available at https://github.com/Vanint/DatasetExpansion.\n","authors":["Yifan Zhang","Daquan Zhou","Bryan Hooi","Kai Wang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2211.13976v6.pdf","comment":"NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion"},{"id":"http://arxiv.org/abs/2310.06369v1","updated":"2023-10-10T07:11:25Z","published":"2023-10-10T07:11:25Z","title":"Geometrically Aligned Transfer Encoder for Inductive Transfer in\n  Regression Tasks","summary":"  Transfer learning is a crucial technique for handling a small amount of data\nthat is potentially related to other abundant data. However, most of the\nexisting methods are focused on classification tasks using images and language\ndatasets. Therefore, in order to expand the transfer learning scheme to\nregression tasks, we propose a novel transfer technique based on differential\ngeometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this\nmethod, we interpret the latent vectors from the model to exist on a Riemannian\ncurved manifold. We find a proper diffeomorphism between pairs of tasks to\nensure that every arbitrary point maps to a locally flat coordinate in the\noverlapping region, allowing the transfer of knowledge from the source to the\ntarget data. This also serves as an effective regularizer for the model to\nbehave in extrapolation regions. In this article, we demonstrate that GATE\noutperforms conventional methods and exhibits stable behavior in both the\nlatent space and extrapolation regions for various molecular graph datasets.\n","authors":["Sung Moon Ko","Sumin Lee","Dae-Woong Jeong","Woohyung Lim","Sehui Han"],"pdf_url":"https://arxiv.org/pdf/2310.06369v1.pdf","comment":"12+11 pages, 6+1 figures, 0+7 tables"},{"id":"http://arxiv.org/abs/2310.06367v1","updated":"2023-10-10T07:08:35Z","published":"2023-10-10T07:08:35Z","title":"DrugCLIP: Contrastive Protein-Molecule Representation Learning for\n  Virtual Screening","summary":"  Virtual screening, which identifies potential drugs from vast compound\ndatabases to bind with a particular protein pocket, is a critical step in\nAI-assisted drug discovery. Traditional docking methods are highly\ntime-consuming, and can only work with a restricted search library in real-life\napplications. Recent supervised learning approaches using scoring functions for\nbinding-affinity prediction, although promising, have not yet surpassed docking\nmethods due to their strong dependency on limited data with reliable\nbinding-affinity labels. In this paper, we propose a novel contrastive learning\nframework, DrugCLIP, by reformulating virtual screening as a dense retrieval\ntask and employing contrastive learning to align representations of binding\nprotein pockets and molecules from a large quantity of pairwise data without\nexplicit binding-affinity scores. We also introduce a biological-knowledge\ninspired data augmentation strategy to learn better protein-molecule\nrepresentations. Extensive experiments show that DrugCLIP significantly\noutperforms traditional docking and supervised learning methods on diverse\nvirtual screening benchmarks with highly reduced computation time, especially\nin zero-shot setting.\n","authors":["Bowen Gao","Bo Qiang","Haichuan Tan","Minsi Ren","Yinjun Jia","Minsi Lu","Jingjing Liu","Weiying Ma","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2310.06367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06358v1","updated":"2023-10-10T06:52:20Z","published":"2023-10-10T06:52:20Z","title":"Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and\n  Shortest Paths-based Centrality Metrics","summary":"  We perform factor analysis on the raw data of the four major neighborhood and\nshortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and\nCloseness) and propose a novel quantitative measure called the\nCore-Intermediate-Peripheral (CIP) Index to capture the extent with which a\nnode could play the role of a core node (nodes at the center of a network with\nlarger values for any centrality metric) vis-a-vis a peripheral node (nodes\nthat exist at the periphery of a network with lower values for any centrality\nmetric). We conduct factor analysis (varimax-based rotation of the\nEigenvectors) on the transpose matrix of the raw centrality metrics dataset,\nwith the node ids as features, under the hypothesis that there are two factors\n(core and peripheral) that drive the values incurred by the nodes with respect\nto the centrality metrics. We test our approach on a diverse suite of 12\ncomplex real-world networks.\n","authors":["Natarajan Meghanathan"],"pdf_url":"https://arxiv.org/pdf/2310.06358v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2308.11053v3","updated":"2023-10-10T06:46:21Z","published":"2023-08-21T21:36:56Z","title":"Ultra Dual-Path Compression For Joint Echo Cancellation And Noise\n  Suppression","summary":"  Echo cancellation and noise reduction are essential for full-duplex\ncommunication, yet most existing neural networks have high computational costs\nand are inflexible in tuning model complexity. In this paper, we introduce\ntime-frequency dual-path compression to achieve a wide range of compression\nratios on computational cost. Specifically, for frequency compression,\ntrainable filters are used to replace manually designed filters for dimension\nreduction. For time compression, only using frame skipped prediction causes\nlarge performance degradation, which can be alleviated by a post-processing\nnetwork with full sequence modeling. We have found that under fixed compression\nratios, dual-path compression combining both the time and frequency methods\nwill give further performance improvement, covering compression ratios from 4x\nto 32x with little model size change. Moreover, the proposed models show\ncompetitive performance compared with fast FullSubNet and DeepFilterNet.\n","authors":["Hangting Chen","Jianwei Yu","Yi Luo","Rongzhi Gu","Weihua Li","Zhuocheng Lu","Chao Weng"],"pdf_url":"https://arxiv.org/pdf/2308.11053v3.pdf","comment":"Proceedings of INTERSPEECH"},{"id":"http://arxiv.org/abs/2309.15223v2","updated":"2023-10-10T06:30:47Z","published":"2023-09-26T19:41:34Z","title":"Low-rank Adaptation of Large Language Model Rescoring for\n  Parameter-Efficient Speech Recognition","summary":"  We propose a neural language modeling system based on low-rank adaptation\n(LoRA) for speech recognition output rescoring. Although pretrained language\nmodels (LMs) like BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pretraining stage and\nadapting the pretrained models to specific domains limit their practical use in\nrescoring. Here we present a method based on low-rank decomposition to train a\nrescoring BERT model and adapt it to new domains using only a fraction (0.08%)\nof the pretrained parameters. These inserted matrices are optimized through a\ndiscriminative training objective along with a correlation-based regularization\nloss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is\nevaluated on LibriSpeech and internal datasets with decreased training times by\nfactors between 5.4 and 3.6.\n","authors":["Yu Yu","Chao-Han Huck Yang","Jari Kolehmainen","Prashanth G. Shivakumar","Yile Gu","Sungho Ryu","Roger Ren","Qi Luo","Aditya Gourav","I-Fan Chen","Yi-Chieh Liu","Tuan Dinh","Ankur Gandhe","Denis Filimonov","Shalini Ghosh","Andreas Stolcke","Ariya Rastow","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2309.15223v2.pdf","comment":"Accepted to IEEE ASRU 2023. Internal Review Approved. Revised 2nd\n  version with Andreas and Huck. The first version is in Sep 29th. 8 pages"},{"id":"http://arxiv.org/abs/2310.04673v2","updated":"2023-10-10T06:26:54Z","published":"2023-10-07T03:17:59Z","title":"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","summary":"  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks. However, there has\nbeen limited research on applying similar frameworks to audio tasks. Previously\nproposed large language models for audio tasks either lack sufficient\nquantitative evaluations, or are limited to tasks for recognizing and\nunderstanding audio content, or significantly underperform existing\nstate-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified\nGPT model for audio recognition, understanding, and generation. LauraGPT is a\nversatile language model that can process both audio and text inputs and\ngenerate outputs in either modalities. It can perform a wide range of tasks\nrelated to content, semantics, paralinguistics, and audio-signal analysis. Some\nof its noteworthy tasks include automatic speech recognition, speech-to-text\ntranslation, text-to-speech synthesis, machine translation, speech enhancement,\nautomated audio captioning, speech emotion recognition, and spoken language\nunderstanding. To achieve this goal, we use a combination of continuous and\ndiscrete features for audio. We encode input audio into continuous\nrepresentations using an audio encoder and decode output audio from discrete\ncodec codes. We then fine-tune a large decoder-only Transformer-based language\nmodel on multiple audio-to-text, text-to-audio, audio-to-audio, and\ntext-to-text tasks using a supervised multitask learning approach. Extensive\nexperiments show that LauraGPT achieves competitive or superior performance\ncompared to existing SOTA models on various audio processing benchmarks.\n","authors":["Jiaming Wang","Zhihao Du","Qian Chen","Yunfei Chu","Zhifu Gao","Zerui Li","Kai Hu","Xiaohuan Zhou","Jin Xu","Ziyang Ma","Wen Wang","Siqi Zheng","Chang Zhou","Zhijie Yan","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04673v2.pdf","comment":"10 pages, under review"},{"id":"http://arxiv.org/abs/2310.06343v1","updated":"2023-10-10T06:26:05Z","published":"2023-10-10T06:26:05Z","title":"Boosting Continuous Control with Consistency Policy","summary":"  Due to its training stability and strong expression, the diffusion model has\nattracted considerable attention in offline reinforcement learning. However,\nseveral challenges have also come with it: 1) The demand for a large number of\ndiffusion steps makes the diffusion-model-based methods time inefficient and\nlimits their applications in real-time control; 2) How to achieve policy\nimprovement with accurate guidance for diffusion model-based policy is still an\nopen problem. Inspired by the consistency model, we propose a novel\ntime-efficiency method named Consistency Policy with Q-Learning (CPQL), which\nderives action from noise by a single step. By establishing a mapping from the\nreverse diffusion trajectories to the desired policy, we simultaneously address\nthe issues of time efficiency and inaccurate guidance when updating diffusion\nmodel-based policy with the learned Q-function. We demonstrate that CPQL can\nachieve policy improvement with accurate guidance for offline reinforcement\nlearning, and can be seamlessly extended for online RL tasks. Experimental\nresults indicate that CPQL achieves new state-of-the-art performance on 11\noffline and 21 online tasks, significantly improving inference speed by nearly\n45 times compared to Diffusion-QL. We will release our code later.\n","authors":["Yuhui Chen","Haoran Li","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06343v1.pdf","comment":"18 pages, 9 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.06434v1","updated":"2023-10-10T09:04:33Z","published":"2023-10-10T09:04:33Z","title":"Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n  for Speech Recognition","summary":"  We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.\n","authors":["Srijith Radhakrishnan","Chao-Han Huck Yang","Sumeer Ahmad Khan","Rohit Kumar","Narsis A. Kiani","David Gomez-Cabrero","Jesper N. Tegner"],"pdf_url":"https://arxiv.org/pdf/2310.06434v1.pdf","comment":"Accepted to EMNLP 2023. 10 pages. This work has been done in October\n  2022 and was submitted to EMNLP 23 once the draft was finalized. GitHub:\n  https://github.com/Srijith-rkr/Whispering-LLaMA"},{"id":"http://arxiv.org/abs/2310.06412v1","updated":"2023-10-10T08:29:34Z","published":"2023-10-10T08:29:34Z","title":"Encoder-Decoder-Based Intra-Frame Block Partitioning Decision","summary":"  The recursive intra-frame block partitioning decision process, a crucial\ncomponent of the next-generation video coding standards, exerts significant\ninfluence over the encoding time. In this paper, we propose an encoder-decoder\nneural network (NN) to accelerate this process. Specifically, a CNN is utilized\nto compress the pixel data of the largest coding unit (LCU) into a fixed-length\nvector. Subsequently, a Transformer decoder is employed to transcribe the\nfixed-length vector into a variable-length vector, which represents the block\npartitioning outcomes of the encoding LCU. The vector transcription process\nadheres to the constraints imposed by the block partitioning algorithm. By\nfully parallelizing the NN prediction in the intra-mode decision, substantial\ntime savings can be attained during the decision phase. The experimental\nresults obtained from high-definition (HD) sequences coding demonstrate that\nthis framework achieves a remarkable 87.84\\% reduction in encoding time, with a\nrelatively small loss (8.09\\%) of coding performance compared to AVS3 HPM4.0.\n","authors":["Yucheng Jiang","Han Peng","Yan Song","Jie Yu","Peng Zhang","Songping Mai"],"pdf_url":"https://arxiv.org/pdf/2310.06412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04673v2","updated":"2023-10-10T06:26:54Z","published":"2023-10-07T03:17:59Z","title":"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","summary":"  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks. However, there has\nbeen limited research on applying similar frameworks to audio tasks. Previously\nproposed large language models for audio tasks either lack sufficient\nquantitative evaluations, or are limited to tasks for recognizing and\nunderstanding audio content, or significantly underperform existing\nstate-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified\nGPT model for audio recognition, understanding, and generation. LauraGPT is a\nversatile language model that can process both audio and text inputs and\ngenerate outputs in either modalities. It can perform a wide range of tasks\nrelated to content, semantics, paralinguistics, and audio-signal analysis. Some\nof its noteworthy tasks include automatic speech recognition, speech-to-text\ntranslation, text-to-speech synthesis, machine translation, speech enhancement,\nautomated audio captioning, speech emotion recognition, and spoken language\nunderstanding. To achieve this goal, we use a combination of continuous and\ndiscrete features for audio. We encode input audio into continuous\nrepresentations using an audio encoder and decode output audio from discrete\ncodec codes. We then fine-tune a large decoder-only Transformer-based language\nmodel on multiple audio-to-text, text-to-audio, audio-to-audio, and\ntext-to-text tasks using a supervised multitask learning approach. Extensive\nexperiments show that LauraGPT achieves competitive or superior performance\ncompared to existing SOTA models on various audio processing benchmarks.\n","authors":["Jiaming Wang","Zhihao Du","Qian Chen","Yunfei Chu","Zhifu Gao","Zerui Li","Kai Hu","Xiaohuan Zhou","Jin Xu","Ziyang Ma","Wen Wang","Siqi Zheng","Chang Zhou","Zhijie Yan","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04673v2.pdf","comment":"10 pages, under review"},{"id":"http://arxiv.org/abs/2310.06311v1","updated":"2023-10-10T05:09:05Z","published":"2023-10-10T05:09:05Z","title":"Improving Compositional Text-to-image Generation with Large\n  Vision-Language Models","summary":"  Recent advancements in text-to-image models, particularly diffusion models,\nhave shown significant promise. However, compositional text-to-image models\nfrequently encounter difficulties in generating high-quality images that\naccurately align with input texts describing multiple objects, variable\nattributes, and intricate spatial relationships. To address this limitation, we\nemploy large vision-language models (LVLMs) for multi-dimensional assessment of\nthe alignment between generated images and their corresponding input texts.\nUtilizing this assessment, we fine-tune the diffusion model to enhance its\nalignment capabilities. During the inference phase, an initial image is\nproduced using the fine-tuned diffusion model. The LVLM is then employed to\npinpoint areas of misalignment in the initial image, which are subsequently\ncorrected using the image editing algorithm until no further misalignments are\ndetected by the LVLM. The resultant image is consequently more closely aligned\nwith the input text. Our experimental results validate that the proposed\nmethodology significantly improves text-image alignment in compositional image\ngeneration, particularly with respect to object number, attribute binding,\nspatial relationships, and aesthetic quality.\n","authors":["Song Wen","Guian Fang","Renrui Zhang","Peng Gao","Hao Dong","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2310.06311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06309v1","updated":"2023-10-10T04:58:43Z","published":"2023-10-10T04:58:43Z","title":"Encoding and Decoding Narratives: Datafication and Alternative Access\n  Models for Audiovisual Archives","summary":"  Situated in the intersection of audiovisual archives, computational methods,\nand immersive interactions, this work probes the increasingly important\naccessibility issues from a two-fold approach. Firstly, the work proposes an\nontological data model to handle complex descriptors (metadata, feature\nvectors, etc.) with regard to user interactions. Secondly, this work examines\ntext-to-video retrieval from an implementation perspective by proposing a\nclassifier-enhanced workflow to deal with complex and hybrid queries and a\ntraining data augmentation workflow to improve performance. This work serves as\nthe foundation for experimenting with novel public-facing access models to\nlarge audiovisual archives\n","authors":["Yuchen Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06309v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.05825"},{"id":"http://arxiv.org/abs/2310.06238v1","updated":"2023-10-10T01:22:41Z","published":"2023-10-10T01:22:41Z","title":"Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for\n  Unbiased Question-Answering","summary":"  In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.\n","authors":["Xiulong Liu","Zhikang Dong","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05174v2","updated":"2023-10-10T22:58:45Z","published":"2023-01-12T18:00:00Z","title":"Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A\n  Reproducibility Study","summary":"  Most approaches to cross-modal retrieval (CMR) focus either on object-centric\ndatasets, meaning that each document depicts or describes a single object, or\non scene-centric datasets, meaning that each image depicts or describes a\ncomplex scene that involves multiple objects and relations between them. We\nposit that a robust CMR model should generalize well across both dataset types.\nDespite recent advances in CMR, the reproducibility of the results and their\ngeneralizability across different dataset types has not been studied before. We\naddress this gap and focus on the reproducibility of the state-of-the-art CMR\nresults when evaluated on object-centric and scene-centric datasets. We select\ntwo state-of-the-art CMR models with different architectures: (i) CLIP; and\n(ii) X-VLM. Additionally, we select two scene-centric datasets, and three\nobject-centric datasets, and determine the relative performance of the selected\nmodels on these datasets. We focus on reproducibility, replicability, and\ngeneralizability of the outcomes of previously published CMR experiments. We\ndiscover that the experiments are not fully reproducible and replicable.\nBesides, the relative performance results partially generalize across\nobject-centric and scene-centric datasets. On top of that, the scores obtained\non object-centric datasets are much lower than the scores obtained on\nscene-centric datasets. For reproducibility and transparency we make our source\ncode and the trained models publicly available.\n","authors":["Mariya Hendriksen","Svitlana Vakulenko","Ernst Kuiper","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2301.05174v2.pdf","comment":"18 pages, accepted as a reproducibility paper at ECIR 2023"},{"id":"http://arxiv.org/abs/2310.06958v1","updated":"2023-10-10T19:21:41Z","published":"2023-10-10T19:21:41Z","title":"Comparing the robustness of modern no-reference image- and video-quality\n  metrics to adversarial attacks","summary":"  Nowadays neural-network-based image- and video-quality metrics show better\nperformance compared to traditional methods. However, they also became more\nvulnerable to adversarial attacks that increase metrics' scores without\nimproving visual quality. The existing benchmarks of quality metrics compare\ntheir performance in terms of correlation with subjective quality and\ncalculation time. However, the adversarial robustness of image-quality metrics\nis also an area worth researching. In this paper, we analyse modern metrics'\nrobustness to different adversarial attacks. We adopted adversarial attacks\nfrom computer vision tasks and compared attacks' efficiency against 15\nno-reference image/video-quality metrics. Some metrics showed high resistance\nto adversarial attacks which makes their usage in benchmarks safer than\nvulnerable metrics. The benchmark accepts new metrics submissions for\nresearchers who want to make their metrics more robust to attacks or to find\nsuch metrics for their needs. Try our benchmark using pip install\nrobustness-benchmark.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Sergey Lavrushkin","Ekaterina Shumitskaya","Maksim Velikanov","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06939v1","updated":"2023-10-10T18:50:27Z","published":"2023-10-10T18:50:27Z","title":"On the Role of Font Formats in Building Efficient Web Applications","summary":"  The success of a web application is closely linked to its performance, which\npositively impacts user satisfaction and contributes to energy-saving efforts.\nAmong the various optimization techniques, one specific subject focuses on\nimproving the utilization of web fonts. This study investigates the impact of\ndifferent font formats on client-side resource consumption, such as CPU,\nmemory, load time, and energy. In a controlled experiment, we evaluate\nperformance metrics using the four font formats: OTF, TTF, WOFF, and WOFF2. The\nresults of the study show that there are significant differences between all\npair-wise format comparisons regarding all performance metrics. Overall, WOFF2\nperforms best, except in terms of memory allocation. Through the study and\nexamination of literature, this research contributes (1) an overview of\nmethodologies to enhance web performance through font utilization, (2) a\nspecific exploration of the four prevalent font formats in an experimental\nsetup, and (3) practical recommendations for scientific professionals and\npractitioners.\n","authors":["Benedikt Dornauer","Wolfgang Vigl","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2310.06939v1.pdf","comment":"Preprint: Product-Focused Software Process Improvement 24th\n  International Conference, PROFES 2023, Dornbirn, Austria, December 10-13,\n  2023, Proceedings"}]},"2023-10-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2305.05658v2","updated":"2023-10-11T17:59:44Z","published":"2023-05-09T17:52:59Z","title":"TidyBot: Personalized Robot Assistance with Large Language Models","summary":"  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n","authors":["Jimmy Wu","Rika Antonova","Adam Kan","Marion Lepert","Andy Zeng","Shuran Song","Jeannette Bohg","Szymon Rusinkiewicz","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2305.05658v2.pdf","comment":"Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language\n  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2023. Project page:\n  https://tidybot.cs.princeton.edu"},{"id":"http://arxiv.org/abs/2310.07715v1","updated":"2023-10-11T17:59:36Z","published":"2023-10-11T17:59:36Z","title":"To Build Our Future, We Must Know Our Past: Contextualizing Paradigm\n  Shifts in Natural Language Processing","summary":"  NLP is in a period of disruptive change that is impacting our methodologies,\nfunding sources, and public perception. In this work, we seek to understand how\nto shape our future by better understanding our past. We study factors that\nshape NLP as a field, including culture, incentives, and infrastructure by\nconducting long-form interviews with 26 NLP researchers of varying seniority,\nresearch area, institution, and social identity. Our interviewees identify\ncyclical patterns in the field, as well as new shifts without historical\nparallel, including changes in benchmark culture and software infrastructure.\nWe complement this discussion with quantitative analysis of citation,\nauthorship, and language use in the ACL Anthology over time. We conclude by\ndiscussing shared visions, concerns, and hopes for the future of NLP. We hope\nthat this study of our field's past and present can prompt informed discussion\nof our community's implicit norms and more deliberate action to consciously\nshape the future.\n","authors":["Sireesh Gururaja","Amanda Bertsch","Clara Na","David Gray Widder","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2310.07715v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07713v1","updated":"2023-10-11T17:59:05Z","published":"2023-10-11T17:59:05Z","title":"InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining","summary":"  Pretraining auto-regressive large language models (LLMs) with retrieval\ndemonstrates better perplexity and factual accuracy by leveraging external\ndatabases. However, the size of existing pretrained retrieval-augmented LLM is\nstill limited (e.g., Retro has 7.5B parameters), which limits the effectiveness\nof instruction tuning and zero-shot generalization. In this work, we introduce\nRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.\nSpecifically, we continue to pretrain the 43B GPT model on additional 100\nbillion tokens using the Retro augmentation method by retrieving from 1.2\ntrillion tokens. The obtained foundation model, Retro 48B, largely outperforms\nthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,\nInstructRetro demonstrates significant improvement over the instruction tuned\nGPT on zero-shot question answering (QA) tasks. Specifically, the average\nimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-form\nQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.\nSurprisingly, we find that one can ablate the encoder from InstructRetro\narchitecture and directly use its decoder backbone, while achieving comparable\nresults. We hypothesize that pretraining with retrieval makes its decoder good\nat incorporating context for QA. Our results highlights the promising direction\nto obtain a better GPT decoder for QA through continued pretraining with\nretrieval before instruction tuning.\n","authors":["Boxin Wang","Wei Ping","Lawrence McAfee","Peng Xu","Bo Li","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2310.07713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07712v1","updated":"2023-10-11T17:59:02Z","published":"2023-10-11T17:59:02Z","title":"Found in the Middle: Permutation Self-Consistency Improves Listwise\n  Ranking in Large Language Models","summary":"  Large language models (LLMs) exhibit positional bias in how they use context,\nwhich especially complicates listwise ranking. To address this, we propose\npermutation self-consistency, a form of self-consistency over ranking list\noutputs of black-box LLMs. Our key idea is to marginalize out different list\norders in the prompt to produce an order-independent ranking with less\npositional bias. First, given some input prompt, we repeatedly shuffle the list\nin the prompt and pass it through the LLM while holding the instructions the\nsame. Next, we aggregate the resulting sample of rankings by computing the\ncentral ranking closest in distance to all of them, marginalizing out prompt\norder biases in the process. Theoretically, we prove the robustness of our\nmethod, showing convergence to the true ranking in the presence of random\nperturbations. Empirically, on five list-ranking datasets in sorting and\npassage reranking, our approach improves scores from conventional inference by\nup to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous\nstate of the art in passage reranking. Our code is at\nhttps://github.com/castorini/perm-sc.\n","authors":["Raphael Tang","Xinyu Zhang","Xueguang Ma","Jimmy Lin","Ferhan Ture"],"pdf_url":"https://arxiv.org/pdf/2310.07712v1.pdf","comment":"First two authors contributed equally; 10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07710v1","updated":"2023-10-11T17:57:35Z","published":"2023-10-11T17:57:35Z","title":"DiPmark: A Stealthy, Efficient and Resilient Watermark for Large\n  Language Models","summary":"  Watermarking techniques offer a promising way to secure data via embedding\ncovert information into the data. A paramount challenge in the domain lies in\npreserving the distribution of original data during watermarking. Our research\nextends and refines existing watermarking framework, placing emphasis on the\nimportance of a distribution-preserving (DiP) watermark. Contrary to the\ncurrent strategies, our proposed DiPmark preserves the original token\ndistribution during watermarking (stealthy), is detectable without access to\nthe language model API or weights (efficient), and is robust to moderate\nchanges of tokens (resilient). This is achieved by incorporating a novel\nreweight strategy, combined with a hash function that assigns unique\n\\textit{i.i.d.} ciphers based on the context. The empirical benchmarks of our\napproach underscore its stealthiness, efficiency, and resilience, making it a\nrobust solution for watermarking tasks that demand impeccable quality\npreservation.\n","authors":["Yihan Wu","Zhengmian Hu","Hongyang Zhang","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07707v1","updated":"2023-10-11T17:57:14Z","published":"2023-10-11T17:57:14Z","title":"MatFormer: Nested Transformer for Elastic Inference","summary":"  Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n& encoders), modalities (language & vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.\n","authors":[" Devvrit","Sneha Kudugunta","Aditya Kusupati","Tim Dettmers","Kaifeng Chen","Inderjit Dhillon","Yulia Tsvetkov","Hannaneh Hajishirzi","Sham Kakade","Ali Farhadi","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.07707v1.pdf","comment":"31 pages, 12 figures, first three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.07704v1","updated":"2023-10-11T17:55:15Z","published":"2023-10-11T17:55:15Z","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity","summary":"  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret\n","authors":["Haoxuan You","Haotian Zhang","Zhe Gan","Xianzhi Du","Bowen Zhang","Zirui Wang","Liangliang Cao","Shih-Fu Chang","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07704v1.pdf","comment":"30 pages, 10 figures. Code/Project Website:\n  https://github.com/apple/ml-ferret"},{"id":"http://arxiv.org/abs/2310.07700v1","updated":"2023-10-11T17:51:28Z","published":"2023-10-11T17:51:28Z","title":"Knowledge-enhanced Memory Model for Emotional Support Conversation","summary":"  The prevalence of mental disorders has become a significant issue, leading to\nthe increased focus on Emotional Support Conversation as an effective\nsupplement for mental health support. Existing methods have achieved compelling\nresults, however, they still face three challenges: 1) variability of emotions,\n2) practicality of the response, and 3) intricate strategy modeling. To address\nthese challenges, we propose a novel knowledge-enhanced Memory mODEl for\nemotional suppoRt coNversation (MODERN). Specifically, we first devise a\nknowledge-enriched dialogue context encoding to perceive the dynamic emotion\nchange of different periods of the conversation for coherent user state\nmodeling and select context-related concepts from ConceptNet for practical\nresponse generation. Thereafter, we implement a novel memory-enhanced strategy\nmodeling module to model the semantic patterns behind the strategy categories.\nExtensive experiments on a widely used large-scale dataset verify the\nsuperiority of our model over cutting-edge baselines.\n","authors":["Mengzhao Jia","Qianglong Chen","Liqiang Jing","Dawei Fu","Renyu Li"],"pdf_url":"https://arxiv.org/pdf/2310.07700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08896v3","updated":"2023-10-11T17:43:28Z","published":"2023-03-15T19:31:21Z","title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models","summary":"  Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.\n","authors":["Potsawee Manakul","Adian Liusie","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2303.08896v3.pdf","comment":"EMNLP 2023 (main conference)"},{"id":"http://arxiv.org/abs/2310.04381v2","updated":"2023-10-11T17:36:12Z","published":"2023-10-06T17:19:40Z","title":"Hermes: Unlocking Security Analysis of Cellular Network Protocols by\n  Synthesizing Finite State Machines from Natural Language Specifications","summary":"  In this paper, we present Hermes, an end-to-end framework to automatically\ngenerate formal representations from natural language cellular specifications.\nWe first develop a neural constituency parser, NEUTREX, to process\ntransition-relevant texts and extract transition components (i.e., states,\nconditions, and actions). We also design a domain-specific language to\ntranslate these transition components to logical formulas by leveraging\ndependency parse trees. Finally, we compile these logical formulas to generate\ntransitions and create the formal model as finite state machines. To\ndemonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and\n5G RRC specifications and obtain an overall accuracy of 81-87%, which is a\nsubstantial improvement over the state-of-the-art. Our security analysis of the\nextracted models uncovers 3 new vulnerabilities and identifies 19 previous\nattacks in 4G and 5G specifications, and 7 deviations in commercial 4G\nbasebands.\n","authors":["Abdullah Al Ishtiaq","Sarkar Snigdha Sarathi Das","Syed Md Mukit Rashid","Ali Ranjbar","Kai Tu","Tianwei Wu","Zhezheng Song","Weixuan Wang","Mujtahid Akon","Rui Zhang","Syed Rafiul Hussain"],"pdf_url":"https://arxiv.org/pdf/2310.04381v2.pdf","comment":"Accepted at USENIX Security 24"},{"id":"http://arxiv.org/abs/2306.12424v2","updated":"2023-10-11T17:34:19Z","published":"2023-06-21T17:59:51Z","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution","summary":"  We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n","authors":["Siobhan Mackenzie Hall","Fernanda Gonalves Abrantes","Hanwen Zhu","Grace Sodunke","Aleksandar Shtedritski","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2306.12424v2.pdf","comment":"Data and code available at https://github.com/oxai/visogender"},{"id":"http://arxiv.org/abs/2310.07676v1","updated":"2023-10-11T17:21:03Z","published":"2023-10-11T17:21:03Z","title":"Composite Backdoor Attacks Against Large Language Models","summary":"  Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. The unique\ncharacteristics of our CBA can be tailored for various practical scenarios,\ne.g., targeting specific user groups. Our work highlights the necessity of\nincreased security research on the trustworthiness of foundation LLMs.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08703v3","updated":"2023-10-11T17:00:34Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v3.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2310.07659v1","updated":"2023-10-11T17:00:29Z","published":"2023-10-11T17:00:29Z","title":"Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for\n  Knowledge-Grounded Dialogue","summary":"  Accurate knowledge selection is critical in knowledge-grounded dialogue\nsystems. Towards a closer look at it, we offer a novel perspective to organize\nexisting literature, i.e., knowledge selection coupled with, after, and before\ngeneration. We focus on the third under-explored category of study, which can\nnot only select knowledge accurately in advance, but has the advantage to\nreduce the learning, adjustment, and interpretation burden of subsequent\nresponse generation models, especially LLMs. We propose GATE, a\ngenerator-agnostic knowledge selection method, to prepare knowledge for\nsubsequent response generation models by selecting context-related knowledge\namong different knowledge structures and variable knowledge requirements.\nExperimental results demonstrate the superiority of GATE, and indicate that\nknowledge selection before generation is a lightweight yet effective way to\nfacilitate LLMs (e.g., ChatGPT) to generate more informative responses.\n","authors":["Qin Lang","Zhang Yao","Liang Hongru","Wang jun","Yang Zhenglu"],"pdf_url":"https://arxiv.org/pdf/2310.07659v1.pdf","comment":"Accepted by EMNLP2023 main conference"},{"id":"http://arxiv.org/abs/2310.07654v1","updated":"2023-10-11T16:54:57Z","published":"2023-10-11T16:54:57Z","title":"Audio-Visual Neural Syntax Acquisition","summary":"  We study phrase structure induction from visually-grounded speech. The core\nidea is to first segment the speech waveform into sequences of word segments,\nand subsequently induce phrase structure using the inferred segment-level\ncontinuous representations. We present the Audio-Visual Neural Syntax Learner\n(AV-NSL) that learns phrase structure by listening to audio and looking at\nimages, without ever being exposed to text. By training on paired images and\nspoken captions, AV-NSL exhibits the capability to infer meaningful phrase\nstructures that are comparable to those derived by naturally-supervised text\nparsers, for both English and German. Our findings extend prior work in\nunsupervised language acquisition from speech and grounded grammar induction,\nand present one approach to bridge the gap between the two topics.\n","authors":["Cheng-I Jeff Lai","Freda Shi","Puyuan Peng","Yoon Kim","Kevin Gimpel","Shiyu Chang","Yung-Sung Chuang","Saurabhchand Bhati","David Cox","David Harwath","Yang Zhang","Karen Livescu","James Glass"],"pdf_url":"https://arxiv.org/pdf/2310.07654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13172v2","updated":"2023-10-11T16:51:50Z","published":"2023-05-22T16:00:00Z","title":"Editing Large Language Models: Problems, Methods, and Opportunities","summary":"  Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Yunzhi Yao","Peng Wang","Bozhong Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13172v2.pdf","comment":"EMNLP 2023. Updated with new experiments"},{"id":"http://arxiv.org/abs/2310.07652v1","updated":"2023-10-11T16:51:46Z","published":"2023-10-11T16:51:46Z","title":"LLM4Vis: Explainable Visualization Recommendation using ChatGPT","summary":"  Data visualization is a powerful tool for exploring and communicating\ninsights in various domains. To automate visualization choice for datasets, a\ntask known as visualization recommendation has been proposed. Various\nmachine-learning-based approaches have been developed for this purpose, but\nthey often require a large corpus of dataset-visualization pairs for training\nand lack natural explanations for their results. To address this research gap,\nwe propose LLM4Vis, a novel ChatGPT-based prompting approach to perform\nvisualization recommendation and return human-like explanations using very few\ndemonstration examples. Our approach involves feature description,\ndemonstration example selection, explanation generation, demonstration example\nconstruction, and inference steps. To obtain demonstration examples with\nhigh-quality explanations, we propose a new explanation generation\nbootstrapping to iteratively refine generated explanations by considering the\nprevious generation and template-based hint. Evaluations on the VizML dataset\nshow that LLM4Vis outperforms or performs similarly to supervised learning\nmodels like Random Forest, Decision Tree, and MLP in both few-shot and\nzero-shot settings. The qualitative evaluation also shows the effectiveness of\nexplanations generated by LLM4Vis. We make our code publicly available at\n\\href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.\n","authors":["Lei Wang","Songheng Zhang","Yun Wang","Ee-Peng Lim","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07652v1.pdf","comment":"EMNLP 2023 (Industry Track)"},{"id":"http://arxiv.org/abs/2307.15770v2","updated":"2023-10-11T16:49:29Z","published":"2023-07-28T18:58:16Z","title":"CHATREPORT: Democratizing Sustainability Disclosure Analysis through\n  LLM-based Tools","summary":"  In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.\n","authors":["Jingwei Ni","Julia Bingler","Chiara Colesanti-Senni","Mathias Kraus","Glen Gostlow","Tobias Schimanski","Dominik Stammbach","Saeid Ashraf Vaghefi","Qian Wang","Nicolas Webersinke","Tobias Wekhof","Tingyu Yu","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2307.15770v2.pdf","comment":"6 pages. arXiv admin note: text overlap with arXiv:2306.15518"},{"id":"http://arxiv.org/abs/2310.07644v1","updated":"2023-10-11T16:40:57Z","published":"2023-10-11T16:40:57Z","title":"Rethinking the BERT-like Pretraining for DNA Sequences","summary":"  With the success of large-scale pretraining in NLP, there is an increasing\ntrend of applying it to the domain of life sciences. In particular, pretraining\nmethods based on DNA sequences have garnered growing attention due to their\npotential to capture generic information about genes. However, existing\npretraining methods for DNA sequences largely rely on direct adoptions of BERT\npretraining from NLP, lacking a comprehensive understanding and a specifically\ntailored approach. To address this research gap, we first conducted a series of\nexploratory experiments and gained several insightful observations: 1) In the\nfine-tuning phase of downstream tasks, when using K-mer overlapping\ntokenization instead of K-mer non-overlapping tokenization, both overlapping\nand non-overlapping pretraining weights show consistent performance\nimprovement.2) During the pre-training process, using K-mer overlapping\ntokenization quickly produces clear K-mer embeddings and reduces the loss to a\nvery low level, while using K-mer non-overlapping tokenization results in less\ndistinct embeddings and continuously decreases the loss. 3) Using overlapping\ntokenization causes the self-attention in the intermediate layers of\npre-trained models to tend to overly focus on certain tokens, reflecting that\nthese layers are not adequately optimized. In summary, overlapping tokenization\ncan benefit the fine-tuning of downstream tasks but leads to inadequate\npretraining with fast convergence. To unleash the pretraining potential, we\nintroduce a novel approach called RandomMask, which gradually increases the\ntask difficulty of BERT-like pretraining by continuously expanding its mask\nboundary, forcing the model to learn more knowledge. RandomMask is simple but\neffective, achieving top-tier performance across 26 datasets of 28 datasets\nspanning 7 downstream tasks.\n","authors":["Chaoqi Liang","Weiqiang Bai","Lifeng Qiao","Yuchen Ren","Jianle Sun","Peng Ye","Hongliang Yan","Xinzhu Ma","Wangmeng Zuo","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.07644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07641v1","updated":"2023-10-11T16:38:11Z","published":"2023-10-11T16:38:11Z","title":"Evaluating Large Language Models at Evaluating Instruction Following","summary":"  As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these \"LLM evaluators\", particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.\n","authors":["Zhiyuan Zeng","Jiatong Yu","Tianyu Gao","Yu Meng","Tanya Goyal","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07641v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2306.13047v3","updated":"2023-10-11T16:33:39Z","published":"2023-06-22T17:13:08Z","title":"Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with\n  a Focus on Candidate Response Distribution","summary":"  Multiple choice exams are widely used to assess candidates across a diverse\nrange of domains and tasks. To moderate question quality, newly proposed\nquestions often pass through pre-test evaluation stages before being deployed\ninto real-world exams. Currently, this evaluation process is manually\nintensive, which can lead to time lags in the question development cycle.\nStreamlining this process via automation can significantly enhance efficiency,\nhowever, there's a current lack of datasets with adequate pre-test analysis\ninformation. In this paper we analyse the Cambridge Multiple-Choice Questions\nReading Dataset; a multiple-choice comprehension dataset of questions at\ndifferent target levels, with corresponding candidate selection distributions.\nWe introduce the task of candidate distribution matching, propose several\nevaluation metrics for the task, and demonstrate that automatic systems trained\non RACE++ can be leveraged as baselines for our task. We further demonstrate\nthat these automatic systems can be used for practical pre-test evaluation\ntasks such as detecting underperforming distractors, where our detection\nsystems can automatically identify poor distractors that few candidates select.\n","authors":["Adian Liusie","Vatsal Raina","Andrew Mullooly","Kate Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2306.13047v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07629v1","updated":"2023-10-11T16:18:13Z","published":"2023-10-11T16:18:13Z","title":"The Past, Present and Better Future of Feedback Learning in Large\n  Language Models for Subjective Human Preferences and Values","summary":"  Human feedback is increasingly used to steer the behaviours of Large Language\nModels (LLMs). However, it is unclear how to collect and incorporate feedback\nin a way that is efficient, effective and unbiased, especially for highly\nsubjective human preferences and values. In this paper, we survey existing\napproaches for learning from human feedback, drawing on 95 papers primarily\nfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLM\ntrends for integrating human feedback into language models. Second, we give an\noverview of present techniques and practices, as well as the motivations for\nusing feedback; conceptual frameworks for defining values and preferences; and\nhow feedback is collected and from whom. Finally, we encourage a better future\nof feedback learning in LLMs by raising five unresolved conceptual and\npractical challenges.\n","authors":["Hannah Rose Kirk","Andrew M. Bean","Bertie Vidgen","Paul Rttger","Scott A. Hale"],"pdf_url":"https://arxiv.org/pdf/2310.07629v1.pdf","comment":"Accepted for the 2023 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP, Main)"},{"id":"http://arxiv.org/abs/2303.08268v3","updated":"2023-10-11T16:17:20Z","published":"2023-03-14T23:01:27Z","title":"Chat with the Environment: Interactive Multimodal Perception Using Large\n  Language Models","summary":"  Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.\n","authors":["Xufeng Zhao","Mengdi Li","Cornelius Weber","Muhammad Burhan Hafez","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2303.08268v3.pdf","comment":"IROS2023, Detroit. See the project website at\n  https://matcha-agent.github.io"},{"id":"http://arxiv.org/abs/2310.07611v1","updated":"2023-10-11T15:56:00Z","published":"2023-10-11T15:56:00Z","title":"Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in\n  Self-Refined Open-Source Models","summary":"  The dominance of proprietary LLMs has led to restricted access and raised\ninformation privacy concerns. High-performing open-source alternatives are\ncrucial for information-sensitive and high-volume applications but often lag\nbehind in performance. To address this gap, we propose (1) A untargeted variant\nof iterative self-critique and self-refinement devoid of external influence.\n(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score\n(PeRFICS) - to find the optimal model for a given task considering refined\nperformance and cost. Our experiments show that SoTA open source models of\nvarying sizes from 7B - 65B, on average, improve 8.2% from their baseline\nperformance. Strikingly, even models with extremely small memory footprints,\nsuch as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%\nimprovement in high-creativity, open ended tasks on the Vicuna benchmark.\nVicuna-13B takes it a step further and outperforms ChatGPT post-refinement.\nThis work has profound implications for resource-constrained and\ninformation-sensitive environments seeking to leverage LLMs without incurring\nprohibitive costs, compromising on performance and privacy. The domain-agnostic\nself-refinement process coupled with our novel ranking metric facilitates\ninformed decision-making in model selection, thereby reducing costs and\ndemocratizing access to high-performing language models, as evidenced by case\nstudies.\n","authors":["Sumuk Shashidhar","Abhinav Chinta","Vaibhav Sahai","Zhenhailong Wang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2310.07611v1.pdf","comment":"Initial Preprint. Accepted to Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07609v1","updated":"2023-10-11T15:51:53Z","published":"2023-10-11T15:51:53Z","title":"QACHECK: A Demonstration System for Question-Guided Multi-Hop\n  Fact-Checking","summary":"  Fact-checking real-world claims often requires complex, multi-step reasoning\ndue to the absence of direct evidence to support or refute them. However,\nexisting fact-checking systems often lack transparency in their\ndecision-making, making it challenging for users to comprehend their reasoning\nprocess. To address this, we propose the Question-guided Multi-hop\nFact-Checking (QACHECK) system, which guides the model's reasoning process by\nasking a series of questions critical for verifying a claim. QACHECK has five\nkey modules: a claim verifier, a question generator, a question-answering\nmodule, a QA validator, and a reasoner. Users can input a claim into QACHECK,\nwhich then predicts its veracity and provides a comprehensive report detailing\nits reasoning process, guided by a sequence of (question, answer) pairs.\nQACHECK also provides the source of evidence supporting each question,\nfostering a transparent, explainable, and user-friendly fact-checking process.\nA recorded video of QACHECK is at https://www.youtube.com/watch?v=ju8kxSldM64\n","authors":["Liangming Pan","Xinyuan Lu","Min-Yen Kan","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2310.07609v1.pdf","comment":"Accepted at EMNLP 2023 System Demonstrations Track"},{"id":"http://arxiv.org/abs/2310.07588v1","updated":"2023-10-11T15:28:44Z","published":"2023-10-11T15:28:44Z","title":"Accurate Use of Label Dependency in Multi-Label Text Classification\n  Through the Lens of Causality","summary":"  Multi-Label Text Classification (MLTC) aims to assign the most relevant\nlabels to each given text. Existing methods demonstrate that label dependency\ncan help to improve the model's performance. However, the introduction of label\ndependency may cause the model to suffer from unwanted prediction bias. In this\nstudy, we attribute the bias to the model's misuse of label dependency, i.e.,\nthe model tends to utilize the correlation shortcut in label dependency rather\nthan fusing text information and label dependency for prediction. Motivated by\ncausal inference, we propose a CounterFactual Text Classifier (CFTC) to\neliminate the correlation bias, and make causality-based predictions.\nSpecifically, our CFTC first adopts the predict-then-modify backbone to extract\nprecise label information embedded in label dependency, then blocks the\ncorrelation shortcut through the counterfactual de-bias technique with the help\nof the human causal graph. Experimental results on three datasets demonstrate\nthat our CFTC significantly outperforms the baselines and effectively\neliminates the correlation bias in datasets.\n","authors":["Caoyun Fan","Wenqing Chen","Jidong Tian","Yitian Li","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2310.07588v1.pdf","comment":"Applied Intelligence 2023"},{"id":"http://arxiv.org/abs/2304.14933v2","updated":"2023-10-11T15:08:51Z","published":"2023-04-28T15:43:21Z","title":"An Empirical Study of Multimodal Model Merging","summary":"  Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n","authors":["Yi-Lin Sung","Linjie Li","Kevin Lin","Zhe Gan","Mohit Bansal","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2304.14933v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2308.12067v2","updated":"2023-10-11T14:49:26Z","published":"2023-08-23T11:27:30Z","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4","summary":"  Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.\n","authors":["Lai Wei","Zihao Jiang","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07521v1","updated":"2023-10-11T14:18:03Z","published":"2023-10-11T14:18:03Z","title":"Survey on Factuality in Large Language Models: Knowledge, Retrieval and\n  Domain-Specificity","summary":"  This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.\n","authors":["Cunxiang Wang","Xiaoze Liu","Yuanhao Yue","Xiangru Tang","Tianhang Zhang","Cheng Jiayang","Yunzhi Yao","Wenyang Gao","Xuming Hu","Zehan Qi","Yidong Wang","Linyi Yang","Jindong Wang","Xing Xie","Zheng Zhang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07521v1.pdf","comment":"43 pages; 300+ references"},{"id":"http://arxiv.org/abs/2310.07488v1","updated":"2023-10-11T13:35:05Z","published":"2023-10-11T13:35:05Z","title":"KwaiYiiMath: Technical Report","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable abilities in handling a variety of natural language processing (NLP)\ndownstream tasks, even on mathematical tasks requiring multi-step reasoning. In\nthis report, we introduce the KwaiYiiMath which enhances the mathematical\nreasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)\nand Reinforced Learning from Human Feedback (RLHF), including on both English\nand Chinese mathematical tasks. Meanwhile, we also constructed a small-scale\nChinese primary school mathematics test set (named KMath), consisting of 188\nexamples to evaluate the correctness of the problem-solving process generated\nby the models. Empirical studies demonstrate that KwaiYiiMath can achieve\nstate-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with\nthe similar size models, respectively.\n","authors":["Jiayi Fu","Lei Lin","Xiaoyang Gao","Pengli Liu","Zhengzong Chen","Zhirui Yang","Shengnan Zhang","Xue Zheng","Yan Li","Yuliang Liu","Xucheng Ye","Yiqiao Liao","Chao Liao","Bin Chen","Chengru Song","Junchen Wan","Zijia Lin","Fuzheng Zhang","Zhongyuan Wang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2310.07488v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2310.07487v1","updated":"2023-10-11T13:34:22Z","published":"2023-10-11T13:34:22Z","title":"Cognate Transformer for Automated Phonological Reconstruction and\n  Cognate Reflex Prediction","summary":"  Phonological reconstruction is one of the central problems in historical\nlinguistics where a proto-word of an ancestral language is determined from the\nobserved cognate words of daughter languages. Computational approaches to\nhistorical linguistics attempt to automate the task by learning models on\navailable linguistic data. Several ideas and techniques drawn from\ncomputational biology have been successfully applied in the area of\ncomputational historical linguistics. Following these lines, we adapt MSA\nTransformer, a protein language model, to the problem of automated phonological\nreconstruction. MSA Transformer trains on multiple sequence alignments as input\nand is, thus, apt for application on aligned cognate words. We, hence, name our\nmodel as Cognate Transformer. We also apply the model on another associated\ntask, namely, cognate reflex prediction, where a reflex word in a daughter\nlanguage is predicted based on cognate words from other daughter languages. We\nshow that our model outperforms the existing models on both tasks, especially\nwhen it is pre-trained on masked word prediction task.\n","authors":["V. S. D. S. Mahesh Akavarapu","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2310.07487v1.pdf","comment":"Accepted to appear at the conference of EMNLP-2023"},{"id":"http://arxiv.org/abs/2310.07423v1","updated":"2023-10-11T12:15:24Z","published":"2023-10-11T12:15:24Z","title":"Adapting the adapters for code-switching in multilingual ASR","summary":"  Recently, large pre-trained multilingual speech models have shown potential\nin scaling Automatic Speech Recognition (ASR) to many low-resource languages.\nSome of these models employ language adapters in their formulation, which helps\nto improve monolingual performance and avoids some of the drawbacks of\nmulti-lingual modeling on resource-rich languages. However, this formulation\nrestricts the usability of these models on code-switched speech, where two\nlanguages are mixed together in the same utterance. In this work, we propose\nways to effectively fine-tune such models on code-switched speech, by\nassimilating information from both language adapters at each language\nadaptation point in the network. We also model code-switching as a sequence of\nlatent binary sequences that can be used to guide the flow of information from\neach language adapter at the frame level. The proposed approaches are evaluated\non three code-switched datasets encompassing Arabic, Mandarin, and Hindi\nlanguages paired with English, showing consistent improvements in\ncode-switching performance with at least 10\\% absolute reduction in CER across\nall test sets.\n","authors":["Atharva Kulkarni","Ajinkya Kulkarni","Miguel Couceiro","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2310.07423v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.07403v1","updated":"2023-10-11T11:39:36Z","published":"2023-10-11T11:39:36Z","title":"DASpeech: Directed Acyclic Transformer for Fast and High-quality\n  Speech-to-Speech Translation","summary":"  Direct speech-to-speech translation (S2ST) translates speech from one\nlanguage into another using a single model. However, due to the presence of\nlinguistic and acoustic diversity, the target speech follows a complex\nmultimodal distribution, posing challenges to achieving both high-quality\ntranslations and fast decoding speeds for S2ST models. In this paper, we\npropose DASpeech, a non-autoregressive direct S2ST model which realizes both\nfast and high-quality S2ST. To better capture the complex distribution of the\ntarget speech, DASpeech adopts the two-pass architecture to decompose the\ngeneration process into two steps, where a linguistic decoder first generates\nthe target text, and an acoustic decoder then generates the target speech based\non the hidden states of the linguistic decoder. Specifically, we use the\ndecoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as\nthe acoustic decoder. DA-Transformer models translations with a directed\nacyclic graph (DAG). To consider all potential paths in the DAG during\ntraining, we calculate the expected hidden states for each target token via\ndynamic programming, and feed them into the acoustic decoder to predict the\ntarget mel-spectrogram. During inference, we select the most probable path and\ntake hidden states on that path as input to the acoustic decoder. Experiments\non the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or\neven better performance than the state-of-the-art S2ST model Translatotron 2,\nwhile preserving up to 18.53x speedup compared to the autoregressive baseline.\nCompared with the previous non-autoregressive S2ST model, DASpeech does not\nrely on knowledge distillation and iterative decoding, achieving significant\nimprovements in both translation quality and decoding speed. Furthermore,\nDASpeech shows the ability to preserve the speaker's voice of the source speech\nduring translation.\n","authors":["Qingkai Fang","Yan Zhou","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2310.07403v1.pdf","comment":"NeurIPS 2023. Audio samples are available at\n  https://ictnlp.github.io/daspeech-demo/"},{"id":"http://arxiv.org/abs/2310.07397v1","updated":"2023-10-11T11:32:57Z","published":"2023-10-11T11:32:57Z","title":"Target-oriented Proactive Dialogue Systems with Personalization: Problem\n  Formulation and Dataset Curation","summary":"  Target-oriented dialogue systems, designed to proactively steer conversations\ntoward predefined targets or accomplish specific system-side goals, are an\nexciting area in conversational AI. In this work, by formulating a <dialogue\nact, topic> pair as the conversation target, we explore a novel problem of\npersonalized target-oriented dialogue by considering personalization during the\ntarget accomplishment process. However, there remains an emergent need for\nhigh-quality datasets, and building one from scratch requires tremendous human\neffort. To address this, we propose an automatic dataset curation framework\nusing a role-playing approach. Based on this framework, we construct a\nlarge-scale personalized target-oriented dialogue dataset, TopDial, which\ncomprises about 18K multi-turn dialogues. The experimental results show that\nthis dataset is of high quality and could contribute to exploring personalized\ntarget-oriented dialogue.\n","authors":["Jian Wang","Yi Cheng","Dongding Lin","Chak Tou Leong","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2310.07397v1.pdf","comment":"Accepted to EMNLP-2023 main conference"},{"id":"http://arxiv.org/abs/2310.07387v1","updated":"2023-10-11T11:08:20Z","published":"2023-10-11T11:08:20Z","title":"Linguistic laws in biology","summary":"  Linguistic laws, the common statistical patterns of human language, have been\ninvestigated by quantitative linguists for nearly a century. Recently,\nbiologists from a range of disciplines have started to explore the prevalence\nof these laws beyond language, finding patterns consistent with linguistic laws\nacross multiple levels of biological organisation, from molecular (genomes,\ngenes, and proteins) to organismal (animal behaviour) to ecological\n(populations and ecosystems). We propose a new conceptual framework for the\nstudy of linguistic laws in biology, comprising and integrating distinct levels\nof analysis, from description to prediction to theory building. Adopting this\nframework will provide critical new insights into the fundamental rules of\norganisation underpinning natural systems, unifying linguistic laws and core\ntheory in biology.\n","authors":["Stuart Semple","Ramon Ferrer-i-Cancho","Morgan L. Gustison"],"pdf_url":"https://arxiv.org/pdf/2310.07387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08732v3","updated":"2023-10-11T10:51:12Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v3.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.06692v2","updated":"2023-10-11T10:05:29Z","published":"2023-10-10T15:10:03Z","title":"Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task\n  Scenarios with Large Language Models","summary":"  Large language models (LLMs) have unveiled remarkable reasoning capabilities\nby exploiting chain-of-thought (CoT) prompting, which generates intermediate\nreasoning chains to serve as the rationale for deriving the answer. However,\ncurrent CoT methods either simply employ general prompts such as Let's think\nstep by step, or heavily rely on handcrafted task-specific demonstrations to\nattain preferable performances, thereby engendering an inescapable gap between\nperformance and generalization. To bridge this gap, we propose Meta-CoT, a\ngeneralizable CoT prompting method in mixed-task scenarios where the type of\ninput questions is unknown. Meta-CoT firstly categorizes the scenario based on\nthe input question and subsequently constructs diverse demonstrations from the\ncorresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys\nremarkable performances on ten public benchmark reasoning tasks and superior\ngeneralization capabilities. Notably, Meta-CoT achieves the state-of-the-art\nresult on SVAMP (93.7%) without any additional program-aided methods. Our\nfurther experiments on five out-of-distribution datasets verify the stability\nand generality of Meta-CoT.\n","authors":["Anni Zou","Zhuosheng Zhang","Hai Zhao","Xiangru Tang"],"pdf_url":"https://arxiv.org/pdf/2310.06692v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.07347v1","updated":"2023-10-11T09:55:46Z","published":"2023-10-11T09:55:46Z","title":"Fast-ELECTRA for Efficient Pre-training","summary":"  ELECTRA pre-trains language models by detecting tokens in a sequence that\nhave been replaced by an auxiliary model. Although ELECTRA offers a significant\nboost in efficiency, its potential is constrained by the training cost brought\nby the auxiliary model. Notably, this model, which is jointly trained with the\nmain model, only serves to assist the training of the main model and is\ndiscarded post-training. This results in a substantial amount of training cost\nbeing expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which\nleverages an existing language model as the auxiliary model. To construct a\nlearning curriculum for the main model, we smooth its output distribution via\ntemperature scaling following a descending schedule. Our approach rivals the\nperformance of state-of-the-art ELECTRA-style pre-training methods, while\nsignificantly eliminating the computation and memory cost brought by the joint\ntraining of the auxiliary model. Our method also reduces the sensitivity to\nhyper-parameters and enhances the pre-training stability.\n","authors":["Chengyu Dong","Liyuan Liu","Hao Cheng","Jingbo Shang","Jianfeng Gao","Xiaodong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07345v1","updated":"2023-10-11T09:53:17Z","published":"2023-10-11T09:53:17Z","title":"Investigating the Effect of Language Models in Sequence Discriminative\n  Training for Neural Transducers","summary":"  In this work, we investigate the effect of language models (LMs) with\ndifferent context lengths and label units (phoneme vs. word) used in sequence\ndiscriminative training for phoneme-based neural transducers. Both lattice-free\nand N-best-list approaches are examined. For lattice-free methods with\nphoneme-level LMs, we propose a method to approximate the context history to\nemploy LMs with full-context dependency. This approximation can be extended to\narbitrary context length and enables the usage of word-level LMs in\nlattice-free methods. Moreover, a systematic comparison is conducted across\nlattice-free and N-best-list-based methods. Experimental results on Librispeech\nshow that using the word-level LM in training outperforms the phoneme-level LM.\nBesides, we find that the context size of the LM used for probability\ncomputation has a limited effect on performance. Moreover, our results reveal\nthe pivotal importance of the hypothesis space quality in sequence\ndiscriminative training.\n","authors":["Zijian Yang","Wei Zhou","Ralf Schlter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2310.07345v1.pdf","comment":"accepted at ASRU 2023"},{"id":"http://arxiv.org/abs/2304.14767v2","updated":"2023-10-11T09:49:59Z","published":"2023-04-28T11:26:17Z","title":"Dissecting Recall of Factual Associations in Auto-Regressive Language\n  Models","summary":"  Transformer-based language models (LMs) are known to capture factual\nknowledge in their parameters. While previous work looked into where factual\nassociations are stored, only little is known about how they are retrieved\ninternally during inference. We investigate this question through the lens of\ninformation flow. Given a subject-relation query, we study how the model\naggregates information about the subject and relation to predict the correct\nattribute. With interventions on attention edges, we first identify two\ncritical points where information propagates to the prediction: one from the\nrelation positions followed by another from the subject positions. Next, by\nanalyzing the information at these points, we unveil a three-step internal\nmechanism for attribute extraction. First, the representation at the\nlast-subject position goes through an enrichment process, driven by the early\nMLP sublayers, to encode many subject-related attributes. Second, information\nfrom the relation propagates to the prediction. Third, the prediction\nrepresentation \"queries\" the enriched subject to extract the attribute. Perhaps\nsurprisingly, this extraction is typically done via attention heads, which\noften encode subject-attribute mappings in their parameters. Overall, our\nfindings introduce a comprehensive view of how factual associations are stored\nand extracted internally in LMs, facilitating future research on knowledge\nlocalization and editing.\n","authors":["Mor Geva","Jasmijn Bastings","Katja Filippova","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2304.14767v2.pdf","comment":"Accepted at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07343v1","updated":"2023-10-11T09:46:32Z","published":"2023-10-11T09:46:32Z","title":"How Do Large Language Models Capture the Ever-changing World Knowledge?\n  A Review of Recent Advances","summary":"  Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms\n","authors":["Zihan Zhang","Meng Fang","Ling Chen","Mohammad-Reza Namazi-Rad","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07343v1.pdf","comment":"EMNLP 2023 main conference, paper link at\n  https://github.com/hyintell/awesome-refreshing-llms"},{"id":"http://arxiv.org/abs/2304.11082v4","updated":"2023-10-11T09:45:15Z","published":"2023-04-19T17:50:09Z","title":"Fundamental Limitations of Alignment in Large Language Models","summary":"  An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that within the limits of this framework, for any\nbehavior that has a finite probability of being exhibited by the model, there\nexist prompts that can trigger the model into outputting this behavior, with\nprobability that increases with the length of the prompt. This implies that any\nalignment process that attenuates an undesired behavior but does not remove it\naltogether, is not safe against adversarial prompting attacks. Furthermore, our\nframework hints at the mechanism by which leading alignment approaches such as\nreinforcement learning from human feedback make the LLM prone to being prompted\ninto the undesired behaviors. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.\n","authors":["Yotam Wolf","Noam Wies","Oshri Avnery","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2304.11082v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07328v1","updated":"2023-10-11T09:18:09Z","published":"2023-10-11T09:18:09Z","title":"An Empirical Study of Instruction-tuning Large Language Models in\n  Chinese","summary":"  The success of ChatGPT validates the potential of large language models\n(LLMs) in artificial general intelligence (AGI). Subsequently, the release of\nLLMs has sparked the open-source community's interest in instruction-tuning,\nwhich is deemed to accelerate ChatGPT's replication process. However, research\non instruction-tuning LLMs in Chinese, the world's most spoken language, is\nstill in its early stages. Therefore, this paper makes an in-depth empirical\nstudy of instruction-tuning LLMs in Chinese, which can serve as a cookbook that\nprovides valuable findings for effectively customizing LLMs that can better\nrespond to Chinese instructions. Specifically, we systematically explore the\nimpact of LLM bases, parameter-efficient methods, instruction data types, which\nare the three most important elements for instruction-tuning. Besides, we also\nconduct experiment to study the impact of other factors, e.g., chain-of-thought\ndata and human-value alignment. We hope that this empirical study can make a\nmodest contribution to the open Chinese version of ChatGPT. This paper will\nrelease a powerful Chinese LLMs that is comparable to ChatGLM. The code and\ndata are available at https://github.com/PhoebusSi/Alpaca-CoT.\n","authors":["Qingyi Si","Tong Wang","Zheng Lin","Xu Zhang","Yanan Cao","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07328v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07321v1","updated":"2023-10-11T09:09:55Z","published":"2023-10-11T09:09:55Z","title":"On the Impact of Cross-Domain Data on German Language Models","summary":"  Traditionally, large language models have been either trained on general web\ncrawls or domain-specific data. However, recent successes of generative large\nlanguage models, have shed light on the benefits of cross-domain datasets. To\nexamine the significance of prioritizing data diversity over quality, we\npresent a German dataset comprising texts from five domains, along with another\ndataset aimed at containing high-quality data. Through training a series of\nmodels ranging between 122M and 750M parameters on both datasets, we conduct a\ncomprehensive benchmark on multiple downstream tasks. Our findings demonstrate\nthat the models trained on the cross-domain dataset outperform those trained on\nquality data alone, leading to improvements up to $4.45\\%$ over the previous\nstate-of-the-art. The models are available at\nhttps://huggingface.co/ikim-uk-essen\n","authors":["Amin Dada","Aokun Chen","Cheng Peng","Kaleb E Smith","Ahmad Idrissi-Yaghir","Constantin Marc Seibold","Jianning Li","Lars Heiliger","Christoph M. Friedrich","Daniel Truhn","Jan Egger","Jiang Bian","Jens Kleesiek","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07321v1.pdf","comment":"13 pages, 1 figure, accepted at Findings of the Association for\n  Computational Linguistics: EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07306v1","updated":"2023-10-11T08:40:06Z","published":"2023-10-11T08:40:06Z","title":"SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification\n  Model","summary":"  This paper presents a Soft Labeling and Noisy Mixup-based open intent\nclassification model (SNOiC). Most of the previous works have used\nthreshold-based methods to identify open intents, which are prone to\noverfitting and may produce biased predictions. Additionally, the need for more\navailable data for an open intent class presents another limitation for these\nexisting models. SNOiC combines Soft Labeling and Noisy Mixup strategies to\nreduce the biasing and generate pseudo-data for open intent class. The\nexperimental results on four benchmark datasets show that the SNOiC model\nachieves a minimum and maximum performance of 68.72\\% and 94.71\\%,\nrespectively, in identifying open intents. Moreover, compared to\nstate-of-the-art models, the SNOiC model improves the performance of\nidentifying open intents by 0.93\\% (minimum) and 12.76\\% (maximum). The model's\nefficacy is further established by analyzing various parameters used in the\nproposed model. An ablation study is also conducted, which involves creating\nthree model variants to validate the effectiveness of the SNOiC model.\n","authors":["Aditi Kanwar","Aditi Seetha","Satyendra Singh Chouhan","Rajdeep Niyogi"],"pdf_url":"https://arxiv.org/pdf/2310.07306v1.pdf","comment":"9 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07301v1","updated":"2023-10-11T08:36:43Z","published":"2023-10-11T08:36:43Z","title":"Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions","summary":"  Impressive progress has been made on chat models based on Large Language\nModels (LLMs) recently; however, there is a noticeable lag in multi-turn\nconversations between open-source chat models (e.g., Alpaca and Vicuna) and the\nleading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we\nattribute the lag to the lack of enough high-quality multi-turn\ninstruction-tuning data. The available instruction-tuning data for the\ncommunity are either single-turn conversations or multi-turn ones with certain\nissues, such as non-human-like instructions, less detailed responses, or rare\ntopic shifts. In this paper, we address these challenges by introducing Parrot,\na highly scalable solution designed to automatically generate high-quality\ninstruction-tuning data, which are then used to enhance the effectiveness of\nchat models in multi-turn conversations. Specifically, we start by training the\nParrot-Ask model, which is designed to emulate real users in generating\ninstructions. We then utilize Parrot-Ask to engage in multi-turn conversations\nwith ChatGPT across a diverse range of topics, resulting in a collection of 40K\nhigh-quality multi-turn dialogues (Parrot-40K). These data are subsequently\nemployed to train a chat model that we have named Parrot-Chat. We demonstrate\nthat the dialogues gathered from Parrot-Ask markedly outperform existing\nmulti-turn instruction-following datasets in critical metrics, including topic\ndiversity, number of turns, and resemblance to human conversation. With only\n40K training examples, Parrot-Chat achieves strong performance against other\n13B open-source models across a range of instruction-following benchmarks, and\nparticularly excels in evaluations of multi-turn capabilities. We make all\ncodes, datasets, and two versions of the Parrot-Ask model based on LLaMA2-13B\nand KuaiYii-13B available at https://github.com/kwai/KwaiYii/Parrot.\n","authors":["Yuchong Sun","Che Liu","Jinwen Huang","Ruihua Song","Fuzheng Zhang","Di Zhang","Zhongyuan Wang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2310.07301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07678v2","updated":"2023-10-11T08:34:42Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07299v1","updated":"2023-10-11T08:33:23Z","published":"2023-10-11T08:33:23Z","title":"RobustGEC: Robust Grammatical Error Correction Against Subtle Context\n  Perturbation","summary":"  Grammatical Error Correction (GEC) systems play a vital role in assisting\npeople with their daily writing tasks. However, users may sometimes come across\na GEC system that initially performs well but fails to correct errors when the\ninputs are slightly modified. To ensure an ideal user experience, a reliable\nGEC system should have the ability to provide consistent and accurate\nsuggestions when encountering irrelevant context perturbations, which we refer\nto as context robustness. In this paper, we introduce RobustGEC, a benchmark\ndesigned to evaluate the context robustness of GEC systems. RobustGEC comprises\n5,000 GEC cases, each with one original error-correct sentence pair and five\nvariants carefully devised by human annotators. Utilizing RobustGEC, we reveal\nthat state-of-the-art GEC systems still lack sufficient robustness against\ncontext perturbations. In addition, we propose a simple yet effective method\nfor remitting this issue.\n","authors":["Yue Zhang","Leyang Cui","Enbo Zhao","Wei Bi","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2310.07299v1.pdf","comment":"Accepted to EMNLP 2023 (main conference, long paper)"},{"id":"http://arxiv.org/abs/2310.07289v1","updated":"2023-10-11T08:22:37Z","published":"2023-10-11T08:22:37Z","title":"Beyond Factuality: A Comprehensive Evaluation of Large Language Models\n  as Knowledge Generators","summary":"  Large language models (LLMs) outperform information retrieval techniques for\ndownstream knowledge-intensive tasks when being prompted to generate world\nknowledge. However, community concerns abound regarding the factuality and\npotential implications of using this uncensored knowledge. In light of this, we\nintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to\nsystematically and automatically evaluate generated knowledge from six\nimportant perspectives -- Factuality, Relevance, Coherence, Informativeness,\nHelpfulness and Validity. We conduct an extensive empirical analysis of the\ngenerated knowledge from three different types of LLMs on two widely studied\nknowledge-intensive tasks, i.e., open-domain question answering and\nknowledge-grounded dialogue. Surprisingly, our study reveals that the\nfactuality of generated knowledge, even if lower, does not significantly hinder\ndownstream tasks. Instead, the relevance and coherence of the outputs are more\nimportant than small factual mistakes. Further, we show how to use CONNER to\nimprove knowledge-intensive tasks by designing two strategies: Prompt\nEngineering and Knowledge Selection. Our evaluation code and LLM-generated\nknowledge with human annotations will be released to facilitate future\nresearch.\n","authors":["Liang Chen","Yang Deng","Yatao Bian","Zeyu Qin","Bingzhe Wu","Tat-Seng Chua","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2310.07289v1.pdf","comment":"Accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.07284v1","updated":"2023-10-11T08:17:54Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\npotential variation or even absence of pre-registered cues. To address this\nlimitation, this study investigates the integration of natural language to\nenhance the flexibility and controllability of existing TSE models.\nSpecifically, we propose a model named LLM-TSE, wherein a large language model\n(LLM) to extract useful semantic cues from the user's typed text input, which\ncan complement the pre-registered cues or work independently to control the TSE\nprocess. Our experimental results demonstrate competitive performance when only\ntext-based cues are presented, and a new state-of-the-art is set when combined\nwith pre-registered acoustic cues. To the best of our knowledge, this is the\nfirst work that has successfully incorporated text-based cues to guide target\nspeaker extraction, which can be a cornerstone for cocktail party problem\nresearch.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07282v1","updated":"2023-10-11T08:16:35Z","published":"2023-10-11T08:16:35Z","title":"An Analysis on Large Language Models in Healthcare: A Case Study of\n  BioBERT","summary":"  This paper conducts a comprehensive investigation into applying large\nlanguage models, particularly on BioBERT, in healthcare. It begins with\nthoroughly examining previous natural language processing (NLP) approaches in\nhealthcare, shedding light on the limitations and challenges these methods\nface. Following that, this research explores the path that led to the\nincorporation of BioBERT into healthcare applications, highlighting its\nsuitability for addressing the specific requirements of tasks related to\nbiomedical text mining. The analysis outlines a systematic methodology for\nfine-tuning BioBERT to meet the unique needs of the healthcare domain. This\napproach includes various components, including the gathering of data from a\nwide range of healthcare sources, data annotation for tasks like identifying\nmedical entities and categorizing them, and the application of specialized\npreprocessing techniques tailored to handle the complexities found in\nbiomedical texts. Additionally, the paper covers aspects related to model\nevaluation, with a focus on healthcare benchmarks and functions like processing\nof natural language in biomedical, question-answering, clinical document\nclassification, and medical entity recognition. It explores techniques to\nimprove the model's interpretability and validates its performance compared to\nexisting healthcare-focused language models. The paper thoroughly examines\nethical considerations, particularly patient privacy and data security. It\nhighlights the benefits of incorporating BioBERT into healthcare contexts,\nincluding enhanced clinical decision support and more efficient information\nretrieval. Nevertheless, it acknowledges the impediments and complexities of\nthis integration, encompassing concerns regarding data privacy, transparency,\nresource-intensive requirements, and the necessity for model customization to\nalign with diverse healthcare domains.\n","authors":["Shyni Sharaf","V. S. Anoop"],"pdf_url":"https://arxiv.org/pdf/2310.07282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03347v4","updated":"2023-10-11T08:13:28Z","published":"2023-04-06T19:53:59Z","title":"Towards Interpretable Mental Health Analysis with Large Language Models","summary":"  The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.\n","authors":["Kailai Yang","Shaoxiong Ji","Tianlin Zhang","Qianqian Xie","Ziyan Kuang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2304.03347v4.pdf","comment":"Accepted by EMNLP 2023 main conference as a long paper"},{"id":"http://arxiv.org/abs/2310.07279v1","updated":"2023-10-11T08:07:22Z","published":"2023-10-11T08:07:22Z","title":"Enhancing expressivity transfer in textless speech-to-speech translation","summary":"  Textless speech-to-speech translation systems are rapidly advancing, thanks\nto the integration of self-supervised learning techniques. However, existing\nstate-of-the-art systems fall short when it comes to capturing and transferring\nexpressivity accurately across different languages. Expressivity plays a vital\nrole in conveying emotions, nuances, and cultural subtleties, thereby enhancing\ncommunication across diverse languages. To address this issue this study\npresents a novel method that operates at the discrete speech unit level and\nleverages multilingual emotion embeddings to capture language-agnostic\ninformation. Specifically, we demonstrate how these embeddings can be used to\neffectively predict the pitch and duration of speech units in the target\nlanguage. Through objective and subjective experiments conducted on a\nFrench-to-English translation task, our findings highlight the superior\nexpressivity transfer achieved by our approach compared to current\nstate-of-the-art systems.\n","authors":["Jarod Duret","Benjamin O'Brien","Yannick Estve","Titouan Parcollet"],"pdf_url":"https://arxiv.org/pdf/2310.07279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07276v1","updated":"2023-10-11T07:57:08Z","published":"2023-10-11T07:57:08Z","title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical\n  Knowledge and Natural Language Associations","summary":"  Recent advancements in biological research leverage the integration of\nmolecules, proteins, and natural language to enhance drug discovery. However,\ncurrent models exhibit several limitations, such as the generation of invalid\nmolecular SMILES, underutilization of contextual information, and equal\ntreatment of structured and unstructured knowledge. To address these issues, we\npropose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches\ncross-modal integration in biology with chemical knowledge and natural language\nassociations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular\nrepresentations and extracts knowledge from the surrounding context of\nbio-entities in unstructured biological literature. Furthermore,\n$\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,\nleading to more effective utilization of information. After fine-tuning, BioT5\nshows superior performance across a wide range of tasks, demonstrating its\nstrong capability of capturing underlying relations and properties of\nbio-entities. Our code is available at\n$\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n","authors":["Qizhi Pei","Wei Zhang","Jinhua Zhu","Kehan Wu","Kaiyuan Gao","Lijun Wu","Yingce Xia","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2310.07276v1.pdf","comment":"Empirical Methods in Natural Language Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2310.07251v1","updated":"2023-10-11T07:27:34Z","published":"2023-10-11T07:27:34Z","title":"Ethical Reasoning over Moral Alignment: A Case and Framework for\n  In-Context Ethical Policies in LLMs","summary":"  In this position paper, we argue that instead of morally aligning LLMs to\nspecific set of ethical principles, we should infuse generic ethical reasoning\ncapabilities into them so that they can handle value pluralism at a global\nscale. When provided with an ethical policy, an LLM should be capable of making\ndecisions that are ethically consistent to the policy. We develop a framework\nthat integrates moral dilemmas with moral principles pertaining to different\nforamlisms of normative ethics, and at different levels of abstractions.\nInitial experiments with GPT-x models shows that while GPT-4 is a nearly\nperfect ethical reasoner, the models still have bias towards the moral values\nof Western and English speaking societies.\n","authors":["Abhinav Rao","Aditi Khandelwal","Kumar Tanmay","Utkarsh Agarwal","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2310.07251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06450v2","updated":"2023-10-11T07:04:04Z","published":"2023-10-10T09:20:14Z","title":"Constructive Large Language Models Alignment with Diverse Feedback","summary":"  In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.\n","authors":["Tianshu Yu","Ting-En Lin","Yuchuan Wu","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.06450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07225v1","updated":"2023-10-11T06:26:19Z","published":"2023-10-11T06:26:19Z","title":"Exploring the Landscape of Large Language Models In Medical Question\n  Answering: Observations and Open Questions","summary":"  Large Language Models (LLMs) have shown promise in medical question answering\nby achieving passing scores in standardised exams and have been suggested as\ntools for supporting healthcare workers. Deploying LLMs into such a high-risk\ncontext requires a clear understanding of the limitations of these models. With\nthe rapid development and release of new LLMs, it is especially valuable to\nidentify patterns which exist across models and may, therefore, continue to\nappear in newer versions. In this paper, we evaluate a wide range of popular\nLLMs on their knowledge of medical questions in order to better understand\ntheir properties as a group. From this comparison, we provide preliminary\nobservations and raise open questions for further research.\n","authors":["Karolina Korgul","Andrew M. Bean","Felix Krones","Robert McCraith","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2310.07225v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.05028v3","updated":"2023-10-11T06:16:30Z","published":"2023-10-08T06:17:39Z","title":"Revisiting Large Language Models as Zero-shot Relation Extractors","summary":"  Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.\n","authors":["Guozheng Li","Peng Wang","Wenjun Ke"],"pdf_url":"https://arxiv.org/pdf/2310.05028v3.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2303.08518v3","updated":"2023-10-11T05:40:41Z","published":"2023-03-15T10:53:49Z","title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation","summary":"  Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Junyu Bi","Yuefeng Zhan","Jianfeng Liu","Yujing Wang","Hao Sun","Furu Wei","Denvy Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08518v3.pdf","comment":"EMNLP 2023 Main Conference"},{"id":"http://arxiv.org/abs/2305.16340v2","updated":"2023-10-11T05:32:13Z","published":"2023-05-24T03:47:22Z","title":"Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model","summary":"  Transformers have shown dominant performance across a range of domains\nincluding language and vision. However, their computational cost grows\nquadratically with the sequence length, making their usage prohibitive for\nresource-constrained applications. To counter this, our approach is to divide\nthe whole sequence into segments and use local attention mechanism on the\nindividual segments. We propose a segmented recurrent transformer (SRformer)\nthat combines segmented (local) attention with recurrent attention. The loss\ncaused by reducing the attention window length is compensated by aggregating\ninformation across segments with recurrent attention. SRformer leverages\nRecurrent Accumulate-and-Fire (RAF) neurons' inherent memory to update the\ncumulative product of keys and values. The segmented attention and lightweight\nRAF neurons ensure the efficiency of the proposed transformer. Such an approach\nleads to models with sequential processing capability at a lower\ncomputation/memory cost. We apply the proposed method to T5 and BART\ntransformers. The modified models are tested on summarization datasets\nincluding CNN-dailymail, XSUM, ArXiv, and MediaSUM. Notably, using segmented\ninputs of varied sizes, the proposed model achieves $6-22\\%$ higher ROUGE1\nscores than a segmented transformer and outperforms other recurrent transformer\napproaches. Furthermore, compared to full attention, the proposed model reduces\nthe computational complexity of cross attention by around $40\\%$.\n","authors":["Yinghan Long","Sayeed Shafayet Chowdhury","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2305.16340v2.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.14251v2","updated":"2023-10-11T05:27:50Z","published":"2023-05-23T17:06:00Z","title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long\n  Form Text Generation","summary":"  Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI -- and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via `pip install factscore`.\n","authors":["Sewon Min","Kalpesh Krishna","Xinxi Lyu","Mike Lewis","Wen-tau Yih","Pang Wei Koh","Mohit Iyyer","Luke Zettlemoyer","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2305.14251v2.pdf","comment":"25 pages; 7 figures. Published as a main conference paper at EMNLP\n  2023. Code available at https://github.com/shmsw25/FActScore"},{"id":"http://arxiv.org/abs/2204.07994v2","updated":"2023-10-11T05:12:10Z","published":"2022-04-17T12:33:34Z","title":"Knowledgeable Salient Span Mask for Enhancing Language Models as\n  Knowledge Base","summary":"  Pre-trained language models (PLMs) like BERT have made significant progress\nin various downstream NLP tasks. However, by asking models to do cloze-style\ntests, recent work finds that PLMs are short in acquiring knowledge from\nunstructured text. To understand the internal behaviour of PLMs in retrieving\nknowledge, we first define knowledge-baring (K-B) tokens and knowledge-free\n(K-F) tokens for unstructured text and ask professional annotators to label\nsome samples manually. Then, we find that PLMs are more likely to give wrong\npredictions on K-B tokens and attend less attention to those tokens inside the\nself-attention module. Based on these observations, we develop two solutions to\nhelp the model learn more knowledge from unstructured text in a fully\nself-supervised manner. Experiments on knowledge-intensive tasks show the\neffectiveness of the proposed methods. To our best knowledge, we are the first\nto explore fully self-supervised learning of knowledge in continual\npre-training.\n","authors":["Cunxiang Wang","Fuli Luo","Yanyang Li","Runxin Xu","Fei Huang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2204.07994v2.pdf","comment":"NLPCC-2023"},{"id":"http://arxiv.org/abs/2309.17421v2","updated":"2023-10-11T05:07:37Z","published":"2023-09-29T17:34:51Z","title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)","summary":"  Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n","authors":["Zhengyuan Yang","Linjie Li","Kevin Lin","Jianfeng Wang","Chung-Ching Lin","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2309.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06200v2","updated":"2023-10-11T05:00:10Z","published":"2023-10-09T23:02:07Z","title":"The Importance of Prompt Tuning for Automated Neuron Explanations","summary":"  Recent advances have greatly increased the capabilities of large language\nmodels (LLMs), but our understanding of the models and their safety has not\nprogressed as fast. In this paper we aim to understand LLMs deeper by studying\ntheir individual neurons. We build upon previous work showing large language\nmodels such as GPT-4 can be useful in explaining what each neuron in a language\nmodel does. Specifically, we analyze the effect of the prompt used to generate\nexplanations and show that reformatting the explanation prompt in a more\nnatural way can significantly improve neuron explanation quality and greatly\nreduce computational cost. We demonstrate the effects of our new prompts in\nthree different ways, incorporating both automated and human evaluations.\n","authors":["Justin Lee","Tuomas Oikarinen","Arjun Chatha","Keng-Chi Chang","Yilan Chen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2310.06200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07188v1","updated":"2023-10-11T04:30:18Z","published":"2023-10-11T04:30:18Z","title":"Adaptive Gating in Mixture-of-Experts based Language Models","summary":"  Large language models, such as OpenAI's ChatGPT, have demonstrated\nexceptional language understanding capabilities in various NLP tasks. Sparsely\nactivated mixture-of-experts (MoE) has emerged as a promising solution for\nscaling models while maintaining a constant number of computational operations.\nExisting MoE model adopts a fixed gating network where each token is computed\nby the same number of experts. However, this approach contradicts our intuition\nthat the tokens in each sequence vary in terms of their linguistic complexity\nand, consequently, require different computational costs. Little is discussed\nin prior research on the trade-off between computation per token and model\nperformance. This paper introduces adaptive gating in MoE, a flexible training\nstrategy that allows tokens to be processed by a variable number of experts\nbased on expert probability distribution. The proposed framework preserves\nsparsity while improving training efficiency. Additionally, curriculum learning\nis leveraged to further reduce training time. Extensive experiments on diverse\nNLP tasks show that adaptive gating reduces at most 22.5% training time while\nmaintaining inference quality. Moreover, we conduct a comprehensive analysis of\nthe routing decisions and present our insights when adaptive gating is used.\n","authors":["Jiamin Li","Qiang Su","Yitao Yang","Yimin Jiang","Cong Wang","Hong Xu"],"pdf_url":"https://arxiv.org/pdf/2310.07188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12920v4","updated":"2023-10-11T04:08:26Z","published":"2023-01-30T14:19:29Z","title":"Active Learning for Multilingual Semantic Parser","summary":"  Current multilingual semantic parsing (MSP) datasets are almost all collected\nby translating the utterances in the existing datasets from the resource-rich\nlanguage to the target language. However, manual translation is costly. To\nreduce the translation effort, this paper proposes the first active learning\nprocedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing\ndatasets to be translated. We also propose a novel selection method that\nprioritizes the examples diversifying the logical form structures with more\nlexical choices, and a novel hyperparameter tuning method that needs no extra\nannotation cost. Our experiments show that AL-MSP significantly reduces\ntranslation costs with ideal selection methods. Our selection method with\nproper hyperparameters yields better parsing performance than the other\nbaselines on two multilingual datasets.\n","authors":["Zhuang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2301.12920v4.pdf","comment":"EACL 2023 (findings)"},{"id":"http://arxiv.org/abs/2310.07177v1","updated":"2023-10-11T04:03:42Z","published":"2023-10-11T04:03:42Z","title":"Online Speculative Decoding","summary":"  Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.\n","authors":["Xiaoxuan Liu","Lanxiang Hu","Peter Bailis","Ion Stoica","Zhijie Deng","Alvin Cheung","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07170v1","updated":"2023-10-11T03:39:46Z","published":"2023-10-11T03:39:46Z","title":"PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a\n  Language Model","summary":"  Despite the remarkable progress in natural language understanding with\npretrained Transformers, neural language models often do not handle commonsense\nknowledge well. Toward commonsense-aware models, there have been attempts to\nobtain knowledge, ranging from automatic acquisition to crowdsourcing. However,\nit is difficult to obtain a high-quality knowledge base at a low cost,\nespecially from scratch. In this paper, we propose PHALM, a method of building\na knowledge graph from scratch, by prompting both crowdworkers and a large\nlanguage model (LLM). We used this method to build a Japanese event knowledge\ngraph and trained Japanese commonsense generation models. Experimental results\nrevealed the acceptability of the built graph and inferences generated by the\ntrained models. We also report the difference in prompting humans and an LLM.\nOur code, data, and models are available at\ngithub.com/nlp-waseda/comet-atomic-ja.\n","authors":["Tatsuya Ide","Eiki Murata","Daisuke Kawahara","Takato Yamazaki","Shengzhe Li","Kenta Shinzato","Toshinori Sato"],"pdf_url":"https://arxiv.org/pdf/2310.07170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15074v2","updated":"2023-10-11T03:38:56Z","published":"2023-05-24T11:55:59Z","title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models","summary":"  The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40\\%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.\n","authors":["Daman Arora","Himanshu Gaurav Singh"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2305.15074v2.pdf","comment":"v2"},{"id":"http://arxiv.org/abs/2310.07161v1","updated":"2023-10-11T03:19:22Z","published":"2023-10-11T03:19:22Z","title":"Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms","summary":"  Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,\nthe complexities introduced by acoustic transformations merit rigorous\nanalysis. This research, rooted in the exploration of proprietary sender-side\ndenoising effects, meticulously evaluates platforms such as Google Meets and\nZoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,\nensuring a structured examination tailored to various denoising settings and\nreceiver interfaces. A methodological novelty is introduced via the Oaxaca\ndecomposition, traditionally an econometric tool, repurposed herein to analyze\nacoustic-phonetic perturbations within VoIP systems. To further ground the\nimplications of these transformations, psychoacoustic metrics, specifically\nPESQ and STOI, were harnessed to furnish a comprehensive understanding of\nspeech alterations. Cumulatively, the insights garnered underscore the\nintricate landscape of VoIP-influenced acoustic dynamics. In addition to the\nprimary findings, a multitude of metrics are reported, extending the research\npurview. Moreover, out-of-domain benchmarking for both time and time-frequency\ndomain speech enhancement models is included, thereby enhancing the depth and\napplicability of this inquiry.\n","authors":["Joseph Konan","Ojas Bhargave","Shikhar Agnihotri","Shuo Han","Yunyang Zeng","Ankit Shah","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2310.07161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02796v2","updated":"2023-10-11T03:16:23Z","published":"2023-07-06T06:11:51Z","title":"VerifAI: Verified Generative AI","summary":"  Generative AI has made significant strides, yet concerns about the accuracy\nand reliability of its outputs continue to grow. Such inaccuracies can have\nserious consequences such as inaccurate decision-making, the spread of false\ninformation, privacy violations, legal liabilities, and more. Although efforts\nto address these risks are underway, including explainable AI and responsible\nAI practices such as transparency, privacy protection, bias mitigation, and\nsocial and environmental responsibility, misinformation caused by generative AI\nwill remain a significant challenge. We propose that verifying the outputs of\ngenerative AI from a data management perspective is an emerging issue for\ngenerative AI. This involves analyzing the underlying data from multi-modal\ndata lakes, including text files, tables, and knowledge graphs, and assessing\nits quality and consistency. By doing so, we can establish a stronger\nfoundation for evaluating the outputs of generative AI models. Such an approach\ncan ensure the correctness of generative AI, promote transparency, and enable\ndecision-making with greater confidence. Our vision is to promote the\ndevelopment of verifiable generative AI and contribute to a more trustworthy\nand responsible use of AI.\n","authors":["Nan Tang","Chenyu Yang","Ju Fan","Lei Cao","Yuyu Luo","Alon Halevy"],"pdf_url":"https://arxiv.org/pdf/2307.02796v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07155v1","updated":"2023-10-11T03:01:42Z","published":"2023-10-11T03:01:42Z","title":"\"A Tale of Two Movements\": Identifying and Comparing Perspectives in\n  #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly\n  Supervised Graph-based Structured Prediction","summary":"  Social media has become a major driver of social change, by facilitating the\nformation of online social movements. Automatically understanding the\nperspectives driving the movement and the voices opposing it, is a challenging\ntask as annotated data is difficult to obtain. We propose a weakly supervised\ngraph-based approach that explicitly models perspectives in\n#BackLivesMatter-related tweets. Our proposed approach utilizes a\nsocial-linguistic representation of the data. We convert the text to a graph by\nbreaking it into structured elements and connect it with the social network of\nauthors, then structured prediction is done over the elements for identifying\nperspectives. Our approach uses a small seed set of labeled examples. We\nexperiment with large language models for generating artificial training\nexamples, compare them to manual annotation, and find that it achieves\ncomparable performance. We perform quantitative and qualitative analyses using\na human-annotated test set. Our model outperforms multitask baselines by a\nlarge margin, successfully characterizing the perspectives supporting and\nopposing #BLM.\n","authors":["Shamik Roy","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2310.07155v1.pdf","comment":"Accepted version to Findings of EMNLP 2023 (camera ready coming soon)"},{"id":"http://arxiv.org/abs/2310.07147v1","updated":"2023-10-11T02:47:40Z","published":"2023-10-11T02:47:40Z","title":"QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources","summary":"  Large Language Models (LLMs) have showcased remarkable impacts across a wide\nspectrum of natural language processing tasks. Fine-tuning these pre-trained\nmodels on downstream datasets provides further significant performance gains,\nbut this process has been challenging due to its extraordinary resource\nrequirements. To this end, existing efforts focus on parameter-efficient\nfine-tuning, which, unfortunately, fail to capitalize on the powerful potential\nof full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized\nFull-parameter Tuning framework for LLMs that enables memory-efficient\nfine-tuning without harming performance. Our framework incorporates two novel\nideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the\nmomentum and has consistent update magnitudes for each parameter, an inherent\nadvantage for robust quantization; and (ii) we quantize all model states and\nstore them as integer values, and present a gradient flow and parameter update\nscheme for the quantized weights. As a result, QFT reduces the model state\nmemory to 21% of the standard solution while achieving comparable performance,\ne.g., tuning a LLaMA-7B model requires only <30GB of memory, satisfied by a\nsingle A6000 GPU.\n","authors":["Zhikai Li","Xiaoxuan Liu","Banghua Zhu","Zhen Dong","Qingyi Gu","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2310.07147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07146v1","updated":"2023-10-11T02:47:21Z","published":"2023-10-11T02:47:21Z","title":"Empowering Psychotherapy with Large Language Models: Cognitive\n  Distortion Detection through Diagnosis of Thought Prompting","summary":"  Mental illness remains one of the most critical public health issues of our\ntime, due to the severe scarcity and accessibility limit of professionals.\nPsychotherapy requires high-level expertise to conduct deep, complex reasoning\nand analysis on the cognition modeling of the patients. In the era of Large\nLanguage Models, we believe it is the right time to develop AI assistance for\ncomputational psychotherapy. We study the task of cognitive distortion\ndetection and propose the Diagnosis of Thought (DoT) prompting. DoT performs\ndiagnosis on the patient's speech via three stages: subjectivity assessment to\nseparate the facts and the thoughts; contrastive reasoning to elicit the\nreasoning processes supporting and contradicting the thoughts; and schema\nanalysis to summarize the cognition schemas. The generated diagnosis rationales\nthrough the three stages are essential for assisting the professionals.\nExperiments demonstrate that DoT obtains significant improvements over ChatGPT\nfor cognitive distortion detection, while generating high-quality rationales\napproved by human experts.\n","authors":["Zhiyu Chen","Yujie Lu","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07146v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.05280v2","updated":"2023-10-11T02:32:10Z","published":"2023-10-08T21:03:18Z","title":"Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona\n  Biases in Dialogue Systems","summary":"  Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. Generic personas refer to an individual from a demographic group\n(e.g. an Asian person), whereas specific personas can be actual names of\nhistorical figures. While the adoption of personas allows dialogue systems to\nbe more engaging and approachable to users, it also carries the potential risk\nof exacerbating social biases in model responses, further causing societal\nharms through interactions with users. In this paper, we systematically study\n\"persona biases\", which we define to be the sensitivity of harmful dialogue\nmodel behaviors to different persona adoptions. We categorize persona biases\ninto biases in harmful expression and harmful agreement, as well as establish a\ncomprehensive evaluation framework to measure persona biases in five aspects:\nOffensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic\nAgreement. Additionally, we propose to comprehensively investigate persona\nbiases through experimenting with UniversalPersona, a systematized persona\ndataset with a comprehensive list of both generic and specific model personas.\nThrough benchmarking on four different models, including Blender, ChatGPT,\nAlpaca, and Vicuna, our study uncovers significant persona biases in these\ndialogue systems.Findings of our study underscores the immediate need to\nrevisit the use of persona traits in dialogue agents, to ensure their safe\napplication.\n","authors":["Yixin Wan","Jieyu Zhao","Aman Chadha","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2310.05280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07137v1","updated":"2023-10-11T02:22:28Z","published":"2023-10-11T02:22:28Z","title":"AE-smnsMLC: Multi-Label Classification with Semantic Matching and\n  Negative Label Sampling for Product Attribute Value Extraction","summary":"  Product attribute value extraction plays an important role for many\nreal-world applications in e-Commerce such as product search and\nrecommendation. Previous methods treat it as a sequence labeling task that\nneeds more annotation for position of values in the product text. This limits\ntheir application to real-world scenario in which only attribute values are\nweakly-annotated for each product without their position. Moreover, these\nmethods only use product text (i.e., product title and description) and do not\nconsider the semantic connection between the multiple attribute values of a\ngiven product and its text, which can help attribute value extraction. In this\npaper, we reformulate this task as a multi-label classification task that can\nbe applied for real-world scenario in which only annotation of attribute values\nis available to train models (i.e., annotation of positional information of\nattribute values is not available). We propose a classification model with\nsemantic matching and negative label sampling for attribute value extraction.\nSemantic matching aims to capture semantic interactions between attribute\nvalues of a given product and its text. Negative label sampling aims to enhance\nthe model's ability of distinguishing similar values belonging to the same\nattribute. Experimental results on three subsets of a large real-world\ne-Commerce dataset demonstrate the effectiveness and superiority of our\nproposed model.\n","authors":["Zhongfen Deng","Wei-Te Chen","Lei Chen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2310.07137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07135v1","updated":"2023-10-11T02:16:12Z","published":"2023-10-11T02:16:12Z","title":"Comparing Styles across Languages","summary":"  Understanding how styles differ across languages is advantageous for training\nboth humans and computers to generate culturally appropriate text. We introduce\nan explanation framework to extract stylistic differences from multilingual LMs\nand compare styles across languages. Our framework (1) generates comprehensive\nstyle lexica in any language and (2) consolidates feature importances from LMs\ninto comparable lexical categories. We apply this framework to compare\npoliteness, creating the first holistic multilingual politeness dataset and\nexploring how politeness varies across four languages. Our approach enables an\neffective evaluation of how distinct linguistic categories contribute to\nstylistic variations and provides interpretable insights into how people\ncommunicate differently around the world.\n","authors":["Shreya Havaldar","Matthew Pressimone","Eric Wong","Lyle Ungar"],"pdf_url":"https://arxiv.org/pdf/2310.07135v1.pdf","comment":"To appear in EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07106v1","updated":"2023-10-11T01:03:42Z","published":"2023-10-11T01:03:42Z","title":"The Temporal Structure of Language Processing in the Human Brain\n  Corresponds to The Layered Hierarchy of Deep Language Models","summary":"  Deep Language Models (DLMs) provide a novel computational paradigm for\nunderstanding the mechanisms of natural language processing in the human brain.\nUnlike traditional psycholinguistic models, DLMs use layered sequences of\ncontinuous numerical vectors to represent words and context, allowing a\nplethora of emerging applications such as human-like text generation. In this\npaper we show evidence that the layered hierarchy of DLMs may be used to model\nthe temporal dynamics of language comprehension in the brain by demonstrating a\nstrong correlation between DLM layer depth and the time at which layers are\nmost predictive of the human brain. Our ability to temporally resolve\nindividual layers benefits from our use of electrocorticography (ECoG) data,\nwhich has a much higher temporal resolution than noninvasive methods like fMRI.\nUsing ECoG, we record neural activity from participants listening to a\n30-minute narrative while also feeding the same narrative to a high-performing\nDLM (GPT2-XL). We then extract contextual embeddings from the different layers\nof the DLM and use linear encoding models to predict neural activity. We first\nfocus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our\nmodel to track the increasing temporal receptive window along the linguistic\nprocessing hierarchy from auditory to syntactic and semantic areas. Our results\nreveal a connection between human language processing and DLMs, with the DLM's\nlayer-by-layer accumulation of contextual information mirroring the timing of\nneural activity in high-order language areas.\n","authors":["Ariel Goldstein","Eric Ham","Mariano Schain","Samuel Nastase","Zaid Zada","Avigail Dabush","Bobbi Aubrey","Harshvardhan Gazula","Amir Feder","Werner K Doyle","Sasha Devore","Patricia Dugan","Daniel Friedman","Roi Reichart","Michael Brenner","Avinatan Hassidim","Orrin Devinsky","Adeen Flinker","Omer Levy","Uri Hasson"],"pdf_url":"https://arxiv.org/pdf/2310.07106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07096v1","updated":"2023-10-11T00:38:57Z","published":"2023-10-11T00:38:57Z","title":"Sparse Universal Transformer","summary":"  The Universal Transformer (UT) is a variant of the Transformer that shares\nparameters across its layers. Empirical evidence shows that UTs have better\ncompositional generalization than Vanilla Transformers (VTs) in formal language\ntasks. The parameter-sharing also affords it better parameter efficiency than\nVTs. Despite its many advantages, scaling UT parameters is much more compute\nand memory intensive than scaling up a VT. This paper proposes the Sparse\nUniversal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE)\nand a new stick-breaking-based dynamic halting mechanism to reduce UT's\ncomputation complexity while retaining its parameter efficiency and\ngeneralization ability. Experiments show that SUT achieves the same performance\nas strong baseline models while only using half computation and parameters on\nWMT'14 and strong generalization results on formal language tasks (Logical\ninference and CFQ). The new halting mechanism also enables around 50\\%\nreduction in computation during inference with very little performance decrease\non formal language tasks.\n","authors":["Shawn Tan","Yikang Shen","Zhenfang Chen","Aaron Courville","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2310.07096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09442v3","updated":"2023-10-11T00:37:33Z","published":"2023-06-15T18:49:50Z","title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch","summary":"  Deploying large language models (LMs) can pose hazards from harmful outputs\nsuch as toxic or false text. Prior work has introduced automated tools that\nelicit harmful outputs to identify these risks. While this is a valuable step\ntoward securing models, these approaches rely on a pre-existing way to\nefficiently classify undesirable outputs. Using a pre-existing classifier does\nnot allow for red-teaming to be tailored to the target model. Furthermore, when\nfailures can be easily classified in advance, red-teaming has limited marginal\nvalue because problems can be avoided by simply filtering training data and/or\nmodel outputs. Here, we consider red-teaming \"from scratch,\" in which the\nadversary does not begin with a way to classify failures. Our framework\nconsists of three steps: 1) Exploring the model's range of behaviors in the\ndesired context; 2) Establishing a definition and measurement for undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure to develop diverse adversarial\nprompts. We use this approach to red-team GPT-3 to discover classes of inputs\nthat elicit false statements. In doing so, we construct the CommonClaim dataset\nof 20,000 statements labeled by humans as common-knowledge-true, common\nknowledge-false, or neither. We are making code and data available.\n","authors":["Stephen Casper","Jason Lin","Joe Kwon","Gatlen Culp","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2306.09442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07093v1","updated":"2023-10-11T00:18:29Z","published":"2023-10-11T00:18:29Z","title":"Argumentative Stance Prediction: An Exploratory Study on Multimodality\n  and Few-Shot Learning","summary":"  To advance argumentative stance prediction as a multimodal problem, the First\nShared Task in Multimodal Argument Mining hosted stance prediction in crucial\nsocial topics of gun control and abortion. Our exploratory study attempts to\nevaluate the necessity of images for stance prediction in tweets and compare\nout-of-the-box text-based large-language models (LLM) in few-shot settings\nagainst fine-tuned unimodal and multimodal models. Our work suggests an\nensemble of fine-tuned text-based language models (0.817 F1-score) outperforms\nboth the multimodal (0.677 F1-score) and text-based few-shot prediction using a\nrecent state-of-the-art LLM (0.550 F1-score). In addition to the differences in\nperformance, our findings suggest that the multimodal models tend to perform\nbetter when image content is summarized as natural language over their native\npixel structure and, using in-context examples improves few-shot performance of\nLLMs.\n","authors":["Arushi Sharma","Abhibha Gupta","Maneesh Bilalpur"],"pdf_url":"https://arxiv.org/pdf/2310.07093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07091v1","updated":"2023-10-11T00:14:40Z","published":"2023-10-11T00:14:40Z","title":"Jaeger: A Concatenation-Based Multi-Transformer VQA Model","summary":"  Document-based Visual Question Answering poses a challenging task between\nlinguistic sense disambiguation and fine-grained multimodal retrieval. Although\nthere has been encouraging progress in document-based question answering due to\nthe utilization of large language and open-world prior models\\cite{1}, several\nchallenges persist, including prolonged response times, extended inference\ndurations, and imprecision in matching. In order to overcome these challenges,\nwe propose Jaegar, a concatenation-based multi-transformer VQA model. To derive\nquestion features, we leverage the exceptional capabilities of RoBERTa\nlarge\\cite{2} and GPT2-xl\\cite{3} as feature extractors. Subsequently, we\nsubject the outputs from both models to a concatenation process. This operation\nallows the model to consider information from diverse sources concurrently,\nstrengthening its representational capability. By leveraging pre-trained models\nfor feature extraction, our approach has the potential to amplify the\nperformance of these models through concatenation. After concatenation, we\napply dimensionality reduction to the output features, reducing the model's\ncomputational effectiveness and inference time. Empirical results demonstrate\nthat our proposed model achieves competitive performance on Task C of the\nPDF-VQA Dataset. If the user adds any new data, they should make sure to style\nit as per the instructions provided in previous sections.\n","authors":["Jieting Long","Zewei Shi","Penghao Jiang","Yidong Gan"],"pdf_url":"https://arxiv.org/pdf/2310.07091v1.pdf","comment":"This paper is the technical research paper of CIKM 2023 DocIU\n  challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored\n  by Google, Microsoft, and the Centre for data-driven geoscience"},{"id":"http://arxiv.org/abs/2310.07088v1","updated":"2023-10-11T00:01:41Z","published":"2023-10-11T00:01:41Z","title":"Diversity of Thought Improves Reasoning Abilities of Large Language\n  Models","summary":"  Large language models (LLMs) are documented to struggle in settings that\nrequire complex reasoning. Nevertheless, instructing the model to break down\nthe problem into smaller reasoning steps (Wei et al., 2022), or ensembling\nvarious generations through modifying decoding steps (Wang et al., 2023) boosts\nperformance. Current methods assume that the input prompt is fixed and expect\nthe decoding strategies to introduce the diversity needed for ensembling. In\nthis work, we relax this assumption and discuss how one can create and leverage\nvariations of the input prompt as a means to diversity of thought to improve\nmodel performance. We propose a method that automatically improves prompt\ndiversity by soliciting feedback from the LLM to ideate approaches that fit for\nthe problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse\nreasoning path Self-Ensemble) across multiple inference calls. We also propose\na cost-effective alternative where diverse prompts are used within a single\ninference call; we call this IDIV-SE (In-call DIVerse reasoning path\nSelf-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform\nthe previously discussed baselines using both GPT-3.5 and GPT-4 on several\nreasoning benchmarks, without modifying the decoding process. Additionally,\nDIV-SE advances state-of-the-art performance on recent planning benchmarks\n(Valmeekam et al., 2023), exceeding the highest previously reported accuracy by\nat least 29.6 percentage points on the most challenging 4/5 Blocksworld task.\nOur results shed light on how to enforce prompt diversity toward LLM reasoning\nand thereby improve the pareto frontier of the accuracy-cost trade-off.\n","authors":["Ranjita Naik","Varun Chandrasekaran","Mert Yuksekgonul","Hamid Palangi","Besmira Nushi"],"pdf_url":"https://arxiv.org/pdf/2310.07088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10966v5","updated":"2023-10-11T23:59:52Z","published":"2023-09-19T23:39:07Z","title":"MBR and QE Finetuning: Training-time Distillation of the Best and Most\n  Expensive Decoding Methods","summary":"  Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that MAP decoding is not optimal, because model probabilities\ndo not always align with human preferences. Stronger decoding methods,\nincluding Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\ndecoding, have since been proposed to mitigate the model-perplexity-vs-quality\nmismatch. While these decoding methods achieve state-of-the-art performance,\nthey are prohibitively expensive to compute. In this work, we propose MBR\nfinetuning and QE finetuning which distill the quality gains from these\ndecoding methods at training time, while using an efficient decoding algorithm\nat inference time. Using the canonical NLG task of Neural Machine Translation\n(NMT), we show that even with self-training, these finetuning methods\nsignificantly outperform the base model. Moreover, when using an external LLM\nas a teacher model, these finetuning methods outperform finetuning on\nhuman-generated references. These findings suggest new ways to leverage\nmonolingual data to achieve improvements in model quality that are on par with,\nor even exceed, improvements from human-curated data, while maintaining maximum\nefficiency during decoding.\n","authors":["Mara Finkelstein","Subhajit Naskar","Mehdi Mirzazadeh","Apurva Shah","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2309.10966v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03135v3","updated":"2023-10-11T23:38:36Z","published":"2023-07-06T17:05:26Z","title":"Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability","summary":"  Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood\n","authors":["Xuanlin Li","Yunhao Fang","Minghua Liu","Zhan Ling","Zhuowen Tu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2307.03135v3.pdf","comment":"Published at International Conference on Computer Vision (ICCV) 2023.\n  Poster at\n  https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf"},{"id":"http://arxiv.org/abs/2310.07931v1","updated":"2023-10-11T23:01:29Z","published":"2023-10-11T23:01:29Z","title":"D2 Pruning: Message Passing for Balancing Diversity and Difficulty in\n  Data Pruning","summary":"  Analytical theories suggest that higher-quality data can lead to lower test\nerrors in models trained on a fixed data budget. Moreover, a model can be\ntrained on a lower compute budget without compromising performance if a dataset\ncan be stripped of its redundancies. Coreset selection (or data pruning) seeks\nto select a subset of the training data so as to maximize the performance of\nmodels trained on this subset, also referred to as coreset. There are two\ndominant approaches: (1) geometry-based data selection for maximizing data\ndiversity in the coreset, and (2) functions that assign difficulty scores to\nsamples based on training dynamics. Optimizing for data diversity leads to a\ncoreset that is biased towards easier samples, whereas, selection by difficulty\nranking omits easy samples that are necessary for the training of deep learning\nmodels. This demonstrates that data diversity and importance scores are two\ncomplementary factors that need to be jointly considered during coreset\nselection. We represent a dataset as an undirected graph and propose a novel\npruning algorithm, D2 Pruning, that uses forward and reverse message passing\nover this dataset graph for coreset selection. D2 Pruning updates the\ndifficulty scores of each example by incorporating the difficulty of its\nneighboring examples in the dataset graph. Then, these updated difficulty\nscores direct a graph-based sampling method to select a coreset that\nencapsulates both diverse and difficult regions of the dataset space. We\nevaluate supervised and self-supervised versions of our method on various\nvision and language datasets. Results show that D2 Pruning improves coreset\nselection over previous state-of-the-art methods for up to 70% pruning rates.\nAdditionally, we find that using D2 Pruning for filtering large multimodal\ndatasets leads to increased diversity in the dataset and improved\ngeneralization of pretrained models.\n","authors":["Adyasha Maharana","Prateek Yadav","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.07931v1.pdf","comment":"17 pages (Our code is available at\n  https://github.com/adymaharana/d2pruning)"},{"id":"http://arxiv.org/abs/2310.07929v1","updated":"2023-10-11T22:57:03Z","published":"2023-10-11T22:57:03Z","title":"Crosslingual Structural Priming and the Pre-Training Dynamics of\n  Bilingual Language Models","summary":"  Do multilingual language models share abstract grammatical representations\nacross languages, and if so, when do these develop? Following Sinclair et al.\n(2022), we use structural priming to test for abstract grammatical\nrepresentations with causal effects on model outputs. We extend the approach to\na Dutch-English bilingual setting, and we evaluate a Dutch-English language\nmodel during pre-training. We find that crosslingual structural priming effects\nemerge early after exposure to the second language, with less than 1M tokens of\ndata in that language. We discuss implications for data contamination,\nlow-resource transfer, and how abstract grammatical representations emerge in\nmultilingual models.\n","authors":["Catherine Arnett","Tyler A. Chang","James A. Michaelov","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2310.07929v1.pdf","comment":"Extended abstract accepted to the 3rd Multilingual Representation\n  Learning workshop at EMNLP 2023"},{"id":"http://arxiv.org/abs/2308.03853v2","updated":"2023-10-11T22:41:37Z","published":"2023-08-07T18:03:10Z","title":"Exploring zero-shot capability of large language models in inferences\n  from medical oncology notes","summary":"  Both medical care and observational studies in oncology require a thorough\nunderstanding of a patient's disease progression and treatment history, often\nelaborately documented in clinical notes. Despite their vital role, no current\noncology information representation and annotation schema fully encapsulates\nthe diversity of information recorded within these notes. Although large\nlanguage models (LLMs) have recently exhibited impressive performance on\nvarious medical natural language processing tasks, due to the current lack of\ncomprehensively annotated oncology datasets, an extensive evaluation of LLMs in\nextracting and reasoning with the complex rhetoric in oncology notes remains\nunderstudied. We developed a detailed schema for annotating textual oncology\ninformation, encompassing patient characteristics, tumor characteristics,\ntests, treatments, and temporality. Using a corpus of 40 de-identified breast\nand pancreatic cancer progress notes at University of California, San\nFrancisco, we applied this schema to assess the abilities of three\nrecently-released LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to perform\nzero-shot extraction of detailed oncological history from two narrative\nsections of clinical progress notes. Our team annotated 9028 entities, 9986\nmodifiers, and 5312 relationships. The GPT-4 model exhibited overall best\nperformance, with an average BLEU score of 0.68, an average ROUGE score of\n0.71, and an average accuracy of 67% on complex tasks (expert manual evaluation\non subset). Notably, it was proficient in tumor characteristic and medication\nextraction, and demonstrated superior performance in advanced tasks of\ninferring symptoms due to cancer and considerations of future medications.\nGPT-4 may already be usable to extract important facts from cancer progress\nnotes needed for clinical research, complex population management, and\ndocumenting quality patient care.\n","authors":["Madhumita Sushil","Vanessa E. Kennedy","Divneet Mandair","Brenda Y. Miao","Travis Zack","Atul J. Butte"],"pdf_url":"https://arxiv.org/pdf/2308.03853v2.pdf","comment":"Source code available at:\n  https://github.com/MadhumitaSushil/OncLLMExtraction"},{"id":"http://arxiv.org/abs/2310.07923v1","updated":"2023-10-11T22:35:18Z","published":"2023-10-11T22:35:18Z","title":"The Expresssive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps adds a\nclear new ability (under standard complexity conjectures): recognizing all\nregular languages. Our results also imply that linear steps keep transformer\ndecoders within context-sensitive languages, and polynomial steps make them\nrecognize exactly the class of polynomial-time solvable problems -- the first\nexact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v1.pdf","comment":"9-page preprint"},{"id":"http://arxiv.org/abs/2310.07911v1","updated":"2023-10-11T21:38:40Z","published":"2023-10-11T21:38:40Z","title":"Pit One Against Many: Leveraging Attention-head Embeddings for\n  Parameter-efficient Multi-head Attention","summary":"  Scaling pre-trained language models has resulted in large performance gains\nin various natural language processing tasks but comes with a large cost in\nmemory requirements. Inspired by the position embeddings in transformers, we\naim to simplify and reduce the memory footprint of the multi-head attention\n(MHA) mechanism. We propose an alternative module that uses only a single\nshared projection matrix and multiple head embeddings (MHE), i.e. one per head.\nWe empirically demonstrate that our MHE attention is substantially more memory\nefficient compared to alternative attention mechanisms while achieving high\npredictive performance retention ratio to vanilla MHA on several downstream\ntasks. MHE attention only requires a negligible fraction of additional\nparameters ($3nd$, where $n$ is the number of attention heads and $d$ the size\nof the head embeddings) compared to a single-head attention, while MHA requires\n$(3n^2-3n)d^2-3nd$ additional parameters.\n","authors":["Huiyin Xue","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2310.07911v1.pdf","comment":"Accepted at EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2305.09656v3","updated":"2023-10-11T21:38:22Z","published":"2023-05-16T17:55:51Z","title":"SatLM: Satisfiability-Aided Language Models Using Declarative Prompting","summary":"  Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving problems that require more sophisticated planning and\nsearch. In this paper, we propose a new satisfiability-aided language modeling\n(SatLM) approach for improving the reasoning capabilities of LLMs. We use an\nLLM to generate a declarative task specification rather than an imperative\nprogram and leverage an off-the-shelf automated theorem prover to derive the\nfinal answer. This approach has two key advantages. The declarative\nspecification is closer to the problem description than the reasoning steps\nare, so the LLM can parse it out of the description more accurately.\nFurthermore, by offloading the actual reasoning task to an automated theorem\nprover, our approach can guarantee the correctness of the answer with respect\nto the parsed specification and avoid planning errors in the solving process.\nWe evaluate SATLM on 8 different datasets and show that it consistently\noutperforms program-aided LMs in the imperative paradigm. In particular, SATLM\noutperforms program-aided LMs by 23% on a challenging subset of the GSM\narithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and\nBoardgameQA, surpassing previous models that are trained on the respective\ntraining sets.\n","authors":["Xi Ye","Qiaochu Chen","Isil Dillig","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2305.09656v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.07889v1","updated":"2023-10-11T20:52:30Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation. Our approach uses off-the-shelf vision systems\n(for image captioning and object detection) to convert an agent's egocentric\npanoramic view at each time step into natural language descriptions. We then\nfinetune a pretrained language model to select an action, based on the current\nview and the trajectory history, that would best fulfill the navigation\ninstructions. In contrast to the standard setup which adapts a pretrained\nlanguage model to work directly with continuous visual features from pretrained\nvision models, our approach instead uses (discrete) language as the perceptual\nrepresentation. We explore two use cases of our language-based navigation\n(LangNav) approach on the R2R vision-and-language navigation benchmark:\ngenerating synthetic trajectories from a prompted large language model (GPT-4)\nwith which to finetune a smaller language model; and sim-to-real transfer where\nwe transfer a policy learned on a simulated environment (ALFRED) to a\nreal-world environment (R2R). Our approach is found to improve upon strong\nbaselines that rely on visual features in settings where only a few gold\ntrajectories (10-100) are available, demonstrating the potential of using\nlanguage as a perceptual representation for navigation tasks.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07875v1","updated":"2023-10-11T20:34:42Z","published":"2023-10-11T20:34:42Z","title":"TabLib: A Dataset of 627M Tables with Context","summary":"  It is well-established that large, diverse datasets play a pivotal role in\nthe performance of modern AI systems for text and image modalities. However,\nthere are no datasets for tabular data of comparable size and diversity to\nthose available for text and images. Thus we present \"TabLib'', a compilation\nof 627 million tables totaling 69 TiB, along with 867B tokens of context.\nTabLib was extracted from numerous file formats, including CSV, HTML, SQLite,\nPDF, Excel, and others, sourced from GitHub and Common Crawl. The size and\ndiversity of TabLib offer considerable promise in the table modality,\nreminiscent of the original promise of foundational datasets for text and\nimages, such as The Pile and LAION.\n","authors":["Gus Eggert","Kevin Huo","Mike Biven","Justin Waugh"],"pdf_url":"https://arxiv.org/pdf/2310.07875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07856v1","updated":"2023-10-11T19:58:07Z","published":"2023-10-11T19:58:07Z","title":"Assessing Evaluation Metrics for Neural Test Oracle Generation","summary":"  In this work, we revisit existing oracle generation studies plus ChatGPT to\nempirically investigate the current standing of their performance in both\nNLG-based and test adequacy metrics. Specifically, we train and run four\nstate-of-the-art test oracle generation models on five NLG-based and two test\nadequacy metrics for our analysis. We apply two different correlation analyses\nbetween these two different sets of metrics. Surprisingly, we found no\nsignificant correlation between the NLG-based metrics and test adequacy\nmetrics. For instance, oracles generated from ChatGPT on the project\nactivemq-artemis had the highest performance on all the NLG-based metrics among\nthe studied NOGs, however, it had the most number of projects with a decrease\nin test adequacy metrics compared to all the studied NOGs. We further conduct a\nqualitative analysis to explore the reasons behind our observations, we found\nthat oracles with high NLG-based metrics but low test adequacy metrics tend to\nhave complex or multiple chained method invocations within the oracle's\nparameters, making it hard for the model to generate completely, affecting the\ntest adequacy metrics. On the other hand, oracles with low NLG-based metrics\nbut high test adequacy metrics tend to have to call different assertion types\nor a different method that functions similarly to the ones in the ground truth.\nOverall, this work complements prior studies on test oracle generation with an\nextensive performance evaluation with both NLG and test adequacy metrics and\nprovides guidelines for better assessment of deep learning applications in\nsoftware test generation in the future.\n","authors":["Jiho Shin","Hadi Hemmati","Moshi Wei","Song Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07856v1.pdf","comment":"10 pages + reference"},{"id":"http://arxiv.org/abs/2305.09548v2","updated":"2023-10-11T19:57:43Z","published":"2023-05-16T15:45:59Z","title":"Measuring Social Dimensions of Self-Presentation in Social Media\n  Biographies with an Identity-based Approach","summary":"  Social media users on sites like Twitter, Instagram, and Tiktok use the\nprofile description, or bio, field of user profiles to present themselves to\nthe world. In contrast to the ``offline'' world, where social context often\nencourages us to adopt a single identity, the profile description is a\nfree-text field in which users are encouraged to present the self using\nmultiple, sometimes conflicting, social identities. While sociologists, social\npsychologists, sociolinguists, and increasingly computational social\nscientists, have developed a large and growing array of methods to estimate the\nmeaning of individual social identities, little work has attended to the ways\nin which social meanings emerge from the collections of social identities\npresent in social media bios. The present work proposes and evaluate three\nnovel, identity-based methods to measure the social dimensions of meaning\nexpressed in Twitter bios. We show that these models outperform reasonable\nbaselines with respect to 1) predicting which sets of identities are more\nlikely to co-occur within a single biography and 2) quantifying perceptions of\nentire social media biographies along salient dimensions of social meaning on\nTwitter, in particular partisanship. We demonstrate the utility of our method\nin a computational social science setting by using model outputs to better\nunderstand how self presentation along dimensions of partisanship, religion,\nage, and gender are related to the sharing of URLs on Twitter from low versus\nhigh quality news sites.\n","authors":["Navid Madani","Rabiraj Bandyopadhyay","Briony Swire-Thompson","Michael Miller Yoder","Kenneth Joseph"],"pdf_url":"https://arxiv.org/pdf/2305.09548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07849v1","updated":"2023-10-11T19:51:13Z","published":"2023-10-11T19:51:13Z","title":"Synthetic Data Generation with Large Language Models for Text\n  Classification: Potential and Limitations","summary":"  The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.\n","authors":["Zhuoyan Li","Hangxiao Zhu","Zhuoran Lu","Ming Yin"],"pdf_url":"https://arxiv.org/pdf/2310.07849v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07848v1","updated":"2023-10-11T19:50:59Z","published":"2023-10-11T19:50:59Z","title":"Framework for Question-Answering in Sanskrit through Automated\n  Construction of Knowledge Graphs","summary":"  Sanskrit (sa\\d{m}sk\\d{r}ta) enjoys one of the largest and most varied\nliterature in the whole world. Extracting the knowledge from it, however, is a\nchallenging task due to multiple reasons including complexity of the language\nand paucity of standard natural language processing tools. In this paper, we\ntarget the problem of building knowledge graphs for particular types of\nrelationships from sa\\d{m}sk\\d{r}ta texts. We build a natural language\nquestion-answering system in sa\\d{m}sk\\d{r}ta that uses the knowledge graph to\nanswer factoid questions. We design a framework for the overall system and\nimplement two separate instances of the system on human relationships from\nmah\\=abh\\=arata and r\\=am\\=aya\\d{n}a, and one instance on synonymous\nrelationships from bh\\=avaprak\\=a\\'sa nigha\\d{n}\\d{t}u, a technical text from\n\\=ayurveda. We show that about 50% of the factoid questions can be answered\ncorrectly by the system. More importantly, we analyse the shortcomings of the\nsystem in detail for each step, and discuss the possible ways forward.\n","authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2310.07848v1.pdf","comment":"Accepted at 6th International Sanskrit Computational Linguistics\n  Symposium (ISCLS) 2019"},{"id":"http://arxiv.org/abs/2310.07830v1","updated":"2023-10-11T19:16:09Z","published":"2023-10-11T19:16:09Z","title":"Does Synthetic Data Make Large Language Models More Efficient?","summary":"  Natural Language Processing (NLP) has undergone transformative changes with\nthe advent of deep learning methodologies. One challenge persistently\nconfronting researchers is the scarcity of high-quality, annotated datasets\nthat drive these models. This paper explores the nuances of synthetic data\ngeneration in NLP, with a focal point on template-based question generation. By\nassessing its advantages, including data augmentation potential and the\nintroduction of structured variety, we juxtapose these benefits against\ninherent limitations, such as the risk of overfitting and the constraints posed\nby pre-defined templates. Drawing from empirical evaluations, we demonstrate\nthe impact of template-based synthetic data on the performance of modern\ntransformer models. We conclude by emphasizing the delicate balance required\nbetween synthetic and real-world data, and the future trajectories of\nintegrating synthetic data in model training pipelines. The findings aim to\nguide NLP practitioners in harnessing synthetic data's potential, ensuring\noptimal model performance in diverse applications.\n","authors":["Sia Gholami","Marwan Omar"],"pdf_url":"https://arxiv.org/pdf/2310.07830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07826v1","updated":"2023-10-11T19:09:07Z","published":"2023-10-11T19:09:07Z","title":"Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language\n  Annotation","summary":"  One of the primary obstacles in the advancement of Natural Language\nProcessing (NLP) technologies for low-resource languages is the lack of\nannotated datasets for training and testing machine learning models. In this\npaper, we present Antarlekhaka, a tool for manual annotation of a comprehensive\nset of tasks relevant to NLP. The tool is Unicode-compatible,\nlanguage-agnostic, Web-deployable and supports distributed annotation by\nmultiple simultaneous annotators. The system sports user-friendly interfaces\nfor 8 categories of annotation tasks. These, in turn, enable the annotation of\na considerably larger set of NLP tasks. The task categories include two\nlinguistic tasks not handled by any other tool, namely, sentence boundary\ndetection and deciding canonical word order, which are important tasks for text\nthat is in the form of poetry. We propose the idea of sequential annotation\nbased on small text units, where an annotator performs several tasks related to\na single text unit before proceeding to the next unit. The research\napplications of the proposed mode of multi-task annotation are also discussed.\nAntarlekhaka outperforms other annotation tools in objective evaluation. It has\nbeen also used for two real-life annotation tasks on two different languages,\nnamely, Sanskrit and Bengali. The tool is available at\nhttps://github.com/Antarlekhaka/code.\n","authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2310.07826v1.pdf","comment":"Accepted: 3rd Workshop for Natural Language Processing Open Source\n  Software (NLP-OSS) @ EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07821v1","updated":"2023-10-11T19:02:57Z","published":"2023-10-11T19:02:57Z","title":"Non-autoregressive Text Editing with Copy-aware Latent Alignments","summary":"  Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the\nfield of text editing, with the aim of addressing the slow autoregressive\ninference problem posed by the former. Despite promising results, Seq2Edit\napproaches still face several challenges such as inflexibility in generation\nand difficulty in generalizing to other languages. In this work, we propose a\nnovel non-autoregressive text editing method to circumvent the above issues, by\nmodeling the edit process with latent CTC alignments. We make a crucial\nextension to CTC by introducing the copy operation into the edit space, thus\nenabling more efficient management of textual overlap in editing. We conduct\nextensive experiments on GEC and sentence fusion tasks, showing that our\nproposed method significantly outperforms existing Seq2Edit models and achieves\nsimilar or even better results than Seq2Seq with over $4\\times$ speedup.\nMoreover, it demonstrates good generalizability on German and Russian. In-depth\nanalyses reveal the strengths of our method in terms of the robustness under\nvarious scenarios and generating fluent and flexible outputs.\n","authors":["Yu Zhang","Yue Zhang","Leyang Cui","Guohong Fu"],"pdf_url":"https://arxiv.org/pdf/2310.07821v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07819v1","updated":"2023-10-11T19:00:40Z","published":"2023-10-11T19:00:40Z","title":"Faithfulness Measurable Masked Language Models","summary":"  A common approach to explain NLP models, is to use importance measures that\nexpress which tokens are important for a prediction. Unfortunately, such\nexplanations are often wrong despite being persuasive. Therefore, it is\nessential to measure their faithfulness. One such metric is if tokens are truly\nimportant, then masking them should result in worse model performance. However,\ntoken masking introduces out-of-distribution issues and existing solutions are\ncomputationally expensive and employ proxy-models. Furthermore, other metrics\nare very limited in scope. In this work, we propose an inherently faithfulness\nmeasurable model that addresses these challenges. This is achieved by using a\nnovel fine-tuning method that incorporates masking, such that masking tokens\nbecome in-distribution by design. This differs from existing approaches, which\nare completely model-agnostic but are inapplicable in practice. We demonstrate\nthe generality of our approach by applying it to various tasks and validate it\nusing statistical in-distribution tests. Additionally, because masking is\nin-distribution, importance measures which themselves use masking become more\nfaithful, thus our model becomes more explainable.\n","authors":["Andreas Madsen","Siva Reddy","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2310.07819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07818v1","updated":"2023-10-11T18:59:48Z","published":"2023-10-11T18:59:48Z","title":"Exploring the Relationship between Analogy Identification and Sentence\n  Structure Encoding in Large Language Models","summary":"  Identifying analogies plays a pivotal role in human cognition and language\nproficiency. In the last decade, there has been extensive research on word\nanalogies in the form of ``A is to B as C is to D.'' However, there is a\ngrowing interest in analogies that involve longer text, such as sentences and\ncollections of sentences, which convey analogous meanings. While the current\nNLP research community evaluates the ability of Large Language Models (LLMs) to\nidentify such analogies, the underlying reasons behind these abilities warrant\ndeeper investigation. Furthermore, the capability of LLMs to encode both\nsyntactic and semantic structures of language within their embeddings has\ngarnered significant attention with the surge in their utilization. In this\nwork, we examine the relationship between the abilities of multiple LLMs to\nidentify sentence analogies, and their capacity to encode syntactic and\nsemantic structures. Through our analysis, we find that analogy identification\nability of LLMs is positively correlated with their ability to encode syntactic\nand semantic structures of sentences. Specifically, we find that the LLMs which\ncapture syntactic structures better, also have higher abilities in identifying\nsentence analogies.\n","authors":["Thilini Wijesiriwardene","Ruwan Wickramarachchi","Aishwarya Naresh Reganti","Vinija Jain","Aman Chadha","Amit Sheth","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2310.07818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07815v1","updated":"2023-10-11T18:56:15Z","published":"2023-10-11T18:56:15Z","title":"Language Models As Semantic Indexers","summary":"  Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss and there is usually an inherent mismatch\nbetween the distribution of embeddings within the latent space produced by text\nencoders and the anticipated distribution required for semantic indexing.\nNevertheless, it is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMINDEXER, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. The learned semantic indexer\ncan facilitate various downstream tasks, such as recommendation and retrieval.\nWe conduct experiments on three tasks including recommendation, product search,\nand document retrieval on five datasets from various domains, where LMINDEXER\noutperforms competitive baselines significantly and consistently.\n","authors":["Bowen Jin","Hansi Zeng","Guoyin Wang","Xiusi Chen","Tianxin Wei","Ruirui Li","Zhengyang Wang","Zheng Li","Yang Li","Hanqing Lu","Suhang Wang","Jiawei Han","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07815v1.pdf","comment":"9 pages, 3 appendix pages"},{"id":"http://arxiv.org/abs/2310.07803v1","updated":"2023-10-11T18:36:13Z","published":"2023-10-11T18:36:13Z","title":"A general mechanism of humor: reformulating the semantic overlap","summary":"  This article proposes a cognitive mechanism of humour of general\napplicability, not restricted to verbal communication. It is indebted to\nRaskin's concept of script overlap, and conforms to the incongruity-resolution\ntheoretical framework, but it is built on the notion of constraint, an abstract\ncorrespondence between sets of data. Under this view, script overlap is an\noutcome of a more abstractly described phenomenon, constraint overlap. The\nimportant concept of the overlooked argument is introduced to characterise the\ntwo overlapping constraints -- overt and covert. Their inputs and outputs are\nnot directly encoded in utterances, but implicated by them, and their overlap\nresults in another overlap at the level of the communicated utterances, that\nthe incongruity reveals. Our hypothesis assumes as a given that the evocation\nof such constraints is a cognitive effect of the inferential process by which a\nhearer interprets utterances. We base this assumption on Hofstadter's theory of\nanalogy-making as the essence of human thought. By substituting \"stimuli\" of\nany kind for \"utterances\" in this model, we obtain a mechanism as easily\napplicable to non-verbal communication -- slapstick, cartoons -- and we propose\nit describes the necessary and sufficient conditions for a communicative act in\nany modality to carry humour.\n","authors":["Javier Martnez"],"pdf_url":"https://arxiv.org/pdf/2310.07803v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.07795v1","updated":"2023-10-11T18:30:37Z","published":"2023-10-11T18:30:37Z","title":"Ontology Enrichment for Effective Fine-grained Entity Typing","summary":"  Fine-grained entity typing (FET) is the task of identifying specific entity\ntypes at a fine-grained level for entity mentions based on their contextual\ninformation. Conventional methods for FET require extensive human annotation,\nwhich is time-consuming and costly. Recent studies have been developing weakly\nsupervised or zero-shot approaches. We study the setting of zero-shot FET where\nonly an ontology is provided. However, most existing ontology structures lack\nrich supporting information and even contain ambiguous relations, making them\nineffective in guiding FET. Recently developed language models, though\npromising in various few-shot and zero-shot NLP tasks, may face challenges in\nzero-shot FET due to their lack of interaction with task-specific ontology. In\nthis study, we propose OnEFET, where we (1) enrich each node in the ontology\nstructure with two types of extra information: instance information for\ntraining sample augmentation and topic information to relate types to contexts,\nand (2) develop a coarse-to-fine typing algorithm that exploits the enriched\ninformation by training an entailment model with contrasting topics and\ninstance-based augmented training samples. Our experiments show that OnEFET\nachieves high-quality fine-grained entity typing without human annotation,\noutperforming existing zero-shot methods by a large margin and rivaling\nsupervised methods.\n","authors":["Siru Ouyang","Jiaxin Huang","Pranav Pillai","Yunyi Zhang","Yu Zhang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.07795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v1","updated":"2023-10-11T18:27:12Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional carefully\ndesigned embedding-based and rule-based models dominate. The question remains\nopen of whether pre-trained LLMs can understand structured temporal relational\ndata and replace them as the foundation model for temporal relational\nforecasting. Therefore, we bring temporal knowledge forecasting into the\ngenerative setting. However, challenges occur in the huge chasms between\ncomplex temporal graph data structure and sequential natural expressions LLMs\ncan handle, and between the enormous data sizes of tKGs and heavy computation\ncosts of finetuning LLMs. To address these challenges, we propose a novel\nretrieval augmented generation framework that performs generative forecasting\non tKGs named GenTKG, which combines a temporal logical rule-based retrieval\nstrategy and lightweight parameter-efficient instruction tuning. Extensive\nexperiments have shown that GenTKG outperforms conventional methods of temporal\nrelational forecasting under low computation resources. GenTKG also highlights\nremarkable transferability with exceeding performance on unseen datasets\nwithout re-training. Our work reveals the huge potential of LLMs in the tKG\ndomain and opens a new frontier for generative forecasting on tKGs.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v1.pdf","comment":"8 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.07716v1","updated":"2023-10-11T17:59:56Z","published":"2023-10-11T17:59:56Z","title":"PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection","summary":"  Object anomaly detection is an important problem in the field of machine\nvision and has seen remarkable progress recently. However, two significant\nchallenges hinder its research and application. First, existing datasets lack\ncomprehensive visual information from various pose angles. They usually have an\nunrealistic assumption that the anomaly-free training dataset is pose-aligned,\nand the testing samples have the same pose as the training data. However, in\npractice, anomaly may exist in any regions on a object, the training and query\nsamples may have different poses, calling for the study on pose-agnostic\nanomaly detection. Second, the absence of a consensus on experimental protocols\nfor pose-agnostic anomaly detection leads to unfair comparisons of different\nmethods, hindering the research on pose-agnostic anomaly detection. To address\nthese issues, we develop Multi-pose Anomaly Detection (MAD) dataset and\nPose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to\naddress the pose-agnostic anomaly detection problem. Specifically, we build MAD\nusing 20 complex-shaped LEGO toys including 4K views with various poses, and\nhigh-quality and diverse 3D anomalies in both simulated and real environments.\nAdditionally, we propose a novel method OmniposeAD, trained using MAD,\nspecifically designed for pose-agnostic anomaly detection. Through\ncomprehensive evaluations, we demonstrate the relevance of our dataset and\nmethod. Furthermore, we provide an open-source benchmark library, including\ndataset and baseline methods that cover 8 anomaly detection paradigms, to\nfacilitate future research and application in this domain. Code, data, and\nmodels are publicly available at https://github.com/EricLee0224/PAD.\n","authors":["Qiang Zhou","Weize Li","Lihan Jiang","Guoliang Wang","Guyue Zhou","Shanghang Zhang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.07716v1.pdf","comment":"Accepted by NeurIPS 2023. Codes are available at\n  https://github.com/EricLee0224/PAD"},{"id":"http://arxiv.org/abs/2305.05658v2","updated":"2023-10-11T17:59:44Z","published":"2023-05-09T17:52:59Z","title":"TidyBot: Personalized Robot Assistance with Large Language Models","summary":"  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n","authors":["Jimmy Wu","Rika Antonova","Adam Kan","Marion Lepert","Andy Zeng","Shuran Song","Jeannette Bohg","Szymon Rusinkiewicz","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2305.05658v2.pdf","comment":"Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language\n  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2023. Project page:\n  https://tidybot.cs.princeton.edu"},{"id":"http://arxiv.org/abs/2310.07707v1","updated":"2023-10-11T17:57:14Z","published":"2023-10-11T17:57:14Z","title":"MatFormer: Nested Transformer for Elastic Inference","summary":"  Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n& encoders), modalities (language & vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.\n","authors":[" Devvrit","Sneha Kudugunta","Aditya Kusupati","Tim Dettmers","Kaifeng Chen","Inderjit Dhillon","Yulia Tsvetkov","Hannaneh Hajishirzi","Sham Kakade","Ali Farhadi","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.07707v1.pdf","comment":"31 pages, 12 figures, first three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.07704v1","updated":"2023-10-11T17:55:15Z","published":"2023-10-11T17:55:15Z","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity","summary":"  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret\n","authors":["Haoxuan You","Haotian Zhang","Zhe Gan","Xianzhi Du","Bowen Zhang","Zirui Wang","Liangliang Cao","Shih-Fu Chang","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07704v1.pdf","comment":"30 pages, 10 figures. Code/Project Website:\n  https://github.com/apple/ml-ferret"},{"id":"http://arxiv.org/abs/2310.07702v1","updated":"2023-10-11T17:52:39Z","published":"2023-10-11T17:52:39Z","title":"ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with\n  Diffusion Models","summary":"  In this work, we investigate the capability of generating images from\npre-trained diffusion models at much higher resolutions than the training image\nsizes. In addition, the generated images should have arbitrary image aspect\nratios. When generating images directly at a higher resolution, 1024 x 1024,\nwith the pre-trained Stable Diffusion using training images of resolution 512 x\n512, we observe persistent problems of object repetition and unreasonable\nobject structures. Existing works for higher-resolution generation, such as\nattention-based and joint-diffusion approaches, cannot well address these\nissues. As a new perspective, we examine the structural components of the U-Net\nin diffusion models and identify the crucial cause as the limited perception\nfield of convolutional kernels. Based on this key observation, we propose a\nsimple yet effective re-dilation that can dynamically adjust the convolutional\nperception field during inference. We further propose the dispersed convolution\nand noise-damped classifier-free guidance, which can enable\nultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our\napproach does not require any training or optimization. Extensive experiments\ndemonstrate that our approach can address the repetition issue well and achieve\nstate-of-the-art performance on higher-resolution image synthesis, especially\nin texture details. Our work also suggests that a pre-trained diffusion model\ntrained on low-resolution images can be directly used for high-resolution\nvisual generation without further tuning, which may provide insights for future\nresearch on ultra-high-resolution image and video synthesis.\n","authors":["Yingqing He","Shaoshu Yang","Haoxin Chen","Xiaodong Cun","Menghan Xia","Yong Zhang","Xintao Wang","Ran He","Qifeng Chen","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2310.07702v1.pdf","comment":"Project page: https://yingqinghe.github.io/scalecrafter/ Github:\n  https://github.com/YingqingHe/ScaleCrafter"},{"id":"http://arxiv.org/abs/2310.07699v1","updated":"2023-10-11T17:49:13Z","published":"2023-10-11T17:49:13Z","title":"From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched\n  Captions","summary":"  Web-crawled datasets are pivotal to the success of pre-training\nvision-language models, exemplified by CLIP. However, web-crawled AltTexts can\nbe noisy and potentially irrelevant to images, thereby undermining the crucial\nimage-text alignment. Existing methods for rewriting captions using large\nlanguage models (LLMs) have shown promise on small, curated datasets like CC3M\nand CC12M. Nevertheless, their efficacy on massive web-captured captions is\nconstrained by the inherent noise and randomness in such data. In this study,\nwe address this limitation by focusing on two key aspects: data quality and\ndata variety. Unlike recent LLM rewriting techniques, we emphasize exploiting\nvisual concepts and their integration into the captions to improve data\nquality. For data variety, we propose a novel mixed training scheme that\noptimally leverages AltTexts alongside newly generated Visual-enriched Captions\n(VeC). We use CLIP as one example and adapt the method for CLIP training on\nlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive\nevaluation of VeCLIP across small, medium, and large scales of raw data. Our\nresults show significant advantages in image-text alignment and overall model\nperformance, underscoring the effectiveness of VeCLIP in improving CLIP\ntraining. For example, VeCLIP achieves a remarkable over 20% improvement in\nCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,\nwe also achieve a notable over 3% improvement while using only 14% of the data\nemployed in the vanilla CLIP and 11% in ALIGN.\n","authors":["Zhengfeng Lai","Haotian Zhang","Wentao Wu","Haoping Bai","Aleksei Timofeev","Xianzhi Du","Zhe Gan","Jiulong Shan","Chen-Nee Chuah","Yinfei Yang","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07699v1.pdf","comment":"CV/ML"},{"id":"http://arxiv.org/abs/2310.07697v1","updated":"2023-10-11T17:46:28Z","published":"2023-10-11T17:46:28Z","title":"ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation","summary":"  Recent works have successfully extended large-scale text-to-image models to\nthe video domain, producing promising results but at a high computational cost\nand requiring a large amount of video data. In this work, we introduce\nConditionVideo, a training-free approach to text-to-video generation based on\nthe provided condition, video, and input text, by leveraging the power of\noff-the-shelf text-to-image generation methods (e.g., Stable Diffusion).\nConditionVideo generates realistic dynamic videos from random noise or given\nscene videos. Our method explicitly disentangles the motion representation into\ncondition-guided and scenery motion components. To this end, the ConditionVideo\nmodel is designed with a UNet branch and a control branch. To improve temporal\ncoherence, we introduce sparse bi-directional spatial-temporal attention\n(sBiST-Attn). The 3D control network extends the conventional 2D controlnet\nmodel, aiming to strengthen conditional generation accuracy by additionally\nleveraging the bi-directional frames in the temporal domain. Our method\nexhibits superior performance in terms of frame consistency, clip score, and\nconditional accuracy, outperforming other compared methods.\n","authors":["Bo Peng","Xinyuan Chen","Yaohui Wang","Chaochao Lu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.07697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09535v3","updated":"2023-10-11T17:46:21Z","published":"2023-03-16T17:51:13Z","title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing","summary":"  The diffusion-based generative models have achieved remarkable success in\ntext-based image generation. However, since it contains enormous randomness in\ngeneration progress, it is still challenging to apply such models for\nreal-world visual content editing, especially in videos. In this paper, we\npropose FateZero, a zero-shot text-based editing method on real-world videos\nwithout per-prompt training or use-specific mask. To edit videos consistently,\nwe propose several techniques based on the pre-trained models. Firstly, in\ncontrast to the straightforward DDIM inversion technique, our approach captures\nintermediate attention maps during inversion, which effectively retain both\nstructural and motion information. These maps are directly fused in the editing\nprocess rather than generated during denoising. To further minimize semantic\nleakage of the source video, we then fuse self-attentions with a blending mask\nobtained by cross-attention features from the source prompt. Furthermore, we\nhave implemented a reform of the self-attention mechanism in denoising UNet by\nintroducing spatial-temporal attention to ensure frame consistency. Yet\nsuccinct, our method is the first one to show the ability of zero-shot\ntext-driven video style and local attribute editing from the trained\ntext-to-image model. We also have a better zero-shot shape-aware editing\nability based on the text-to-video model. Extensive experiments demonstrate our\nsuperior temporal consistency and editing capability than previous works.\n","authors":["Chenyang Qi","Xiaodong Cun","Yong Zhang","Chenyang Lei","Xintao Wang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09535v3.pdf","comment":"Accepted to ICCV 2023 as an Oral Presentation. Project page:\n  https://fate-zero-edit.github.io ; GitHub repository:\n  https://github.com/ChenyangQiQi/FateZero"},{"id":"http://arxiv.org/abs/2303.05078v2","updated":"2023-10-11T17:46:03Z","published":"2023-03-09T07:26:49Z","title":"Efficient Transformer-based 3D Object Detection with Dynamic Token\n  Halting","summary":"  Balancing efficiency and accuracy is a long-standing problem for deploying\ndeep learning models. The trade-off is even more important for real-time\nsafety-critical systems like autonomous vehicles. In this paper, we propose an\neffective approach for accelerating transformer-based 3D object detectors by\ndynamically halting tokens at different layers depending on their contribution\nto the detection task. Although halting a token is a non-differentiable\noperation, our method allows for differentiable end-to-end learning by\nleveraging an equivalent differentiable forward-pass. Furthermore, our\nframework allows halted tokens to be reused to inform the model's predictions\nthrough a straightforward token recycling mechanism. Our method significantly\nimproves the Pareto frontier of efficiency versus accuracy when compared with\nthe existing approaches. By halting tokens and increasing model capacity, we\nare able to improve the baseline model's performance without increasing the\nmodel's latency on the Waymo Open Dataset.\n","authors":["Mao Ye","Gregory P. Meyer","Yuning Chai","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.05078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07687v1","updated":"2023-10-11T17:36:17Z","published":"2023-10-11T17:36:17Z","title":"Orbital Polarimetric Tomography of a Flare Near the Sagittarius A*\n  Supermassive Black Hole","summary":"  The interaction between the supermassive black hole at the center of the\nMilky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces\nhigh energy flares seen in X-ray, infrared and radio. One mechanism for\nobserved flares is the formation of compact bright regions that appear within\nthe accretion disk and close to the event horizon. Understanding these flares\ncan provide a window into black hole accretion processes. Although\nsophisticated simulations predict the formation of these flares, their\nstructure has yet to be recovered by observations. Here we show the first\nthree-dimensional (3D) reconstruction of an emission flare in orbit recovered\nfrom ALMA light curves observed on April 11, 2017. Our recovery results show\ncompact bright regions at a distance of roughly 6 times the event horizon.\nMoreover, our recovery suggests a clockwise rotation in a low-inclination\norbital plane, a result consistent with prior studies by EHT and GRAVITY\ncollaborations. To recover this emission structure we solve a highly ill-posed\ntomography problem by integrating a neural 3D representation (an emergent\nartificial intelligence approach for 3D reconstruction) with a gravitational\nmodel for black holes. Although the recovered 3D structure is subject, and\nsometimes sensitive, to the model assumptions, under physically motivated\nchoices we find that our results are stable and our approach is successful on\nsimulated data. We anticipate that in the future, this approach could be used\nto analyze a richer collection of time-series data that could shed light on the\nmechanisms governing black hole and plasma dynamics.\n","authors":["Aviad Levis","Andrew A. Chael","Katherine L. Bouman","Maciek Wielgus","Pratul P. Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2310.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12424v2","updated":"2023-10-11T17:34:19Z","published":"2023-06-21T17:59:51Z","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution","summary":"  We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n","authors":["Siobhan Mackenzie Hall","Fernanda Gonalves Abrantes","Hanwen Zhu","Grace Sodunke","Aleksandar Shtedritski","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2306.12424v2.pdf","comment":"Data and code available at https://github.com/oxai/visogender"},{"id":"http://arxiv.org/abs/2310.07682v1","updated":"2023-10-11T17:32:24Z","published":"2023-10-11T17:32:24Z","title":"Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas\n  from Hematoxylin and Eosin Images","summary":"  MET protein overexpression is a targetable event in non-small cell lung\ncancer (NSCLC) and is the subject of active drug development. Challenges in\nidentifying patients for these therapies include lack of access to validated\ntesting, such as standardized immunohistochemistry (IHC) assessment, and\nconsumption of valuable tissue for a single gene/protein assay. Development of\npre-screening algorithms using routinely available digitized hematoxylin and\neosin (H&E)-stained slides to predict MET overexpression could promote testing\nfor those who will benefit most. While assessment of MET expression using IHC\nis currently not routinely performed in NSCLC, next-generation sequencing is\ncommon and in some cases includes RNA expression panel testing. In this work,\nwe leveraged a large database of matched H&E slides and RNA expression data to\ntrain a weakly supervised model to predict MET RNA overexpression directly from\nH&E images. This model was evaluated on an independent holdout test set of 300\nover-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th\npercentile interval: 0.66 - 0.74) with stable performance characteristics\nacross different patient clinical variables and robust to synthetic noise on\nthe test set. These results suggest that H&E-based predictive models could be\nuseful to prioritize patients for confirmatory testing of MET protein or MET\ngene expression status.\n","authors":["Kshitij Ingale","Sun Hae Hong","Josh S. K. Bell","Abbas Rizvi","Amy Welch","Lingdao Sha","Irvin Ho","Kunal Nagpal","Aicha BenTaieb","Rohan P Joshi","Martin C Stumpe"],"pdf_url":"https://arxiv.org/pdf/2310.07682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07678v1","updated":"2023-10-11T17:21:48Z","published":"2023-10-11T17:21:48Z","title":"Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM","summary":"  With the proliferation of image-based applications in various domains, the\nneed for accurate and interpretable image similarity measures has become\nincreasingly critical. Existing image similarity models often lack\ntransparency, making it challenging to understand the reasons why two images\nare considered similar. In this paper, we propose the concept of explainable\nimage similarity, where the goal is the development of an approach, which is\ncapable of providing similarity scores along with visual factual and\ncounterfactual explanations. Along this line, we present a new framework, which\nintegrates Siamese Networks and Grad-CAM for providing explainable image\nsimilarity and discuss the potential benefits and challenges of adopting this\napproach. In addition, we provide a comprehensive discussion about factual and\ncounterfactual explanations provided by the proposed framework for assisting\ndecision making. The proposed approach has the potential to enhance the\ninterpretability, trustworthiness and user acceptance of image-based systems in\nreal-world image similarity applications. The implementation code can be found\nin https://github.com/ioannislivieris/Grad_CAM_Siamese.git.\n","authors":["Ioannis E. Livieris","Emmanuel Pintelas","Niki Kiriakidou","Panagiotis Pintelas"],"pdf_url":"https://arxiv.org/pdf/2310.07678v1.pdf","comment":"The manuscript has been submitted for publication in \"Journal of\n  Imaging\""},{"id":"http://arxiv.org/abs/2310.07669v1","updated":"2023-10-11T17:18:15Z","published":"2023-10-11T17:18:15Z","title":"HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D\n  Semantic Segmentation","summary":"  Signals from different modalities each have their own combination algebra\nwhich affects their sampling processing. RGB is mostly linear; depth is a\ngeometric signal following the operations of mathematical morphology. If a\nnetwork obtaining RGB-D input has both kinds of operators available in its\nlayers, it should be able to give effective output with fewer parameters. In\nthis paper, morphological elements in conjunction with more familiar linear\nmodules are used to construct a mixed linear-morphological network called\nHaarNet. This is the first large-scale linear-morphological hybrid, evaluated\non a set of sizeable real-world datasets. In the network, morphological Haar\nsampling is applied to both feature channels in several layers, which splits\nextreme values and high-frequency information such that both can be processed\nto improve both modalities. Moreover, morphologically parameterised ReLU is\nused, and morphologically-sound up-sampling is applied to obtain a\nfull-resolution output. Experiments show that HaarNet is competitive with a\nstate-of-the-art CNN, implying that morphological networks are a promising\nresearch direction for geometry-based learning tasks.\n","authors":["Rick Groenendijk","Leo Dorst","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2310.07669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07664v1","updated":"2023-10-11T17:09:19Z","published":"2023-10-11T17:09:19Z","title":"Accelerating Vision Transformers Based on Heterogeneous Attention\n  Patterns","summary":"  Recently, Vision Transformers (ViTs) have attracted a lot of attention in the\nfield of computer vision. Generally, the powerful representative capacity of\nViTs mainly benefits from the self-attention mechanism, which has a high\ncomputation complexity. To accelerate ViTs, we propose an integrated\ncompression pipeline based on observed heterogeneous attention patterns across\nlayers. On one hand, different images share more similar attention patterns in\nearly layers than later layers, indicating that the dynamic query-by-key\nself-attention matrix may be replaced with a static self-attention matrix in\nearly layers. Then, we propose a dynamic-guided static self-attention (DGSSA)\nmethod where the matrix inherits self-attention information from the replaced\ndynamic self-attention to effectively improve the feature representation\nability of ViTs. On the other hand, the attention maps have more low-rank\npatterns, which reflect token redundancy, in later layers than early layers. In\na view of linear dimension reduction, we further propose a method of global\naggregation pyramid (GLAD) to reduce the number of tokens in later layers of\nViTs, such as Deit. Experimentally, the integrated compression pipeline of\nDGSSA and GLAD can accelerate up to 121% run-time throughput compared with\nDeiT, which surpasses all SOTA approaches.\n","authors":["Deli Yu","Teng Xi","Jianwei Li","Baopu Li","Gang Zhang","Haocheng Feng","Junyu Han","Jingtuo Liu","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07663v1","updated":"2023-10-11T17:03:21Z","published":"2023-10-11T17:03:21Z","title":"Deep Video Inpainting Guided by Audio-Visual Self-Supervision","summary":"  Humans can easily imagine a scene from auditory information based on their\nprior knowledge of audio-visual events. In this paper, we mimic this innate\nhuman ability in deep learning models to improve the quality of video\ninpainting. To implement the prior knowledge, we first train the audio-visual\nnetwork, which learns the correspondence between auditory and visual\ninformation. Then, the audio-visual network is employed as a guider that\nconveys the prior knowledge of audio-visual correspondence to the video\ninpainting network. This prior knowledge is transferred through our proposed\ntwo novel losses: audio-visual attention loss and audio-visual pseudo-class\nconsistency loss. These two losses further improve the performance of the video\ninpainting by encouraging the inpainting result to have a high correspondence\nto its synchronized audio. Experimental results demonstrate that our proposed\nmethod can restore a wider domain of video scenes and is particularly effective\nwhen the sounding object in the scene is partially blinded.\n","authors":["Kyuyeon Kim","Junsik Jung","Woo Jae Kim","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.07663v1.pdf","comment":"Accepted at ICASSP 2022"},{"id":"http://arxiv.org/abs/2305.13172v2","updated":"2023-10-11T16:51:50Z","published":"2023-05-22T16:00:00Z","title":"Editing Large Language Models: Problems, Methods, and Opportunities","summary":"  Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Yunzhi Yao","Peng Wang","Bozhong Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13172v2.pdf","comment":"EMNLP 2023. Updated with new experiments"},{"id":"http://arxiv.org/abs/2310.07638v1","updated":"2023-10-11T16:33:30Z","published":"2023-10-11T16:33:30Z","title":"Context-Enhanced Detector For Building Detection From Remote Sensing\n  Images","summary":"  The field of building detection from remote sensing images has made\nsignificant progress, but faces challenges in achieving high-accuracy detection\ndue to the diversity in building appearances and the complexity of vast scenes.\nTo address these challenges, we propose a novel approach called\nContext-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade\nstructure to enhance the extraction of contextual information and improve\nbuilding detection accuracy. Specifically, we introduce two modules: the\nSemantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale\ncontexts and incorporates an attention mechanism to capture long-range\ninteractions, and the Instance Context Mining Module (ICMM), which captures\ninstance-level relationship context by constructing a spatial relationship\ngraph and aggregating instance features. Additionally, we introduce a semantic\nsegmentation loss based on pseudo-masks to guide contextual information\nextraction. Our method achieves state-of-the-art performance on three building\ndetection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.\n","authors":["Ziyue Huang","Mingming Zhang","Qingjie Liu","Wei Wang","Zhe Dong","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07638v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.07633v1","updated":"2023-10-11T16:28:24Z","published":"2023-10-11T16:28:24Z","title":"Attention-Map Augmentation for Hypercomplex Breast Cancer Classification","summary":"  Breast cancer is the most widespread neoplasm among women and early detection\nof this disease is critical. Deep learning techniques have become of great\ninterest to improve diagnostic performance. Nonetheless, discriminating between\nmalignant and benign masses from whole mammograms remains challenging due to\nthem being almost identical to an untrained eye and the region of interest\n(ROI) occupying a minuscule portion of the entire image. In this paper, we\npropose a framework, parameterized hypercomplex attention maps (PHAM), to\novercome these problems. Specifically, we deploy an augmentation step based on\ncomputing attention maps. Then, the attention maps are used to condition the\nclassification step by constructing a multi-dimensional input comprised of the\noriginal breast cancer image and the corresponding attention map. In this step,\na parameterized hypercomplex neural network (PHNN) is employed to perform\nbreast cancer classification. The framework offers two main advantages. First,\nattention maps provide critical information regarding the ROI and allow the\nneural model to concentrate on it. Second, the hypercomplex architecture has\nthe ability to model local relations between input dimensions thanks to\nhypercomplex algebra rules, thus properly exploiting the information provided\nby the attention map. We demonstrate the efficacy of the proposed framework on\nboth mammography images as well as histopathological ones, surpassing\nattention-based state-of-the-art networks and the real-valued counterpart of\nour method. The code of our work is available at\nhttps://github.com/elelo22/AttentionBCS.\n","authors":["Eleonora Lopez","Filippo Betello","Federico Carmignani","Eleonora Grassucci","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07633v1.pdf","comment":"Submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2310.07632v1","updated":"2023-10-11T16:25:45Z","published":"2023-10-11T16:25:45Z","title":"Prompt Backdoors in Visual Prompt Learning","summary":"  Fine-tuning large pre-trained computer vision models is infeasible for\nresource-limited users. Visual prompt learning (VPL) has thus emerged to\nprovide an efficient and flexible alternative to model fine-tuning through\nVisual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider\noptimizes a visual prompt given downstream data, and downstream users can use\nthis prompt together with the large pre-trained model for prediction. However,\nthis new learning paradigm may also pose security risks when the VPPTaaS\nprovider instead provides a malicious visual prompt. In this paper, we take the\nfirst step to explore such risks through the lens of backdoor attacks.\nSpecifically, we propose BadVisualPrompt, a simple yet effective backdoor\nattack against VPL. For example, poisoning $5\\%$ CIFAR10 training data leads to\nabove $99\\%$ attack success rates with only negligible model accuracy drop by\n$1.5\\%$. In particular, we identify and then address a new technical challenge\nrelated to interactions between the backdoor trigger and visual prompt, which\ndoes not exist in conventional, model-level backdoors. Moreover, we provide\nin-depth analyses of seven backdoor defenses from model, prompt, and input\nlevels. Overall, all these defenses are either ineffective or impractical to\nmitigate our BadVisualPrompt, implying the critical vulnerability of VPL.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07623v1","updated":"2023-10-11T16:06:14Z","published":"2023-10-11T16:06:14Z","title":"Dual Quaternion Rotational and Translational Equivariance in 3D Rigid\n  Motion Modelling","summary":"  Objects' rigid motions in 3D space are described by rotations and\ntranslations of a highly-correlated set of points, each with associated $x,y,z$\ncoordinates that real-valued networks consider as separate entities, losing\ninformation. Previous works exploit quaternion algebra and their ability to\nmodel rotations in 3D space. However, these algebras do not properly encode\ntranslations, leading to sub-optimal performance in 3D learning tasks. To\novercome these limitations, we employ a dual quaternion representation of rigid\nmotions in the 3D space that jointly describes rotations and translations of\npoint sets, processing each of the points as a single entity. Our approach is\ntranslation and rotation equivariant, so it does not suffer from shifts in the\ndata and better learns object trajectories, as we validate in the experimental\nevaluations. Models endowed with this formulation outperform previous\napproaches in a human pose forecasting application, attesting to the\neffectiveness of the proposed dual quaternion formulation for rigid motions in\n3D space.\n","authors":["Guilherme Vieira","Eleonora Grassucci","Marcos Eduardo Valle","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07623v1.pdf","comment":"Accepted at IEEE MLSP 2023 (Honorable Mention Top 10% Outstanding\n  Paper)"},{"id":"http://arxiv.org/abs/2310.07602v1","updated":"2023-10-11T15:41:52Z","published":"2023-10-11T15:41:52Z","title":"Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous\n  Driving","summary":"  Radar has stronger adaptability in adverse scenarios for autonomous driving\nenvironmental perception compared to widely adopted cameras and LiDARs.\nCompared with commonly used 3D radars, latest 4D radars have precise vertical\nresolution and higher point cloud density, making it a highly promising sensor\nfor autonomous driving in complex environmental perception. However, due to the\nmuch higher noise than LiDAR, manufacturers choose different filtering\nstrategies, resulting in an inverse ratio between noise level and point cloud\ndensity. There is still a lack of comparative analysis on which method is\nbeneficial for deep learning-based perception algorithms in autonomous driving.\nOne of the main reasons is that current datasets only adopt one type of 4D\nradar, making it difficult to compare different 4D radars in the same scene.\nTherefore, in this paper, we introduce a novel large-scale multi-modal dataset\nfeaturing, for the first time, two types of 4D radars captured simultaneously.\nThis dataset enables further research into effective 4D radar perception\nalgorithms.Our dataset consists of 151 consecutive series, most of which last\n20 seconds and contain 10,007 meticulously synchronized and annotated frames.\nMoreover, our dataset captures a variety of challenging driving scenarios,\nincluding many road conditions, weather conditions, nighttime and daytime with\ndifferent lighting intensities and periods. Our dataset annotates consecutive\nframes, which can be applied to 3D object detection and tracking, and also\nsupports the study of multi-modal tasks. We experimentally validate our\ndataset, providing valuable results for studying different types of 4D radars.\nThis dataset is released on https://github.com/adept-thu/Dual-Radar.\n","authors":["Xinyu Zhang","Li Wang","Jian Chen","Cheng Fang","Lei Yang","Ziying Song","Guangqi Yang","Yichen Wang","Xiaofei Zhang","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2310.07602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07591v1","updated":"2023-10-11T15:33:10Z","published":"2023-10-11T15:33:10Z","title":"PeP: a Point enhanced Painting method for unified point cloud tasks","summary":"  Point encoder is of vital importance for point cloud recognition. As the very\nbeginning step of whole model pipeline, adding features from diverse sources\nand providing stronger feature encoding mechanism would provide better input\nfor downstream modules. In our work, we proposed a novel PeP module to tackle\nabove issue. PeP contains two main parts, a refined point painting method and a\nLM-based point encoder. Experiments results on the nuScenes and KITTI datasets\nvalidate the superior performance of our PeP. The advantages leads to strong\nperformance on both semantic segmentation and object detection, in both lidar\nand multi-modal settings. Notably, our PeP module is model agnostic and\nplug-and-play. Our code will be publicly available soon.\n","authors":["Zichao Dong","Hang Ji","Xufeng Huang","Weikun Zhang","Xin Zhan","Junbo Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07585v1","updated":"2023-10-11T15:21:40Z","published":"2023-10-11T15:21:40Z","title":"A Discrepancy Aware Framework for Robust Anomaly Detection","summary":"  Defect detection is a critical research area in artificial intelligence.\nRecently, synthetic data-based self-supervised learning has shown great\npotential on this task. Although many sophisticated synthesizing strategies\nexist, little research has been done to investigate the robustness of models\nwhen faced with different strategies. In this paper, we focus on this issue and\nfind that existing methods are highly sensitive to them. To alleviate this\nissue, we present a Discrepancy Aware Framework (DAF), which demonstrates\nrobust performance consistently with simple and cheap strategies across\ndifferent anomaly detection benchmarks. We hypothesize that the high\nsensitivity to synthetic data of existing self-supervised methods arises from\ntheir heavy reliance on the visual appearance of synthetic data during\ndecoding. In contrast, our method leverages an appearance-agnostic cue to guide\nthe decoder in identifying defects, thereby alleviating its reliance on\nsynthetic appearance. To this end, inspired by existing knowledge distillation\nmethods, we employ a teacher-student network, which is trained based on\nsynthesized outliers, to compute the discrepancy map as the cue. Extensive\nexperiments on two challenging datasets prove the robustness of our method.\nUnder the simple synthesis strategies, it outperforms existing methods by a\nlarge margin. Furthermore, it also achieves the state-of-the-art localization\nperformance. Code is available at: https://github.com/caiyuxuan1120/DAF.\n","authors":["Yuxuan Cai","Dingkang Liang","Dongliang Luo","Xinwei He","Xin Yang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2310.07585v1.pdf","comment":"Accepted by IEEE Transactions on Industrial Informatics. Code is\n  available at: https://github.com/caiyuxuan1120/DAF"},{"id":"http://arxiv.org/abs/2310.07584v1","updated":"2023-10-11T15:20:44Z","published":"2023-10-11T15:20:44Z","title":"Centrality of the Fingerprint Core Location","summary":"  Fingerprints have long been recognized as a unique and reliable means of\npersonal identification. Central to the analysis and enhancement of\nfingerprints is the concept of the fingerprint core. Although the location of\nthe core is used in many applications, to the best of our knowledge, this study\nis the first to investigate the empirical distribution of the core over a\nlarge, combined dataset of rolled, as well as plain fingerprint recordings. We\nidentify and investigate the extent of incomplete rolling during the rolled\nfingerprint acquisition and investigate the centrality of the core. After\ncorrecting for the incomplete rolling, we find that the core deviates from the\nfingerprint center by 5.7% $\\pm$ 5.2% to 7.6% $\\pm$ 6.9%, depending on the\nfinger. Additionally, we find that the assumption of normal distribution of the\ncore position of plain fingerprint recordings cannot be rejected, but for\nrolled ones it can. Therefore, we use a multi-step process to find the\ndistribution of the rolled fingerprint recordings. The process consists of an\nAnderson-Darling normality test, the Bayesian Information Criterion to reduce\nthe number of possible candidate distributions and finally a Generalized Monte\nCarlo goodness-of-fit procedure to find the best fitting distribution. We find\nthe non-central Fischer distribution best describes the cores' horizontal\npositions. Finally, we investigate the correlation between mean core position\noffset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint\nrecordings where the core sits slightly below the fingerprint center.\n","authors":["Laurenz Ruzicka","Bernhard Strobl","Bernhard Kohn","Clemens Heitzinger"],"pdf_url":"https://arxiv.org/pdf/2310.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07573v1","updated":"2023-10-11T15:15:05Z","published":"2023-10-11T15:15:05Z","title":"Relational Prior Knowledge Graphs for Detection and Instance\n  Segmentation","summary":"  Humans have a remarkable ability to perceive and reason about the world\naround them by understanding the relationships between objects. In this paper,\nwe investigate the effectiveness of using such relationships for object\ndetection and instance segmentation. To this end, we propose a Relational\nPrior-based Feature Enhancement Model (RP-FEM), a graph transformer that\nenhances object proposal features using relational priors. The proposed\narchitecture operates on top of scene graphs obtained from initial proposals\nand aims to concurrently learn relational context modeling for object detection\nand instance segmentation. Experimental evaluations on COCO show that the\nutilization of scene graphs, augmented with relational priors, offer benefits\nfor object detection and instance segmentation. RP-FEM demonstrates its\ncapacity to suppress improbable class predictions within the image while also\npreventing the model from generating duplicate predictions, leading to\nimprovements over the baseline model on which it is built.\n","authors":["Osman lger","Yu Wang","Ysbrand Galama","Sezer Karaoglu","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2310.07573v1.pdf","comment":"Published in ICCV2023 SG2RL Workshop"},{"id":"http://arxiv.org/abs/2310.07572v1","updated":"2023-10-11T15:14:54Z","published":"2023-10-11T15:14:54Z","title":"Impact of Label Types on Training SWIN Models with Overhead Imagery","summary":"  Understanding the impact of data set design on model training and performance\ncan help alleviate the costs associated with generating remote sensing and\noverhead labeled data. This work examined the impact of training shifted window\ntransformers using bounding boxes and segmentation labels, where the latter are\nmore expensive to produce. We examined classification tasks by comparing models\ntrained with both target and backgrounds against models trained with only\ntarget pixels, extracted by segmentation labels. For object detection models,\nwe compared performance using either label type when training. We found that\nthe models trained on only target pixels do not show performance improvement\nfor classification tasks, appearing to conflate background pixels in the\nevaluation set with target pixels. For object detection, we found that models\ntrained with either label type showed equivalent performance across testing. We\nfound that bounding boxes appeared to be sufficient for tasks that did not\nrequire more complex labels, such as object segmentation. Continuing work to\ndetermine consistency of this result across data types and model architectures\ncould potentially result in substantial savings in generating remote sensing\ndata sets for deep learning.\n","authors":["Ryan Ford","Kenneth Hutchison","Nicholas Felts","Benjamin Cheng","Jesse Lew","Kyle Jackson"],"pdf_url":"https://arxiv.org/pdf/2310.07572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15751v3","updated":"2023-10-11T15:13:02Z","published":"2022-11-28T20:11:37Z","title":"Edge Video Analytics: A Survey on Applications, Systems and Enabling\n  Techniques","summary":"  Video, as a key driver in the global explosion of digital information, can\ncreate tremendous benefits for human society. Governments and enterprises are\ndeploying innumerable cameras for a variety of applications, e.g., law\nenforcement, emergency management, traffic control, and security surveillance,\nall facilitated by video analytics (VA). This trend is spurred by the rapid\nadvancement of deep learning (DL), which enables more precise models for object\nclassification, detection, and tracking. Meanwhile, with the proliferation of\nInternet-connected devices, massive amounts of data are generated daily,\noverwhelming the cloud. Edge computing, an emerging paradigm that moves\nworkloads and services from the network core to the network edge, has been\nwidely recognized as a promising solution. The resulting new intersection, edge\nvideo analytics (EVA), begins to attract widespread attention. Nevertheless,\nonly a few loosely-related surveys exist on this topic. The basic concepts of\nEVA (e.g., definition, architectures) were not fully elucidated due to the\nrapid development of this domain. To fill these gaps, we provide a\ncomprehensive survey of the recent efforts on EVA. In this paper, we first\nreview the fundamentals of edge computing, followed by an overview of VA. EVA\nsystems and their enabling techniques are discussed next. In addition, we\nintroduce prevalent frameworks and datasets to aid future researchers in the\ndevelopment of EVA systems. Finally, we discuss existing challenges and foresee\nfuture research directions. We believe this survey will help readers comprehend\nthe relationship between VA and edge computing, and spark new ideas on EVA.\n","authors":["Renjie Xu","Saiedeh Razavi","Rong Zheng"],"pdf_url":"https://arxiv.org/pdf/2211.15751v3.pdf","comment":"Accepted in IEEE Communications Surveys and Tutorials, 2023"},{"id":"http://arxiv.org/abs/2304.14933v2","updated":"2023-10-11T15:08:51Z","published":"2023-04-28T15:43:21Z","title":"An Empirical Study of Multimodal Model Merging","summary":"  Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n","authors":["Yi-Lin Sung","Linjie Li","Kevin Lin","Zhe Gan","Mohit Bansal","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2304.14933v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.07555v1","updated":"2023-10-11T15:00:11Z","published":"2023-10-11T15:00:11Z","title":"Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape\n  Bias by Distorted Shape","summary":"  Deep learning models are known to exhibit a strong texture bias, while human\ntends to rely heavily on global shape for object recognition. The current\nbenchmark for evaluating a model's shape bias is a set of style-transferred\nimages with the assumption that resistance to the attack of style transfer is\nrelated to the development of shape sensitivity in the model. In this work, we\nshow that networks trained with style-transfer images indeed learn to ignore\nstyle, but its shape bias arises primarily from local shapes. We provide a\nDistorted Shape Testbench (DiST) as an alternative measurement of global shape\nsensitivity. Our test includes 2400 original images from ImageNet-1K, each of\nwhich is accompanied by two images with the global shapes of the original image\ndistorted while preserving its texture via the texture synthesis program. We\nfound that (1) models that performed well on the previous shape bias evaluation\ndo not fare well in the proposed DiST; (2) the widely adopted ViT models do not\nshow significant advantages over Convolutional Neural Networks (CNNs) on this\nbenchmark despite that ViTs rank higher on the previous shape bias tests. (3)\ntraining with DiST images bridges the significant gap between human and\nexisting SOTA models' performance while preserving the models' accuracy on\nstandard image classification tasks; training with DiST images and\nstyle-transferred images are complementary, and can be combined to train\nnetwork together to enhance both the global and local shape sensitivity of the\nnetwork. Our code will be host at: https://github.com/leelabcnbc/DiST\n","authors":["Ziqi Wen","Tianqin Li","Tai Sing Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07552v1","updated":"2023-10-11T14:54:40Z","published":"2023-10-11T14:54:40Z","title":"ProtoHPE: Prototype-guided High-frequency Patch Enhancement for\n  Visible-Infrared Person Re-identification","summary":"  Visible-infrared person re-identification is challenging due to the large\nmodality gap. To bridge the gap, most studies heavily rely on the correlation\nof visible-infrared holistic person images, which may perform poorly under\nsevere distribution shifts. In contrast, we find that some cross-modal\ncorrelated high-frequency components contain discriminative visual patterns and\nare less affected by variations such as wavelength, pose, and background\nclutter than holistic images. Therefore, we are motivated to bridge the\nmodality gap based on such high-frequency components, and propose\n\\textbf{Proto}type-guided \\textbf{H}igh-frequency \\textbf{P}atch\n\\textbf{E}nhancement (ProtoHPE) with two core designs. \\textbf{First}, to\nenhance the representation ability of cross-modal correlated high-frequency\ncomponents, we split patches with such components by Wavelet Transform and\nexponential moving average Vision Transformer (ViT), then empower ViT to take\nthe split patches as auxiliary input. \\textbf{Second}, to obtain semantically\ncompact and discriminative high-frequency representations of the same identity,\nwe propose Multimodal Prototypical Contrast. To be specific, it hierarchically\ncaptures the comprehensive semantics of different modal instances, facilitating\nthe aggregation of high-frequency representations belonging to the same\nidentity. With it, ViT can capture key high-frequency components during\ninference without relying on ProtoHPE, thus bringing no extra complexity.\nExtensive experiments validate the effectiveness of ProtoHPE.\n","authors":["Guiwei Zhang","Yongfei Zhang","Zichang Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07548v1","updated":"2023-10-11T14:50:52Z","published":"2023-10-11T14:50:52Z","title":"Attribute Localization and Revision Network for Zero-Shot Learning","summary":"  Zero-shot learning enables the model to recognize unseen categories with the\naid of auxiliary semantic information such as attributes. Current works\nproposed to detect attributes from local image regions and align extracted\nfeatures with class-level semantics. In this paper, we find that the choice\nbetween local and global features is not a zero-sum game, global features can\nalso contribute to the understanding of attributes. In addition, aligning\nattribute features with class-level semantics ignores potential intra-class\nattribute variation. To mitigate these disadvantages, we present Attribute\nLocalization and Revision Network in this paper. First, we design Attribute\nLocalization Module (ALM) to capture both local and global features from image\nregions, a novel module called Scale Control Unit is incorporated to fuse\nglobal and local representations. Second, we propose Attribute Revision Module\n(ARM), which generates image-level semantics by revising the ground-truth value\nof each attribute, compensating for performance degradation caused by ignoring\nintra-class variation. Finally, the output of ALM will be aligned with revised\nsemantics produced by ARM to achieve the training process. Comprehensive\nexperimental results on three widely used benchmarks demonstrate the\neffectiveness of our model in the zero-shot prediction task.\n","authors":["Junzhe Xu","Suling Duan","Chenwei Tang","Zhenan He","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2310.07548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12067v2","updated":"2023-10-11T14:49:26Z","published":"2023-08-23T11:27:30Z","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4","summary":"  Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.\n","authors":["Lai Wei","Zihao Jiang","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07534v1","updated":"2023-10-11T14:39:12Z","published":"2023-10-11T14:39:12Z","title":"Human-Centered Evaluation of XAI Methods","summary":"  In the ever-evolving field of Artificial Intelligence, a critical challenge\nhas been to decipher the decision-making processes within the so-called \"black\nboxes\" in deep learning. Over recent years, a plethora of methods have emerged,\ndedicated to explaining decisions across diverse tasks. Particularly in tasks\nlike image classification, these methods typically identify and emphasize the\npivotal pixels that most influence a classifier's prediction. Interestingly,\nthis approach mirrors human behavior: when asked to explain our rationale for\nclassifying an image, we often point to the most salient features or aspects.\nCapitalizing on this parallel, our research embarked on a user-centric study.\nWe sought to objectively measure the interpretability of three leading\nexplanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3)\nLayer-wise Relevance Propagation. Intriguingly, our results highlight that\nwhile the regions spotlighted by these methods can vary widely, they all offer\nhumans a nearly equivalent depth of understanding. This enables users to\ndiscern and categorize images efficiently, reinforcing the value of these\nmethods in enhancing AI transparency.\n","authors":["Karam Dawoud","Wojciech Samek","Sebastian Lapuschkin","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2310.07534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12813v2","updated":"2023-10-11T14:35:26Z","published":"2023-07-24T14:06:54Z","title":"Described Object Detection: Liberating Object Detection with Flexible\n  Expressions","summary":"  Detecting objects based on language information is a popular task that\nincludes Open-Vocabulary object Detection (OVD) and Referring Expression\nComprehension (REC). In this paper, we advance them to a more practical setting\ncalled Described Object Detection (DOD) by expanding category names to flexible\nlanguage expressions for OVD and overcoming the limitation of REC only\ngrounding the pre-existing object. We establish the research foundation for DOD\nby constructing a Description Detection Dataset ($D^3$). This dataset features\nflexible language expressions, whether short category names or long\ndescriptions, and annotating all described objects on all images without\nomission. By evaluating previous SOTA methods on $D^3$, we find some\ntroublemakers that fail current REC, OVD, and bi-functional methods. REC\nmethods struggle with confidence scores, rejecting negative instances, and\nmulti-target scenarios, while OVD methods face constraints with long and\ncomplex descriptions. Recent bi-functional methods also do not work well on DOD\ndue to their separated training procedures and inference strategies for REC and\nOVD tasks. Building upon the aforementioned findings, we propose a baseline\nthat largely improves REC methods by reconstructing the training data and\nintroducing a binary classification sub-task, outperforming existing methods.\nData and code are available at https://github.com/shikras/d-cube and related\nworks are tracked in\nhttps://github.com/Charles-Xie/awesome-described-object-detection.\n","authors":["Chi Xie","Zhao Zhang","Yixuan Wu","Feng Zhu","Rui Zhao","Shuang Liang"],"pdf_url":"https://arxiv.org/pdf/2307.12813v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.01950v2","updated":"2023-10-11T14:21:29Z","published":"2023-04-01T09:16:40Z","title":"MP-FedCL: Multiprototype Federated Contrastive Learning for Edge\n  Intelligence","summary":"  Federated learning-assisted edge intelligence enables privacy protection in\nmodern intelligent services. However, not independent and identically\ndistributed (non-IID) distribution among edge clients can impair the local\nmodel performance. The existing single prototype-based strategy represents a\nclass by using the mean of the feature space. However, feature spaces are\nusually not clustered, and a single prototype may not represent a class well.\nMotivated by this, this paper proposes a multi-prototype federated contrastive\nlearning approach (MP-FedCL) which demonstrates the effectiveness of using a\nmulti-prototype strategy over a single-prototype under non-IID settings,\nincluding both label and feature skewness. Specifically, a multi-prototype\ncomputation strategy based on \\textit{k-means} is first proposed to capture\ndifferent embedding representations for each class space, using multiple\nprototypes ($k$ centroids) to represent a class in the embedding space. In each\nglobal round, the computed multiple prototypes and their respective model\nparameters are sent to the edge server for aggregation into a global prototype\npool, which is then sent back to all clients to guide their local training.\nFinally, local training for each client minimizes their own supervised learning\ntasks and learns from shared prototypes in the global prototype pool through\nsupervised contrastive learning, which encourages them to learn knowledge\nrelated to their own class from others and reduces the absorption of unrelated\nknowledge in each global iteration. Experimental results on MNIST, Digit-5,\nOffice-10, and DomainNet show that our method outperforms multiple baselines,\nwith an average test accuracy improvement of about 4.6\\% and 10.4\\% under\nfeature and label non-IID distributions, respectively.\n","authors":["Yu Qiao","Md. Shirajum Munir","Apurba Adhikary","Huy Q. Le","Avi Deb Raha","Chaoning Zhang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2304.01950v2.pdf","comment":"Accepted by IEEE Internet of Things"},{"id":"http://arxiv.org/abs/2310.07522v1","updated":"2023-10-11T14:19:05Z","published":"2023-10-11T14:19:05Z","title":"S4C: Self-Supervised Semantic Scene Completion with Neural Fields","summary":"  3D semantic scene understanding is a fundamental challenge in computer\nvision. It enables mobile agents to autonomously plan and navigate arbitrary\nenvironments. SSC formalizes this challenge as jointly estimating dense\ngeometry and semantic information from sparse observations of a scene. Current\nmethods for SSC are generally trained on 3D ground truth based on aggregated\nLiDAR scans. This process relies on special sensors and annotation by hand\nwhich are costly and do not scale well. To overcome this issue, our work\npresents the first self-supervised approach to SSC called S4C that does not\nrely on 3D ground truth data. Our proposed method can reconstruct a scene from\na single image and only relies on videos and pseudo segmentation ground truth\ngenerated from off-the-shelf image segmentation network during training. Unlike\nexisting methods, which use discrete voxel grids, we represent scenes as\nimplicit semantic fields. This formulation allows querying any point within the\ncamera frustum for occupancy and semantic class. Our architecture is trained\nthrough rendering-based self-supervised losses. Nonetheless, our method\nachieves performance close to fully supervised state-of-the-art methods.\nAdditionally, our method demonstrates strong generalization capabilities and\ncan synthesize accurate segmentation maps for far away viewpoints.\n","authors":["Adrian Hayler","Felix Wimbauer","Dominik Muhle","Christian Rupprecht","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.07522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07517v1","updated":"2023-10-11T14:15:25Z","published":"2023-10-11T14:15:25Z","title":"CM-PIE: Cross-modal perception for interactive-enhanced audio-visual\n  video parsing","summary":"  Audio-visual video parsing is the task of categorizing a video at the segment\nlevel with weak labels, and predicting them as audible or visible events.\nRecent methods for this task leverage the attention mechanism to capture the\nsemantic correlations among the whole video across the audio-visual modalities.\nHowever, these approaches have overlooked the importance of individual segments\nwithin a video and the relationship among them, and tend to rely on a single\nmodality when learning features. In this paper, we propose a novel\ninteractive-enhanced cross-modal perception method~(CM-PIE), which can learn\nfine-grained features by applying a segment-based attention module.\nFurthermore, a cross-modal aggregation block is introduced to jointly optimize\nthe semantic representation of audio and visual signals by enhancing\ninter-modal interactions. The experimental results show that our model offers\nimproved parsing performance on the Look, Listen, and Parse dataset compared to\nother methods.\n","authors":["Yaru Chen","Ruohao Guo","Xubo Liu","Peipei Wu","Guangyao Li","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07517v1.pdf","comment":"5 pages, 3 figures, 15 references"},{"id":"http://arxiv.org/abs/2310.07511v1","updated":"2023-10-11T14:07:05Z","published":"2023-10-11T14:07:05Z","title":"A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes\n  via Deviation Relationship Learning","summary":"  Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets. Given the diversity in earth anomaly types, a\nunified anomaly detector across modalities and scenes should be cost-effective\nand flexible to new earth observation sources and anomaly types. However, the\ncurrent anomaly detectors are limited to a single modality and single scene,\nsince they aim to learn the varying background distribution. Motivated by the\nuniversal anomaly deviation pattern, in that anomalies exhibit deviations from\ntheir local context, we exploit this characteristic to build a unified anomaly\ndetector. Firstly, we reformulate the anomaly detection task as an undirected\nbilayer graph based on the deviation relationship, where the anomaly score is\nmodeled as the conditional probability, given the pattern of the background and\nnormal objects. The learning objective is then expressed as a conditional\nprobability ranking problem. Furthermore, we design an instantiation of the\nreformulation in the data, architecture, and optimization aspects. Simulated\nspectral and spatial anomalies drive the instantiated architecture. The model\nis optimized directly for the conditional probability ranking. The proposed\nmodel was validated in five modalities including the hyperspectral, visible\nlight, synthetic aperture radar (SAR), infrared and low light to show its\nunified detection ability.\n","authors":["Jingtao Li","Xinyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.07511v1.pdf","comment":"Journal paper"},{"id":"http://arxiv.org/abs/2310.07510v1","updated":"2023-10-11T14:06:04Z","published":"2023-10-11T14:06:04Z","title":"Heuristic Vision Pre-Training with Self-Supervised and Supervised\n  Multi-Task Learning","summary":"  To mimic human vision with the way of recognizing the diverse and open world,\nfoundation vision models are much critical. While recent techniques of\nself-supervised learning show the promising potentiality of this mission, we\nargue that signals from labelled data are also important for common-sense\nrecognition, and properly chosen pre-text tasks can facilitate the efficiency\nof vision representation learning. To this end, we propose a novel pre-training\nframework by adopting both self-supervised and supervised visual pre-text tasks\nin a multi-task manner. Specifically, given an image, we take a heuristic way\nby considering its intrinsic style properties, inside objects with their\nlocations and correlations, and how it looks like in 3D space for basic visual\nunderstanding. However, large-scale object bounding boxes and correlations are\nusually hard to achieve. Alternatively, we develop a hybrid method by\nleveraging both multi-label classification and self-supervised learning. On the\none hand, under the multi-label supervision, the pre-trained model can explore\nthe detailed information of an image, e.g., image types, objects, and part of\nsemantic relations. On the other hand, self-supervised learning tasks, with\nrespect to Masked Image Modeling (MIM) and contrastive learning, can help the\nmodel learn pixel details and patch correlations. Results show that our\npre-trained models can deliver results on par with or better than\nstate-of-the-art (SOTA) results on multiple visual tasks. For example, with a\nvanilla Swin-B backbone, we achieve 85.3\\% top-1 accuracy on ImageNet-1K\nclassification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6\nmIoU on ADE-20K semantic segmentation when using Upernet. The performance shows\nthe ability of our vision foundation model to serve general purpose vision\ntasks.\n","authors":["Zhiming Qian"],"pdf_url":"https://arxiv.org/pdf/2310.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07506v1","updated":"2023-10-11T14:02:11Z","published":"2023-10-11T14:02:11Z","title":"Leveraging Hierarchical Feature Sharing for Efficient Dataset\n  Condensation","summary":"  Given a real-world dataset, data condensation (DC) aims to synthesize a\nsignificantly smaller dataset that captures the knowledge of this dataset for\nmodel training with high performance. Recent works propose to enhance DC with\ndata parameterization, which condenses data into parameterized data containers\nrather than pixel space. The intuition behind data parameterization is to\nencode shared features of images to avoid additional storage costs. In this\npaper, we recognize that images share common features in a hierarchical way due\nto the inherent hierarchical structure of the classification system, which is\noverlooked by current data parameterization methods. To better align DC with\nthis hierarchical nature and encourage more efficient information sharing\ninside data containers, we propose a novel data parameterization architecture,\nHierarchical Memory Network (HMN). HMN stores condensed data in a three-tier\nstructure, representing the dataset-level, class-level, and instance-level\nfeatures. Another helpful property of the hierarchical architecture is that HMN\nnaturally ensures good independence among images despite achieving information\nsharing. This enables instance-level pruning for HMN to reduce redundant\ninformation, thereby further minimizing redundancy and enhancing performance.\nWe evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and\nTiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results\nshow that our proposed method outperforms all baselines, even when trained with\na batch-based loss consuming less GPU memory.\n","authors":["Haizhong Zheng","Jiachen Sun","Shutong Wu","Bhavya Kailkhura","Zhuoqing Mao","Chaowei Xiao","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2310.07506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07504v1","updated":"2023-10-11T14:01:36Z","published":"2023-10-11T14:01:36Z","title":"PtychoDV: Vision Transformer-Based Deep Unrolling Network for\n  Ptychographic Image Reconstruction","summary":"  Ptychography is an imaging technique that captures multiple overlapping\nsnapshots of a sample, illuminated coherently by a moving localized probe. The\nimage recovery from ptychographic data is generally achieved via an iterative\nalgorithm that solves a nonlinear phase-field problem derived from measured\ndiffraction patterns. However, these approaches have high computational cost.\nIn this paper, we introduce PtychoDV, a novel deep model-based network designed\nfor efficient, high-quality ptychographic image reconstruction. PtychoDV\ncomprises a vision transformer that generates an initial image from the set of\nraw measurements, taking into consideration their mutual correlations. This is\nfollowed by a deep unrolling network that refines the initial image using\nlearnable convolutional priors and the ptychography measurement model.\nExperimental results on simulated data demonstrate that PtychoDV is capable of\noutperforming existing deep learning methods for this problem, and\nsignificantly reduces computational cost compared to iterative methodologies,\nwhile maintaining competitive performance.\n","authors":["Weijie Gan","Qiuchen Zhai","Michael Thompson McCann","Cristina Garcia Cardona","Ulugbek S. Kamilov","Brendt Wohlberg"],"pdf_url":"https://arxiv.org/pdf/2310.07504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08313v3","updated":"2023-10-11T13:55:29Z","published":"2023-08-16T12:18:27Z","title":"ECPC-IDS:A benchmark endometrail cancer PET/CT image dataset for\n  evaluation of semantic segmentation and detection of hypermetabolic regions","summary":"  Endometrial cancer is one of the most common tumors in the female\nreproductive system and is the third most common gynecological malignancy that\ncauses death after ovarian and cervical cancer. Early diagnosis can\nsignificantly improve the 5-year survival rate of patients. With the\ndevelopment of artificial intelligence, computer-assisted diagnosis plays an\nincreasingly important role in improving the accuracy and objectivity of\ndiagnosis, as well as reducing the workload of doctors. However, the absence of\npublicly available endometrial cancer image datasets restricts the application\nof computer-assisted diagnostic techniques.In this paper, a publicly available\nEndometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation\nand Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically,\nthe segmentation section includes PET and CT images, with a total of 7159\nimages in multiple formats. In order to prove the effectiveness of segmentation\nmethods on ECPC-IDS, five classical deep learning semantic segmentation methods\nare selected to test the image segmentation task. The object detection section\nalso includes PET and CT images, with a total of 3579 images and XML files with\nannotation information. Six deep learning methods are selected for experiments\non the detection task.This study conduct extensive experiments using deep\nlearning-based semantic segmentation and object detection methods to\ndemonstrate the differences between various methods on ECPC-IDS. As far as we\nknow, this is the first publicly available dataset of endometrial cancer with a\nlarge number of multiple images, including a large amount of information\nrequired for image and target detection. ECPC-IDS can aid researchers in\nexploring new algorithms to enhance computer-assisted technology, benefiting\nboth clinical doctors and patients greatly.\n","authors":["Dechao Tang","Tianming Du","Deguo Ma","Zhiyu Ma","Hongzan Sun","Marcin Grzegorzek","Huiyan Jiang","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2308.08313v3.pdf","comment":"14 pages,6 figures"},{"id":"http://arxiv.org/abs/2310.07492v1","updated":"2023-10-11T13:39:11Z","published":"2023-10-11T13:39:11Z","title":"Boosting Black-box Attack to Deep Neural Networks with Conditional\n  Diffusion Models","summary":"  Existing black-box attacks have demonstrated promising potential in creating\nadversarial examples (AE) to deceive deep learning models. Most of these\nattacks need to handle a vast optimization space and require a large number of\nqueries, hence exhibiting limited practical impacts in real-world scenarios. In\nthis paper, we propose a novel black-box attack strategy, Conditional Diffusion\nModel Attack (CDMA), to improve the query efficiency of generating AEs under\nquery-limited situations. The key insight of CDMA is to formulate the task of\nAE synthesis as a distribution transformation problem, i.e., benign examples\nand their corresponding AEs can be regarded as coming from two distinctive\ndistributions and can transform from each other with a particular converter.\nUnlike the conventional \\textit{query-and-optimization} approach, we generate\neligible AEs with direct conditional transform using the aforementioned data\nconverter, which can significantly reduce the number of queries needed. CDMA\nadopts the conditional Denoising Diffusion Probabilistic Model as the\nconverter, which can learn the transformation from clean samples to AEs, and\nensure the smooth development of perturbed noise resistant to various defense\nstrategies. We demonstrate the effectiveness and efficiency of CDMA by\ncomparing it with nine state-of-the-art black-box attacks across three\nbenchmark datasets. On average, CDMA can reduce the query count to a handful of\ntimes; in most cases, the query count is only ONE. We also show that CDMA can\nobtain $>99\\%$ attack success rate for untarget attacks over all datasets and\ntargeted attack over CIFAR-10 with the noise budget of $\\epsilon=16$.\n","authors":["Renyang Liu","Wei Zhou","Tianwei Zhang","Kangjie Chen","Jun Zhao","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2310.07492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11161v2","updated":"2023-10-11T13:29:23Z","published":"2023-04-02T16:03:44Z","title":"altiro3D: Scene representation from single image and novel view\n  synthesis","summary":"  We introduce altiro3D, a free extended library developed to represent reality\nstarting from a given original RGB image or flat video. It allows to generate a\nlight-field (or Native) image or video and get a realistic 3D experience. To\nsynthesize N-number of virtual images and add them sequentially into a Quilt\ncollage, we apply MiDaS models for the monocular depth estimation, simple\nOpenCV and Telea inpainting techniques to map all pixels, and implement a\n'Fast' algorithm to handle 3D projection camera and scene transformations along\nN-viewpoints. We use the degree of depth to move proportionally the pixels,\nassuming the original image to be at the center of all the viewpoints. altiro3D\ncan also be used with DIBR algorithm to compute intermediate snapshots from a\nequivalent 'Real (slower)' camera with N-geometric viewpoints, which requires\nto calibrate a priori several intrinsic and extrinsic camera parameters. We\nadopt a pixel- and device-based Lookup Table to optimize computing time. The\nmultiple viewpoints and video generated from a single image or frame can be\ndisplayed in a free-view LCD display.\n","authors":["E. Canessa","L. Tenze"],"pdf_url":"https://arxiv.org/pdf/2304.11161v2.pdf","comment":"In press (2023) Springer International Journal of Information\n  Technology (IJIT) 10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.07473v1","updated":"2023-10-11T13:19:29Z","published":"2023-10-11T13:19:29Z","title":"FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation","summary":"  Learning to navigate to an image-specified goal is an important but\nchallenging task for autonomous systems. The agent is required to reason the\ngoal location from where a picture is shot. Existing methods try to solve this\nproblem by learning a navigation policy, which captures semantic features of\nthe goal image and observation image independently and lastly fuses them for\npredicting a sequence of navigation actions. However, these methods suffer from\ntwo major limitations. 1) They may miss detailed information in the goal image,\nand thus fail to reason the goal location. 2) More critically, it is hard to\nfocus on the goal-relevant regions in the observation image, because they\nattempt to understand observation without goal conditioning. In this paper, we\naim to overcome these limitations by designing a Fine-grained Goal Prompting\n(FGPrompt) method for image-goal navigation. In particular, we leverage\nfine-grained and high-resolution feature maps in the goal image as prompts to\nperform conditioned embedding, which preserves detailed information in the goal\nimage and guides the observation encoder to pay attention to goal-relevant\nregions. Compared with existing methods on the image-goal navigation benchmark,\nour method brings significant performance improvement on 3 benchmark datasets\n(i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the\nstate-of-the-art success rate by 8% with only 1/50 model size. Project page:\nhttps://xinyusun.github.io/fgprompt-pages\n","authors":["Xinyu Sun","Peihao Chen","Jugang Fan","Thomas H. Li","Jian Chen","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07473v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.07449v1","updated":"2023-10-11T12:51:16Z","published":"2023-10-11T12:51:16Z","title":"PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction","summary":"  Neural surface reconstruction is sensitive to the camera pose noise, even if\nstate-of-the-art pose estimators like COLMAP or ARKit are used. More\nimportantly, existing Pose-NeRF joint optimisation methods have struggled to\nimprove pose accuracy in challenging real-world scenarios. To overcome the\nchallenges, we introduce the pose residual field (\\textbf{PoRF}), a novel\nimplicit representation that uses an MLP for regressing pose updates. This is\nmore robust than the conventional pose parameter optimisation due to parameter\nsharing that leverages global information over the entire sequence.\nFurthermore, we propose an epipolar geometry loss to enhance the supervision\nthat leverages the correspondences exported from COLMAP results without the\nextra computational overhead. Our method yields promising results. On the DTU\ndataset, we reduce the rotation error by 78\\% for COLMAP poses, leading to the\ndecreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the\nMobileBrick dataset that contains casually captured unbounded 360-degree\nvideos, our method refines ARKit poses and improves the reconstruction F1 score\nfrom 69.18 to 75.67, outperforming that with the dataset provided ground-truth\npose (75.14). These achievements demonstrate the efficacy of our approach in\nrefining camera poses and improving the accuracy of neural surface\nreconstruction in real-world scenarios.\n","authors":["Jia-Wang Bian","Wenjing Bian","Victor Adrian Prisacariu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2310.07449v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07440v1","updated":"2023-10-11T12:46:11Z","published":"2023-10-11T12:46:11Z","title":"Distance-based Weighted Transformer Network for Image Completion","summary":"  The challenge of image generation has been effectively modeled as a problem\nof structure priors or transformation. However, existing models have\nunsatisfactory performance in understanding the global input image structures\nbecause of particular inherent features (for example, local inductive prior).\nRecent studies have shown that self-attention is an efficient modeling\ntechnique for image completion problems. In this paper, we propose a new\narchitecture that relies on Distance-based Weighted Transformer (DWT) to better\nunderstand the relationships between an image's components. In our model, we\nleverage the strengths of both Convolutional Neural Networks (CNNs) and DWT\nblocks to enhance the image completion process. Specifically, CNNs are used to\naugment the local texture information of coarse priors and DWT blocks are used\nto recover certain coarse textures and coherent visual structures. Unlike\ncurrent approaches that generally use CNNs to create feature maps, we use the\nDWT to encode global dependencies and compute distance-based weighted feature\nmaps, which substantially minimizes the problem of visual ambiguities.\nMeanwhile, to better produce repeated textures, we introduce Residual Fast\nFourier Convolution (Res-FFC) blocks to combine the encoder's skip features\nwith the coarse features provided by our generator. Furthermore, a simple yet\neffective technique is proposed to normalize the non-zero values of\nconvolutions, and fine-tune the network layers for regularization of the\ngradient norms to provide an efficient training stabiliser. Extensive\nquantitative and qualitative experiments on three challenging datasets\ndemonstrate the superiority of our proposed model compared to existing\napproaches.\n","authors":["Pourya Shamsolmoali","Masoumeh Zareapoor","Huiyu Zhou","Xuelong Li","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2310.07440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07438v1","updated":"2023-10-11T12:41:32Z","published":"2023-10-11T12:41:32Z","title":"DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for\n  Trajectory Prediction","summary":"  Predicting temporally consistent road users' trajectories in a multi-agent\nsetting is a challenging task due to unknown characteristics of agents and\ntheir varying intentions. Besides using semantic map information and modeling\ninteractions, it is important to build an effective mechanism capable of\nreasoning about behaviors at different levels of granularity. To this end, we\npropose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE)\nmethod. Unlike past arts, our approach 1) dynamically predicts agents' goals\nirrespective of particular road structures, such as lanes, allowing the method\nto produce a more accurate estimation of destinations; 2) achieves map\ncompliant predictions by generating future trajectories in a coarse-to-fine\nfashion, where the coarser predictions at a lower frame rate serve as\nintermediate goals; and 3) uses an attention module designed to temporally\nalign predicted trajectories via masked attention. Using the common Argoverse\nbenchmark dataset, we show that our method achieves state-of-the-art\nperformance on various metrics, and further investigate the contributions of\nproposed modules via comprehensive ablation studies.\n","authors":["Rezaul Karim","Soheil Mohamad Alizadeh Shabestary","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2310.07438v1.pdf","comment":"6 tables 4 figures"},{"id":"http://arxiv.org/abs/2205.09615v4","updated":"2023-10-11T12:09:35Z","published":"2022-05-19T15:13:00Z","title":"EXACT: How to Train Your Accuracy","summary":"  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n","authors":["Ivan Karpukhin","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2205.09615v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07419v1","updated":"2023-10-11T12:05:44Z","published":"2023-10-11T12:05:44Z","title":"Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing\n  Else","summary":"  Recent advances in text-to-image diffusion models have enabled the\nphotorealistic generation of images from text prompts. Despite the great\nprogress, existing models still struggle to generate compositional\nmulti-concept images naturally, limiting their ability to visualize human\nimagination. While several recent works have attempted to address this issue,\nthey either introduce additional training or adopt guidance at inference time.\nIn this work, we consider a more ambitious goal: natural multi-concept\ngeneration using a pre-trained diffusion model, and with almost no extra cost.\nTo achieve this goal, we identify the limitations in the text embeddings used\nfor the pre-trained text-to-image diffusion models. Specifically, we observe\nconcept dominance and non-localized contribution that severely degrade\nmulti-concept generation performance. We further design a minimal low-cost\nsolution that overcomes the above issues by tweaking (not re-training) the text\nembeddings for more realistic multi-concept text-to-image generation. Our\nCorrection by Similarities method tweaks the embedding of concepts by\ncollecting semantic features from most similar tokens to localize the\ncontribution. To avoid mixing features of concepts, we also apply Cross-Token\nNon-Maximum Suppression, which excludes the overlap of contributions from\ndifferent concepts. Experiments show that our approach outperforms previous\nmethods in text-to-image, image manipulation, and personalization tasks,\ndespite not introducing additional training or inference costs to the diffusion\nsteps.\n","authors":["Hazarapet Tunanyan","Dejia Xu","Shant Navasardyan","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2310.07419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07416v1","updated":"2023-10-11T12:01:52Z","published":"2023-10-11T12:01:52Z","title":"A Novel Voronoi-based Convolutional Neural Network Framework for Pushing\n  Person Detection in Crowd Videos","summary":"  Analyzing the microscopic dynamics of pushing behavior within crowds can\noffer valuable insights into crowd patterns and interactions. By identifying\ninstances of pushing in crowd videos, a deeper understanding of when, where,\nand why such behavior occurs can be achieved. This knowledge is crucial to\ncreating more effective crowd management strategies, optimizing crowd flow, and\nenhancing overall crowd experiences. However, manually identifying pushing\nbehavior at the microscopic level is challenging, and the existing automatic\napproaches cannot detect such microscopic behavior. Thus, this article\nintroduces a novel automatic framework for identifying pushing in videos of\ncrowds on a microscopic level. The framework comprises two main components: i)\nFeature extraction and ii) Video labeling. In the feature extraction component,\na new Voronoi-based method is developed for determining the local regions\nassociated with each person in the input video. Subsequently, these regions are\nfed into EfficientNetV1B0 Convolutional Neural Network to extract the deep\nfeatures of each person over time. In the second component, a combination of a\nfully connected layer with a Sigmoid activation function is employed to analyze\nthese deep features and annotate the individuals involved in pushing within the\nvideo. The framework is trained and evaluated on a new dataset created using\nsix real-world experiments, including their corresponding ground truths. The\nexperimental findings indicate that the suggested framework outperforms seven\nbaseline methods that are employed for comparative analysis purposes.\n","authors":["Ahmed Alia","Mohammed Maree","Mohcine Chraibi","Armin Seyfried"],"pdf_url":"https://arxiv.org/pdf/2310.07416v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2309.14065v4","updated":"2023-10-11T11:43:41Z","published":"2023-09-25T11:57:16Z","title":"AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile\n  Platform Real-Time RGB-D Semantic Segmentation","summary":"  In the realm of robotic intelligence, achieving efficient and precise RGB-D\nsemantic segmentation is a key cornerstone. State-of-the-art multimodal\nsemantic segmentation methods, primarily rooted in symmetrical skeleton\nnetworks, find it challenging to harmonize computational efficiency and\nprecision. In this work, we propose AsymFormer, a novel network for real-time\nRGB-D semantic segmentation, which targets the minimization of superfluous\nparameters by optimizing the distribution of computational resources and\nintroduces an asymmetrical backbone to allow for the effective fusion of\nmultimodal features. Furthermore, we explore techniques to bolster network\naccuracy by redefining feature selection and extracting multi-modal\nself-similarity features without a substantial increase in the parameter count,\nthereby ensuring real-time execution on robotic platforms. Additionally, a\nLocal Attention-Guided Feature Selection (LAFS) module is used to selectively\nfuse features from different modalities by leveraging their dependencies.\nSubsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding\n(CMA) module is introduced to further extract cross-modal representations. This\nmethod is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer\ndemonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on\nSUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after\nimplementing mixed precision quantization, it attains an impressive inference\nspeed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal\nmethods, thereby demonstrating that AsymFormer can strike a balance between\nhigh accuracy and efficiency for RGB-D semantic segmentation.\n","authors":["Siqi Du","Weixi Wang","Renzhong Guo","Shengjun Tang"],"pdf_url":"https://arxiv.org/pdf/2309.14065v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04707v2","updated":"2023-10-11T11:30:32Z","published":"2023-03-08T16:48:24Z","title":"DiM: Distilling Dataset into Generative Model","summary":"  Dataset distillation reduces the network training cost by synthesizing small\nand informative datasets from large-scale ones. Despite the success of the\nrecent dataset distillation algorithms, three drawbacks still limit their wider\napplication: i). the synthetic images perform poorly on large architectures;\nii). they need to be re-optimized when the distillation ratio changes; iii).\nthe limited diversity restricts the performance when the distillation ratio is\nlarge. In this paper, we propose a novel distillation scheme to\n\\textbf{D}istill information of large train sets \\textbf{i}nto generative\n\\textbf{M}odels, named DiM. Specifically, DiM learns to use a generative model\nto store the information of the target dataset. During the distillation phase,\nwe minimize the differences in logits predicted by a models pool between real\nand generated images. At the deployment stage, the generative model synthesizes\nvarious training samples from random noises on the fly. Due to the simple yet\neffective designs, the trained DiM can be directly applied to different\ndistillation ratios and large architectures without extra cost. We validate the\nproposed DiM across 4 datasets and achieve state-of-the-art results on all of\nthem. To the best of our knowledge, we are the first to achieve higher accuracy\non complex architectures than simple ones, such as 75.1\\% with ResNet-18 and\n72.6\\% with ConvNet-3 on ten images per class of CIFAR-10. Besides, DiM\noutperforms previous methods with 10\\% $\\sim$ 22\\% when images per class are 1\nand 10 on the SVHN dataset.\n","authors":["Kai Wang","Jianyang Gu","Daquan Zhou","Zheng Zhu","Wei Jiang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2303.04707v2.pdf","comment":"Distilling datasets into generative models"},{"id":"http://arxiv.org/abs/2310.05682v2","updated":"2023-10-11T11:28:40Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07394v1","updated":"2023-10-11T11:26:35Z","published":"2023-10-11T11:26:35Z","title":"CLIP for Lightweight Semantic Segmentation","summary":"  The large-scale pretrained model CLIP, trained on 400 million image-text\npairs, offers a promising paradigm for tackling vision tasks, albeit at the\nimage level. Later works, such as DenseCLIP and LSeg, extend this paradigm to\ndense prediction, including semantic segmentation, and have achieved excellent\nresults. However, the above methods either rely on CLIP-pretrained visual\nbackbones or use none-pretrained but heavy backbones such as Swin, while\nfalling ineffective when applied to lightweight backbones. The reason for this\nis that the lightweitht networks, feature extraction ability of which are\nrelatively limited, meet difficulty embedding the image feature aligned with\ntext embeddings perfectly. In this work, we present a new feature fusion module\nwhich tackles this problem and enables language-guided paradigm to be applied\nto lightweight networks. Specifically, the module is a parallel design of CNN\nand transformer with a two-way bridge in between, where CNN extracts spatial\ninformation and visual context of the feature map from the image encoder, and\nthe transformer propagates text embeddings from the text encoder forward. The\ncore of the module is the bidirectional fusion of visual and text feature\nacross the bridge which prompts their proximity and alignment in embedding\nspace. The module is model-agnostic, which can not only make language-guided\nlightweight semantic segmentation practical, but also fully exploit the\npretrained knowledge of language priors and achieve better performance than\nprevious SOTA work, such as DenseCLIP, whatever the vision backbone is.\nExtensive experiments have been conducted to demonstrate the superiority of our\nmethod.\n","authors":["Ke Jin","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07379v1","updated":"2023-10-11T10:54:44Z","published":"2023-10-11T10:54:44Z","title":"Causal Unsupervised Semantic Segmentation","summary":"  Unsupervised semantic segmentation aims to achieve high-quality semantic\ngrouping without human-labeled annotations. With the advent of self-supervised\npre-training, various frameworks utilize the pre-trained features to train\nprediction heads for unsupervised dense prediction. However, a significant\nchallenge in this unsupervised setup is determining the appropriate level of\nclustering required for segmenting concepts. To address it, we propose a novel\nframework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages\ninsights from causal inference. Specifically, we bridge intervention-oriented\napproach (i.e., frontdoor adjustment) to define suitable two-step tasks for\nunsupervised prediction. The first step involves constructing a concept\nclusterbook as a mediator, which represents possible concept prototypes at\ndifferent levels of granularity in a discretized form. Then, the mediator\nestablishes an explicit link to the subsequent concept-wise self-supervised\nlearning for pixel-level grouping. Through extensive experiments and analyses\non various datasets, we corroborate the effectiveness of CAUSE and achieve\nstate-of-the-art performance in unsupervised semantic segmentation.\n","authors":["Junho Kim","Byung-Kwan Lee","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2310.07379v1.pdf","comment":"code available:\n  https://github.com/ByungKwanLee/Causal-Unsupervised-Segmentation"},{"id":"http://arxiv.org/abs/2310.07376v1","updated":"2023-10-11T10:50:15Z","published":"2023-10-11T10:50:15Z","title":"Point Cloud Denoising and Outlier Detection with Local Geometric\n  Structure by Dynamic Graph CNN","summary":"  The digitalization of society is rapidly developing toward the realization of\nthe digital twin and metaverse. In particular, point clouds are attracting\nattention as a media format for 3D space. Point cloud data is contaminated with\nnoise and outliers due to measurement errors. Therefore, denoising and outlier\ndetection are necessary for point cloud processing. Among them, PointCleanNet\nis an effective method for point cloud denoising and outlier detection.\nHowever, it does not consider the local geometric structure of the patch. We\nsolve this problem by applying two types of graph convolutional layer designed\nbased on the Dynamic Graph CNN. Experimental results show that the proposed\nmethods outperform the conventional method in AUPR, which indicates outlier\ndetection accuracy, and Chamfer Distance, which indicates denoising accuracy.\n","authors":["Kosuke Nakayama","Hiroto Fukuta","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2310.07376v1.pdf","comment":"2023 IEEE 12th Global Conference on Consumer Electronics (GCCE 2023)"},{"id":"http://arxiv.org/abs/2303.16570v2","updated":"2023-10-11T10:41:11Z","published":"2023-03-29T10:08:29Z","title":"Point2Vec for Self-Supervised Representation Learning on Point Clouds","summary":"  Recently, the self-supervised learning framework data2vec has shown inspiring\nperformance for various modalities using a masked student-teacher approach.\nHowever, it remains open whether such a framework generalizes to the unique\nchallenges of 3D point clouds. To answer this question, we extend data2vec to\nthe point cloud domain and report encouraging results on several downstream\ntasks. In an in-depth analysis, we discover that the leakage of positional\ninformation reveals the overall object shape to the student even under heavy\nmasking and thus hampers data2vec to learn strong representations for point\nclouds. We address this 3D-specific shortcoming by proposing point2vec, which\nunleashes the full potential of data2vec-like pre-training on point clouds. Our\nexperiments show that point2vec outperforms other self-supervised methods on\nshape classification and few-shot learning on ModelNet40 and ScanObjectNN,\nwhile achieving competitive results on part segmentation on ShapeNetParts.\nThese results suggest that the learned representations are strong and\ntransferable, highlighting point2vec as a promising direction for\nself-supervised learning of point cloud representations.\n","authors":["Karim Abou Zeid","Jonas Schult","Alexander Hermans","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2303.16570v2.pdf","comment":"Accepted at GCPR 2023. Project page at\n  https://vision.rwth-aachen.de/point2vec"},{"id":"http://arxiv.org/abs/2307.00773v3","updated":"2023-10-11T10:29:59Z","published":"2023-07-03T06:33:49Z","title":"DifFSS: Diffusion Model for Few-Shot Semantic Segmentation","summary":"  Diffusion models have demonstrated excellent performance in image generation.\nAlthough various few-shot semantic segmentation (FSS) models with different\nnetwork structures have been proposed, performance improvement has reached a\nbottleneck. This paper presents the first work to leverage the diffusion model\nfor FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve\nthe performance of the state-of-the-art FSS models by a large margin without\nmodifying their network structure. Specifically, we utilize the powerful\ngeneration ability of diffusion models to generate diverse auxiliary support\nimages by using the semantic mask, scribble or soft HED boundary of the support\nimage as control conditions. This generation process simulates the variety\nwithin the class of the query image, such as color, texture variation,\nlighting, $etc$. As a result, FSS models can refer to more diverse support\nimages, yielding more robust representations, thereby achieving a consistent\nimprovement in segmentation performance. Extensive experiments on three\npublicly available datasets based on existing advanced FSS models demonstrate\nthe effectiveness of the diffusion model for FSS task. Furthermore, we explore\nin detail the impact of different input settings of the diffusion model on\nsegmentation performance. Hopefully, this completely new paradigm will bring\ninspiration to the study of FSS task integrated with AI-generated content. Code\nis available at https://github.com/TrinitialChan/DifFSS\n","authors":["Weimin Tan","Siyuan Chen","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2307.00773v3.pdf","comment":"code is available at https://github.com/TrinitialChan/DifFSS"},{"id":"http://arxiv.org/abs/2305.03989v2","updated":"2023-10-11T10:26:27Z","published":"2023-05-06T09:29:12Z","title":"LEO: Generative Latent Image Animator for Human Video Synthesis","summary":"  Spatio-temporal coherency is a major challenge in synthesizing high quality\nvideos, particularly in synthesizing human videos that contain rich global and\nlocal deformations. To resolve this challenge, previous approaches have\nresorted to different features in the generation process aimed at representing\nappearance and motion. However, in the absence of strict mechanisms to\nguarantee such disentanglement, a separation of motion from appearance has\nremained challenging, resulting in spatial distortions and temporal jittering\nthat break the spatio-temporal coherency. Motivated by this, we here propose\nLEO, a novel framework for human video synthesis, placing emphasis on\nspatio-temporal coherency. Our key idea is to represent motion as a sequence of\nflow maps in the generation process, which inherently isolate motion from\nappearance. We implement this idea via a flow-based image animator and a Latent\nMotion Diffusion Model (LMDM). The former bridges a space of motion codes with\nthe space of flow maps, and synthesizes video frames in a warp-and-inpaint\nmanner. LMDM learns to capture motion prior in the training data by\nsynthesizing sequences of motion codes. Extensive quantitative and qualitative\nanalysis suggests that LEO significantly improves coherent synthesis of human\nvideos over previous methods on the datasets TaichiHD, FaceForensics and\nCelebV-HQ. In addition, the effective disentanglement of appearance and motion\nin LEO allows for two additional tasks, namely infinite-length human video\nsynthesis, as well as content-preserving video editing.\n","authors":["Yaohui Wang","Xin Ma","Xinyuan Chen","Antitza Dantcheva","Bo Dai","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.03989v2.pdf","comment":"Project webpage: https://wyhsirius.github.io/LEO-project/"},{"id":"http://arxiv.org/abs/2310.07361v1","updated":"2023-10-11T10:21:34Z","published":"2023-10-11T10:21:34Z","title":"Domain Generalization Guided by Gradient Signal to Noise Ratio of\n  Parameters","summary":"  Overfitting to the source domain is a common issue in gradient-based training\nof deep neural networks. To compensate for the over-parameterized models,\nnumerous regularization techniques have been introduced such as those based on\ndropout. While these methods achieve significant improvements on classical\nbenchmarks such as ImageNet, their performance diminishes with the introduction\nof domain shift in the test set i.e. when the unseen data comes from a\nsignificantly different distribution. In this paper, we move away from the\nclassical approach of Bernoulli sampled dropout mask construction and propose\nto base the selection on gradient-signal-to-noise ratio (GSNR) of network's\nparameters. Specifically, at each training step, parameters with high GSNR will\nbe discarded. Furthermore, we alleviate the burden of manually searching for\nthe optimal dropout ratio by leveraging a meta-learning approach. We evaluate\nour method on standard domain generalization benchmarks and achieve competitive\nresults on classification and face anti-spoofing problems.\n","authors":["Mateusz Michalkiewicz","Masoud Faraki","Xiang Yu","Manmohan Chandraker","Mahsa Baktashmotlagh"],"pdf_url":"https://arxiv.org/pdf/2310.07361v1.pdf","comment":"Paper was accepted to ICCV 2023"},{"id":"http://arxiv.org/abs/2310.07359v1","updated":"2023-10-11T10:17:41Z","published":"2023-10-11T10:17:41Z","title":"Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance\n  Images Using a Hybrid GAN-CNN Method","summary":"  Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive\ncycles of hypomania and depression. Since diagnosing BD relies on subjective\nbehavioral assessments over a long period, a solid diagnosis based on objective\ncriteria is not straightforward. The current study responded to the described\nobstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural\nMRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI\nsamples rather than conventional datasets such as functional MRI (fMRI),\nelectroencephalography (EEG), and behavioral symptoms while removing the data\ninsufficiency usually encountered when dealing with sMRI samples. The impact of\nvarious augmentation ratios is also tested using 5-fold cross-validation. Based\non the results, this study obtains an accuracy rate of 75.8%, a sensitivity of\n60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while\nutilizing less than 6% sample counts. Next, it is demonstrated that a 2- D\nlayer-based GAN generator can effectively reproduce complex 3D brain samples, a\nmore straightforward technique than manual image processing. Lastly, the\noptimum augmentation threshold for the current study using 172 sMRI samples is\n50%, showing the applicability of the described method for larger sMRI\ndatasets. In conclusion, it is established that data augmentation using GAN\nimproves the accuracy of the CNN classifier using sMRI samples, thus developing\nmore reliable decision support systems to assist practitioners in identifying\nBD patients more reliably and in a shorter period\n","authors":["Masood Hamed Saghayan","Mohammad Hossein Zolfagharnasab","Ali Khadem","Farzam Matinfar","Hassan Rashidi"],"pdf_url":"https://arxiv.org/pdf/2310.07359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07355v1","updated":"2023-10-11T10:12:43Z","published":"2023-10-11T10:12:43Z","title":"IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training","summary":"  In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.\n","authors":["Che Liu","Sibo Cheng","Miaojing Shi","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2310.07355v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2307.00040v2","updated":"2023-10-11T10:11:36Z","published":"2023-06-30T17:37:48Z","title":"DisCo: Disentangled Control for Realistic Human Dance Generation","summary":"  Generative AI has made significant strides in computer vision, particularly\nin text-driven image/video synthesis (T2I/T2V). Despite the notable\nadvancements, it remains challenging in human-centric content synthesis such as\nrealistic dance generation. Current methodologies, primarily tailored for human\nmotion transfer, encounter difficulties when confronted with real-world dance\nscenarios (e.g., social media dance) which require to generalize across a wide\nspectrum of poses and intricate human details. In this paper, we depart from\nthe traditional paradigm of human motion transfer and emphasize two additional\ncritical attributes for the synthesis of human dance content in social media\ncontexts: (i) Generalizability: the model should be able to generalize beyond\ngeneric human viewpoints as well as unseen human subjects, backgrounds, and\nposes; (ii) Compositionality: it should allow for composition of seen/unseen\nsubjects, backgrounds, and poses from different sources seamlessly. To address\nthese challenges, we introduce DisCo, which includes a novel model architecture\nwith disentangled control to improve the compositionality of dance synthesis,\nand an effective human attribute pre-training for better generalizability to\nunseen humans. Extensive qualitative and quantitative results demonstrate that\nDisCo can generate high-quality human dance images and videos with diverse\nappearances and flexible motions. Code, demo, video and visualization are\navailable at: https://disco-dance.github.io/.\n","authors":["Tan Wang","Linjie Li","Kevin Lin","Yuanhao Zhai","Chung-Ching Lin","Zhengyuan Yang","Hanwang Zhang","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2307.00040v2.pdf","comment":"Project Page: https://disco-dance.github.io/ ; Add temporal module ;\n  Synchronize FVD computation with MCVD ; More baselines and visualizations"},{"id":"http://arxiv.org/abs/2307.02869v2","updated":"2023-10-11T10:03:08Z","published":"2023-07-06T09:12:13Z","title":"MomentDiff: Generative Video Moment Retrieval from Random to Real","summary":"  Video moment retrieval pursues an efficient and generalized solution to\nidentify the specific temporal segments within an untrimmed video that\ncorrespond to a given language description. To achieve this goal, we provide a\ngenerative diffusion-based framework called MomentDiff, which simulates a\ntypical human retrieval process from random browsing to gradual localization.\nSpecifically, we first diffuse the real span to random noise, and learn to\ndenoise the random noise to the original span with the guidance of similarity\nbetween text and video. This allows the model to learn a mapping from arbitrary\nrandom locations to real moments, enabling the ability to locate segments from\nrandom initialization. Once trained, MomentDiff could sample random temporal\nsegments as initial guesses and iteratively refine them to generate an accurate\ntemporal boundary. Different from discriminative works (e.g., based on\nlearnable proposals or queries), MomentDiff with random initialized spans could\nresist the temporal location biases from datasets. To evaluate the influence of\nthe temporal location biases, we propose two anti-bias datasets with location\ndistribution shifts, named Charades-STA-Len and Charades-STA-Mom. The\nexperimental results demonstrate that our efficient framework consistently\noutperforms state-of-the-art methods on three public benchmarks, and exhibits\nbetter generalization and robustness on the proposed anti-bias datasets. The\ncode, model, and anti-bias evaluation datasets are available at\nhttps://github.com/IMCCretrieval/MomentDiff.\n","authors":["Pandeng Li","Chen-Wei Xie","Hongtao Xie","Liming Zhao","Lei Zhang","Yun Zheng","Deli Zhao","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.02869v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.06841v2","updated":"2023-10-11T09:53:51Z","published":"2023-04-13T22:20:54Z","title":"Video alignment using unsupervised learning of local and global features","summary":"  In this paper, we tackle the problem of video alignment, the process of\nmatching the frames of a pair of videos containing similar actions. The main\nchallenge in video alignment is that accurate correspondence should be\nestablished despite the differences in the execution processes and appearances\nbetween the two videos. We introduce an unsupervised method for alignment that\nuses global and local features of the frames. In particular, we introduce\neffective features for each video frame using three machine vision tools:\nperson detection, pose estimation, and VGG network. Then, the features are\nprocessed and combined to construct a multidimensional time series that\nrepresents the video. The resulting time series are used to align videos of the\nsame actions using a novel version of dynamic time warping named Diagonalized\nDynamic Time Warping(DDTW). The main advantage of our approach is that no\ntraining is required, which makes it applicable for any new type of action\nwithout any need to collect training samples for it. For evaluation, we\nconsidered video synchronization and phase classification tasks on the Penn\naction dataset. Also, for an effective evaluation of the video synchronization\ntask, we present a new metric called Enclosed Area Error(EAE). The results show\nthat our method outperforms previous state-of-the-art methods, such as TCC, and\nother self-supervised and weakly supervised methods.\n","authors":["Niloufar Fakhfour","Mohammad ShahverdiKondori","Hoda Mohammadzade"],"pdf_url":"https://arxiv.org/pdf/2304.06841v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.02044v2","updated":"2023-10-11T09:21:23Z","published":"2023-10-03T13:35:49Z","title":"Video Transformers under Occlusion: How Physics and Background\n  Attributes Impact Large Models for Robotic Manipulation","summary":"  As transformer architectures and dataset sizes continue to scale, the need to\nunderstand the specific dataset factors affecting model performance becomes\nincreasingly urgent. This paper investigates how object physics attributes\n(color, friction coefficient, shape) and background characteristics (static,\ndynamic, background complexity) influence the performance of Video Transformers\nin trajectory prediction tasks under occlusion. Beyond mere occlusion\nchallenges, this study aims to investigate three questions: How do object\nphysics attributes and background characteristics influence the model\nperformance? What kinds of attributes are most influential to the model\ngeneralization? Is there a data saturation point for large transformer model\nperformance within a single task? To facilitate this research, we present\nOccluManip, a real-world video-based robot pushing dataset comprising 460,000\nconsistent recordings of objects with different physics and varying\nbackgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible\ntemporal length along with target object trajectories are collected,\naccommodating tasks with different temporal requirements. Additionally, we\npropose Video Occlusion Transformer (VOT), a generic video-transformer-based\nnetwork achieving an average 96% accuracy across all 18 sub-datasets provided\nin OccluManip. OccluManip and VOT will be released at:\nhttps://github.com/ShutongJIN/OccluManip.git\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v2.pdf","comment":"Under review at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2310.07324v1","updated":"2023-10-11T09:14:30Z","published":"2023-10-11T09:14:30Z","title":"Guided Attention for Interpretable Motion Captioning","summary":"  While much effort has been invested in generating human motion from text,\nrelatively few studies have been dedicated to the reverse direction, that is,\ngenerating text from motion. Much of the research focuses on maximizing\ngeneration quality without any regard for the interpretability of the\narchitectures, particularly regarding the influence of particular body parts in\nthe generation and the temporal synchronization of words with specific\nmovements and actions. This study explores the combination of movement encoders\nwith spatio-temporal attention models and proposes strategies to guide the\nattention during training to highlight perceptually pertinent areas of the\nskeleton in time. We show that adding guided attention with adaptive gate leads\nto interpretable captioning while improving performance compared to higher\nparameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we\nobtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr\nof 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D,\nwe obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA\n+6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our\ncode implementation and reproduction details will be soon available at\nhttps://github.com/rd20karim/M2T-Interpretable/tree/main.\n","authors":["Karim Radouane","Andon Tchechmedjiev","Sylvie Ranwez","Julien Lagarde"],"pdf_url":"https://arxiv.org/pdf/2310.07324v1.pdf","comment":"arXiv preprint"},{"id":"http://arxiv.org/abs/2310.07322v1","updated":"2023-10-11T09:12:42Z","published":"2023-10-11T09:12:42Z","title":"A webcam-based machine learning approach for three-dimensional range of\n  motion evaluation","summary":"  Background. Joint range of motion (ROM) is an important quantitative measure\nfor physical therapy. Commonly relying on a goniometer, accurate and reliable\nROM measurement requires extensive training and practice. This, in turn,\nimposes a significant barrier for those who have limited in-person access to\nhealthcare.\n  Objective. The current study presents and evaluates an alternative machine\nlearning-based ROM evaluation method that could be remotely accessed via a\nwebcam.\n  Methods. To evaluate its reliability, the ROM measurements for a diverse set\nof joints (neck, spine, and upper and lower extremities) derived using this\nmethod were compared to those obtained from a marker-based optical motion\ncapture system.\n  Results. Data collected from 25 healthy adults demonstrated that the webcam\nsolution exhibited high test-retest reliability, with substantial to almost\nperfect intraclass correlation coefficients for most joints. Compared with the\nmarker-based system, the webcam-based system demonstrated substantial to almost\nperfect inter-rater reliability for some joints, and lower inter-rater\nreliability for other joints (e.g., shoulder flexion and elbow flexion), which\ncould be attributed to the reduced sensitivity to joint locations at the apex\nof the movement.\n  Conclusions. The proposed webcam-based method exhibited high test-retest and\ninter-rater reliability, making it a versatile alternative for existing ROM\nevaluation methods in clinical practice and the tele-implementation of physical\ntherapy and rehabilitation.\n","authors":["Xiaoye Michael Wang","Derek T. Smith","Qin Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07310v1","updated":"2023-10-11T08:47:29Z","published":"2023-10-11T08:47:29Z","title":"Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine\n  Learning in Epigraphy","summary":"  Epigraphy increasingly turns to modern artificial intelligence (AI)\ntechnologies such as machine learning (ML) for extracting insights from ancient\ninscriptions. However, scarce labeled data for training ML algorithms severely\nlimits current techniques, especially for ancient scripts like Old Aramaic. Our\nresearch pioneers an innovative methodology for generating synthetic training\ndata tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic\nAramaic letter datasets, incorporating textural features, lighting, damage, and\naugmentations to mimic real-world inscription diversity. Despite minimal real\nexamples, we engineer a dataset of 250,000 training and 25,000 validation\nimages covering the 22 letter classes in the Aramaic alphabet. This\ncomprehensive corpus provides a robust volume of data for training a residual\nneural network (ResNet) to classify highly degraded Aramaic letters. The ResNet\nmodel demonstrates high accuracy in classifying real images from the 8th\ncentury BCE Hadad statue inscription. Additional experiments validate\nperformance on varying materials and styles, proving effective generalization.\nOur results validate the model's capabilities in handling diverse real-world\nscenarios, proving the viability of our synthetic data approach and avoiding\nthe dependence on scarce training data that has constrained epigraphic\nanalysis. Our innovative framework elevates interpretation accuracy on damaged\ninscriptions, thus enhancing knowledge extraction from these historical\nresources.\n","authors":["Andrei C. Aioanei","Regine Hunziker-Rodewald","Konstantin Klein","Dominik L. Michels"],"pdf_url":"https://arxiv.org/pdf/2310.07310v1.pdf","comment":"41 pages, 19 images"},{"id":"http://arxiv.org/abs/2206.07255v2","updated":"2023-10-11T08:41:34Z","published":"2022-06-15T02:35:51Z","title":"GRAM-HD: 3D-Consistent Image Generation at High Resolution with\n  Generative Radiance Manifolds","summary":"  Recent works have shown that 3D-aware GANs trained on unstructured single\nimage collections can generate multiview images of novel instances. The key\nunderpinnings to achieve this are a 3D radiance field generator and a volume\nrendering process. However, existing methods either cannot generate\nhigh-resolution images (e.g., up to 256X256) due to the high computation cost\nof neural volume rendering, or rely on 2D CNNs for image-space upsampling which\njeopardizes the 3D consistency across different views. This paper proposes a\nnovel 3D-aware GAN that can generate high resolution images (up to 1024X1024)\nwhile keeping strict 3D consistency as in volume rendering. Our motivation is\nto achieve super-resolution directly in the 3D space to preserve 3D\nconsistency. We avoid the otherwise prohibitively-expensive computation cost by\napplying 2D convolutions on a set of 2D radiance manifolds defined in the\nrecent generative radiance manifold (GRAM) approach, and apply dedicated loss\nfunctions for effective GAN training at high resolution. Experiments on FFHQ\nand AFHQv2 datasets show that our method can produce high-quality 3D-consistent\nresults that significantly outperform existing methods. It makes a significant\nstep towards closing the gap between traditional 2D image generation and\n3D-consistent free-view generation.\n","authors":["Jianfeng Xiang","Jiaolong Yang","Yu Deng","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2206.07255v2.pdf","comment":"ICCV2023 camera ready version (more results and method comparisons).\n  Project page: https://jeffreyxiang.github.io/GRAM-HD/"},{"id":"http://arxiv.org/abs/2301.11986v2","updated":"2023-10-11T08:25:26Z","published":"2023-01-27T20:54:58Z","title":"Enhancing Face Recognition with Latent Space Data Augmentation and\n  Facial Posture Reconstruction","summary":"  The small amount of training data for many state-of-the-art deep\nlearning-based Face Recognition (FR) systems causes a marked deterioration in\ntheir performance. Although a considerable amount of research has addressed\nthis issue by inventing new data augmentation techniques, using either input\nspace transformations or Generative Adversarial Networks (GAN) for feature\nspace augmentations, these techniques have yet to satisfy expectations. In this\npaper, we propose an approach named the Face Representation Augmentation (FRA)\nfor augmenting face datasets. To the best of our knowledge, FRA is the first\nmethod that shifts its focus towards manipulating the face embeddings generated\nby any face representation learning algorithm to create new embeddings\nrepresenting the same identity and facial emotion but with an altered posture.\nExtensive experiments conducted in this study convince of the efficacy of our\nmethodology and its power to provide noiseless, completely new facial\nrepresentations to improve the training procedure of any FR algorithm.\nTherefore, FRA can help the recent state-of-the-art FR methods by providing\nmore data for training FR systems. The proposed method, using experiments\nconducted on the Karolinska Directed Emotional Faces (KDEF) dataset, improves\nthe identity classification accuracies by 9.52 %, 10.04 %, and 16.60 %, in\ncomparison with the base models of MagFace, ArcFace, and CosFace, respectively.\n","authors":["Soroush Hashemifar","Abdolreza Marefat","Javad Hassannataj Joloudari","Hamid Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2301.11986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04189v2","updated":"2023-10-11T08:01:11Z","published":"2023-10-06T12:08:15Z","title":"Bridging the Gap between Human Motion and Action Semantics via Kinematic\n  Phrases","summary":"  The goal of motion understanding is to establish a reliable mapping between\nmotion and action semantics, while it is a challenging many-to-many problem. An\nabstract action semantic (i.e., walk forwards) could be conveyed by\nperceptually diverse motions (walk with arms up or swinging), while a motion\ncould carry different semantics w.r.t. its context and intention. This makes an\nelegant mapping between them difficult. Previous attempts adopted\ndirect-mapping paradigms with limited reliability. Also, current automatic\nmetrics fail to provide reliable assessments of the consistency between motions\nand action semantics. We identify the source of these problems as the\nsignificant gap between the two modalities. To alleviate this gap, we propose\nKinematic Phrases (KP) that take the objective kinematic facts of human motion\nwith proper abstraction, interpretability, and generality characteristics.\nBased on KP as a mediator, we can unify a motion knowledge base and build a\nmotion understanding system. Meanwhile, KP can be automatically converted from\nmotions and to text descriptions with no subjective bias, inspiring Kinematic\nPrompt Generation (KPG) as a novel automatic motion generation benchmark. In\nextensive experiments, our approach shows superiority over other methods. Our\ncode and data would be made publicly available at https://foruck.github.io/KP.\n","authors":["Xinpeng Liu","Yong-Lu Li","Ailing Zeng","Zizheng Zhou","Yang You","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2310.04189v2.pdf","comment":"Yong-Lu Li and Cewu Lu are the corresponding authors. Project page is\n  available at https://foruck.github.io/KP/"},{"id":"http://arxiv.org/abs/2310.07265v1","updated":"2023-10-11T07:45:37Z","published":"2023-10-11T07:45:37Z","title":"Distilling Efficient Vision Transformers from CNNs for Semantic\n  Segmentation","summary":"  In this paper, we tackle a new problem: how to transfer knowledge from the\npre-trained cumbersome yet well-performed CNN-based model to learn a compact\nVision Transformer (ViT)-based model while maintaining its learning capacity?\nDue to the completely different characteristics of ViT and CNN and the\nlong-existing capacity gap between teacher and student models in Knowledge\nDistillation (KD), directly transferring the cross-model knowledge is\nnon-trivial. To this end, we subtly leverage the visual and\nlinguistic-compatible feature character of ViT (i.e., student), and its\ncapacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD\nframework, dubbed C2VKD. Importantly, as the teacher's features are\nheterogeneous to those of the student, we first propose a novel\nvisual-linguistic feature distillation (VLFD) module that explores efficient KD\namong the aligned visual and linguistic-compatible representations. Moreover,\ndue to the large capacity gap between the teacher and student and the\ninevitable prediction errors of the teacher, we then propose a pixel-wise\ndecoupled distillation (PDD) module to supervise the student under the\ncombination of labels and teacher's predictions from the decoupled target and\nnon-target classes. Experiments on three semantic segmentation benchmark\ndatasets consistently show that the increment of mIoU of our method is over\n200% of the SoTA KD methods\n","authors":["Xu Zheng","Yunhao Luo","Pengyuan Zhou","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13720v5","updated":"2023-10-11T07:39:48Z","published":"2023-06-23T18:08:00Z","title":"Decoupled Diffusion Models: Image to Zero and Zero to Noise","summary":"  Recent diffusion probabilistic models (DPMs) have shown remarkable abilities\nof generated content, however, they often suffer from complex forward\nprocesses, resulting in inefficient solutions for the reversed process and\nprolonged sampling times. In this paper, we aim to address the aforementioned\nchallenges by focusing on the diffusion process itself that we propose to\ndecouple the intricate diffusion process into two comparatively simpler process\nto improve the generative efficacy and speed. In particular, we present a novel\ndiffusion paradigm named DDM (Decoupled Diffusion Models) based on the Ito\ndiffusion process, in which the image distribution is approximated by an\nexplicit transition probability while the noise path is controlled by the\nstandard Wiener process. We find that decoupling the diffusion process reduces\nthe learning difficulty and the explicit transition probability improves the\ngenerative speed significantly. We prove a new training objective for DPM,\nwhich enables the model to learn to predict the noise and image components\nseparately. Moreover, given the novel forward diffusion equation, we derive the\nreverse denoising formula of DDM that naturally supports fewer steps of\ngeneration without ordinary differential equation (ODE) based accelerators. Our\nexperiments demonstrate that DDM outperforms previous DPMs by a large margin in\nfewer function evaluations setting and gets comparable performances in long\nfunction evaluations setting. We also show that our framework can be applied to\nimage-conditioned generation and high-resolution image synthesis, and that it\ncan generate high-quality images with only 10 function evaluations.\n","authors":["Yuhang Huang","Liang Zheng","Zheng Qin","Xinwang Liu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2306.13720v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07259v1","updated":"2023-10-11T07:37:13Z","published":"2023-10-11T07:37:13Z","title":"Uncovering Hidden Connections: Iterative Tracking and Reasoning for\n  Video-grounded Dialog","summary":"  In contrast to conventional visual question answering, video-grounded dialog\nnecessitates a profound understanding of both dialog history and video content\nfor accurate response generation. Despite commendable strides made by existing\nmethodologies, they often grapple with the challenges of incrementally\nunderstanding intricate dialog histories and assimilating video information. In\nresponse to this gap, we present an iterative tracking and reasoning strategy\nthat amalgamates a textual encoder, a visual encoder, and a generator. At its\ncore, our textual encoder is fortified with a path tracking and aggregation\nmechanism, adept at gleaning nuances from dialog history that are pivotal to\ndeciphering the posed questions. Concurrently, our visual encoder harnesses an\niterative reasoning network, meticulously crafted to distill and emphasize\ncritical visual markers from videos, enhancing the depth of visual\ncomprehension. Culminating this enriched information, we employ the pre-trained\nGPT-2 model as our response generator, stitching together coherent and\ncontextually apt answers. Our empirical assessments, conducted on two renowned\ndatasets, testify to the prowess and adaptability of our proposed design.\n","authors":["Haoyu Zhang","Meng Liu","Yaowei Wang","Da Cao","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2310.07259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07255v1","updated":"2023-10-11T07:30:37Z","published":"2023-10-11T07:30:37Z","title":"ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and\n  Multispectral Data Fusion","summary":"  Deep learning-based hyperspectral image (HSI) super-resolution, which aims to\ngenerate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image\n(HSI) and multispectral image (MSI) with deep neural networks (DNNs), has\nattracted lots of attention. However, neural networks require large amounts of\ntraining data, hindering their application in real-world scenarios. In this\nletter, we propose a novel adversarial automatic data augmentation framework\nADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich\ndata diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes\nan augmentor network and two downsampling networks jointly by adversarial\nlearning so that we can learn more robust downsampling networks for training\nthe upsampling network. Extensive experiments on two public classical\nhyperspectral datasets demonstrate the effectiveness of our ADASR compared to\nthe state-of-the-art methods.\n","authors":["Jinghui Qin","Lihuang Fang","Ruitao Lu","Liang Lin","Yukai Shi"],"pdf_url":"https://arxiv.org/pdf/2310.07255v1.pdf","comment":"This paper has been accepted by IEEE Geoscience and Remote Sensing\n  Letters. Code is released at https://github.com/fangfang11-plog/ADASR"},{"id":"http://arxiv.org/abs/2310.07252v1","updated":"2023-10-11T07:30:01Z","published":"2023-10-11T07:30:01Z","title":"A Comparative Study of Pre-trained CNNs and GRU-Based Attention for\n  Image Caption Generation","summary":"  Image captioning is a challenging task involving generating a textual\ndescription for an image using computer vision and natural language processing\ntechniques. This paper proposes a deep neural framework for image caption\ngeneration using a GRU-based attention mechanism. Our approach employs multiple\npre-trained convolutional neural networks as the encoder to extract features\nfrom the image and a GRU-based language model as the decoder to generate\ndescriptive sentences. To improve performance, we integrate the Bahdanau\nattention model with the GRU decoder to enable learning to focus on specific\nimage parts. We evaluate our approach using the MSCOCO and Flickr30k datasets\nand show that it achieves competitive scores compared to state-of-the-art\nmethods. Our proposed framework can bridge the gap between computer vision and\nnatural language and can be extended to specific domains.\n","authors":["Rashid Khan","Bingding Huang","Haseeb Hassan","Asim Zaman","Zhongfu Ye"],"pdf_url":"https://arxiv.org/pdf/2310.07252v1.pdf","comment":"15pages, 10 figures, 5 tables. 2023 the 5th International Conference\n  on Robotics and Computer Vision (ICRCV 2023). arXiv admin note: substantial\n  text overlap with arXiv:2203.01594"},{"id":"http://arxiv.org/abs/2310.07250v1","updated":"2023-10-11T07:27:28Z","published":"2023-10-11T07:27:28Z","title":"Synthesizing Missing MRI Sequences from Available Modalities using\n  Generative Adversarial Networks in BraTS Dataset","summary":"  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic\nresonance imaging (MRI) plays a significant role in the diagnosis, treatment\nplanning, and follow-up of glioblastoma patients due to its non-invasive and\nradiation-free nature. The International Brain Tumor Segmentation (BraTS)\nchallenge has contributed to generating numerous AI algorithms to accurately\nand efficiently segment glioblastoma sub-compartments using four structural\n(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not\nalways be available. To address this issue, Generative Adversarial Networks\n(GANs) can be used to synthesize the missing MRI sequences. In this paper, we\nimplement and utilize an open-source GAN approach that takes any three MRI\nsequences as input to generate the missing fourth structural sequence. Our\nproposed approach is contributed to the community-driven generally nuanced deep\nlearning framework (GaNDLF) and demonstrates promising results in synthesizing\nhigh-quality and realistic MRI sequences, enabling clinicians to improve their\ndiagnostic capabilities and support the application of AI methods to brain\ntumor MRI quantification.\n","authors":["Ibrahim Ethem Hamamci"],"pdf_url":"https://arxiv.org/pdf/2310.07250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07248v1","updated":"2023-10-11T07:25:50Z","published":"2023-10-11T07:25:50Z","title":"IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via\n  Improved Box-dice and Contrastive Latent-anchors","summary":"  Box-supervised polyp segmentation attracts increasing attention for its\ncost-effective potential. Existing solutions often rely on learning-free\nmethods or pretrained models to laboriously generate pseudo masks, triggering\nDice constraint subsequently. In this paper, we found that a model guided by\nthe simplest box-filled masks can accurately predict polyp locations/sizes, but\nsuffers from shape collapsing. In response, we propose two innovative learning\nfashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and\ncombine them to train a robust box-supervised model IBoxCLA. The core idea\nbehind IBoxCLA is to decouple the learning of location/size and shape, allowing\nfor focused constraints on each of them. Specifically, IBox transforms the\nsegmentation map into a proxy map using shape decoupling and confusion-region\nswapping sequentially. Within the proxy map, shapes are disentangled, while\nlocations/sizes are encoded as box-like responses. By constraining the proxy\nmap instead of the raw prediction, the box-filled mask can well supervise\nIBoxCLA without misleading its shape learning. Furthermore, CLA contributes to\nshape learning by generating two types of latent anchors, which are learned and\nupdated using momentum and segmented polyps to steadily represent polyp and\nbackground features. The latent anchors facilitate IBoxCLA to capture\ndiscriminative features within and outside boxes in a contrastive manner,\nyielding clearer boundaries. We benchmark IBoxCLA on five public polyp\ndatasets. The experimental results demonstrate the competitive performance of\nIBoxCLA compared to recent fully-supervised polyp segmentation methods, and its\nsuperiority over other box-supervised state-of-the-arts with a relative\nincrease of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.\n","authors":["Zhiwei Wang","Qiang Hu","Hongkuan Shi","Li He","Man He","Wenxuan Dai","Ting Li","Yitong Zhang","Dun Li","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07247v1","updated":"2023-10-11T07:24:27Z","published":"2023-10-11T07:24:27Z","title":"Optimizing the Placement of Roadside LiDARs for Autonomous Driving","summary":"  Multi-agent cooperative perception is an increasingly popular topic in the\nfield of autonomous driving, where roadside LiDARs play an essential role.\nHowever, how to optimize the placement of roadside LiDARs is a crucial but\noften overlooked problem. This paper proposes an approach to optimize the\nplacement of roadside LiDARs by selecting optimized positions within the scene\nfor better perception performance. To efficiently obtain the best combination\nof locations, a greedy algorithm based on perceptual gain is proposed, which\nselects the location that can maximize the perceptual gain sequentially. We\ndefine perceptual gain as the increased perceptual capability when a new LiDAR\nis placed. To obtain the perception capability, we propose a perception\npredictor that learns to evaluate LiDAR placement using only a single point\ncloud frame. A dataset named Roadside-Opt is created using the CARLA simulator\nto facilitate research on the roadside LiDAR placement problem.\n","authors":["Wentao Jiang","Hao Xiang","Xinyu Cai","Runsheng Xu","Jiaqi Ma","Yikang Li","Gim Hee Lee","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07245v1","updated":"2023-10-11T07:22:37Z","published":"2023-10-11T07:22:37Z","title":"Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs","summary":"  Visual crowd counting estimates the density of the crowd using deep learning\nmodels such as convolution neural networks (CNNs). The performance of the model\nheavily relies on the quality of the training data that constitutes crowd\nimages. In harsh weather such as fog, dust, and low light conditions, the\ninference performance may severely degrade on the noisy and blur images. In\nthis paper, we propose the use of Pix2Pix generative adversarial network (GAN)\nto first denoise the crowd images prior to passing them to the counting model.\nA Pix2Pix network is trained using synthetic noisy images generated from\noriginal crowd images and then the pretrained generator is then used in the\ninference engine to estimate the crowd density in unseen, noisy crowd images.\nThe performance is tested on JHU-Crowd dataset to validate the significance of\nthe proposed method particularly when high reliability and accuracy are\nrequired.\n","authors":["Muhammad Asif Khan","Hamid Menouar","Ridha Hamila"],"pdf_url":"https://arxiv.org/pdf/2310.07245v1.pdf","comment":"The paper has been accepted for presentation in IEEE 38th\n  International Conference on Image and Vision Computing New Zealand (IVCNZ\n  2023). The final manuscript can be accessed at ieeexplore"},{"id":"http://arxiv.org/abs/2310.04991v3","updated":"2023-10-11T07:20:32Z","published":"2023-10-08T03:35:27Z","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and\n  Decoupling","summary":"  This paper proposes Video-Teller, a video-language foundation model that\nleverages multi-modal fusion and fine-grained modality alignment to\nsignificantly enhance the video-to-text generation task. Video-Teller boosts\nthe training efficiency by utilizing frozen pretrained vision and language\nmodules. It capitalizes on the robust linguistic capabilities of large language\nmodels, enabling the generation of both concise and elaborate video\ndescriptions. To effectively integrate visual and auditory information,\nVideo-Teller builds upon the image-based BLIP-2 model and introduces a cascaded\nQ-Former which fuses information across frames and ASR texts. To better guide\nvideo summarization, we introduce a fine-grained modality alignment objective,\nwhere the cascaded Q-Former's output embedding is trained to align with the\ncaption/summary embedding created by a pretrained text auto-encoder.\nExperimental results demonstrate the efficacy of our proposed video-language\nfoundation model in accurately comprehending videos and generating coherent and\nprecise language descriptions. It is worth noting that the fine-grained\nalignment enhances the model's capabilities (4% improvement of CIDEr score on\nMSR-VTT) with only 13% extra parameters in training and zero additional cost in\ninference.\n","authors":["Haogeng Liu","Qihang Fan","Tingkai Liu","Linjie Yang","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05447v2","updated":"2023-10-11T07:10:49Z","published":"2023-10-09T06:43:48Z","title":"Towards Fair and Comprehensive Comparisons for Image-Based 3D Object\n  Detection","summary":"  In this work, we build a modular-designed codebase, formulate strong training\nrecipes, design an error diagnosis toolbox, and discuss current methods for\nimage-based 3D object detection. In particular, different from other highly\nmature tasks, e.g., 2D object detection, the community of image-based 3D object\ndetection is still evolving, where methods often adopt different training\nrecipes and tricks resulting in unfair evaluations and comparisons. What is\nworse, these tricks may overwhelm their proposed designs in performance, even\nleading to wrong conclusions. To address this issue, we build a module-designed\ncodebase and formulate unified training standards for the community.\nFurthermore, we also design an error diagnosis toolbox to measure the detailed\ncharacterization of detection models. Using these tools, we analyze current\nmethods in-depth under varying settings and provide discussions for some open\nquestions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes\ndatasets, which have led to different dominant methods for these datasets. We\nhope that this work will facilitate future research in image-based 3D object\ndetection. Our codes will be released at\n\\url{https://github.com/OpenGVLab/3dodi}\n","authors":["Xinzhu Ma","Yongtao Wang","Yinmin Zhang","Zhiyi Xia","Yuan Meng","Zhihui Wang","Haojie Li","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.05447v2.pdf","comment":"ICCV23, code will be released soon"},{"id":"http://arxiv.org/abs/2301.07807v3","updated":"2023-10-11T07:07:18Z","published":"2023-01-18T22:38:03Z","title":"Measuring uncertainty in human visual segmentation","summary":"  Segmenting visual stimuli into distinct groups of features and visual objects\nis central to visual function. Classical psychophysical methods have helped\nuncover many rules of human perceptual segmentation, and recent progress in\nmachine learning has produced successful algorithms. Yet, the computational\nlogic of human segmentation remains unclear, partially because we lack\nwell-controlled paradigms to measure perceptual segmentation maps and compare\nmodels quantitatively. Here we propose a new, integrated approach: given an\nimage, we measure multiple pixel-based same--different judgments and perform\nmodel--based reconstruction of the underlying segmentation map. The\nreconstruction is robust to several experimental manipulations and captures the\nvariability of individual participants. We demonstrate the validity of the\napproach on human segmentation of natural images and composite textures. We\nshow that image uncertainty affects measured human variability, and it\ninfluences how participants weigh different visual features. Because any\nputative segmentation algorithm can be inserted to perform the reconstruction,\nour paradigm affords quantitative tests of theories of perception as well as\nnew benchmarks for segmentation algorithms.\n","authors":["Jonathan Vacher","Claire Launay","Pascal Mamassian","Ruben Coen-Cagli"],"pdf_url":"https://arxiv.org/pdf/2301.07807v3.pdf","comment":"32 pages, 9 figures, 5 appendix, 5 figures in appendix"},{"id":"http://arxiv.org/abs/2305.17382v3","updated":"2023-10-11T07:02:45Z","published":"2023-05-27T06:24:43Z","title":"APRIL-GAN: A Zero-/Few-Shot Anomaly Classification and Segmentation\n  Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on\n  Zero-shot AD and 4th Place on Few-shot AD","summary":"  In this technical report, we briefly introduce our solution for the\nZero/Few-shot Track of the Visual Anomaly and Novelty Detection (VAND) 2023\nChallenge. For industrial visual inspection, building a single model that can\nbe rapidly adapted to numerous categories without or with only a few normal\nreference images is a promising research direction. This is primarily because\nof the vast variety of the product types. For the zero-shot track, we propose a\nsolution based on the CLIP model by adding extra linear layers. These layers\nare used to map the image features to the joint embedding space, so that they\ncan compare with the text features to generate the anomaly maps. Besides, when\nthe reference images are available, we utilize multiple memory banks to store\ntheir features and compare them with the features of the test images during the\ntesting phase. In this challenge, our method achieved first place in the\nzero-shot track, especially excelling in segmentation with an impressive F1\nscore improvement of 0.0489 over the second-ranked participant. Furthermore, in\nthe few-shot track, we secured the fourth position overall, with our\nclassification F1 score of 0.8687 ranking first among all participating teams.\n","authors":["Xuhai Chen","Yue Han","Jiangning Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.17382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16254v3","updated":"2023-10-11T06:59:47Z","published":"2023-03-28T18:59:17Z","title":"CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using\n  Transformer-based Neural Representations","summary":"  Cryo-electron microscopy (cryo-EM) allows for the high-resolution\nreconstruction of 3D structures of proteins and other biomolecules. Successful\nreconstruction of both shape and movement greatly helps understand the\nfundamental processes of life. However, it is still challenging to reconstruct\nthe continuous motions of 3D structures from hundreds of thousands of noisy and\nrandomly oriented 2D cryo-EM images. Recent advancements use Fourier domain\ncoordinate-based neural networks to continuously model 3D conformations, yet\nthey often struggle to capture local flexible regions accurately. We propose\nCryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction.\nOur approach leverages an implicit feature volume directly in the real domain\nas the 3D representation. We further introduce a novel query-based deformation\ntransformer decoder to improve the reconstruction quality. Our approach is\ncapable of refining pre-computed pose estimations and locating flexible\nregions. In experiments, our method outperforms current approaches on three\npublic datasets (1 synthetic and 2 experimental) and a new synthetic dataset of\nPEDV spike protein. The code and new synthetic dataset will be released for\nbetter reproducibility of our results. Project page:\nhttps://cryoformer.github.io.\n","authors":["Xinhang Liu","Yan Zeng","Yifan Qin","Hao Li","Jiakai Zhang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16254v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07237v1","updated":"2023-10-11T06:58:22Z","published":"2023-10-11T06:58:22Z","title":"SAGE-ICP: Semantic Information-Assisted ICP","summary":"  Robust and accurate pose estimation in unknown environments is an essential\npart of robotic applications. We focus on LiDAR-based point-to-point ICP\ncombined with effective semantic information. This paper proposes a novel\nsemantic information-assisted ICP method named SAGE-ICP, which leverages\nsemantics in odometry. The semantic information for the whole scan is timely\nand efficiently extracted by a 3D convolution network, and these point-wise\nlabels are deeply involved in every part of the registration, including\nsemantic voxel downsampling, data association, adaptive local map, and dynamic\nvehicle removal. Unlike previous semantic-aided approaches, the proposed method\ncan improve localization accuracy in large-scale scenes even if the semantic\ninformation has certain errors. Experimental evaluations on KITTI and KITTI-360\nshow that our method outperforms the baseline methods, and improves accuracy\nwhile maintaining real-time performance, i.e., runs faster than the sensor\nframe rate.\n","authors":["Jiaming Cui","Jiming Chen","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07237v1.pdf","comment":"6+1 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07236v1","updated":"2023-10-11T06:56:08Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07236v1.pdf","comment":"Project Page: https://adamesh.github.io"},{"id":"http://arxiv.org/abs/2309.13438v3","updated":"2023-10-11T06:43:08Z","published":"2023-09-23T17:29:38Z","title":"Rethinking Superpixel Segmentation from Biologically Inspired Mechanisms","summary":"  Recently, advancements in deep learning-based superpixel segmentation methods\nhave brought about improvements in both the efficiency and the performance of\nsegmentation. However, a significant challenge remains in generating\nsuperpixels that strictly adhere to object boundaries while conveying rich\nvisual significance, especially when cross-surface color correlations may\ninterfere with objects. Drawing inspiration from neural structure and visual\nmechanisms, we propose a biological network architecture comprising an Enhanced\nScreening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel\nsegmentation. The ESM enhances semantic information by simulating the\ninteractive projection mechanisms of the visual cortex. Additionally, the BAL\nemulates the spatial frequency characteristics of visual cortical cells to\nfacilitate the generation of superpixels with strong boundary adherence. We\ndemonstrate the effectiveness of our approach through evaluations on both the\nBSDS500 dataset and the NYUv2 dataset.\n","authors":["Tingyu Zhao","Bo Peng","Yuan Sun","Daipeng Yang","Zhenguang Zhang","Xi Wu"],"pdf_url":"https://arxiv.org/pdf/2309.13438v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13311v2","updated":"2023-10-11T06:28:41Z","published":"2023-05-22T17:59:45Z","title":"VDT: General-purpose Video Diffusion Transformers via Mask Modeling","summary":"  This work introduces Video Diffusion Transformer (VDT), which pioneers the\nuse of transformers in diffusion-based video generation. It features\ntransformer blocks with modularized temporal and spatial attention modules to\nleverage the rich spatial-temporal representation inherited in transformers. We\nalso propose a unified spatial-temporal mask modeling mechanism, seamlessly\nintegrated with the model, to cater to diverse video generation scenarios. VDT\noffers several appealing benefits. 1) It excels at capturing temporal\ndependencies to produce temporally consistent video frames and even simulate\nthe physics and dynamics of 3D objects over time. 2) It facilitates flexible\nconditioning information, \\eg, simple concatenation in the token space,\neffectively unifying different token lengths and modalities. 3) Pairing with\nour proposed spatial-temporal mask modeling mechanism, it becomes a\ngeneral-purpose video diffuser for harnessing a range of tasks, including\nunconditional generation, video prediction, interpolation, animation, and\ncompletion, etc. Extensive experiments on these tasks spanning various\nscenarios, including autonomous driving, natural weather, human action, and\nphysics-based simulation, demonstrate the effectiveness of VDT. Additionally,\nwe present comprehensive studies on how \\model handles conditioning information\nwith the mask modeling mechanism, which we believe will benefit future research\nand advance the field. Project page: https:VDT-2023.github.io\n","authors":["Haoyu Lu","Guoxing Yang","Nanyi Fei","Yuqi Huo","Zhiwu Lu","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2305.13311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07223v1","updated":"2023-10-11T06:13:50Z","published":"2023-10-11T06:13:50Z","title":"Deep Learning for blind spectral unmixing of LULC classes with MODIS\n  multispectral time series and ancillary data","summary":"  Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)\ntypes. Spectral unmixing is a technique to extract information from mixed\npixels into their constituent LULC types and corresponding abundance fractions.\nTraditionally, solving this task has relied on either classical methods that\nrequire prior knowledge of endmembers or machine learning methods that avoid\nexplicit endmembers calculation, also known as blind spectral unmixing (BSU).\nMost BSU studies based on Deep Learning (DL) focus on one time-step\nhyperspectral data, yet its acquisition remains quite costly compared with\nmultispectral data. To our knowledge, here we provide the first study on BSU of\nLULC classes using multispectral time series data with DL models. We further\nboost the performance of a Long-Short Term Memory (LSTM)-based model by\nincorporating geographic plus topographic (geo-topographic) and climatic\nancillary information. Our experiments show that combining spectral-temporal\ninput data together with geo-topographic and climatic information substantially\nimproves the abundance estimation of LULC classes in mixed pixels. To carry out\nthis study, we built a new labeled dataset of the region of Andalusia (Spain)\nwith monthly multispectral time series of pixels for the year 2013 from MODIS\nat 460m resolution, for two hierarchical levels of LULC classes, named\nAndalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset\nprovides, at the pixel level, a multispectral time series plus ancillary\ninformation annotated with the abundance of each LULC class inside each pixel.\nThe dataset and code are available to the public.\n","authors":["Jos Rodrguez-Ortega","Rohaifa Khaldi","Domingo Alcaraz-Segura","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2310.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07222v1","updated":"2023-10-11T06:11:42Z","published":"2023-10-11T06:11:42Z","title":"Uni-paint: A Unified Framework for Multimodal Image Inpainting with\n  Pretrained Diffusion Model","summary":"  Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have\ndemonstrated impressive image generation capabilities and have also been\nsuccessfully applied to image inpainting. However, in practice, users often\nrequire more control over the inpainting process beyond textual guidance,\nespecially when they want to composite objects with customized appearance,\ncolor, shape, and layout. Unfortunately, existing diffusion-based inpainting\nmethods are limited to single-modal guidance and require task-specific\ntraining, hindering their cross-modal scalability. To address these\nlimitations, we propose Uni-paint, a unified framework for multimodal\ninpainting that offers various modes of guidance, including unconditional,\ntext-driven, stroke-driven, exemplar-driven inpainting, as well as a\ncombination of these modes. Furthermore, our Uni-paint is based on pretrained\nStable Diffusion and does not require task-specific training on specific\ndatasets, enabling few-shot generalizability to customized images. We have\nconducted extensive qualitative and quantitative evaluations that show our\napproach achieves comparable results to existing single-modal methods while\noffering multimodal inpainting capabilities not available in other methods.\nCode will be available at https://github.com/ysy31415/unipaint.\n","authors":["Shiyuan Yang","Xiaodong Chen","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2310.07222v1.pdf","comment":"Accepted by ACMMM'23"},{"id":"http://arxiv.org/abs/2310.04895v2","updated":"2023-10-11T05:59:53Z","published":"2023-10-07T18:47:17Z","title":"Cell Tracking-by-detection using Elliptical Bounding Boxes","summary":"  Cell detection and tracking are paramount for bio-analysis. Recent approaches\nrely on the tracking-by-model evolution paradigm, which usually consists of\ntraining end-to-end deep learning models to detect and track the cells on the\nframes with promising results. However, such methods require extensive amounts\nof annotated data, which is time-consuming to obtain and often requires\nspecialized annotators. This work proposes a new approach based on the\nclassical tracking-by-detection paradigm that alleviates the requirement of\nannotated data. More precisely, it approximates the cell shapes as oriented\nellipses and then uses generic-purpose oriented object detectors to identify\nthe cells in each frame. We then rely on a global data association algorithm\nthat explores temporal cell similarity using probability distance metrics,\nconsidering that the ellipses relate to two-dimensional Gaussian distributions.\nOur results show that our method can achieve detection and tracking results\ncompetitively with state-of-the-art techniques that require considerably more\nextensive data annotation. Our code is available at:\nhttps://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.\n","authors":["Lucas N. Kirsten","Cludio R. Jung"],"pdf_url":"https://arxiv.org/pdf/2310.04895v2.pdf","comment":"Paper under review on IEEE/ACM Transactions on Computational Biology\n  and Bioinformatics"},{"id":"http://arxiv.org/abs/2310.07212v1","updated":"2023-10-11T05:58:14Z","published":"2023-10-11T05:58:14Z","title":"Multi-Task Learning-Enabled Automatic Vessel Draft Reading for\n  Intelligent Maritime Surveillance","summary":"  The accurate and efficient vessel draft reading (VDR) is an important\ncomponent of intelligent maritime surveillance, which could be exploited to\nassist in judging whether the vessel is normally loaded or overloaded. The\ncomputer vision technique with an excellent price-to-performance ratio has\nbecome a popular medium to estimate vessel draft depth. However, the\ntraditional estimation methods easily suffer from several limitations, such as\nsensitivity to low-quality images, high computational cost, etc. In this work,\nwe propose a multi-task learning-enabled computational method (termed MTL-VDR)\nfor generating highly reliable VDR. In particular, our MTL-VDR mainly consists\nof four components, i.e., draft mark detection, draft scale recognition,\nvessel/water segmentation, and final draft depth estimation. We first construct\na benchmark dataset related to draft mark detection and employ a powerful and\nefficient convolutional neural network to accurately perform the detection\ntask. The multi-task learning method is then proposed for simultaneous draft\nscale recognition and vessel/water segmentation. To obtain more robust VDR\nunder complex conditions (e.g., damaged and stained scales, etc.), the accurate\ndraft scales are generated by an automatic correction method, which is\npresented based on the spatial distribution rules of draft scales. Finally, an\nadaptive computational method is exploited to yield an accurate and robust\ndraft depth. Extensive experiments have been implemented on the realistic\ndataset to compare our MTL-VDR with state-of-the-art methods. The results have\ndemonstrated its superior performance in terms of accuracy, robustness, and\nefficiency. The computational speed exceeds 40 FPS, which satisfies the\nrequirements of real-time maritime surveillance to guarantee vessel traffic\nsafety.\n","authors":["Jingxiang Qu","Ryan Wen Liu","Chenjie Zhao","Yu Guo","Sendren Sheng-Dong Xu","Fenghua Zhu","Yisheng Lv"],"pdf_url":"https://arxiv.org/pdf/2310.07212v1.pdf","comment":"12 pages,11 figures, submitted to IEEE T-ITS"},{"id":"http://arxiv.org/abs/2310.07209v1","updated":"2023-10-11T05:49:47Z","published":"2023-10-11T05:49:47Z","title":"Multi-task Explainable Skin Lesion Classification","summary":"  Skin cancer is one of the deadliest diseases and has a high mortality rate if\nleft untreated. The diagnosis generally starts with visual screening and is\nfollowed by a biopsy or histopathological examination. Early detection can aid\nin lowering mortality rates. Visual screening can be limited by the experience\nof the doctor. Due to the long tail distribution of dermatological datasets and\nsignificant intra-variability between classes, automatic classification\nutilizing computer-aided methods becomes challenging. In this work, we propose\na multitask few-shot-based approach for skin lesions that generalizes well with\nfew labelled data to address the small sample space challenge. The proposed\napproach comprises a fusion of a segmentation network that acts as an attention\nmodule and classification network. The output of the segmentation network helps\nto focus on the most discriminatory features while making a decision by the\nclassification network. To further enhance the classification performance, we\nhave combined segmentation and classification loss in a weighted manner. We\nhave also included the visualization results that explain the decisions made by\nthe algorithm. Three dermatological datasets are used to evaluate the proposed\nmethod thoroughly. We also conducted cross-database experiments to ensure that\nthe proposed approach is generalizable across similar datasets. Experimental\nresults demonstrate the efficacy of the proposed work.\n","authors":["Mahapara Khurshid","Mayank Vatsa","Richa Singh"],"pdf_url":"https://arxiv.org/pdf/2310.07209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v3","updated":"2023-10-11T05:46:28Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high. In light of these observations, we conclude that TTA for\nsegmentation presents significant challenges, and simply using classic TTA\nmethods cannot address this problem well.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang","Yonghui Xu","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2310.05341v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05917v2","updated":"2023-10-11T05:41:16Z","published":"2023-10-09T17:59:12Z","title":"Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input","summary":"  Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.\n","authors":["Donglai Xiang","Fabian Prada","Zhe Cao","Kaiwen Guo","Chenglei Wu","Jessica Hodgins","Timur Bagautdinov"],"pdf_url":"https://arxiv.org/pdf/2310.05917v2.pdf","comment":"SIGGRAPH Asia 2023 Conference Paper. Project website:\n  https://xiangdonglai.github.io/www-sa23-drivable-clothing/"},{"id":"http://arxiv.org/abs/2310.07206v1","updated":"2023-10-11T05:34:36Z","published":"2023-10-11T05:34:36Z","title":"DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via\n  Physics Simulation","summary":"  This paper addresses the task of 3D pose estimation for a hand interacting\nwith an object from a single image observation. When modeling hand-object\ninteraction, previous works mainly exploit proximity cues, while overlooking\nthe dynamical nature that the hand must stably grasp the object to counteract\ngravity and thus preventing the object from slipping or falling. These works\nfail to leverage dynamical constraints in the estimation and consequently often\nproduce unstable results. Meanwhile, refining unstable configurations with\nphysics-based reasoning remains challenging, both by the complexity of contact\ndynamics and by the lack of effective and efficient physics inference in the\ndata-driven learning framework. To address both issues, we present DeepSimHO: a\nnovel deep-learning pipeline that combines forward physics simulation and\nbackward gradient approximation with a neural network. Specifically, for an\ninitial hand-object pose estimated by a base network, we forward it to a\nphysics simulator to evaluate its stability. However, due to non-smooth contact\ngeometry and penetration, existing differentiable simulators can not provide\nreliable state gradient. To remedy this, we further introduce a deep network to\nlearn the stability evaluation process from the simulator, while smoothly\napproximating its gradient and thus enabling effective back-propagation.\nExtensive experiments show that our method noticeably improves the stability of\nthe estimation and achieves superior efficiency over test-time optimization.\nThe code is available at https://github.com/rongakowang/DeepSimHO.\n","authors":["Rong Wang","Wei Mao","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2310.07206v1.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.17421v2","updated":"2023-10-11T05:07:37Z","published":"2023-09-29T17:34:51Z","title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)","summary":"  Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n","authors":["Zhengyuan Yang","Linjie Li","Kevin Lin","Jianfeng Wang","Chung-Ching Lin","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2309.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07189v1","updated":"2023-10-11T04:38:21Z","published":"2023-10-11T04:38:21Z","title":"SpikePoint: An Efficient Point-based Spiking Neural Network for Event\n  Cameras Action Recognition","summary":"  Event cameras are bio-inspired sensors that respond to local changes in light\nintensity and feature low latency, high energy efficiency, and high dynamic\nrange. Meanwhile, Spiking Neural Networks (SNNs) have gained significant\nattention due to their remarkable efficiency and fault tolerance. By\nsynergistically harnessing the energy efficiency inherent in event cameras and\nthe spike-based processing capabilities of SNNs, their integration could enable\nultra-low-power application scenarios, such as action recognition tasks.\nHowever, existing approaches often entail converting asynchronous events into\nconventional frames, leading to additional data mapping efforts and a loss of\nsparsity, contradicting the design concept of SNNs and event cameras. To\naddress this challenge, we propose SpikePoint, a novel end-to-end point-based\nSNN architecture. SpikePoint excels at processing sparse event cloud data,\neffectively extracting both global and local features through a singular-stage\nstructure. Leveraging the surrogate training method, SpikePoint achieves high\naccuracy with few parameters and maintains low power consumption, specifically\nemploying the identity mapping feature extractor on diverse datasets.\nSpikePoint achieves state-of-the-art (SOTA) performance on four event-based\naction recognition datasets using only 16 timesteps, surpassing other SNN\nmethods. Moreover, it also achieves SOTA performance across all methods on\nthree datasets, utilizing approximately 0.3\\% of the parameters and 0.5\\% of\npower consumption employed by artificial neural networks (ANNs). These results\nemphasize the significance of Point Cloud and pave the way for many\nultra-low-power event-based data processing applications.\n","authors":["Hongwei Ren","Yue Zhou","Yulong Huang","Haotian Fu","Xiaopeng Lin","Jie Song","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.07189v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07186v1","updated":"2023-10-11T04:25:24Z","published":"2023-10-11T04:25:24Z","title":"Multiview Transformer: Rethinking Spatial Information in Hyperspectral\n  Image Classification","summary":"  Identifying the land cover category for each pixel in a hyperspectral image\n(HSI) relies on spectral and spatial information. An HSI cuboid with a specific\npatch size is utilized to extract spatial-spectral feature representation for\nthe central pixel. In this article, we investigate that scene-specific but not\nessential correlations may be recorded in an HSI cuboid. This additional\ninformation improves the model performance on existing HSI datasets and makes\nit hard to properly evaluate the ability of a model. We refer to this problem\nas the spatial overfitting issue and utilize strict experimental settings to\navoid it. We further propose a multiview transformer for HSI classification,\nwhich consists of multiview principal component analysis (MPCA), spectral\nencoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT).\nMPCA performs dimension reduction on an HSI via constructing spectral multiview\nobservations and applying PCA on each view data to extract low-dimensional view\nrepresentation. The combination of view representations, named multiview\nrepresentation, is the dimension reduction output of the MPCA. To aggregate the\nmultiview information, a fully-convolutional SED with a U-shape in spectral\ndimension is introduced to extract a multiview feature map. SPTT transforms the\nmultiview features into tokens using the spatial-pooling tokenization strategy\nand learns robust and discriminative spatial-spectral features for land cover\nidentification. Classification is conducted with a linear classifier.\nExperiments on three HSI datasets with rigid settings demonstrate the\nsuperiority of the proposed multiview transformer over the state-of-the-art\nmethods.\n","authors":["Jie Zhang","Yongshan Zhang","Yicong Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.07186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07184v1","updated":"2023-10-11T04:20:32Z","published":"2023-10-11T04:20:32Z","title":"NeuroInspect: Interpretable Neuron-based Debugging Framework through\n  Class-conditional Visualizations","summary":"  Despite deep learning (DL) has achieved remarkable progress in various\ndomains, the DL models are still prone to making mistakes. This issue\nnecessitates effective debugging tools for DL practitioners to interpret the\ndecision-making process within the networks. However, existing debugging\nmethods often demand extra data or adjustments to the decision process,\nlimiting their applicability. To tackle this problem, we present NeuroInspect,\nan interpretable neuron-based debugging framework with three key stages:\ncounterfactual explanations, feature visualizations, and false correlation\nmitigation. Our debugging framework first pinpoints neurons responsible for\nmistakes in the network and then visualizes features embedded in the neurons to\nbe human-interpretable. To provide these explanations, we introduce\nCLIP-Illusion, a novel feature visualization method that generates images\nrepresenting features conditioned on classes to examine the connection between\nneurons and the decision layer. We alleviate convoluted explanations of the\nconventional visualization approach by employing class information, thereby\nisolating mixed properties. This process offers more human-interpretable\nexplanations for model errors without altering the trained network or requiring\nadditional data. Furthermore, our framework mitigates false correlations\nlearned from a dataset under a stochastic perspective, modifying decisions for\nthe neurons considered as the main causes. We validate the effectiveness of our\nframework by addressing false correlations and improving inferences for classes\nwith the worst performance in real-world settings. Moreover, we demonstrate\nthat NeuroInspect helps debug the mistakes of DL models through evaluation for\nhuman understanding. The code is openly available at\nhttps://github.com/yeongjoonJu/NeuroInspect.\n","authors":["Yeong-Joon Ju","Ji-Hoon Park","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07184v1.pdf","comment":"Summitted to IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2310.07179v1","updated":"2023-10-11T04:05:11Z","published":"2023-10-11T04:05:11Z","title":"rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera","summary":"  Novel view synthesis of satellite images holds a wide range of practical\napplications. While recent advances in the Neural Radiance Field have\npredominantly targeted pin-hole cameras, and models for satellite cameras often\ndemand sufficient input views. This paper presents rpcPRF, a Multiplane Images\n(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).\nUnlike coordinate-based neural radiance fields in need of sufficient views of\none scene, our model is applicable to single or few inputs and performs well on\nimages from unseen scenes. To enable generalization across scenes, we propose\nto use reprojection supervision to induce the predicted MPI to learn the\ncorrect geometry between the 3D coordinates and the images. Moreover, we remove\nthe stringent requirement of dense depth supervision from deep\nmultiview-stereo-based methods by introducing rendering techniques of radiance\nfields. rpcPRF combines the superiority of implicit representations and the\nadvantages of the RPC model, to capture the continuous altitude space while\nlearning the 3D structure. Given an RGB image and its corresponding RPC, the\nend-to-end model learns to synthesize the novel view with a new RPC and\nreconstruct the altitude of the scene. When multiple views are provided as\ninputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC\ndataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF\noutperforms state-of-the-art nerf-based methods by a significant margin in\nterms of image fidelity, reconstruction accuracy, and efficiency, for both\nsingle-view and multiview task.\n","authors":["Tongtong Zhang","Yuanxiang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05136v3","updated":"2023-10-11T04:04:07Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v3.pdf","comment":"Adjust the subject"},{"id":"http://arxiv.org/abs/2310.07176v1","updated":"2023-10-11T04:00:17Z","published":"2023-10-11T04:00:17Z","title":"Improving mitosis detection on histopathology images using large\n  vision-language models","summary":"  In certain types of cancerous tissue, mitotic count has been shown to be\nassociated with tumor proliferation, poor prognosis, and therapeutic\nresistance. Due to the high inter-rater variability of mitotic counting by\npathologists, convolutional neural networks (CNNs) have been employed to reduce\nthe subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained\nwhole slide images. However, most existing models have performance that lags\nbehind expert panel review and only incorporate visual information. In this\nwork, we demonstrate that pre-trained large-scale vision-language models that\nleverage both visual features and natural language improve mitosis detection\naccuracy. We formulate the mitosis detection task as an image captioning task\nand a visual question answering (VQA) task by including metadata such as tumor\nand scanner types as context. The effectiveness of our pipeline is demonstrated\nvia comparison with various baseline models using 9,501 mitotic figures and\n11,051 hard negatives (non-mitotic figures that are difficult to characterize)\nfrom the publicly available Mitosis Domain Generalization Challenge (MIDOG22)\ndataset.\n","authors":["Ruiwen Ding","James Hall","Neil Tenenholtz","Kristen Severson"],"pdf_url":"https://arxiv.org/pdf/2310.07176v1.pdf","comment":"Submitted to IEEE ISBI 2024. Under review"},{"id":"http://arxiv.org/abs/2310.07166v1","updated":"2023-10-11T03:29:13Z","published":"2023-10-11T03:29:13Z","title":"Anchor-based Multi-view Subspace Clustering with Hierarchical Feature\n  Descent","summary":"  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. We dig out as well as deploy the dependency amongst views\nthrough hierarchical feature descent, which leads to a common latent space(\nSTAGE 1). This latent space, for the first time of its kind, is regarded as a\n'resemblance space', as it reveals certain correlations and dependencies of\ndifferent views. To be exact, the one-hot encoding of a category can also be\nreferred to as a resemblance space in its terminal phase. Moreover, due to the\nintrinsic fact that most of the existing multi-view clustering algorithms stem\nfrom k-means clustering and spectral clustering, this results in cubic time\ncomplexity w.r.t. the number of the objects. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\nfurther reduce the computing complexity to linear time cost through a unified\nsampling strategy in resemblance space( STAGE 2), followed by subspace\nclustering to learn the representation collectively( STAGE 3). Extensive\nexperimental results on public benchmark datasets demonstrate that our proposed\nmodel consistently outperforms the state-of-the-art techniques.\n","authors":["Qiyuan Ou","Siwei Wang","Pei Zhang","Sihang Zhou","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17546v2","updated":"2023-10-11T03:19:18Z","published":"2023-03-30T17:13:56Z","title":"PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor","summary":"  Generative image editing has recently witnessed extremely fast-paced growth.\nSome works use high-level conditioning such as text, while others use low-level\nconditioning. Nevertheless, most of them lack fine-grained control over the\nproperties of the different objects present in the image, i.e.\\,object-level\nimage editing. In this work, we tackle the task by perceiving the images as an\namalgamation of various objects and aim to control the properties of each\nobject in a fine-grained manner. Out of these properties, we identify structure\nand appearance as the most intuitive to understand and useful for editing\npurposes. We propose \\textbf{PAIR} Diffusion, a generic framework that can\nenable a diffusion model to control the structure and appearance properties of\neach object in the image. We show that having control over the properties of\neach object in an image leads to comprehensive editing capabilities. Our\nframework allows for various object-level editing operations on real images\nsuch as reference image-based appearance editing, free-form shape editing,\nadding objects, and variations. Thanks to our design, we do not require any\ninversion step. Additionally, we propose multimodal classifier-free guidance\nwhich enables editing images using both reference images and text when using\nour approach with foundational diffusion models. We validate the above claims\nby extensively evaluating our framework on both unconditional and foundational\ndiffusion models. Please refer to\nhttps://vidit98.github.io/publication/conference-paper/pair_diff.html for code\nand model release.\n","authors":["Vidit Goel","Elia Peruzzo","Yifan Jiang","Dejia Xu","Xingqian Xu","Nicu Sebe","Trevor Darrell","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17546v2.pdf","comment":"26 pages and 17 figures"},{"id":"http://arxiv.org/abs/2202.07870v2","updated":"2023-10-11T03:11:14Z","published":"2022-02-16T05:47:31Z","title":"IPD:An Incremental Prototype based DBSCAN for large-scale data with\n  cluster representatives","summary":"  DBSCAN is a fundamental density-based clustering technique that identifies\nany arbitrary shape of the clusters. However, it becomes infeasible while\nhandling big data. On the other hand, centroid-based clustering is important\nfor detecting patterns in a dataset since unprocessed data points can be\nlabeled to their nearest centroid. However, it can not detect non-spherical\nclusters. For a large data, it is not feasible to store and compute labels of\nevery samples. These can be done as and when the information is required. The\npurpose can be accomplished when clustering act as a tool to identify cluster\nrepresentatives and query is served by assigning cluster labels of nearest\nrepresentative. In this paper, we propose an Incremental Prototype-based DBSCAN\n(IPD) algorithm which is designed to identify arbitrary-shaped clusters for\nlarge-scale data. Additionally, it chooses a set of representatives for each\ncluster.\n","authors":["Jayasree Saha","Jayanta Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2202.07870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06925v2","updated":"2023-10-11T02:52:46Z","published":"2023-04-14T05:21:47Z","title":"YOLO-Drone:Airborne real-time detection of dense small objects from\n  high-altitude perspective","summary":"  Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote\nsensing object detection technology, have rapidly gained a broad spectrum of\napplications and emerged as one of the primary research focuses in the field of\ncomputer vision. Although UAV remote sensing systems have the ability to detect\nvarious objects, small-scale objects can be challenging to detect reliably due\nto factors such as object size, image degradation, and real-time limitations.\nTo tackle these issues, a real-time object detection algorithm (YOLO-Drone) is\nproposed and applied to two new UAV platforms as well as a specific light\nsource (silicon-based golden LED). YOLO-Drone presents several novelties: 1)\nincluding a new backbone Darknet59; 2) a new complex feature aggregation module\nMSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial\npyramid pooling modules; 3) and the use of Generalized Intersection over Union\n(GIoU) as the loss function. To evaluate performance, two benchmark datasets,\nUAVDT and VisDrone, along with one homemade dataset acquired at night under\nsilicon-based golden LEDs, are utilized. The experimental results show that, in\nboth UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art\n(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,\nrespectively. With regards to UAVDT, the YOLO-Drone exhibits both high\nreal-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,\nYOLO-Drone achieves high performance under the silicon-based golden LEDs, with\na mAP of up to 87.71%, surpassing the performance of YOLO series under ordinary\nlight sources. To conclude, the proposed YOLO-Drone is a highly effective\nsolution for object detection in UAV applications, particularly for night\ndetection tasks where silicon-based golden light LED technology exhibits\nsignificant superiority.\n","authors":["Li Zhu","Jiahui Xiong","Feng Xiong","Hanzheng Hu","Zhengnan Jiang"],"pdf_url":"https://arxiv.org/pdf/2304.06925v2.pdf","comment":"Some contributing authors are not signed"},{"id":"http://arxiv.org/abs/2310.07149v1","updated":"2023-10-11T02:50:16Z","published":"2023-10-11T02:50:16Z","title":"Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via\n  Edge Concatenation","summary":"  The generalization capability of unsupervised domain adaptation can mitigate\nthe need for extensive pixel-level annotations to train semantic segmentation\nnetworks by training models on synthetic data as a source with\ncomputer-generated annotations. Entropy-based adversarial networks are proposed\nto improve source domain prediction; however, they disregard significant\nexternal information, such as edges, which have the potential to identify and\ndistinguish various objects within an image accurately. To address this issue,\nwe introduce a novel approach to domain adaptation, leveraging the synergy of\ninternal and external information within entropy-based adversarial networks. In\nthis approach, we enrich the discriminator network with edge-predicted\nprobability values within this innovative framework to enhance the clarity of\nclass boundaries. Furthermore, we devised a probability-sharing network that\nintegrates diverse information for more effective segmentation. Incorporating\nobject edges addresses a pivotal aspect of unsupervised domain adaptation that\nhas frequently been neglected in the past -- the precise delineation of object\nboundaries. Conventional unsupervised domain adaptation methods usually center\naround aligning feature distributions and may not explicitly model object\nboundaries. Our approach effectively bridges this gap by offering clear\nguidance on object boundaries, thereby elevating the quality of domain\nadaptation. Our approach undergoes rigorous evaluation on the established\nunsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA\n$\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Mapillary. Experimental\nresults show that the proposed model attains better performance than\nstate-of-the-art methods. The superior performance across different\nunsupervised domain adaptation scenarios highlights the versatility and\nrobustness of the proposed method.\n","authors":["Hye-Seong Hong","Abhishek Kumar","Dong-Gyu Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06282v2","updated":"2023-10-11T02:46:12Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15854v2","updated":"2023-10-11T02:34:23Z","published":"2023-08-30T08:40:15Z","title":"Zero-shot Inversion Process for Image Attribute Editing with Diffusion\n  Models","summary":"  Denoising diffusion models have shown outstanding performance in image\nediting. Existing works tend to use either image-guided methods, which provide\na visual reference but lack control over semantic coherence, or text-guided\nmethods, which ensure faithfulness to text guidance but lack visual quality. To\naddress the problem, we propose the Zero-shot Inversion Process (ZIP), a\nframework that injects a fusion of generated visual reference and text guidance\ninto the semantic latent space of a \\textit{frozen} pre-trained diffusion\nmodel. Only using a tiny neural network, the proposed ZIP produces diverse\ncontent and attributes under the intuitive control of the text prompt.\nMoreover, ZIP shows remarkable robustness for both in-domain and out-of-domain\nattribute manipulation on real images. We perform detailed experiments on\nvarious benchmark datasets. Compared to state-of-the-art methods, ZIP produces\nimages of equivalent quality while providing a realistic editing effect.\n","authors":["Zhanbo Feng","Zenan Ling","Ci Gong","Feng Zhou","Jie Li","Robert C. Qiu"],"pdf_url":"https://arxiv.org/pdf/2308.15854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07138v1","updated":"2023-10-11T02:23:18Z","published":"2023-10-11T02:23:18Z","title":"Denoising Task Routing for Diffusion Models","summary":"  Diffusion models generate highly realistic images through learning a\nmulti-step denoising process, naturally embodying the principles of multi-task\nlearning (MTL). Despite the inherent connection between diffusion models and\nMTL, there remains an unexplored area in designing neural architectures that\nexplicitly incorporate MTL into the framework of diffusion models. In this\npaper, we present Denoising Task Routing (DTR), a simple add-on strategy for\nexisting diffusion model architectures to establish distinct information\npathways for individual tasks within a single architecture by selectively\nactivating subsets of channels in the model. What makes DTR particularly\ncompelling is its seamless integration of prior knowledge of denoising tasks\ninto the framework: (1) Task Affinity: DTR activates similar channels for tasks\nat adjacent timesteps and shifts activated channels as sliding windows through\ntimesteps, capitalizing on the inherent strong affinity between tasks at\nadjacent timesteps. (2) Task Weights: During the early stages (higher\ntimesteps) of the denoising process, DTR assigns a greater number of\ntask-specific channels, leveraging the insight that diffusion models prioritize\nreconstructing global structure and perceptually rich contents in earlier\nstages, and focus on simple noise removal in later stages. Our experiments\ndemonstrate that DTR consistently enhances the performance of diffusion models\nacross various evaluation protocols, all without introducing additional\nparameters. Furthermore, DTR contributes to accelerating convergence during\ntraining. Finally, we show the complementarity between our architectural\napproach and existing MTL optimization techniques, providing a more complete\nview of MTL within the context of diffusion training.\n","authors":["Byeongjun Park","Sangmin Woo","Hyojun Go","Jin-Young Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07131v1","updated":"2023-10-11T02:08:05Z","published":"2023-10-11T02:08:05Z","title":"Echocardiography video synthesis from end diastolic semantic map via\n  diffusion model","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated\nsignificant achievements in various image and video generation tasks, including\nthe domain of medical imaging. However, generating echocardiography videos\nbased on semantic anatomical information remains an unexplored area of\nresearch. This is mostly due to the constraints imposed by the currently\navailable datasets, which lack sufficient scale and comprehensive frame-wise\nannotations for every cardiac cycle. This paper aims to tackle the\naforementioned challenges by expanding upon existing video diffusion models for\nthe purpose of cardiac video synthesis. More specifically, our focus lies in\ngenerating video using semantic maps of the initial frame during the cardiac\ncycle, commonly referred to as end diastole. To further improve the synthesis\nprocess, we integrate spatial adaptive normalization into multiscale feature\nmaps. This enables the inclusion of semantic guidance during synthesis,\nresulting in enhanced realism and coherence of the resultant video sequences.\nExperiments are conducted on the CAMUS dataset, which is a highly used dataset\nin the field of echocardiography. Our model exhibits better performance\ncompared to the standard diffusion technique in terms of multiple metrics,\nincluding FID, FVD, and SSMI.\n","authors":["Phi Nguyen Van","Duc Tran Minh","Hieu Pham Huy","Long Tran Quoc"],"pdf_url":"https://arxiv.org/pdf/2310.07131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01636v2","updated":"2023-10-11T02:02:48Z","published":"2023-10-02T21:02:23Z","title":"Adaptive Visual Scene Understanding: Incremental Scene Graph Generation","summary":"  Scene graph generation (SGG) involves analyzing images to extract meaningful\ninformation about objects and their relationships. Given the dynamic nature of\nthe visual world, it becomes crucial for AI systems to detect new objects and\nestablish their new relationships with existing objects. To address the lack of\ncontinual learning methodologies in SGG, we introduce the comprehensive\nContinual ScenE Graph Generation (CSEGG) dataset along with 3 learning\nscenarios and 8 evaluation metrics. Our research investigates the continual\nlearning performances of existing SGG methods on the retention of previous\nobject entities and relationships as they learn new ones. Moreover, we also\nexplore how continual object detection enhances generalization in classifying\nknown relationships on unknown objects. We conduct extensive experiments\nbenchmarking and analyzing the classical two-stage SGG methods and the most\nrecent transformer-based SGG methods in continual learning settings, and gain\nvaluable insights into the CSEGG problem. We invite the research community to\nexplore this emerging field of study.\n","authors":["Naitik Khandelwal","Xiao Liu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.01636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15142v4","updated":"2023-10-11T01:33:21Z","published":"2023-06-27T02:03:46Z","title":"LRANet: Towards Accurate and Efficient Scene Text Detection with\n  Low-Rank Approximation Network","summary":"  Recently, regression-based methods, which predict parameterized text shapes\nfor text localization, have gained popularity in scene text detection. However,\nthe existing parameterized text shape methods still have limitations in\nmodeling arbitrary-shaped texts due to ignoring the utilization of\ntext-specific shape information. Moreover, the time consumption of the entire\npipeline has been largely overlooked, leading to a suboptimal overall inference\nspeed. To address these issues, we first propose a novel parameterized text\nshape method based on low-rank approximation. Unlike other shape representation\nmethods that employ data-irrelevant parameterization, our approach utilizes\nsingular value decomposition and reconstructs the text shape using a few\neigenvectors learned from labeled text contours. By exploring the shape\ncorrelation among different text contours, our method achieves consistency,\ncompactness, simplicity, and robustness in shape representation. Next, we\npropose a dual assignment scheme for speed acceleration. It adopts a sparse\nassignment branch to accelerate the inference speed, and meanwhile, provides\nample supervised signals for training through a dense assignment branch.\nBuilding upon these designs, we implement an accurate and efficient\narbitrary-shaped text detector named LRANet. Extensive experiments are\nconducted on several challenging benchmarks, demonstrating the superior\naccuracy and efficiency of LRANet compared to state-of-the-art methods. Code\nwill be released soon.\n","authors":["Yuchen Su","Zhineng Chen","Zhiwen Shao","Yuning Du","Zhilong Ji","Jinfeng Bai","Yong Zhou","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2306.15142v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.14349v3","updated":"2023-10-11T01:30:37Z","published":"2021-06-28T00:34:15Z","title":"PNet -- A Deep Learning Based Photometry and Astrometry Bayesian\n  Framework","summary":"  Time domain astronomy has emerged as a vibrant research field in recent\nyears, focusing on celestial objects that exhibit variable magnitudes or\npositions. Given the urgency of conducting follow-up observations for such\nobjects, the development of an algorithm capable of detecting them and\ndetermining their magnitudes and positions has become imperative. Leveraging\nthe advancements in deep neural networks, we present the PNet, an end-to-end\nframework designed not only to detect celestial objects and extract their\nmagnitudes and positions but also to estimate photometry uncertainty. The PNet\ncomprises two essential steps. Firstly, it detects stars and retrieves their\npositions, magnitudes, and calibrated magnitudes. Subsequently, in the second\nphase, the PNet estimates the uncertainty associated with the photometry\nresults, serving as a valuable reference for the light curve classification\nalgorithm. Our algorithm has been tested using both simulated and real\nobservation data, demonstrating the PNet's ability to deliver consistent and\nreliable outcomes. Integration of the PNet into data processing pipelines for\ntime-domain astronomy holds significant potential for enhancing response speed\nand improving the detection capabilities for celestial objects with variable\npositions and magnitudes.\n","authors":["Rui Sun","Peng Jia","Yongyang Sun","Zhimin Yang","Qiang Liu","Hongyan Wei"],"pdf_url":"https://arxiv.org/pdf/2106.14349v3.pdf","comment":"To be published in the AJ and welcome to any comments"},{"id":"http://arxiv.org/abs/2310.04780v2","updated":"2023-10-11T00:38:50Z","published":"2023-10-07T11:45:33Z","title":"IPMix: Label-Preserving Data Augmentation Method for Training Robust\n  Classifiers","summary":"  Data augmentation has been proven effective for training high-accuracy\nconvolutional neural network classifiers by preventing overfitting. However,\nbuilding deep neural networks in real-world scenarios requires not only high\naccuracy on clean data but also robustness when data distributions shift. While\nprior methods have proposed that there is a trade-off between accuracy and\nrobustness, we propose IPMix, a simple data augmentation approach to improve\nrobustness without hurting clean accuracy. IPMix integrates three levels of\ndata augmentation (image-level, patch-level, and pixel-level) into a coherent\nand label-preserving technique to increase the diversity of training data with\nlimited computational overhead. To further improve the robustness, IPMix\nintroduces structural complexity at different levels to generate more diverse\nimages and adopts the random mixing method for multi-scale information fusion.\nExperiments demonstrate that IPMix outperforms state-of-the-art corruption\nrobustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also\nsignificantly improves the other safety measures, including robustness to\nadversarial perturbations, calibration, prediction consistency, and anomaly\ndetection, achieving state-of-the-art or comparable results on several\nbenchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.\n","authors":["Zhenglin Huang","Xianan Bao","Na Zhang","Qingqi Zhang","Xiaomei Tu","Biao Wu","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1910.11103v2","updated":"2023-10-11T00:11:45Z","published":"2019-10-16T23:30:22Z","title":"SPEC2: SPECtral SParsE CNN Accelerator on FPGAs","summary":"  To accelerate inference of Convolutional Neural Networks (CNNs), various\ntechniques have been proposed to reduce computation redundancy. Converting\nconvolutional layers into frequency domain significantly reduces the\ncomputation complexity of the sliding window operations in space domain. On the\nother hand, weight pruning techniques address the redundancy in model\nparameters by converting dense convolutional kernels into sparse ones. To\nobtain high-throughput FPGA implementation, we propose SPEC2 -- the first work\nto prune and accelerate spectral CNNs. First, we propose a systematic pruning\nalgorithm based on Alternative Direction Method of Multipliers (ADMM). The\noffline pruning iteratively sets the majority of spectral weights to zero,\nwithout using any handcrafted heuristics. Then, we design an optimized pipeline\narchitecture on FPGA that has efficient random access into the sparse kernels\nand exploits various dimensions of parallelism in convolutional layers.\nOverall, SPEC2 achieves high inference throughput with extremely low\ncomputation complexity and negligible accuracy degradation. We demonstrate\nSPEC2 by pruning and implementing LeNet and VGG16 on the Xilinx Virtex\nplatform. After pruning 75% of the spectral weights, SPEC2 achieves 0% accuracy\nloss for LeNet, and <1% accuracy loss for VGG16. The resulting accelerators\nachieve up to 24x higher throughput, compared with the state-of-the-art FPGA\nimplementations for VGG16.\n","authors":["Yue Niu","Hanqing Zeng","Ajitesh Srivastava","Kartik Lakhotia","Rajgopal Kannan","Yanzhi Wang","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/1910.11103v2.pdf","comment":"This is a 10-page conference paper in 26TH IEEE International\n  Conference On High Performance Computing, Data, and Analytics (HiPC)"},{"id":"http://arxiv.org/abs/2306.06323v2","updated":"2023-10-11T23:40:03Z","published":"2023-06-10T00:27:37Z","title":"Learning Joint Latent Space EBM Prior Model for Multi-layer Generator","summary":"  This paper studies the fundamental problem of learning multi-layer generator\nmodels. The multi-layer generator model builds multiple layers of latent\nvariables as a prior model on top of the generator, which benefits learning\ncomplex data distribution and hierarchical representations. However, such a\nprior model usually focuses on modeling inter-layer relations between latent\nvariables by assuming non-informative (conditional) Gaussian distributions,\nwhich can be limited in model expressivity. To tackle this issue and learn more\nexpressive prior models, we propose an energy-based model (EBM) on the joint\nlatent space over all layers of latent variables with the multi-layer generator\nas its backbone. Such joint latent space EBM prior model captures the\nintra-layer contextual relations at each layer through layer-wise energy terms,\nand latent variables across different layers are jointly corrected. We develop\na joint training scheme via maximum likelihood estimation (MLE), which involves\nMarkov Chain Monte Carlo (MCMC) sampling for both prior and posterior\ndistributions of the latent variables from different layers. To ensure\nefficient inference and learning, we further propose a variational training\nscheme where an inference model is used to amortize the costly posterior MCMC\nsampling. Our experiments demonstrate that the learned model can be expressive\nin generating high-quality images and capturing hierarchical features for\nbetter outlier detection.\n","authors":["Jiali Cui","Ying Nian Wu","Tian Han"],"pdf_url":"https://arxiv.org/pdf/2306.06323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03135v3","updated":"2023-10-11T23:38:36Z","published":"2023-07-06T17:05:26Z","title":"Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability","summary":"  Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood\n","authors":["Xuanlin Li","Yunhao Fang","Minghua Liu","Zhan Ling","Zhuowen Tu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2307.03135v3.pdf","comment":"Published at International Conference on Computer Vision (ICCV) 2023.\n  Poster at\n  https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf"},{"id":"http://arxiv.org/abs/2310.07932v1","updated":"2023-10-11T23:04:07Z","published":"2023-10-11T23:04:07Z","title":"What Matters to You? Towards Visual Representation Alignment for Robot\n  Learning","summary":"  When operating in service of people, robots need to optimize rewards aligned\nwith end-user preferences. Since robots will rely on raw perceptual inputs like\nRGB images, their rewards will inevitably use visual representations. Recently\nthere has been excitement in using representations from pre-trained visual\nmodels, but key to making these work in robotics is fine-tuning, which is\ntypically done via proxy tasks like dynamics prediction or enforcing temporal\ncycle-consistency. However, all these proxy tasks bypass the human's input on\nwhat matters to them, exacerbating spurious correlations and ultimately leading\nto robot behaviors that are misaligned with user preferences. In this work, we\npropose that robots should leverage human feedback to align their visual\nrepresentations with the end-user and disentangle what matters for the task. We\npropose Representation-Aligned Preference-based Learning (RAPL), a method for\nsolving the visual representation alignment problem and visual reward learning\nproblem through the lens of preference-based learning and optimal transport.\nAcross experiments in X-MAGICAL and in robotic manipulation, we find that\nRAPL's reward consistently generates preferred robot behaviors with high sample\nefficiency, and shows strong zero-shot generalization when the visual\nrepresentation is learned from a different embodiment than the robot's.\n","authors":["Ran Tian","Chenfeng Xu","Masayoshi Tomizuka","Jitendra Malik","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2310.07932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07931v1","updated":"2023-10-11T23:01:29Z","published":"2023-10-11T23:01:29Z","title":"D2 Pruning: Message Passing for Balancing Diversity and Difficulty in\n  Data Pruning","summary":"  Analytical theories suggest that higher-quality data can lead to lower test\nerrors in models trained on a fixed data budget. Moreover, a model can be\ntrained on a lower compute budget without compromising performance if a dataset\ncan be stripped of its redundancies. Coreset selection (or data pruning) seeks\nto select a subset of the training data so as to maximize the performance of\nmodels trained on this subset, also referred to as coreset. There are two\ndominant approaches: (1) geometry-based data selection for maximizing data\ndiversity in the coreset, and (2) functions that assign difficulty scores to\nsamples based on training dynamics. Optimizing for data diversity leads to a\ncoreset that is biased towards easier samples, whereas, selection by difficulty\nranking omits easy samples that are necessary for the training of deep learning\nmodels. This demonstrates that data diversity and importance scores are two\ncomplementary factors that need to be jointly considered during coreset\nselection. We represent a dataset as an undirected graph and propose a novel\npruning algorithm, D2 Pruning, that uses forward and reverse message passing\nover this dataset graph for coreset selection. D2 Pruning updates the\ndifficulty scores of each example by incorporating the difficulty of its\nneighboring examples in the dataset graph. Then, these updated difficulty\nscores direct a graph-based sampling method to select a coreset that\nencapsulates both diverse and difficult regions of the dataset space. We\nevaluate supervised and self-supervised versions of our method on various\nvision and language datasets. Results show that D2 Pruning improves coreset\nselection over previous state-of-the-art methods for up to 70% pruning rates.\nAdditionally, we find that using D2 Pruning for filtering large multimodal\ndatasets leads to increased diversity in the dataset and improved\ngeneralization of pretrained models.\n","authors":["Adyasha Maharana","Prateek Yadav","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.07931v1.pdf","comment":"17 pages (Our code is available at\n  https://github.com/adymaharana/d2pruning)"},{"id":"http://arxiv.org/abs/2308.12364v2","updated":"2023-10-11T22:38:38Z","published":"2023-08-23T18:08:32Z","title":"Saliency-based Video Summarization for Face Anti-spoofing","summary":"  With the growing availability of databases for face presentation attack\ndetection, researchers are increasingly focusing on video-based face\nanti-spoofing methods that involve hundreds to thousands of images for training\nthe models. However, there is currently no clear consensus on the optimal\nnumber of frames in a video to improve face spoofing detection. Inspired by the\nvisual saliency theory, we present a video summarization method for face\nanti-spoofing detection that aims to enhance the performance and efficiency of\ndeep learning models by leveraging visual saliency. In particular, saliency\ninformation is extracted from the differences between the Laplacian and Wiener\nfilter outputs of the source images, enabling identification of the most\nvisually salient regions within each frame. Subsequently, the source images are\ndecomposed into base and detail images, enhancing the representation of the\nmost important information. Weighting maps are then computed based on the\nsaliency information, indicating the importance of each pixel in the image. By\nlinearly combining the base and detail images using the weighting maps, the\nmethod fuses the source images to create a single representative image that\nsummarizes the entire video. The key contribution of the proposed method lies\nin demonstrating how visual saliency can be used as a data-centric approach to\nimprove the performance and efficiency for face presentation attack detection.\nBy focusing on the most salient images or regions within the images, a more\nrepresentative and diverse training set can be created, potentially leading to\nmore effective models. To validate the method's effectiveness, a simple CNN-RNN\ndeep learning architecture was used, and the experimental results showcased\nstate-of-the-art performance on five challenging face anti-spoofing datasets\n","authors":["Usman Muhammad","Mourad Oussalah","Jorma Laaksonen"],"pdf_url":"https://arxiv.org/pdf/2308.12364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14616v3","updated":"2023-10-11T22:15:54Z","published":"2023-09-26T02:09:52Z","title":"NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized\n  Device Coordinates Space","summary":"  Monocular 3D Semantic Scene Completion (SSC) has garnered significant\nattention in recent years due to its potential to predict complex semantics and\ngeometry shapes from a single image, requiring no 3D inputs. In this paper, we\nidentify several critical issues in current state-of-the-art methods, including\nthe Feature Ambiguity of projected 2D features in the ray to the 3D space, the\nPose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D\nconvolution across different depth levels. To address these problems, we devise\na novel Normalized Device Coordinates scene completion network (NDC-Scene) that\ndirectly extends the 2D feature map to a Normalized Device Coordinates (NDC)\nspace, rather than to the world space directly, through progressive restoration\nof the dimension of depth with deconvolution operations. Experiment results\ndemonstrate that transferring the majority of computation from the target 3D\nspace to the proposed normalized device coordinates space benefits monocular\nSSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to\nsimultaneously upsample and fuse the 2D and 3D feature maps, further improving\noverall performance. Our extensive experiments confirm that the proposed method\nconsistently outperforms state-of-the-art methods on both outdoor SemanticKITTI\nand indoor NYUv2 datasets. Our code are available at\nhttps://github.com/Jiawei-Yao0812/NDCScene.\n","authors":["Jiawei Yao","Chuming Li","Keqiang Sun","Yingjie Cai","Hao Li","Wanli Ouyang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2309.14616v3.pdf","comment":"Accepted at ICCV 2023. Project page:\n  https://jiawei-yao0812.github.io/NDC-Scene/"},{"id":"http://arxiv.org/abs/2310.07916v1","updated":"2023-10-11T22:04:33Z","published":"2023-10-11T22:04:33Z","title":"Dynamic Appearance Particle Neural Radiance Field","summary":"  Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof superposition of a static field and a dynamic field. The dynamic field is\nquantised as a collection of {\\em appearance particles}, which carries the\nvisual information of a small dynamic element in the scene and is equipped with\na motion model. All components, including the static field, the visual features\nand motion models of the particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modelling. Experimental results show that DAP-NeRF\nis an effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene.\n","authors":["Ancheng Lin","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2310.07916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07896v1","updated":"2023-10-11T21:07:14Z","published":"2023-10-11T21:07:14Z","title":"NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration","summary":"  Robotic learning for navigation in unfamiliar environments needs to provide\npolicies for both task-oriented navigation (i.e., reaching a goal that the\nrobot has located), and task-agnostic exploration (i.e., searching for a goal\nin a novel setting). Typically, these roles are handled by separate models, for\nexample by using subgoal proposals, planning, or separate navigation\nstrategies. In this paper, we describe how we can train a single unified\ndiffusion policy to handle both goal-directed navigation and goal-agnostic\nexploration, with the latter providing the ability to search novel\nenvironments, and the former providing the ability to reach a user-specified\ngoal once it has been located. We show that this unified policy results in\nbetter overall performance when navigating to visually indicated goals in novel\nenvironments, as compared to approaches that use subgoal proposals from\ngenerative models, or prior methods based on latent variable models. We\ninstantiate our method by using a large-scale Transformer-based policy trained\non data from multiple ground robots, with a diffusion model decoder to flexibly\nhandle both goal-conditioned and goal-agnostic navigation. Our experiments,\nconducted on a real-world mobile robot platform, show effective navigation in\nunseen environments in comparison with five alternative methods, and\ndemonstrate significant improvements in performance and lower collision rates,\ndespite utilizing smaller models than state-of-the-art approaches. For more\nvideos, code, and pre-trained model checkpoints, see\nhttps://general-navigation-models.github.io/nomad/\n","authors":["Ajay Sridhar","Dhruv Shah","Catherine Glossop","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2310.07896v1.pdf","comment":"Project page https://general-navigation-models.github.io/nomad/"},{"id":"http://arxiv.org/abs/2310.07894v1","updated":"2023-10-11T21:04:42Z","published":"2023-10-11T21:04:42Z","title":"Efficient Integrators for Diffusion Generative Models","summary":"  Diffusion models suffer from slow sample generation at inference time.\nTherefore, developing a principled framework for fast deterministic/stochastic\nsampling for a broader class of diffusion models is a promising direction. We\npropose two complementary frameworks for accelerating sample generation in\npre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate\nintegrators generalize DDIM, mapping the reverse diffusion dynamics to a more\namenable space for sampling. In contrast, splitting-based integrators, commonly\nused in molecular dynamics, reduce the numerical simulation error by cleverly\nalternating between numerical updates involving the data and auxiliary\nvariables. After extensively studying these methods empirically and\ntheoretically, we present a hybrid method that leads to the best-reported\nperformance for diffusion models in augmented spaces. Applied to Phase Space\nLangevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and\nstochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network\nfunction evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing\nbaselines, respectively. Our code and model checkpoints will be made publicly\navailable at \\url{https://github.com/mandt-lab/PSLD}.\n","authors":["Kushagra Pandey","Maja Rudolph","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2310.07894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07889v1","updated":"2023-10-11T20:52:30Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation. Our approach uses off-the-shelf vision systems\n(for image captioning and object detection) to convert an agent's egocentric\npanoramic view at each time step into natural language descriptions. We then\nfinetune a pretrained language model to select an action, based on the current\nview and the trajectory history, that would best fulfill the navigation\ninstructions. In contrast to the standard setup which adapts a pretrained\nlanguage model to work directly with continuous visual features from pretrained\nvision models, our approach instead uses (discrete) language as the perceptual\nrepresentation. We explore two use cases of our language-based navigation\n(LangNav) approach on the R2R vision-and-language navigation benchmark:\ngenerating synthetic trajectories from a prompted large language model (GPT-4)\nwith which to finetune a smaller language model; and sim-to-real transfer where\nwe transfer a policy learned on a simulated environment (ALFRED) to a\nreal-world environment (R2R). Our approach is found to improve upon strong\nbaselines that rely on visual features in settings where only a few gold\ntrajectories (10-100) are available, demonstrating the potential of using\nlanguage as a perceptual representation for navigation tasks.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07887v1","updated":"2023-10-11T20:48:20Z","published":"2023-10-11T20:48:20Z","title":"Unsupervised Structured Noise Removal with Variational Lossy Autoencoder","summary":"  Most unsupervised denoising methods are based on the assumption that imaging\nnoise is either pixel-independent, i.e., spatially uncorrelated, or\nsignal-independent, i.e., purely additive. However, in practice many imaging\nsetups, especially in microscopy, suffer from a combination of signal-dependent\nnoise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe\nshaped scanning or readout artifacts). In this paper, we present the first\nunsupervised deep learning-based denoiser that can remove this type of noise\nwithout access to any clean images or a noise model. Unlike self-supervised\ntechniques, our method does not rely on removing pixels by masking or\nsubsampling so can utilize all available information. We implement a\nVariational Autoencoder (VAE) with a specially designed autoregressive decoder\ncapable of modelling the noise component of an image but incapable of\nindependently modelling the underlying clean signal component. As a\nconsequence, our VAE's encoder learns to encode only underlying clean signal\ncontent and to discard imaging noise. We also propose an additional decoder for\nmapping the encoder's latent variables back into image space, thereby sampling\ndenoised images. Experimental results demonstrate that our approach surpasses\nexisting methods for self- and unsupervised image denoising while being robust\nwith respect to the size of the autoregressive receptive field. Code for this\nproject can be found at https://github.com/krulllab/DVLAE.\n","authors":["Benjamin Salmon","Alexander Krull"],"pdf_url":"https://arxiv.org/pdf/2310.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07886v1","updated":"2023-10-11T20:48:19Z","published":"2023-10-11T20:48:19Z","title":"A Survey of Feature Types and Their Contributions for Camera Tampering\n  Detection","summary":"  Camera tamper detection is the ability to detect unauthorized and\nunintentional alterations in surveillance cameras by analyzing the video.\nCamera tampering can occur due to natural events or it can be caused\nintentionally to disrupt surveillance. We cast tampering detection as a change\ndetection problem, and perform a review of the existing literature with\nemphasis on feature types. We formulate tampering detection as a time series\nanalysis problem, and design experiments to study the robustness and capability\nof various feature types. We compute ten features on real-world surveillance\nvideo and apply time series analysis to ascertain their predictability, and\ntheir capability to detect tampering. Finally, we quantify the performance of\nvarious time series models using each feature type to detect tampering.\n","authors":["Pranav Mantini","Shishir K. Shah"],"pdf_url":"https://arxiv.org/pdf/2310.07886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16319v2","updated":"2023-10-11T20:33:37Z","published":"2023-05-25T17:59:50Z","title":"Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical\n  Invariance","summary":"  This paper introduces a novel mathematical property applicable to diverse\nimages, referred to as FINOLA (First-Order Norm+Linear Autoregressive). FINOLA\nrepresents each image in the latent space as a first-order autoregressive\nprocess, in which each regression step simply applies a shared linear model on\nthe normalized value of its immediate neighbor. This intriguing property\nreveals a mathematical invariance that transcends individual images. Expanding\nfrom image grids to continuous coordinates, we unveil the presence of two\nunderlying partial differential equations. We validate the FINOLA property from\ntwo distinct angles: image reconstruction and self-supervised learning.\nFirstly, we demonstrate the ability of FINOLA to auto-regress up to a 256x256\nfeature map (the same resolution to the image) from a single vector placed at\nthe center, successfully reconstructing the original image by only using three\n3x3 convolution layers as decoder. Secondly, we leverage FINOLA for\nself-supervised learning by employing a simple masked prediction approach.\nEncoding a single unmasked quadrant block, we autoregressively predict the\nsurrounding masked region. Remarkably, this pre-trained representation proves\nhighly effective in image classification and object detection tasks, even when\nintegrated into lightweight networks, all without the need for extensive\nfine-tuning. The code will be made publicly available.\n","authors":["Yinpeng Chen","Xiyang Dai","Dongdong Chen","Mengchen Liu","Lu Yuan","Zicheng Liu","Youzuo Lin"],"pdf_url":"https://arxiv.org/pdf/2305.16319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01748v2","updated":"2023-10-11T20:28:58Z","published":"2023-03-03T07:20:58Z","title":"A Complete Recipe for Diffusion Generative Models","summary":"  Score-based Generative Models (SGMs) have demonstrated exceptional synthesis\noutcomes across various tasks. However, the current design landscape of the\nforward diffusion process remains largely untapped and often relies on physical\nheuristics or simplifying assumptions. Utilizing insights from the development\nof scalable Bayesian posterior samplers, we present a complete recipe for\nformulating forward processes in SGMs, ensuring convergence to the desired\ntarget distribution. Our approach reveals that several existing SGMs can be\nseen as specific manifestations of our framework. Building upon this method, we\nintroduce Phase Space Langevin Diffusion (PSLD), which relies on score-based\nmodeling within an augmented space enriched by auxiliary variables akin to\nphysical phase space. Empirical results exhibit the superior sample quality and\nimproved speed-quality trade-off of PSLD compared to various competing\napproaches on established image synthesis benchmarks. Remarkably, PSLD achieves\nsample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional\nCIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in\nconditional synthesis using pre-trained score networks, offering an appealing\nalternative as an SGM backbone for future advancements. Code and model\ncheckpoints can be accessed at \\url{https://github.com/mandt-lab/PSLD}.\n","authors":["Kushagra Pandey","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2303.01748v2.pdf","comment":"Accepted in ICCV'23 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2210.09222v2","updated":"2023-10-11T19:59:02Z","published":"2022-10-14T08:05:16Z","title":"MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human\n  Activity Recognition","summary":"  Multimodal sensors provide complementary information to develop accurate\nmachine-learning methods for human activity recognition (HAR), but introduce\nsignificantly higher computational load, which reduces efficiency. This paper\nproposes an efficient multimodal neural architecture for HAR using an RGB\ncamera and inertial measurement units (IMUs) called Multimodal Temporal Segment\nAttention Network (MMTSA). MMTSA first transforms IMU sensor data into a\ntemporal and structure-preserving gray-scale image using the Gramian Angular\nField (GAF), representing the inherent properties of human activities. MMTSA\nthen applies a multimodal sparse sampling method to reduce data redundancy.\nLastly, MMTSA adopts an inter-segment attention module for efficient multimodal\nfusion. Using three well-established public datasets, we evaluated MMTSA's\neffectiveness and efficiency in HAR. Results show that our method achieves\nsuperior performance improvements 11.13% of cross-subject F1-score on the MMAct\ndataset than the previous state-of-the-art (SOTA) methods. The ablation study\nand analysis suggest that MMTSA's effectiveness in fusing multimodal data for\naccurate HAR. The efficiency evaluation on an edge device showed that MMTSA\nachieved significantly better accuracy, lower computational load, and lower\ninference latency than SOTA methods.\n","authors":["Ziqi Gao","Yuntao Wang","Jianguo Chen","Junliang Xing","Shwetak Patel","Xin Liu","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2210.09222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07855v1","updated":"2023-10-11T19:57:51Z","published":"2023-10-11T19:57:51Z","title":"CrIBo: Self-Supervised Learning via Cross-Image Object-Level\n  Bootstrapping","summary":"  Leveraging nearest neighbor retrieval for self-supervised representation\nlearning has proven beneficial with object-centric images. However, this\napproach faces limitations when applied to scene-centric datasets, where\nmultiple objects within an image are only implicitly captured in the global\nrepresentation. Such global bootstrapping can lead to undesirable entanglement\nof object representations. Furthermore, even object-centric datasets stand to\nbenefit from a finer-grained bootstrapping approach. In response to these\nchallenges, we introduce a novel Cross-Image Object-Level Bootstrapping method\ntailored to enhance dense visual representation learning. By employing\nobject-level nearest neighbor bootstrapping throughout the training, CrIBo\nemerges as a notably strong and adequate candidate for in-context learning,\nleveraging nearest neighbor retrieval at test time. CrIBo shows\nstate-of-the-art performance on the latter task while being highly competitive\nin more standard downstream segmentation tasks. Our code and pretrained models\nwill be publicly available upon acceptance.\n","authors":["Tim Lebailly","Thomas Stegmller","Behzad Bozorgtabar","Jean-Philippe Thiran","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2310.07855v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.07713v1","updated":"2023-10-11T17:59:05Z","published":"2023-10-11T17:59:05Z","title":"InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining","summary":"  Pretraining auto-regressive large language models (LLMs) with retrieval\ndemonstrates better perplexity and factual accuracy by leveraging external\ndatabases. However, the size of existing pretrained retrieval-augmented LLM is\nstill limited (e.g., Retro has 7.5B parameters), which limits the effectiveness\nof instruction tuning and zero-shot generalization. In this work, we introduce\nRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.\nSpecifically, we continue to pretrain the 43B GPT model on additional 100\nbillion tokens using the Retro augmentation method by retrieving from 1.2\ntrillion tokens. The obtained foundation model, Retro 48B, largely outperforms\nthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,\nInstructRetro demonstrates significant improvement over the instruction tuned\nGPT on zero-shot question answering (QA) tasks. Specifically, the average\nimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-form\nQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.\nSurprisingly, we find that one can ablate the encoder from InstructRetro\narchitecture and directly use its decoder backbone, while achieving comparable\nresults. We hypothesize that pretraining with retrieval makes its decoder good\nat incorporating context for QA. Our results highlights the promising direction\nto obtain a better GPT decoder for QA through continued pretraining with\nretrieval before instruction tuning.\n","authors":["Boxin Wang","Wei Ping","Lawrence McAfee","Peng Xu","Bo Li","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2310.07713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08703v3","updated":"2023-10-11T17:00:34Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v3.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2305.13172v2","updated":"2023-10-11T16:51:50Z","published":"2023-05-22T16:00:00Z","title":"Editing Large Language Models: Problems, Methods, and Opportunities","summary":"  Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Yunzhi Yao","Peng Wang","Bozhong Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13172v2.pdf","comment":"EMNLP 2023. Updated with new experiments"},{"id":"http://arxiv.org/abs/2211.15743v3","updated":"2023-10-11T16:18:59Z","published":"2022-11-28T19:49:02Z","title":"Towards Reliable Item Sampling for Recommendation Evaluation","summary":"  Since Rendle and Krichene argued that commonly used sampling-based evaluation\nmetrics are \"inconsistent\" with respect to the global metrics (even in\nexpectation), there have been a few studies on the sampling-based recommender\nsystem evaluation. Existing methods try either mapping the sampling-based\nmetrics to their global counterparts or more generally, learning the empirical\nrank distribution to estimate the top-$K$ metrics. However, despite existing\nefforts, there is still a lack of rigorous theoretical understanding of the\nproposed metric estimators, and the basic item sampling also suffers from the\n\"blind spot\" issue, i.e., estimation accuracy to recover the top-$K$ metrics\nwhen $K$ is small can still be rather substantial. In this paper, we provide an\nin-depth investigation into these problems and make two innovative\ncontributions. First, we propose a new item-sampling estimator that explicitly\noptimizes the error with respect to the ground truth, and theoretically\nhighlight its subtle difference against prior work. Second, we propose a new\nadaptive sampling method which aims to deal with the \"blind spot\" problem and\nalso demonstrate the expectation-maximization (EM) algorithm can be generalized\nfor such a setting. Our experimental results confirm our statistical analysis\nand the superiority of the proposed works. This study helps lay the theoretical\nfoundation for adopting item sampling metrics for recommendation evaluation,\nand provides strong evidence towards making item sampling a powerful and\nreliable tool for recommendation evaluation.\n","authors":["Dong Li","Ruoming Jin","Zhenming Liu","Bin Ren","Jing Gao","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2211.15743v3.pdf","comment":"aaai2023"},{"id":"http://arxiv.org/abs/2310.07554v1","updated":"2023-10-11T14:59:53Z","published":"2023-10-11T14:59:53Z","title":"Retrieve Anything To Augment Large Language Models","summary":"  Large language models (LLMs) face significant challenges stemming from the\ninherent limitations in knowledge, memory, alignment, and action. These\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\nfrom the external world, such as knowledge base, memory store, demonstration\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\nbridging the gap between LLMs and the external assistance. However,\nconventional methods encounter two pressing issues. On one hand, the\ngeneral-purpose retrievers are not properly optimized for the retrieval\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\nrequired versatility, hindering their performance across the diverse retrieval\naugmentation scenarios.\n  In this work, we present a novel approach, the LLM Embedder, which\ncomprehensively support the diverse needs of LLMs' retrieval augmentation with\none unified embedding model. Training such an unified model is non-trivial, as\nvarious retrieval tasks aim to capture distinct semantic relationships, often\nsubject to mutual interference. To address this challenge, we systematically\noptimize our training methodology. This includes reward formulation based on\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\nfine-tuning with explicit instructions, and the use of homogeneous in-batch\nnegative sampling. These optimization strategies contribute to the outstanding\nempirical performance of the LLM-Embedder. Notably, it yields remarkable\nenhancements in retrieval augmentation for LLMs, surpassing both\ngeneral-purpose and task-specific retrievers in various evaluation scenarios.\nThis project is made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Peitian Zhang","Shitao Xiao","Zheng Liu","Zhicheng Dou","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2310.07554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09447v3","updated":"2023-10-11T13:46:25Z","published":"2023-07-18T17:22:19Z","title":"Deep Neural Aggregation for Recommending Items to Group of Users","summary":"  Modern society devotes a significant amount of time to digital interaction.\nMany of our daily actions are carried out through digital means. This has led\nto the emergence of numerous Artificial Intelligence tools that assist us in\nvarious aspects of our lives. One key tool for the digital society is\nRecommender Systems, intelligent systems that learn from our past actions to\npropose new ones that align with our interests. Some of these systems have\nspecialized in learning from the behavior of user groups to make\nrecommendations to a group of individuals who want to perform a joint task. In\nthis article, we analyze the current state of Group Recommender Systems and\npropose two new models that use emerging Deep Learning architectures.\nExperimental results demonstrate the improvement achieved by employing the\nproposed models compared to the state-of-the-art models using four different\ndatasets. The source code of the models, as well as that of all the experiments\nconducted, is available in a public repository.\n","authors":["Jorge Dueas-Lern","Ral Lara-Cabrera","Fernando Ortega","Jess Bobadilla"],"pdf_url":"https://arxiv.org/pdf/2307.09447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07477v1","updated":"2023-10-11T13:24:38Z","published":"2023-10-11T13:24:38Z","title":"GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized\n  Adaptive Testing","summary":"  Computerized Adaptive Testing(CAT) refers to an online system that adaptively\nselects the best-suited question for students with various abilities based on\ntheir historical response records. Most CAT methods only focus on the quality\nobjective of predicting the student ability accurately, but neglect concept\ndiversity or question exposure control, which are important considerations in\nensuring the performance and validity of CAT. Besides, the students' response\nrecords contain valuable relational information between questions and knowledge\nconcepts. The previous methods ignore this relational information, resulting in\nthe selection of sub-optimal test questions. To address these challenges, we\npropose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly,\nthree objectives, namely quality, diversity and novelty, are introduced into\nthe Scalarized Multi-Objective Reinforcement Learning framework of CAT, which\nrespectively correspond to improving the prediction accuracy, increasing the\nconcept diversity and reducing the question exposure. We use an Actor-Critic\nRecommender to select questions and optimize three objectives simultaneously by\nthe scalarization function. Secondly, we utilize the graph neural network to\nlearn relation-aware embeddings of questions and concepts. These embeddings are\nable to aggregate neighborhood information in the relation graphs between\nquestions and concepts. We conduct experiments on three real-world educational\ndatasets, and show that GMOCAT not only outperforms the state-of-the-art\nmethods in the ability prediction, but also achieve superior performance in\nimproving the concept diversity and alleviating the question exposure. Our code\nis available at https://github.com/justarter/GMOCAT.\n","authors":["Hangyu Wang","Ting Long","Liang Yin","Weinan Zhang","Wei Xia","Qichen Hong","Dingyin Xia","Ruiming Tang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.07477v1.pdf","comment":"KDD23"},{"id":"http://arxiv.org/abs/2303.09902v2","updated":"2023-10-11T12:48:30Z","published":"2023-03-17T11:39:35Z","title":"Contrastive Self-supervised Learning in Recommender Systems: A Survey","summary":"  Deep learning-based recommender systems have achieved remarkable success in\nrecent years. However, these methods usually heavily rely on labeled data\n(i.e., user-item interactions), suffering from problems such as data sparsity\nand cold-start. Self-supervised learning, an emerging paradigm that extracts\ninformation from unlabeled data, provides insights into addressing these\nproblems. Specifically, contrastive self-supervised learning, due to its\nflexibility and promising performance, has attracted considerable interest and\nrecently become a dominant branch in self-supervised learning-based\nrecommendation methods. In this survey, we provide an up-to-date and\ncomprehensive review of current contrastive self-supervised learning-based\nrecommendation methods. Firstly, we propose a unified framework for these\nmethods. We then introduce a taxonomy based on the key components of the\nframework, including view generation strategy, contrastive task, and\ncontrastive objective. For each component, we provide detailed descriptions and\ndiscussions to guide the choice of the appropriate method. Finally, we outline\nopen issues and promising directions for future research.\n","authors":["Mengyuan Jing","Yanmin Zhu","Tianzi Zang","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09902v2.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS)"},{"id":"http://arxiv.org/abs/2305.08732v3","updated":"2023-10-11T10:51:12Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v3.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07346v1","updated":"2023-10-11T09:53:43Z","published":"2023-10-11T09:53:43Z","title":"Preliminary Results of a Scientometric Analysis of the German\n  Information Retrieval Community 2020-2023","summary":"  The German Information Retrieval community is located in two different\nsub-fields: Information and computer science. There are no current studies that\ninvestigate these communities on a scientometric level. Available studies only\nfocus on the information scientific part of the community. We generated a data\nset of 401 recent IR-related publications extracted from six core IR\nconferences from a mainly computer scientific background. We analyze this data\nset at the institutional and researcher level. The data set is publicly\nreleased, and we also demonstrate a mapping use case.\n","authors":["Philipp Schaer","Svetlana Myshkina","Jri Keller"],"pdf_url":"https://arxiv.org/pdf/2310.07346v1.pdf","comment":"Data available at https://github.com/irgroup/LWDA2023-IR-community"},{"id":"http://arxiv.org/abs/2206.12781v4","updated":"2023-10-11T09:03:18Z","published":"2022-06-26T03:59:41Z","title":"Efficiently Leveraging Multi-level User Intent for Session-based\n  Recommendation via Atten-Mixer Network","summary":"  Session-based recommendation (SBR) aims to predict the user's next action\nbased on short and dynamic sessions. Recently, there has been an increasing\ninterest in utilizing various elaborately designed graph neural networks (GNNs)\nto capture the pair-wise relationships among items, seemingly suggesting the\ndesign of more complicated models is the panacea for improving the empirical\nperformance. However, these models achieve relatively marginal improvements\nwith exponential growth in model complexity. In this paper, we dissect the\nclassical GNN-based SBR models and empirically find that some sophisticated GNN\npropagations are redundant, given the readout module plays a significant role\nin GNN-based models. Based on this observation, we intuitively propose to\nremove the GNN propagation part, while the readout module will take on more\nresponsibility in the model reasoning process. To this end, we propose the\nMulti-Level Attention Mixture Network (Atten-Mixer), which leverages both\nconcept-view and instance-view readouts to achieve multi-level reasoning over\nitem transitions. As simply enumerating all possible high-level concepts is\ninfeasible for large real-world recommender systems, we further incorporate\nSBR-related inductive biases, i.e., local invariance and inherent priority to\nprune the search space. Experiments on three benchmarks demonstrate the\neffectiveness and efficiency of our proposal. We also have already launched the\nproposed techniques to a large-scale e-commercial online service since April\n2021, with significant improvements of top-tier business metrics demonstrated\nin the online experiments on live traffic.\n","authors":["Peiyan Zhang","Jiayan Guo","Chaozhuo Li","Yueqi Xie","Jaeboum Kim","Yan Zhang","Xing Xie","Haohan Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2206.12781v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07678v2","updated":"2023-10-11T08:34:42Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v2.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07281v1","updated":"2023-10-11T08:15:10Z","published":"2023-10-11T08:15:10Z","title":"A Completely Locale-independent Session-based Recommender System by\n  Leveraging Trained Model","summary":"  In this paper, we propose a solution that won the 10th prize in the KDD Cup\n2023 Challenge Task 2 (Next Product Recommendation for Underrepresented\nLanguages/Locales). Our approach involves two steps: (i) Identify candidate\nitem sets based on co-visitation, and (ii) Re-ranking the items using LightGBM\nwith locale-independent features, including session-based features and product\nsimilarity. The experiment demonstrated that the locale-independent model\nperformed consistently well across different test locales, and performed even\nbetter when incorporating data from other locales into the training.\n","authors":["Yu Tokutake","Chihiro Yamasaki","Yongzhi Jin","Ayuka Inoue","Kei Harada"],"pdf_url":"https://arxiv.org/pdf/2310.07281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06282v2","updated":"2023-10-11T02:46:12Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07142v1","updated":"2023-10-11T02:36:38Z","published":"2023-10-11T02:36:38Z","title":"Validating Synthetic Usage Data in Living Lab Environments","summary":"  Evaluating retrieval performance without editorial relevance judgments is\nchallenging, but instead, user interactions can be used as relevance signals.\nLiving labs offer a way for small-scale platforms to validate information\nretrieval systems with real users. If enough user interaction data are\navailable, click models can be parameterized from historical sessions to\nevaluate systems before exposing users to experimental rankings. However,\ninteraction data are sparse in living labs, and little is studied about how\nclick models can be validated for reliable user simulations when click data are\navailable in moderate amounts.\n  This work introduces an evaluation approach for validating synthetic usage\ndata generated by click models in data-sparse human-in-the-loop environments\nlike living labs. We ground our methodology on the click model's estimates\nabout a system ranking compared to a reference ranking for which the relative\nperformance is known. Our experiments compare different click models and their\nreliability and robustness as more session log data becomes available. In our\nsetup, simple click models can reliably determine the relative system\nperformance with already 20 logged sessions for 50 queries. In contrast, more\ncomplex click models require more session data for reliable estimates, but they\nare a better choice in simulated interleaving experiments when enough session\ndata are available. While it is easier for click models to distinguish between\nmore diverse systems, it is harder to reproduce the system ranking based on the\nsame retrieval algorithm with different interpolation weights. Our setup is\nentirely open, and we share the code to reproduce the experiments.\n","authors":["Timo Breuer","Norbert Fuhr","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2310.07142v1.pdf","comment":"25 pages + appendix and references, accepted JDIQ journal paper"},{"id":"http://arxiv.org/abs/2310.07137v1","updated":"2023-10-11T02:22:28Z","published":"2023-10-11T02:22:28Z","title":"AE-smnsMLC: Multi-Label Classification with Semantic Matching and\n  Negative Label Sampling for Product Attribute Value Extraction","summary":"  Product attribute value extraction plays an important role for many\nreal-world applications in e-Commerce such as product search and\nrecommendation. Previous methods treat it as a sequence labeling task that\nneeds more annotation for position of values in the product text. This limits\ntheir application to real-world scenario in which only attribute values are\nweakly-annotated for each product without their position. Moreover, these\nmethods only use product text (i.e., product title and description) and do not\nconsider the semantic connection between the multiple attribute values of a\ngiven product and its text, which can help attribute value extraction. In this\npaper, we reformulate this task as a multi-label classification task that can\nbe applied for real-world scenario in which only annotation of attribute values\nis available to train models (i.e., annotation of positional information of\nattribute values is not available). We propose a classification model with\nsemantic matching and negative label sampling for attribute value extraction.\nSemantic matching aims to capture semantic interactions between attribute\nvalues of a given product and its text. Negative label sampling aims to enhance\nthe model's ability of distinguishing similar values belonging to the same\nattribute. Experimental results on three subsets of a large real-world\ne-Commerce dataset demonstrate the effectiveness and superiority of our\nproposed model.\n","authors":["Zhongfen Deng","Wei-Te Chen","Lei Chen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2310.07137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07874v1","updated":"2023-10-11T20:34:17Z","published":"2023-10-11T20:34:17Z","title":"Refined Mechanism Design for Approximately Structured Priors via Active\n  Regression","summary":"  We consider the problem of a revenue-maximizing seller with a large number of\nitems $m$ for sale to $n$ strategic bidders, whose valuations are drawn\nindependently from high-dimensional, unknown prior distributions. It is\nwell-known that optimal and even approximately-optimal mechanisms for this\nsetting are notoriously difficult to characterize or compute, and, even when\nthey can be found, are often rife with various counter-intuitive properties. In\nthis paper, following a model introduced recently by Cai and\nDaskalakis~\\cite{cai2022recommender}, we consider the case that bidders' prior\ndistributions can be well-approximated by a topic model. We design an active\nlearning component, responsible for interacting with the bidders and outputting\nlow-dimensional approximations of their types, and a mechanism design\ncomponent, responsible for robustifying mechanisms for the low-dimensional\nmodel to work for the approximate types of the former component. On the active\nlearning front, we cast our problem in the framework of Randomized Linear\nAlgebra (RLA) for regression problems, allowing us to import several\nbreakthrough results from that line of research, and adapt them to our setting.\nOn the mechanism design front, we remove many restrictive assumptions of prior\nwork on the type of access needed to the underlying distributions and the\nassociated mechanisms. To the best of our knowledge, our work is the first to\nformulate connections between mechanism design, and RLA for active learning of\nregression problems, opening the door for further applications of randomized\nlinear algebra primitives to mechanism design.\n","authors":["Christos Boutsikas","Petros Drineas","Marios Mertzanidis","Alexandros Psomas","Paritosh Verma"],"pdf_url":"https://arxiv.org/pdf/2310.07874v1.pdf","comment":"37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)"},{"id":"http://arxiv.org/abs/2310.07815v1","updated":"2023-10-11T18:56:15Z","published":"2023-10-11T18:56:15Z","title":"Language Models As Semantic Indexers","summary":"  Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss and there is usually an inherent mismatch\nbetween the distribution of embeddings within the latent space produced by text\nencoders and the anticipated distribution required for semantic indexing.\nNevertheless, it is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMINDEXER, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. The learned semantic indexer\ncan facilitate various downstream tasks, such as recommendation and retrieval.\nWe conduct experiments on three tasks including recommendation, product search,\nand document retrieval on five datasets from various domains, where LMINDEXER\noutperforms competitive baselines significantly and consistently.\n","authors":["Bowen Jin","Hansi Zeng","Guoyin Wang","Xiusi Chen","Tianxin Wei","Ruirui Li","Zhengyang Wang","Zheng Li","Yang Li","Hanqing Lu","Suhang Wang","Jiawei Han","Xianfeng Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07815v1.pdf","comment":"9 pages, 3 appendix pages"},{"id":"http://arxiv.org/abs/2310.07786v1","updated":"2023-10-11T18:15:55Z","published":"2023-10-11T18:15:55Z","title":"Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble\n  Sampling","summary":"  Real-world applications of contextual bandits often exhibit non-stationarity\ndue to seasonality, serendipity, and evolving social trends. While a number of\nnon-stationary contextual bandit learning algorithms have been proposed in the\nliterature, they excessively explore due to a lack of prioritization for\ninformation of enduring value, or are designed in ways that do not scale in\nmodern applications with high-dimensional user-specific features and large\naction set, or both. In this paper, we introduce a novel non-stationary\ncontextual bandit algorithm that addresses these concerns. It combines a\nscalable, deep-neural-network-based architecture with a carefully designed\nexploration mechanism that strategically prioritizes collecting information\nwith the most lasting value in a non-stationary environment. Through empirical\nevaluations on two real-world recommendation datasets, which exhibit pronounced\nnon-stationarity, we demonstrate that our approach significantly outperforms\nthe state-of-the-art baselines.\n","authors":["Zheqing Zhu","Yueyang Liu","Xu Kuang","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2310.07786v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2305.05658v2","updated":"2023-10-11T17:59:44Z","published":"2023-05-09T17:52:59Z","title":"TidyBot: Personalized Robot Assistance with Large Language Models","summary":"  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n","authors":["Jimmy Wu","Rika Antonova","Adam Kan","Marion Lepert","Andy Zeng","Shuran Song","Jeannette Bohg","Szymon Rusinkiewicz","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2305.05658v2.pdf","comment":"Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language\n  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2023. Project page:\n  https://tidybot.cs.princeton.edu"},{"id":"http://arxiv.org/abs/2310.07713v1","updated":"2023-10-11T17:59:05Z","published":"2023-10-11T17:59:05Z","title":"InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining","summary":"  Pretraining auto-regressive large language models (LLMs) with retrieval\ndemonstrates better perplexity and factual accuracy by leveraging external\ndatabases. However, the size of existing pretrained retrieval-augmented LLM is\nstill limited (e.g., Retro has 7.5B parameters), which limits the effectiveness\nof instruction tuning and zero-shot generalization. In this work, we introduce\nRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.\nSpecifically, we continue to pretrain the 43B GPT model on additional 100\nbillion tokens using the Retro augmentation method by retrieving from 1.2\ntrillion tokens. The obtained foundation model, Retro 48B, largely outperforms\nthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,\nInstructRetro demonstrates significant improvement over the instruction tuned\nGPT on zero-shot question answering (QA) tasks. Specifically, the average\nimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-form\nQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.\nSurprisingly, we find that one can ablate the encoder from InstructRetro\narchitecture and directly use its decoder backbone, while achieving comparable\nresults. We hypothesize that pretraining with retrieval makes its decoder good\nat incorporating context for QA. Our results highlights the promising direction\nto obtain a better GPT decoder for QA through continued pretraining with\nretrieval before instruction tuning.\n","authors":["Boxin Wang","Wei Ping","Lawrence McAfee","Peng Xu","Bo Li","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2310.07713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07712v1","updated":"2023-10-11T17:59:02Z","published":"2023-10-11T17:59:02Z","title":"Found in the Middle: Permutation Self-Consistency Improves Listwise\n  Ranking in Large Language Models","summary":"  Large language models (LLMs) exhibit positional bias in how they use context,\nwhich especially complicates listwise ranking. To address this, we propose\npermutation self-consistency, a form of self-consistency over ranking list\noutputs of black-box LLMs. Our key idea is to marginalize out different list\norders in the prompt to produce an order-independent ranking with less\npositional bias. First, given some input prompt, we repeatedly shuffle the list\nin the prompt and pass it through the LLM while holding the instructions the\nsame. Next, we aggregate the resulting sample of rankings by computing the\ncentral ranking closest in distance to all of them, marginalizing out prompt\norder biases in the process. Theoretically, we prove the robustness of our\nmethod, showing convergence to the true ranking in the presence of random\nperturbations. Empirically, on five list-ranking datasets in sorting and\npassage reranking, our approach improves scores from conventional inference by\nup to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous\nstate of the art in passage reranking. Our code is at\nhttps://github.com/castorini/perm-sc.\n","authors":["Raphael Tang","Xinyu Zhang","Xueguang Ma","Jimmy Lin","Ferhan Ture"],"pdf_url":"https://arxiv.org/pdf/2310.07712v1.pdf","comment":"First two authors contributed equally; 10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07711v1","updated":"2023-10-11T17:58:25Z","published":"2023-10-11T17:58:25Z","title":"Growing Brains: Co-emergence of Anatomical and Functional Modularity in\n  Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs) trained on compositional tasks can exhibit\nfunctional modularity, in which neurons can be clustered by activity similarity\nand participation in shared computational subtasks. Unlike brains, these RNNs\ndo not exhibit anatomical modularity, in which functional clustering is\ncorrelated with strong recurrent coupling and spatial localization of\nfunctional clusters. Contrasting with functional modularity, which can be\nephemerally dependent on the input, anatomically modular networks form a robust\nsubstrate for solving the same subtasks in the future. To examine whether it is\npossible to grow brain-like anatomical modularity, we apply a recent machine\nlearning method, brain-inspired modular training (BIMT), to a network being\ntrained to solve a set of compositional cognitive tasks. We find that\nfunctional and anatomical clustering emerge together, such that functionally\nsimilar neurons also become spatially localized and interconnected. Moreover,\ncompared to standard $L_1$ or no regularization settings, the model exhibits\nsuperior performance by optimally balancing task performance and network\nsparsity. In addition to achieving brain-like organization in RNNs, our\nfindings also suggest that BIMT holds promise for applications in neuromorphic\ncomputing and enhancing the interpretability of neural network architectures.\n","authors":["Ziming Liu","Mikail Khona","Ila R. Fiete","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2310.07711v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07710v1","updated":"2023-10-11T17:57:35Z","published":"2023-10-11T17:57:35Z","title":"DiPmark: A Stealthy, Efficient and Resilient Watermark for Large\n  Language Models","summary":"  Watermarking techniques offer a promising way to secure data via embedding\ncovert information into the data. A paramount challenge in the domain lies in\npreserving the distribution of original data during watermarking. Our research\nextends and refines existing watermarking framework, placing emphasis on the\nimportance of a distribution-preserving (DiP) watermark. Contrary to the\ncurrent strategies, our proposed DiPmark preserves the original token\ndistribution during watermarking (stealthy), is detectable without access to\nthe language model API or weights (efficient), and is robust to moderate\nchanges of tokens (resilient). This is achieved by incorporating a novel\nreweight strategy, combined with a hash function that assigns unique\n\\textit{i.i.d.} ciphers based on the context. The empirical benchmarks of our\napproach underscore its stealthiness, efficiency, and resilience, making it a\nrobust solution for watermarking tasks that demand impeccable quality\npreservation.\n","authors":["Yihan Wu","Zhengmian Hu","Hongyang Zhang","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07707v1","updated":"2023-10-11T17:57:14Z","published":"2023-10-11T17:57:14Z","title":"MatFormer: Nested Transformer for Elastic Inference","summary":"  Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n& encoders), modalities (language & vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.\n","authors":[" Devvrit","Sneha Kudugunta","Aditya Kusupati","Tim Dettmers","Kaifeng Chen","Inderjit Dhillon","Yulia Tsvetkov","Hannaneh Hajishirzi","Sham Kakade","Ali Farhadi","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.07707v1.pdf","comment":"31 pages, 12 figures, first three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.07699v1","updated":"2023-10-11T17:49:13Z","published":"2023-10-11T17:49:13Z","title":"From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched\n  Captions","summary":"  Web-crawled datasets are pivotal to the success of pre-training\nvision-language models, exemplified by CLIP. However, web-crawled AltTexts can\nbe noisy and potentially irrelevant to images, thereby undermining the crucial\nimage-text alignment. Existing methods for rewriting captions using large\nlanguage models (LLMs) have shown promise on small, curated datasets like CC3M\nand CC12M. Nevertheless, their efficacy on massive web-captured captions is\nconstrained by the inherent noise and randomness in such data. In this study,\nwe address this limitation by focusing on two key aspects: data quality and\ndata variety. Unlike recent LLM rewriting techniques, we emphasize exploiting\nvisual concepts and their integration into the captions to improve data\nquality. For data variety, we propose a novel mixed training scheme that\noptimally leverages AltTexts alongside newly generated Visual-enriched Captions\n(VeC). We use CLIP as one example and adapt the method for CLIP training on\nlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive\nevaluation of VeCLIP across small, medium, and large scales of raw data. Our\nresults show significant advantages in image-text alignment and overall model\nperformance, underscoring the effectiveness of VeCLIP in improving CLIP\ntraining. For example, VeCLIP achieves a remarkable over 20% improvement in\nCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,\nwe also achieve a notable over 3% improvement while using only 14% of the data\nemployed in the vanilla CLIP and 11% in ALIGN.\n","authors":["Zhengfeng Lai","Haotian Zhang","Wentao Wu","Haoping Bai","Aleksei Timofeev","Xianzhi Du","Zhe Gan","Jiulong Shan","Chen-Nee Chuah","Yinfei Yang","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07699v1.pdf","comment":"CV/ML"},{"id":"http://arxiv.org/abs/2310.07698v1","updated":"2023-10-11T17:46:59Z","published":"2023-10-11T17:46:59Z","title":"SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc\n  Explanation","summary":"  Explainable AI seeks to bring light to the decision-making processes of\nblack-box models. Traditional saliency-based methods, while highlighting\ninfluential data segments, often lack semantic understanding. Recent\nadvancements, such as Concept Activation Vectors (CAVs) and Concept Bottleneck\nModels (CBMs), offer concept-based explanations but necessitate human-defined\nconcepts. However, human-annotated concepts are expensive to attain. This paper\nintroduces the Concept Bottleneck Surrogate Models (SurroCBM), a novel\nframework that aims to explain the black-box models with automatically\ndiscovered concepts. SurroCBM identifies shared and unique concepts across\nvarious black-box models and employs an explainable surrogate model for\npost-hoc explanations. An effective training strategy using self-generated data\nis proposed to enhance explanation quality continuously. Through extensive\nexperiments, we demonstrate the efficacy of SurroCBM in concept discovery and\nexplanation, underscoring its potential in advancing the field of explainable\nAI.\n","authors":["Bo Pan","Zhenke Liu","Yifei Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.07698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11196v2","updated":"2023-10-11T17:38:26Z","published":"2022-03-18T20:23:41Z","title":"Performance of Deep Learning models with transfer learning for\n  multiple-step-ahead forecasts in monthly time series","summary":"  Deep Learning and transfer learning models are being used to generate time\nseries forecasts; however, there is scarce evidence about their performance\nprediction that it is more evident for monthly time series. The purpose of this\npaper is to compare Deep Learning models with transfer learning and without\ntransfer learning and other traditional methods used for monthly forecasts to\nanswer three questions about the suitability of Deep Learning and Transfer\nLearning to generate predictions of time series. Time series of M4 and M3\ncompetitions were used for the experiments. The results suggest that deep\nlearning models based on TCN, LSTM, and CNN with transfer learning tend to\nsurpass the performance prediction of other traditional methods. On the other\nhand, TCN and LSTM, trained directly on the target time series, got similar or\nbetter performance than traditional methods for some forecast horizons.\n","authors":["Martn Sols","Luis-Alexander Calvo-Valverde"],"pdf_url":"https://arxiv.org/pdf/2203.11196v2.pdf","comment":"20 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2310.07683v1","updated":"2023-10-11T17:34:56Z","published":"2023-10-11T17:34:56Z","title":"Controllable Data Generation Via Iterative Data-Property Mutual Mappings","summary":"  Deep generative models have been widely used for their ability to generate\nrealistic data samples in various areas, such as images, molecules, text, and\nspeech. One major goal of data generation is controllability, namely to\ngenerate new data with desired properties. Despite growing interest in the area\nof controllable generation, significant challenges still remain, including 1)\ndisentangling desired properties with unrelated latent variables, 2)\nout-of-distribution property control, and 3) objective optimization for\nout-of-distribution property control. To address these challenges, in this\npaper, we propose a general framework to enhance VAE-based data generators with\nproperty controllability and ensure disentanglement. Our proposed objective can\nbe optimized on both data seen and unseen in the training set. We propose a\ntraining procedure to train the objective in a semi-supervised manner by\niteratively conducting mutual mappings between the data and properties. The\nproposed framework is implemented on four VAE-based controllable generators to\nevaluate its performance on property error, disentanglement, generation\nquality, and training time. The results indicate that our proposed framework\nenables more precise control over the properties of generated samples in a\nshort training time, ensuring the disentanglement and keeping the validity of\nthe generated samples.\n","authors":["Bo Pan","Muran Qin","Shiyu Wang","Yifei Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07678v1","updated":"2023-10-11T17:21:48Z","published":"2023-10-11T17:21:48Z","title":"Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM","summary":"  With the proliferation of image-based applications in various domains, the\nneed for accurate and interpretable image similarity measures has become\nincreasingly critical. Existing image similarity models often lack\ntransparency, making it challenging to understand the reasons why two images\nare considered similar. In this paper, we propose the concept of explainable\nimage similarity, where the goal is the development of an approach, which is\ncapable of providing similarity scores along with visual factual and\ncounterfactual explanations. Along this line, we present a new framework, which\nintegrates Siamese Networks and Grad-CAM for providing explainable image\nsimilarity and discuss the potential benefits and challenges of adopting this\napproach. In addition, we provide a comprehensive discussion about factual and\ncounterfactual explanations provided by the proposed framework for assisting\ndecision making. The proposed approach has the potential to enhance the\ninterpretability, trustworthiness and user acceptance of image-based systems in\nreal-world image similarity applications. The implementation code can be found\nin https://github.com/ioannislivieris/Grad_CAM_Siamese.git.\n","authors":["Ioannis E. Livieris","Emmanuel Pintelas","Niki Kiriakidou","Panagiotis Pintelas"],"pdf_url":"https://arxiv.org/pdf/2310.07678v1.pdf","comment":"The manuscript has been submitted for publication in \"Journal of\n  Imaging\""},{"id":"http://arxiv.org/abs/2310.07676v1","updated":"2023-10-11T17:21:03Z","published":"2023-10-11T17:21:03Z","title":"Composite Backdoor Attacks Against Large Language Models","summary":"  Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. The unique\ncharacteristics of our CBA can be tailored for various practical scenarios,\ne.g., targeting specific user groups. Our work highlights the necessity of\nincreased security research on the trustworthiness of foundation LLMs.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07672v1","updated":"2023-10-11T17:18:51Z","published":"2023-10-11T17:18:51Z","title":"Stabilizing Estimates of Shapley Values with Control Variates","summary":"  Shapley values are among the most popular tools for explaining predictions of\nblackbox machine learning models. However, their high computational cost\nmotivates the use of sampling approximations, inducing a considerable degree of\nuncertainty. To stabilize these model explanations, we propose ControlSHAP, an\napproach based on the Monte Carlo technique of control variates. Our\nmethodology is applicable to any machine learning model and requires virtually\nno extra computation or modeling effort. On several high-dimensional datasets,\nwe find it can produce dramatic reductions in the Monte Carlo variability of\nShapley estimates.\n","authors":["Jeremy Goldwasser","Giles Hooker"],"pdf_url":"https://arxiv.org/pdf/2310.07672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07668v1","updated":"2023-10-11T17:17:40Z","published":"2023-10-11T17:17:40Z","title":"GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media","summary":"  The proliferation of social media platforms such as Twitter, Instagram, and\nWeibo has significantly enhanced the dissemination of false information. This\nphenomenon grants both individuals and governmental entities the ability to\nshape public opinions, highlighting the need for deploying effective detection\nmethods. In this paper, we propose GraMuFeN, a model designed to detect fake\ncontent by analyzing both the textual and image content of news. GraMuFeN\ncomprises two primary components: a text encoder and an image encoder. For\ntextual analysis, GraMuFeN treats each text as a graph and employs a Graph\nConvolutional Neural Network (GCN) as the text encoder. Additionally, the\npre-trained ResNet-152, as a Convolutional Neural Network (CNN), has been\nutilized as the image encoder. By integrating the outputs from these two\nencoders and implementing a contrastive similarity loss function, GraMuFeN\nachieves remarkable results. Extensive evaluations conducted on two publicly\navailable benchmark datasets for social media news indicate a 10 % increase in\nmicro F1-Score, signifying improvement over existing state-of-the-art models.\nThese findings underscore the effectiveness of combining GCN and CNN models for\ndetecting fake news in multi-modal data, all while minimizing the additional\ncomputational burden imposed by model parameters.\n","authors":["Makan Kananian","Fatima Badiei","S. AmirAli Gh. Ghahramani"],"pdf_url":"https://arxiv.org/pdf/2310.07668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07667v1","updated":"2023-10-11T17:16:33Z","published":"2023-10-11T17:16:33Z","title":"Global Minima, Recoverability Thresholds, and Higher-Order Structure in\n  GNNS","summary":"  We analyze the performance of graph neural network (GNN) architectures from\nthe perspective of random graph theory. Our approach promises to complement\nexisting lenses on GNN analysis, such as combinatorial expressive power and\nworst-case adversarial analysis, by connecting the performance of GNNs to\ntypical-case properties of the training data. First, we theoretically\ncharacterize the nodewise accuracy of one- and two-layer GCNs relative to the\ncontextual stochastic block model (cSBM) and related models. We additionally\nprove that GCNs cannot beat linear models under certain circumstances. Second,\nwe numerically map the recoverability thresholds, in terms of accuracy, of four\ndiverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under a\nvariety of assumptions about the data. Sample results of this second analysis\ninclude: heavy-tailed degree distributions enhance GNN performance, GNNs can\nwork well on strongly heterophilous graphs, and SAGE and Graph Transformer can\nperform well on arbitrarily noisy edge data, but no architecture handled\nsufficiently noisy feature data well. Finally, we show how both specific\nhigher-order structures in synthetic data and the mix of empirical structures\nin real data have dramatic effects (usually negative) on GNN performance.\n","authors":["Drake Brown","Trevor Garrity","Kaden Parker","Jason Oliphant","Stone Carson","Cole Hanson","Zachary Boyd"],"pdf_url":"https://arxiv.org/pdf/2310.07667v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/1910.08883v4","updated":"2023-10-11T17:14:41Z","published":"2019-10-20T03:14:20Z","title":"High-dimensional and universally consistent k-sample tests","summary":"  The k-sample testing problem involves determining whether $k$ groups of data\npoints are each drawn from the same distribution. The standard method for\nk-sample testing in biomedicine is Multivariate analysis of variance (MANOVA),\ndespite that it depends on strong, and often unsuitable, parametric\nassumptions. Moreover, independence testing and k-sample testing are closely\nrelated, and several universally consistent high-dimensional independence tests\nsuch as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion\n(Hsic) enjoy solid theoretical and empirical properties. In this paper, we\nprove that independence tests achieve universally consistent k-sample testing\nand that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD)\nare precisely equivalent to Dcorr. An empirical evaluation of nonparametric\nindependence tests showed that they generally perform better than the popular\nMANOVA test, even in Gaussian distributed scenarios. The evaluation included\nseveral popular independence statistics and covered a comprehensive set of\nsimulations. Additionally, the testing approach was extended to perform\nmultiway and multilevel tests, which were demonstrated in a simulated study as\nwell as a real-world fMRI brain scans with a set of attributes.\n","authors":["Sambit Panda","Cencheng Shen","Ronan Perry","Jelle Zorn","Antoine Lutz","Carey E. Priebe","Joshua T. Vogelstein"],"pdf_url":"https://arxiv.org/pdf/1910.08883v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07665v1","updated":"2023-10-11T17:11:10Z","published":"2023-10-11T17:11:10Z","title":"Deep Backtracking Counterfactuals for Causally Compliant Explanations","summary":"  Counterfactuals can offer valuable insights by answering what would have been\nobserved under altered circumstances, conditional on a factual observation.\nWhereas the classical interventional interpretation of counterfactuals has been\nstudied extensively, backtracking constitutes a less studied alternative the\nbacktracking principle has emerged as an alternative philosophy where all\ncausal laws are kept intact. In the present work, we introduce a practical\nmethod for computing backtracking counterfactuals in structural causal models\nthat consist of deep generative components. To this end, we impose conditions\non the structural assignments that enable the generation of counterfactuals by\nsolving a tractable constrained optimization problem in the structured latent\nspace of a causal model. Our formulation also facilitates a comparison with\nmethods in the field of counterfactual explanations. Compared to these, our\nmethod represents a versatile, modular and causally compliant alternative. We\ndemonstrate these properties experimentally on a modified version of MNIST and\nCelebA.\n","authors":["Klaus-Rudolf Kladny","Julius von Kgelgen","Bernhard Schlkopf","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2310.07665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06577v2","updated":"2023-10-11T17:10:13Z","published":"2023-01-16T19:27:16Z","title":"Learning from Very Little Data: On the Value of Landscape Analysis for\n  Predicting Software Project Health","summary":"  When data is scarce, software analytics can make many mistakes. For example,\nconsider learning predictors for open source project health (e.g. the number of\nclosed pull requests in twelve months time). The training data for this task\nmay be very small (e.g. five years of data, collected every month means just 60\nrows of training data). The models generated from such tiny data sets can make\nmany prediction errors.\n  Those errors can be tamed by a {\\em landscape analysis} that selects better\nlearner control parameters. Our niSNEAK tool (a)~clusters the data to find the\ngeneral landscape of the hyperparameters; then (b)~explores a few\nrepresentatives from each part of that landscape. niSNEAK is both faster and\nmore effective than prior state-of-the-art hyperparameter optimization\nalgorithms (e.g. FLASH, HYPEROPT, OPTUNA).\n  The configurations found by niSNEAK have far less error than other methods.\nFor example, for project health indicators such as $C$= number of commits;\n$I$=number of closed issues, and $R$=number of closed pull requests, niSNEAK's\n12 month prediction errors are \\{I=0\\%, R=33\\%\\,C=47\\%\\}\n  Based on the above, we recommend landscape analytics (e.g. niSNEAK)\nespecially when learning from very small data sets. This paper only explores\nthe application of niSNEAK to project health. That said, we see nothing in\nprinciple that prevents the application of this technique to a wider range of\nproblems.\n  To assist other researchers in repeating, improving, or even refuting our\nresults, all our scripts and data are available on GitHub at\nhttps://github.com/zxcv123456qwe/niSneak\n","authors":["Andre Lustosa","Tim Menzies"],"pdf_url":"https://arxiv.org/pdf/2301.06577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08703v3","updated":"2023-10-11T17:00:34Z","published":"2023-05-15T15:06:20Z","title":"Schema-adaptable Knowledge Graph Construction","summary":"  Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n","authors":["Hongbin Ye","Honghao Gui","Xin Xu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08703v3.pdf","comment":"EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2310.07658v1","updated":"2023-10-11T17:00:03Z","published":"2023-10-11T17:00:03Z","title":"The First Pathloss Radio Map Prediction Challenge","summary":"  To foster research and facilitate fair comparisons among recently proposed\npathloss radio map prediction methods, we have launched the ICASSP 2023 First\nPathloss Radio Map Prediction Challenge. In this short overview paper, we\nbriefly describe the pathloss prediction problem, the provided datasets, the\nchallenge task and the challenge evaluation methodology. Finally, we present\nthe results of the challenge.\n","authors":["akan Yapar","Fabian Jaensch","Ron Levie","Gitta Kutyniok","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2310.07658v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2310.07654v1","updated":"2023-10-11T16:54:57Z","published":"2023-10-11T16:54:57Z","title":"Audio-Visual Neural Syntax Acquisition","summary":"  We study phrase structure induction from visually-grounded speech. The core\nidea is to first segment the speech waveform into sequences of word segments,\nand subsequently induce phrase structure using the inferred segment-level\ncontinuous representations. We present the Audio-Visual Neural Syntax Learner\n(AV-NSL) that learns phrase structure by listening to audio and looking at\nimages, without ever being exposed to text. By training on paired images and\nspoken captions, AV-NSL exhibits the capability to infer meaningful phrase\nstructures that are comparable to those derived by naturally-supervised text\nparsers, for both English and German. Our findings extend prior work in\nunsupervised language acquisition from speech and grounded grammar induction,\nand present one approach to bridge the gap between the two topics.\n","authors":["Cheng-I Jeff Lai","Freda Shi","Puyuan Peng","Yoon Kim","Kevin Gimpel","Shiyu Chang","Yung-Sung Chuang","Saurabhchand Bhati","David Cox","David Harwath","Yang Zhang","Karen Livescu","James Glass"],"pdf_url":"https://arxiv.org/pdf/2310.07654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13172v2","updated":"2023-10-11T16:51:50Z","published":"2023-05-22T16:00:00Z","title":"Editing Large Language Models: Problems, Methods, and Opportunities","summary":"  Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Yunzhi Yao","Peng Wang","Bozhong Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13172v2.pdf","comment":"EMNLP 2023. Updated with new experiments"},{"id":"http://arxiv.org/abs/2310.07648v1","updated":"2023-10-11T16:45:44Z","published":"2023-10-11T16:45:44Z","title":"Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral\n  Physiological Signals","summary":"  Multimodal emotion recognition from physiological signals is receiving an\nincreasing amount of attention due to the impossibility to control them at will\nunlike behavioral reactions, thus providing more reliable information. Existing\ndeep learning-based methods still rely on extracted handcrafted features, not\ntaking full advantage of the learning ability of neural networks, and often\nadopt a single-modality approach, while human emotions are inherently expressed\nin a multimodal way. In this paper, we propose a hypercomplex multimodal\nnetwork equipped with a novel fusion module comprising parameterized\nhypercomplex multiplications. Indeed, by operating in a hypercomplex domain the\noperations follow algebraic rules which allow to model latent relations among\nlearned feature dimensions for a more effective fusion step. We perform\nclassification of valence and arousal from electroencephalogram (EEG) and\nperipheral physiological signals, employing the publicly available database\nMAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of our\nwork is freely available at https://github.com/ispamm/MHyEEG.\n","authors":["Eleonora Lopez","Eleonora Chiarantano","Eleonora Grassucci","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07648v1.pdf","comment":"Published at IEEE ICASSP workshops 2023"},{"id":"http://arxiv.org/abs/2310.07644v1","updated":"2023-10-11T16:40:57Z","published":"2023-10-11T16:40:57Z","title":"Rethinking the BERT-like Pretraining for DNA Sequences","summary":"  With the success of large-scale pretraining in NLP, there is an increasing\ntrend of applying it to the domain of life sciences. In particular, pretraining\nmethods based on DNA sequences have garnered growing attention due to their\npotential to capture generic information about genes. However, existing\npretraining methods for DNA sequences largely rely on direct adoptions of BERT\npretraining from NLP, lacking a comprehensive understanding and a specifically\ntailored approach. To address this research gap, we first conducted a series of\nexploratory experiments and gained several insightful observations: 1) In the\nfine-tuning phase of downstream tasks, when using K-mer overlapping\ntokenization instead of K-mer non-overlapping tokenization, both overlapping\nand non-overlapping pretraining weights show consistent performance\nimprovement.2) During the pre-training process, using K-mer overlapping\ntokenization quickly produces clear K-mer embeddings and reduces the loss to a\nvery low level, while using K-mer non-overlapping tokenization results in less\ndistinct embeddings and continuously decreases the loss. 3) Using overlapping\ntokenization causes the self-attention in the intermediate layers of\npre-trained models to tend to overly focus on certain tokens, reflecting that\nthese layers are not adequately optimized. In summary, overlapping tokenization\ncan benefit the fine-tuning of downstream tasks but leads to inadequate\npretraining with fast convergence. To unleash the pretraining potential, we\nintroduce a novel approach called RandomMask, which gradually increases the\ntask difficulty of BERT-like pretraining by continuously expanding its mask\nboundary, forcing the model to learn more knowledge. RandomMask is simple but\neffective, achieving top-tier performance across 26 datasets of 28 datasets\nspanning 7 downstream tasks.\n","authors":["Chaoqi Liang","Weiqiang Bai","Lifeng Qiao","Yuchen Ren","Jianle Sun","Peng Ye","Hongliang Yan","Xinzhu Ma","Wangmeng Zuo","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.07644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07641v1","updated":"2023-10-11T16:38:11Z","published":"2023-10-11T16:38:11Z","title":"Evaluating Large Language Models at Evaluating Instruction Following","summary":"  As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these \"LLM evaluators\", particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.\n","authors":["Zhiyuan Zeng","Jiatong Yu","Tianyu Gao","Yu Meng","Tanya Goyal","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07641v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2207.10062v3","updated":"2023-10-11T16:32:49Z","published":"2022-07-20T17:47:54Z","title":"DataPerf: Benchmarks for Data-Centric AI Development","summary":"  Machine learning research has long focused on models rather than datasets,\nand prominent datasets are used for common ML tasks without regard to the\nbreadth, difficulty, and faithfulness of the underlying problems. Neglecting\nthe fundamental importance of data has given rise to inaccuracy, bias, and\nfragility in real-world applications, and research is hindered by saturation\nacross existing dataset benchmarks. In response, we present DataPerf, a\ncommunity-led benchmark suite for evaluating ML datasets and data-centric\nalgorithms. We aim to foster innovation in data-centric AI through competition,\ncomparability, and reproducibility. We enable the ML community to iterate on\ndatasets, instead of just architectures, and we provide an open, online\nplatform with multiple rounds of challenges to support this iterative\ndevelopment. The first iteration of DataPerf contains five benchmarks covering\na wide spectrum of data-centric techniques, tasks, and modalities in vision,\nspeech, acquisition, debugging, and diffusion prompting, and we support hosting\nnew contributed benchmarks from the community. The benchmarks, online\nevaluation platform, and baseline implementations are open source, and the\nMLCommons Association will maintain DataPerf to ensure long-term benefits to\nacademia and industry.\n","authors":["Mark Mazumder","Colby Banbury","Xiaozhe Yao","Bojan Karla","William Gaviria Rojas","Sudnya Diamos","Greg Diamos","Lynn He","Alicia Parrish","Hannah Rose Kirk","Jessica Quaye","Charvi Rastogi","Douwe Kiela","David Jurado","David Kanter","Rafael Mosquera","Juan Ciro","Lora Aroyo","Bilge Acun","Lingjiao Chen","Mehul Smriti Raje","Max Bartolo","Sabri Eyuboglu","Amirata Ghorbani","Emmett Goodman","Oana Inel","Tariq Kane","Christine R. Kirkpatrick","Tzu-Sheng Kuo","Jonas Mueller","Tristan Thrush","Joaquin Vanschoren","Margaret Warren","Adina Williams","Serena Yeung","Newsha Ardalani","Praveen Paritosh","Lilith Bath-Leah","Ce Zhang","James Zou","Carole-Jean Wu","Cody Coleman","Andrew Ng","Peter Mattson","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2207.10062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07632v1","updated":"2023-10-11T16:25:45Z","published":"2023-10-11T16:25:45Z","title":"Prompt Backdoors in Visual Prompt Learning","summary":"  Fine-tuning large pre-trained computer vision models is infeasible for\nresource-limited users. Visual prompt learning (VPL) has thus emerged to\nprovide an efficient and flexible alternative to model fine-tuning through\nVisual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider\noptimizes a visual prompt given downstream data, and downstream users can use\nthis prompt together with the large pre-trained model for prediction. However,\nthis new learning paradigm may also pose security risks when the VPPTaaS\nprovider instead provides a malicious visual prompt. In this paper, we take the\nfirst step to explore such risks through the lens of backdoor attacks.\nSpecifically, we propose BadVisualPrompt, a simple yet effective backdoor\nattack against VPL. For example, poisoning $5\\%$ CIFAR10 training data leads to\nabove $99\\%$ attack success rates with only negligible model accuracy drop by\n$1.5\\%$. In particular, we identify and then address a new technical challenge\nrelated to interactions between the backdoor trigger and visual prompt, which\ndoes not exist in conventional, model-level backdoors. Moreover, we provide\nin-depth analyses of seven backdoor defenses from model, prompt, and input\nlevels. Overall, all these defenses are either ineffective or impractical to\nmitigate our BadVisualPrompt, implying the critical vulnerability of VPL.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07631v1","updated":"2023-10-11T16:24:06Z","published":"2023-10-11T16:24:06Z","title":"Graph Transformer Network for Flood Forecasting with Heterogeneous\n  Covariates","summary":"  Floods can be very destructive causing heavy damage to life, property, and\nlivelihoods. Global climate change and the consequent sea-level rise have\nincreased the occurrence of extreme weather events, resulting in elevated and\nfrequent flood risk. Therefore, accurate and timely flood forecasting in\ncoastal river systems is critical to facilitate good flood management. However,\nthe computational tools currently used are either slow or inaccurate. In this\npaper, we propose a Flood prediction tool using Graph Transformer Network\n(FloodGTN) for river systems. More specifically, FloodGTN learns the\nspatio-temporal dependencies of water levels at different monitoring stations\nusing Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to\nconsider external covariates such as rainfall, tide, and the settings of\nhydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the\nriver. We use a Transformer to learn the attention given to external covariates\nin computing water levels. We apply the FloodGTN tool to data from the South\nFlorida Water Management District, which manages a coastal area prone to\nfrequent storms and hurricanes. Experimental results show that FloodGTN\noutperforms the physics-based model (HEC-RAS) by achieving higher accuracy with\n70% improvement while speeding up run times by at least 500x.\n","authors":["Jimeng Shi","Vitalii Stebliankin","Zhaonan Wang","Shaowen Wang","Giri Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2310.07631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07630v1","updated":"2023-10-11T16:23:07Z","published":"2023-10-11T16:23:07Z","title":"Differentiable Euler Characteristic Transforms for Shape Classification","summary":"  The Euler Characteristic Transform (ECT) has proven to be a powerful\nrepresentation, combining geometrical and topological characteristics of shapes\nand graphs. However, the ECT was hitherto unable to learn task-specific\nrepresentations. We overcome this issue and develop a novel computational layer\nthat enables learning the ECT in an end-to-end fashion. Our method DECT is fast\nand computationally efficient, while exhibiting performance on a par with more\ncomplex models in both graph and point cloud classification tasks. Moreover, we\nshow that this seemingly unexpressive statistic still provides the same\ntopological expressivity as more complex topological deep learning layers\nprovide.\n","authors":["Ernst Roell","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2310.07630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08268v3","updated":"2023-10-11T16:17:20Z","published":"2023-03-14T23:01:27Z","title":"Chat with the Environment: Interactive Multimodal Perception Using Large\n  Language Models","summary":"  Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.\n","authors":["Xufeng Zhao","Mengdi Li","Cornelius Weber","Muhammad Burhan Hafez","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2303.08268v3.pdf","comment":"IROS2023, Detroit. See the project website at\n  https://matcha-agent.github.io"},{"id":"http://arxiv.org/abs/2305.11141v3","updated":"2023-10-11T16:16:18Z","published":"2023-05-18T17:35:35Z","title":"Clifford Group Equivariant Neural Networks","summary":"  We introduce Clifford Group Equivariant Neural Networks: a novel approach for\nconstructing $\\mathrm{O}(n)$- and $\\mathrm{E}(n)$-equivariant models. We\nidentify and study the $\\textit{Clifford group}$, a subgroup inside the\nClifford algebra whose definition we adjust to achieve several favorable\nproperties. Primarily, the group's action forms an orthogonal automorphism that\nextends beyond the typical vector space to the entire Clifford algebra while\nrespecting the multivector grading. This leads to several non-equivalent\nsubrepresentations corresponding to the multivector decomposition. Furthermore,\nwe prove that the action respects not just the vector space structure of the\nClifford algebra but also its multiplicative structure, i.e., the geometric\nproduct. These findings imply that every polynomial in multivectors, An\nadvantage worth mentioning is that we obtain expressive layers that can\nelegantly generalize to inner-product spaces of any dimension. We demonstrate,\nnotably from a single core implementation, state-of-the-art performance on\nseveral distinct tasks, including a three-dimensional $n$-body experiment, a\nfour-dimensional Lorentz-equivariant high-energy physics experiment, and a\nfive-dimensional convex hull experiment.\n","authors":["David Ruhe","Johannes Brandstetter","Patrick Forr"],"pdf_url":"https://arxiv.org/pdf/2305.11141v3.pdf","comment":"Published at NeurIPS 2023 (Oral)"},{"id":"http://arxiv.org/abs/2310.07626v1","updated":"2023-10-11T16:09:09Z","published":"2023-10-11T16:09:09Z","title":"Unsupervised Learning of Sea Surface Height Interpolation from\n  Multi-variate Simulated Satellite Observations","summary":"  Satellite-based remote sensing missions have revolutionized our understanding\nof the Ocean state and dynamics. Among them, spaceborne altimetry provides\nvaluable measurements of Sea Surface Height (SSH), which is used to estimate\nsurface geostrophic currents. However, due to the sensor technology employed,\nimportant gaps occur in SSH observations. Complete SSH maps are produced by the\naltimetry community using linear Optimal Interpolations (OI) such as the\nwidely-used Data Unification and Altimeter Combination System (DUACS). However,\nOI is known for producing overly smooth fields and thus misses some\nmesostructures and eddies. On the other hand, Sea Surface Temperature (SST)\nproducts have much higher data coverage and SST is physically linked to\ngeostrophic currents through advection. We design a realistic twin experiment\nto emulate the satellite observations of SSH and SST to evaluate interpolation\nmethods. We introduce a deep learning network able to use SST information, and\na trainable in two settings: one where we have no access to ground truth during\ntraining and one where it is accessible. Our investigation involves a\ncomparative analysis of the aforementioned network when trained using either\nsupervised or unsupervised loss functions. We assess the quality of SSH\nreconstructions and further evaluate the network's performance in terms of eddy\ndetection and physical properties. We find that it is possible, even in an\nunsupervised setting to use SST to improve reconstruction performance compared\nto SST-agnostic interpolations. We compare our reconstructions to DUACS's and\nreport a decrease of 41\\% in terms of root mean squared error.\n","authors":["Theo Archambault","Arthur Filoche","Anastase Charantonis","Dominique Bereziat","Sylvie Thiria"],"pdf_url":"https://arxiv.org/pdf/2310.07626v1.pdf","comment":"submitted to JAMES. 26 pages"},{"id":"http://arxiv.org/abs/2305.12534v2","updated":"2023-10-11T16:05:21Z","published":"2023-05-21T18:26:31Z","title":"BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer","summary":"  We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL)\nbased fuzzer aimed at finding security vulnerabilities for Web applications.\nBertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs\ngrammar-adhering and attack-provoking mutation operations on them to generate\ncandidate attack vectors. The key insight of BertRLFuzzer is the use of RL with\na BERT model as an agent to guide the fuzzer to efficiently learn\ngrammar-adhering and attack-provoking mutation operators. In order to establish\nthe efficacy of BertRLFuzzer we compare it against a total of 13 black box and\nwhite box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We\nobserved a significant improvement, relative to the nearest competing tool, in\nterms of time to first attack (54% less), new vulnerabilities found (17 new\nvulnerabilities), and attack rate (4.4% more attack vectors generated).\n","authors":["Piyush Jha","Joseph Scott","Jaya Sriram Ganeshna","Mudit Singh","Vijay Ganesh"],"pdf_url":"https://arxiv.org/pdf/2305.12534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07612v1","updated":"2023-10-11T15:56:55Z","published":"2023-10-11T15:56:55Z","title":"PHYDI: Initializing Parameterized Hypercomplex Neural Networks as\n  Identity Functions","summary":"  Neural models based on hypercomplex algebra systems are growing and\nprolificating for a plethora of applications, ranging from computer vision to\nnatural language processing. Hand in hand with their adoption, parameterized\nhypercomplex neural networks (PHNNs) are growing in size and no techniques have\nbeen adopted so far to control their convergence at a large scale. In this\npaper, we study PHNNs convergence and propose parameterized hypercomplex\nidentity initialization (PHYDI), a method to improve their convergence at\ndifferent scales, leading to more robust performance when the number of layers\nscales up, while also reaching the same performance with fewer iterations. We\nshow the effectiveness of this approach in different benchmarks and with common\nPHNNs with ResNets- and Transformer-based architecture. The code is available\nat https://github.com/ispamm/PHYDI.\n","authors":["Matteo Mancanelli","Eleonora Grassucci","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07612v1.pdf","comment":"Accepted at IEEE MLSP 2023 (Honorable Mention TOP 5% Outstanding\n  Papers)"},{"id":"http://arxiv.org/abs/2203.06768v4","updated":"2023-10-11T15:46:16Z","published":"2022-03-13T21:39:24Z","title":"Probabilistically Robust Recourse: Navigating the Trade-offs between\n  Costs and Robustness in Algorithmic Recourse","summary":"  As machine learning models are increasingly being employed to make\nconsequential decisions in real-world settings, it becomes critical to ensure\nthat individuals who are adversely impacted (e.g., loan denied) by the\npredictions of these models are provided with a means for recourse. While\nseveral approaches have been proposed to construct recourses for affected\nindividuals, the recourses output by these methods either achieve low costs\n(i.e., ease-of-implementation) or robustness to small perturbations (i.e.,\nnoisy implementations of recourses), but not both due to the inherent\ntrade-offs between the recourse costs and robustness. Furthermore, prior\napproaches do not provide end users with any agency over navigating the\naforementioned trade-offs. In this work, we address the above challenges by\nproposing the first algorithmic framework which enables users to effectively\nmanage the recourse cost vs. robustness trade-offs. More specifically, our\nframework Probabilistically ROBust rEcourse (\\texttt{PROBE}) lets users choose\nthe probability with which a recourse could get invalidated (recourse\ninvalidation rate) if small changes are made to the recourse i.e., the recourse\nis implemented somewhat noisily. To this end, we propose a novel objective\nfunction which simultaneously minimizes the gap between the achieved\n(resulting) and desired recourse invalidation rates, minimizes recourse costs,\nand also ensures that the resulting recourse achieves a positive model\nprediction. We develop novel theoretical results to characterize the recourse\ninvalidation rates corresponding to any given instance w.r.t. different classes\nof underlying models (e.g., linear models, tree based models etc.), and\nleverage these results to efficiently optimize the proposed objective.\nExperimental evaluation with multiple real world datasets demonstrates the\nefficacy of the proposed framework.\n","authors":["Martin Pawelczyk","Teresa Datta","Johannes van-den-Heuvel","Gjergji Kasneci","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2203.06768v4.pdf","comment":"ICLR 2023, camera ready version"},{"id":"http://arxiv.org/abs/2310.07598v1","updated":"2023-10-11T15:38:53Z","published":"2023-10-11T15:38:53Z","title":"Survey on Imbalanced Data, Representation Learning and SEP Forecasting","summary":"  Deep Learning methods have significantly advanced various data-driven tasks\nsuch as regression, classification, and forecasting. However, much of this\nprogress has been predicated on the strong but often unrealistic assumption\nthat training datasets are balanced with respect to the targets they contain.\nThis misalignment with real-world conditions, where data is frequently\nimbalanced, hampers the effectiveness of such models in practical applications.\nMethods that reconsider that assumption and tackle real-world imbalances have\nbegun to emerge and explore avenues to address this challenge. One such\npromising avenue is representation learning, which enables models to capture\ncomplex data characteristics and generalize better to minority classes. By\nfocusing on a richer representation of the feature space, these techniques hold\nthe potential to mitigate the impact of data imbalance. In this survey, we\npresent deep learning works that step away from the balanced-data assumption,\nemploying strategies like representation learning to better approximate\nreal-world imbalances. We also highlight a critical application in SEP\nforecasting where addressing data imbalance is paramount for success.\n","authors":["Josias Moukpe"],"pdf_url":"https://arxiv.org/pdf/2310.07598v1.pdf","comment":"Survey Paper, 4 figures, 16 pages"},{"id":"http://arxiv.org/abs/2310.07596v1","updated":"2023-10-11T15:37:31Z","published":"2023-10-11T15:37:31Z","title":"Prospective Side Information for Latent MDPs","summary":"  In many interactive decision-making settings, there is latent and unobserved\ninformation that remains fixed. Consider, for example, a dialogue system, where\ncomplete information about a user, such as the user's preferences, is not\ngiven. In such an environment, the latent information remains fixed throughout\neach episode, since the identity of the user does not change during an\ninteraction. This type of environment can be modeled as a Latent Markov\nDecision Process (LMDP), a special instance of Partially Observed Markov\nDecision Processes (POMDPs). Previous work established exponential lower bounds\nin the number of latent contexts for the LMDP class. This puts forward a\nquestion: under which natural assumptions a near-optimal policy of an LMDP can\nbe efficiently learned? In this work, we study the class of LMDPs with {\\em\nprospective side information}, when an agent receives additional, weakly\nrevealing, information on the latent context at the beginning of each episode.\nWe show that, surprisingly, this problem is not captured by contemporary\nsettings and algorithms designed for partially observed environments. We then\nestablish that any sample efficient algorithm must suffer at least\n$\\Omega(K^{2/3})$-regret, as opposed to standard $\\Omega(\\sqrt{K})$ lower\nbounds, and design an algorithm with a matching upper bound.\n","authors":["Jeongyeol Kwon","Yonathan Efroni","Shie Mannor","Constantine Caramanis"],"pdf_url":"https://arxiv.org/pdf/2310.07596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07592v1","updated":"2023-10-11T15:35:20Z","published":"2023-10-11T15:35:20Z","title":"Transformers for Green Semantic Communication: Less Energy, More\n  Semantics","summary":"  Semantic communication aims to transmit meaningful and effective information\nrather than focusing on individual symbols or bits, resulting in benefits like\nreduced latency, bandwidth usage, and higher throughput compared to traditional\ncommunication. However, semantic communication poses significant challenges due\nto the need for universal metrics for benchmarking the joint effects of\nsemantic information loss and practical energy consumption. This research\npresents a novel multi-objective loss function named \"Energy-Optimized Semantic\nLoss\" (EOSL), addressing the challenge of balancing semantic information loss\nand energy consumption. Through comprehensive experiments on transformer\nmodels, including CPU and GPU energy usage, it is demonstrated that EOSL-based\nencoder model selection can save up to 90\\% of energy while achieving a 44\\%\nimprovement in semantic similarity performance during inference in this\nexperiment. This work paves the way for energy-efficient neural network\nselection and the development of greener semantic communication architectures.\n","authors":["Shubhabrata Mukherjee","Cory Beard","Sejun Song"],"pdf_url":"https://arxiv.org/pdf/2310.07592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14137v3","updated":"2023-10-11T15:34:51Z","published":"2022-08-30T10:35:32Z","title":"On the Trade-Off between Actionable Explanations and the Right to be\n  Forgotten","summary":"  As machine learning (ML) models are increasingly being deployed in\nhigh-stakes applications, policymakers have suggested tighter data protection\nregulations (e.g., GDPR, CCPA). One key principle is the \"right to be\nforgotten\" which gives users the right to have their data deleted. Another key\nprinciple is the right to an actionable explanation, also known as algorithmic\nrecourse, allowing users to reverse unfavorable decisions. To date, it is\nunknown whether these two principles can be operationalized simultaneously.\nTherefore, we introduce and study the problem of recourse invalidation in the\ncontext of data deletion requests. More specifically, we theoretically and\nempirically analyze the behavior of popular state-of-the-art algorithms and\ndemonstrate that the recourses generated by these algorithms are likely to be\ninvalidated if a small number of data deletion requests (e.g., 1 or 2) warrant\nupdates of the predictive model. For the setting of differentiable models, we\nsuggest a framework to identify a minimal subset of critical training points\nwhich, when removed, maximize the fraction of invalidated recourses. Using our\nframework, we empirically show that the removal of as little as 2 data\ninstances from the training set can invalidate up to 95 percent of all\nrecourses output by popular state-of-the-art algorithms. Thus, our work raises\nfundamental questions about the compatibility of \"the right to an actionable\nexplanation\" in the context of the \"right to be forgotten\", while also\nproviding constructive insights on the determining factors of recourse\nrobustness.\n","authors":["Martin Pawelczyk","Tobias Leemann","Asia Biega","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2208.14137v3.pdf","comment":"ICLR 2023 camera ready version"},{"id":"http://arxiv.org/abs/2310.07588v1","updated":"2023-10-11T15:28:44Z","published":"2023-10-11T15:28:44Z","title":"Accurate Use of Label Dependency in Multi-Label Text Classification\n  Through the Lens of Causality","summary":"  Multi-Label Text Classification (MLTC) aims to assign the most relevant\nlabels to each given text. Existing methods demonstrate that label dependency\ncan help to improve the model's performance. However, the introduction of label\ndependency may cause the model to suffer from unwanted prediction bias. In this\nstudy, we attribute the bias to the model's misuse of label dependency, i.e.,\nthe model tends to utilize the correlation shortcut in label dependency rather\nthan fusing text information and label dependency for prediction. Motivated by\ncausal inference, we propose a CounterFactual Text Classifier (CFTC) to\neliminate the correlation bias, and make causality-based predictions.\nSpecifically, our CFTC first adopts the predict-then-modify backbone to extract\nprecise label information embedded in label dependency, then blocks the\ncorrelation shortcut through the counterfactual de-bias technique with the help\nof the human causal graph. Experimental results on three datasets demonstrate\nthat our CFTC significantly outperforms the baselines and effectively\neliminates the correlation bias in datasets.\n","authors":["Caoyun Fan","Wenqing Chen","Jidong Tian","Yitian Li","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2310.07588v1.pdf","comment":"Applied Intelligence 2023"},{"id":"http://arxiv.org/abs/2310.07587v1","updated":"2023-10-11T15:28:39Z","published":"2023-10-11T15:28:39Z","title":"Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient\n  Balancer","summary":"  Data privacy and long-tailed distribution are the norms rather than the\nexception in many real-world tasks. This paper investigates a federated\nlong-tailed learning (Fed-LT) task in which each client holds a locally\nheterogeneous dataset; if the datasets can be globally aggregated, they jointly\nexhibit a long-tailed distribution. Under such a setting, existing federated\noptimization and/or centralized long-tailed learning methods hardly apply due\nto challenges in (a) characterizing the global long-tailed distribution under\nprivacy constraints and (b) adjusting the local learning strategy to cope with\nthe head-tail imbalance. In response, we propose a method termed\n$\\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB)\nmodule that re-weights clients' gradients in a closed-loop manner, based on the\nfeedback of global long-tailed distribution evaluated by a Direct Prior\nAnalyzer (DPA) module. Using $\\texttt{Fed-GraB}$, clients can effectively\nalleviate the distribution drift caused by data heterogeneity during the model\ntraining process and obtain a global model with better performance on the\nminority classes while maintaining the performance of the majority classes.\nExtensive experiments demonstrate that $\\texttt{Fed-GraB}$ achieves\nstate-of-the-art performance on representative datasets such as CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist.\n","authors":["Zikai Xiao","Zihan Chen","Songshang Liu","Hualiang Wang","Yang Feng","Jin Hao","Joey Tianyi Zhou","Jian Wu","Howard Hao Yang","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07587v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.11942v2","updated":"2023-10-11T15:21:32Z","published":"2023-09-21T09:57:03Z","title":"On the Probability of Immunity","summary":"  This work is devoted to the study of the probability of immunity, i.e. the\neffect occurs whether exposed or not. We derive necessary and sufficient\nconditions for non-immunity and $\\epsilon$-bounded immunity, i.e. the\nprobability of immunity is zero and $\\epsilon$-bounded, respectively. The\nformer allows us to estimate the probability of benefit (i.e., the effect\noccurs if and only if exposed) from a randomized controlled trial, and the\nlatter allows us to produce bounds of the probability of benefit that are\ntighter than the existing ones. We also introduce the concept of indirect\nimmunity (i.e., through a mediator) and repeat our previous analysis for it.\nFinally, we propose a method for sensitivity analysis of the probability of\nimmunity under unmeasured confounding.\n","authors":["Jose M. Pea"],"pdf_url":"https://arxiv.org/pdf/2309.11942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07582v1","updated":"2023-10-11T15:20:07Z","published":"2023-10-11T15:20:07Z","title":"Linear Latent World Models in Simple Transformers: A Case Study on\n  Othello-GPT","summary":"  Foundation models exhibit significant capabilities in decision-making and\nlogical deductions. Nonetheless, a continuing discourse persists regarding\ntheir genuine understanding of the world as opposed to mere stochastic mimicry.\nThis paper meticulously examines a simple transformer trained for Othello,\nextending prior research to enhance comprehension of the emergent world model\nof Othello-GPT. The investigation reveals that Othello-GPT encapsulates a\nlinear representation of opposing pieces, a factor that causally steers its\ndecision-making process. This paper further elucidates the interplay between\nthe linear world representation and causal decision-making, and their\ndependence on layer depth and model complexity. We have made the code public.\n","authors":["Dean S. Hazineh","Zechen Zhang","Jeffery Chiu"],"pdf_url":"https://arxiv.org/pdf/2310.07582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07579v1","updated":"2023-10-11T15:19:31Z","published":"2023-10-11T15:19:31Z","title":"In-Context Unlearning: Language Models as Few Shot Unlearners","summary":"  Machine unlearning, the study of efficiently removing the impact of specific\ntraining points on the trained model, has garnered increased attention of late,\ndriven by the need to comply with privacy regulations like the \\emph{Right to\nbe Forgotten}. Although unlearning is particularly relevant for LLMs in light\nof the copyright issues they raise, achieving precise unlearning is\ncomputationally infeasible for very large models. To this end, recent work has\nproposed several algorithms which approximate the removal of training data\nwithout retraining the model. These algorithms crucially rely on access to the\nmodel parameters in order to update them, an assumption that may not hold in\npractice due to computational constraints or when the LLM is accessed via API.\nIn this work, we propose a new class of unlearning methods for LLMs we call\n``In-Context Unlearning'', providing inputs in context and without having to\nupdate model parameters. To unlearn a particular training instance, we provide\nthe instance alongside a flipped label and additional correctly labelled\ninstances which are prepended as inputs to the LLM at inference time. Our\nexperimental results demonstrate that these contexts effectively remove\nspecific information from the training set while maintaining performance levels\nthat are competitive with (or in some cases exceed) state-of-the-art unlearning\nmethods that require access to the LLM parameters.\n","authors":["Martin Pawelczyk","Seth Neel","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07576v1","updated":"2023-10-11T15:17:55Z","published":"2023-10-11T15:17:55Z","title":"Analyzing Trendy Twitter Hashtags in the 2022 French Election","summary":"  Regressions trained to predict the future activity of social media users need\nrich features for accurate predictions. Many advanced models exist to generate\nsuch features; however, the time complexities of their computations are often\nprohibitive when they run on enormous data-sets. Some studies have shown that\nsimple semantic network features can be rich enough to use for regressions\nwithout requiring complex computations. We propose a method for using semantic\nnetworks as user-level features for machine learning tasks. We conducted an\nexperiment using a semantic network of 1037 Twitter hashtags from a corpus of\n3.7 million tweets related to the 2022 French presidential election. A\nbipartite graph is formed where hashtags are nodes and weighted edges connect\nthe hashtags reflecting the number of Twitter users that interacted with both\nhashtags. The graph is then transformed into a maximum-spanning tree with the\nmost popular hashtag as its root node to construct a hierarchy amongst the\nhashtags. We then provide a vector feature for each user based on this tree. To\nvalidate the usefulness of our semantic feature we performed a regression\nexperiment to predict the response rate of each user with six emotions like\nanger, enjoyment, or disgust. Our semantic feature performs well with the\nregression with most emotions having $R^2$ above 0.5. These results suggest\nthat our semantic feature could be considered for use in further experiments\npredicting social media response on big data-sets.\n","authors":["Aamir Mandviwalla","Lake Yin","Boleslaw K. Szymanski"],"pdf_url":"https://arxiv.org/pdf/2310.07576v1.pdf","comment":"9 pages, 1 figure, to be published in Complex Networks 2023"},{"id":"http://arxiv.org/abs/2211.15751v3","updated":"2023-10-11T15:13:02Z","published":"2022-11-28T20:11:37Z","title":"Edge Video Analytics: A Survey on Applications, Systems and Enabling\n  Techniques","summary":"  Video, as a key driver in the global explosion of digital information, can\ncreate tremendous benefits for human society. Governments and enterprises are\ndeploying innumerable cameras for a variety of applications, e.g., law\nenforcement, emergency management, traffic control, and security surveillance,\nall facilitated by video analytics (VA). This trend is spurred by the rapid\nadvancement of deep learning (DL), which enables more precise models for object\nclassification, detection, and tracking. Meanwhile, with the proliferation of\nInternet-connected devices, massive amounts of data are generated daily,\noverwhelming the cloud. Edge computing, an emerging paradigm that moves\nworkloads and services from the network core to the network edge, has been\nwidely recognized as a promising solution. The resulting new intersection, edge\nvideo analytics (EVA), begins to attract widespread attention. Nevertheless,\nonly a few loosely-related surveys exist on this topic. The basic concepts of\nEVA (e.g., definition, architectures) were not fully elucidated due to the\nrapid development of this domain. To fill these gaps, we provide a\ncomprehensive survey of the recent efforts on EVA. In this paper, we first\nreview the fundamentals of edge computing, followed by an overview of VA. EVA\nsystems and their enabling techniques are discussed next. In addition, we\nintroduce prevalent frameworks and datasets to aid future researchers in the\ndevelopment of EVA systems. Finally, we discuss existing challenges and foresee\nfuture research directions. We believe this survey will help readers comprehend\nthe relationship between VA and edge computing, and spark new ideas on EVA.\n","authors":["Renjie Xu","Saiedeh Razavi","Rong Zheng"],"pdf_url":"https://arxiv.org/pdf/2211.15751v3.pdf","comment":"Accepted in IEEE Communications Surveys and Tutorials, 2023"},{"id":"http://arxiv.org/abs/2304.06548v2","updated":"2023-10-11T15:09:11Z","published":"2023-04-13T13:57:33Z","title":"Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient\n  Descent Methods","summary":"  This paper presents two computationally efficient algorithms for the\norientation estimation of inertial measurement units (IMUs): the\ncorrentropy-based gradient descent (CGD) and the correntropy-based decoupled\norientation estimation (CDOE). Traditional methods, such as gradient descent\n(GD) and decoupled orientation estimation (DOE), rely on the mean squared error\n(MSE) criterion, making them vulnerable to external acceleration and magnetic\ninterference. To address this issue, we demonstrate that the multi-kernel\ncorrentropy loss (MKCL) is an optimal objective function for maximum likelihood\nestimation (MLE) when the noise follows a type of heavy-tailed distribution. In\ncertain situations, the estimation error of the MKCL is bounded even in the\npresence of arbitrarily large outliers. By replacing the standard MSE cost\nfunction with MKCL, we develop the CGD and CDOE algorithms. We evaluate the\neffectiveness of our proposed methods by comparing them with existing\nalgorithms in various situations. Experimental results indicate that our\nproposed methods (CGD and CDOE) outperform their conventional counterparts (GD\nand DOE), especially when faced with external acceleration and magnetic\ndisturbances. Furthermore, the new algorithms demonstrate significantly lower\ncomputational complexity than Kalman filter-based approaches, making them\nsuitable for applications with low-cost microprocessors.\n","authors":["Shilei Li","Lijing Li","Dawei Shi","Yunjiang Lou","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2304.06548v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2304.14933v2","updated":"2023-10-11T15:08:51Z","published":"2023-04-28T15:43:21Z","title":"An Empirical Study of Multimodal Model Merging","summary":"  Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n","authors":["Yi-Lin Sung","Linjie Li","Kevin Lin","Zhe Gan","Mohit Bansal","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2304.14933v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.05469v2","updated":"2023-10-11T15:07:36Z","published":"2023-10-09T07:26:35Z","title":"Vibroacoustic Frequency Response Prediction with Query-based Operator\n  Networks","summary":"  Understanding vibroacoustic wave propagation in mechanical structures like\nairplanes, cars and houses is crucial to ensure health and comfort of their\nusers. To analyze such systems, designers and engineers primarily consider the\ndynamic response in the frequency domain, which is computed through expensive\nnumerical simulations like the finite element method. In contrast, data-driven\nsurrogate models offer the promise of speeding up these simulations, thereby\nfacilitating tasks like design optimization, uncertainty quantification, and\ndesign space exploration. We present a structured benchmark for a\nrepresentative vibroacoustic problem: Predicting the frequency response for\nvibrating plates with varying forms of beadings. The benchmark features a total\nof 12,000 plate geometries with an associated numerical solution and introduces\nevaluation metrics to quantify the prediction quality. To address the frequency\nresponse prediction task, we propose a novel frequency query operator model,\nwhich is trained to map plate geometries to frequency response functions. By\nintegrating principles from operator learning and implicit models for shape\nencoding, our approach effectively addresses the prediction of resonance peaks\nof frequency responses. We evaluate the method on our vibrating-plates\nbenchmark and find that it outperforms DeepONets, Fourier Neural Operators and\nmore traditional neural network architectures. The code and dataset are\navailable from https://eckerlab.org/code/delden2023_plate.\n","authors":["Jan van Delden","Julius Schultz","Christopher Blech","Sabine C. Langer","Timo Lddecke"],"pdf_url":"https://arxiv.org/pdf/2310.05469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07560v1","updated":"2023-10-11T15:04:33Z","published":"2023-10-11T15:04:33Z","title":"ROMO: Retrieval-enhanced Offline Model-based Optimization","summary":"  Data-driven black-box model-based optimization (MBO) problems arise in a\ngreat number of practical application scenarios, where the goal is to find a\ndesign over the whole space maximizing a black-box target function based on a\nstatic offline dataset. In this work, we consider a more general but\nchallenging MBO setting, named constrained MBO (CoMBO), where only part of the\ndesign space can be optimized while the rest is constrained by the environment.\nA new challenge arising from CoMBO is that most observed designs that satisfy\nthe constraints are mediocre in evaluation. Therefore, we focus on optimizing\nthese mediocre designs in the offline dataset while maintaining the given\nconstraints rather than further boosting the best observed design in the\ntraditional MBO setting. We propose retrieval-enhanced offline model-based\noptimization (ROMO), a new derivable forward approach that retrieves the\noffline dataset and aggregates relevant samples to provide a trusted\nprediction, and use it for gradient-based optimization. ROMO is simple to\nimplement and outperforms state-of-the-art approaches in the CoMBO setting.\nEmpirically, we conduct experiments on a synthetic Hartmann (3D) function\ndataset, an industrial CIO dataset, and a suite of modified tasks in the\nDesign-Bench benchmark. Results show that ROMO performs well in a wide range of\nconstrained optimization tasks.\n","authors":["Mingcheng Chen","Haoran Zhao","Yuxiang Zhao","Hulei Fan","Hongqiao Gao","Yong Yu","Zheng Tian"],"pdf_url":"https://arxiv.org/pdf/2310.07560v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.07558v1","updated":"2023-10-11T15:02:13Z","published":"2023-10-11T15:02:13Z","title":"Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning","summary":"  We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$.\n","authors":["Zeqi Ye","Hansheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12898v3","updated":"2023-10-11T14:59:49Z","published":"2023-06-22T14:07:23Z","title":"Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of\n  InAs/GaAs Quantum Dots","summary":"  Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable\nfor developing various optoelectronic devices such as QD lasers and single\nphoton sources. The applications strongly rely on the density and quality of\nthese dots, which has motivated studies of the growth process control to\nrealize high-quality epi-wafers and devices. Establishing the process\nparameters in molecular beam epitaxy (MBE) for a specific density of QDs is a\nmultidimensional optimization challenge, usually addressed through\ntime-consuming and iterative trial-and-error. Here, we report a real-time\nfeedback control method to realize the growth of QDs with arbitrary density,\nwhich is fully automated and intelligent. We developed a machine learning (ML)\nmodel named 3D ResNet 50 trained using reflection high-energy electron\ndiffraction (RHEED) videos as input instead of static images and providing\nreal-time feedback on surface morphologies for process control. As a result, we\ndemonstrated that ML from previous growth could predict the post-growth density\nof QDs, by successfully tuning the QD densities in near-real time from 1.5E10\ncm-2 down to 3.8E8 cm-2 or up to 1.4E11 cm-2. Compared to traditional methods,\nour approach, with in situ tuning capabilities and excellent reliability, can\ndramatically expedite the material optimization process and improve the\nreproducibility of MBE, constituting significant progress for thin film growth\ntechniques. The concepts and methodologies proved feasible in this work are\npromising to be applied to a variety of material growth processes, which will\nrevolutionize semiconductor manufacturing for optoelectronic and\nmicroelectronic industries.\n","authors":["Chao Shen","Wenkang Zhan","Kaiyao Xin","Manyang Li","Zhenyu Sun","Hui Cong","Chi Xu","Jian Tang","Zhaofeng Wu","Bo Xu","Zhongming Wei","Chunlai Xue","Chao Zhao","Zhanguo Wang"],"pdf_url":"https://arxiv.org/pdf/2306.12898v3.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2308.12067v2","updated":"2023-10-11T14:49:26Z","published":"2023-08-23T11:27:30Z","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4","summary":"  Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.\n","authors":["Lai Wei","Zihao Jiang","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07535v1","updated":"2023-10-11T14:39:51Z","published":"2023-10-11T14:39:51Z","title":"Improving Fairness-Accuracy tradeoff with few Test Samples under\n  Covariate Shift","summary":"  Covariate shift in the test data can significantly downgrade both the\naccuracy and the fairness performance of the model. Ensuring fairness across\ndifferent sensitive groups in such settings is of paramount importance due to\nsocietal implications like criminal justice. We operate under the unsupervised\nregime where only a small set of unlabeled test samples along with a labeled\ntraining set is available. Towards this problem, we make three contributions.\nFirst is a novel composite weighted entropy based objective for prediction\naccuracy which is optimized along with a representation matching loss for\nfairness. We experimentally verify that optimizing with our loss formulation\noutperforms a number of state-of-the-art baselines in the pareto sense with\nrespect to the fairness-accuracy tradeoff on several standard datasets. Our\nsecond contribution is a new setting we term Asymmetric Covariate Shift that,\nto the best of our knowledge, has not been studied before. Asymmetric covariate\nshift occurs when distribution of covariates of one group shifts significantly\ncompared to the other groups and this happens when a dominant group is\nover-represented. While this setting is extremely challenging for current\nbaselines, We show that our proposed method significantly outperforms them. Our\nthird contribution is theoretical, where we show that our weighted entropy term\nalong with prediction loss on the training set approximates test loss under\ncovariate shift. Empirically and through formal sample complexity bounds, we\nshow that this approximation to the unseen test loss does not depend on\nimportance sampling variance which affects many other baselines.\n","authors":["Shreyas Havaldar","Jatin Chauhan","Karthikeyan Shanmugam","Jay Nandy","Aravindan Raghuveer"],"pdf_url":"https://arxiv.org/pdf/2310.07535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07534v1","updated":"2023-10-11T14:39:12Z","published":"2023-10-11T14:39:12Z","title":"Human-Centered Evaluation of XAI Methods","summary":"  In the ever-evolving field of Artificial Intelligence, a critical challenge\nhas been to decipher the decision-making processes within the so-called \"black\nboxes\" in deep learning. Over recent years, a plethora of methods have emerged,\ndedicated to explaining decisions across diverse tasks. Particularly in tasks\nlike image classification, these methods typically identify and emphasize the\npivotal pixels that most influence a classifier's prediction. Interestingly,\nthis approach mirrors human behavior: when asked to explain our rationale for\nclassifying an image, we often point to the most salient features or aspects.\nCapitalizing on this parallel, our research embarked on a user-centric study.\nWe sought to objectively measure the interpretability of three leading\nexplanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3)\nLayer-wise Relevance Propagation. Intriguingly, our results highlight that\nwhile the regions spotlighted by these methods can vary widely, they all offer\nhumans a nearly equivalent depth of understanding. This enables users to\ndiscern and categorize images efficiently, reinforcing the value of these\nmethods in enhancing AI transparency.\n","authors":["Karam Dawoud","Wojciech Samek","Sebastian Lapuschkin","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2310.07534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.13149v3","updated":"2023-10-11T14:32:34Z","published":"2023-07-24T22:22:32Z","title":"Discovering interpretable elastoplasticity models via the neural\n  polynomial method enabled symbolic regressions","summary":"  Conventional neural network elastoplasticity models are often perceived as\nlacking interpretability. This paper introduces a two-step machine learning\napproach that returns mathematical models interpretable by human experts. In\nparticular, we introduce a surrogate model where yield surfaces are expressed\nin terms of a set of single-variable feature mappings obtained from supervised\nlearning. A postprocessing step is then used to re-interpret the set of\nsingle-variable neural network mapping functions into mathematical form through\nsymbolic regression. This divide-and-conquer approach provides several\nimportant advantages. First, it enables us to overcome the scaling issue of\nsymbolic regression algorithms. From a practical perspective, it enhances the\nportability of learned models for partial differential equation solvers written\nin different programming languages. Finally, it enables us to have a concrete\nunderstanding of the attributes of the materials, such as convexity and\nsymmetries of models, through automated derivations and reasoning. Numerical\nexamples have been provided, along with an open-source code to enable third\nparty validation.\n","authors":["Bahador Bahmani","Hyoung Suk Suh","WaiChing Sun"],"pdf_url":"https://arxiv.org/pdf/2307.13149v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07528v1","updated":"2023-10-11T14:29:11Z","published":"2023-10-11T14:29:11Z","title":"Provable Advantage of Parameterized Quantum Circuit in Function\n  Approximation","summary":"  Understanding the power of parameterized quantum circuits (PQCs) in\naccomplishing machine learning tasks is one of the most important questions in\nquantum machine learning. In this paper, we analyze the expressivity of PQCs\nthrough the lens of function approximation. Previously established universal\napproximation theorems for PQCs are mainly nonconstructive, leading us to the\nfollowing question: How large do the PQCs need to be to approximate the target\nfunction up to a given error? We exhibit explicit constructions of data\nre-uploading PQCs for approximating continuous and smooth functions and\nestablish quantitative approximation error bounds in terms of the width, the\ndepth and the number of trainable parameters of the PQCs. To achieve this, we\nutilize techniques from quantum signal processing and linear combinations of\nunitaries to construct PQCs that implement multivariate polynomials. We\nimplement global and local approximation techniques using Bernstein polynomials\nand local Taylor expansion and analyze their performances in the quantum\nsetting. We also compare our proposed PQCs to nearly optimal deep neural\nnetworks in approximating high-dimensional smooth functions, showing that the\nratio between model sizes of PQC and deep neural networks is exponentially\nsmall with respect to the input dimension. This suggests a potentially novel\navenue for showcasing quantum advantages in quantum machine learning.\n","authors":["Zhan Yu","Qiuhao Chen","Yuling Jiao","Yinan Li","Xiliang Lu","Xin Wang","Jerry Zhijian Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07528v1.pdf","comment":"31pages, 3 figures"},{"id":"http://arxiv.org/abs/2304.01950v2","updated":"2023-10-11T14:21:29Z","published":"2023-04-01T09:16:40Z","title":"MP-FedCL: Multiprototype Federated Contrastive Learning for Edge\n  Intelligence","summary":"  Federated learning-assisted edge intelligence enables privacy protection in\nmodern intelligent services. However, not independent and identically\ndistributed (non-IID) distribution among edge clients can impair the local\nmodel performance. The existing single prototype-based strategy represents a\nclass by using the mean of the feature space. However, feature spaces are\nusually not clustered, and a single prototype may not represent a class well.\nMotivated by this, this paper proposes a multi-prototype federated contrastive\nlearning approach (MP-FedCL) which demonstrates the effectiveness of using a\nmulti-prototype strategy over a single-prototype under non-IID settings,\nincluding both label and feature skewness. Specifically, a multi-prototype\ncomputation strategy based on \\textit{k-means} is first proposed to capture\ndifferent embedding representations for each class space, using multiple\nprototypes ($k$ centroids) to represent a class in the embedding space. In each\nglobal round, the computed multiple prototypes and their respective model\nparameters are sent to the edge server for aggregation into a global prototype\npool, which is then sent back to all clients to guide their local training.\nFinally, local training for each client minimizes their own supervised learning\ntasks and learns from shared prototypes in the global prototype pool through\nsupervised contrastive learning, which encourages them to learn knowledge\nrelated to their own class from others and reduces the absorption of unrelated\nknowledge in each global iteration. Experimental results on MNIST, Digit-5,\nOffice-10, and DomainNet show that our method outperforms multiple baselines,\nwith an average test accuracy improvement of about 4.6\\% and 10.4\\% under\nfeature and label non-IID distributions, respectively.\n","authors":["Yu Qiao","Md. Shirajum Munir","Apurba Adhikary","Huy Q. Le","Avi Deb Raha","Chaoning Zhang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2304.01950v2.pdf","comment":"Accepted by IEEE Internet of Things"},{"id":"http://arxiv.org/abs/2111.08117v3","updated":"2023-10-11T14:20:40Z","published":"2021-11-15T22:33:52Z","title":"Neural networks with linear threshold activations: structure and\n  algorithms","summary":"  In this article we present new results on neural networks with linear\nthreshold activation functions. We precisely characterize the class of\nfunctions that are representable by such neural networks and show that 2 hidden\nlayers are necessary and sufficient to represent any function representable in\nthe class. This is a surprising result in the light of recent exact\nrepresentability investigations for neural networks using other popular\nactivation functions like rectified linear units (ReLU). We also give precise\nbounds on the sizes of the neural networks required to represent any function\nin the class. Finally, we design an algorithm to solve the empirical risk\nminimization (ERM) problem to global optimality for these neural networks with\na fixed architecture. The algorithm's running time is polynomial in the size of\nthe data sample, if the input dimension and the size of the network\narchitecture are considered fixed constants. The algorithm is unique in the\nsense that it works for any architecture with any number of layers, whereas\nprevious polynomial time globally optimal algorithms work only for very\nrestricted classes of architectures. Using these insights, we propose a new\nclass of neural networks that we call shortcut linear threshold networks. To\nthe best of our knowledge, this way of designing neural networks has not been\nexplored before in the literature. We show that these neural networks have\nseveral desirable theoretical properties.\n","authors":["Sammy Khalife","Hongyu Cheng","Amitabh Basu"],"pdf_url":"https://arxiv.org/pdf/2111.08117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07518v1","updated":"2023-10-11T14:16:04Z","published":"2023-10-11T14:16:04Z","title":"Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement\n  Learning","summary":"  Posterior sampling allows the exploitation of prior knowledge of the\nenvironment's transition dynamics to improve the sample efficiency of\nreinforcement learning. The prior is typically specified as a class of\nparametric distributions, a task that can be cumbersome in practice, often\nresulting in the choice of uninformative priors. In this work, we propose a\nnovel posterior sampling approach in which the prior is given as a (partial)\ncausal graph over the environment's variables. The latter is often more natural\nto design, such as listing known causal dependencies between biometric features\nin a medical treatment study. Specifically, we propose a hierarchical Bayesian\nprocedure, called C-PSRL, simultaneously learning the full causal graph at the\nhigher level and the parameters of the resulting factored dynamics at the lower\nlevel. For this procedure, we provide an analysis of its Bayesian regret, which\nexplicitly connects the regret rate with the degree of prior knowledge. Our\nnumerical evaluation conducted in illustrative domains confirms that C-PSRL\nstrongly improves the efficiency of posterior sampling with an uninformative\nprior while performing close to posterior sampling with the full causal graph.\n","authors":["Mirco Mutti","Riccardo De Santi","Marcello Restelli","Alexander Marx","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2310.07518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07511v1","updated":"2023-10-11T14:07:05Z","published":"2023-10-11T14:07:05Z","title":"A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes\n  via Deviation Relationship Learning","summary":"  Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets. Given the diversity in earth anomaly types, a\nunified anomaly detector across modalities and scenes should be cost-effective\nand flexible to new earth observation sources and anomaly types. However, the\ncurrent anomaly detectors are limited to a single modality and single scene,\nsince they aim to learn the varying background distribution. Motivated by the\nuniversal anomaly deviation pattern, in that anomalies exhibit deviations from\ntheir local context, we exploit this characteristic to build a unified anomaly\ndetector. Firstly, we reformulate the anomaly detection task as an undirected\nbilayer graph based on the deviation relationship, where the anomaly score is\nmodeled as the conditional probability, given the pattern of the background and\nnormal objects. The learning objective is then expressed as a conditional\nprobability ranking problem. Furthermore, we design an instantiation of the\nreformulation in the data, architecture, and optimization aspects. Simulated\nspectral and spatial anomalies drive the instantiated architecture. The model\nis optimized directly for the conditional probability ranking. The proposed\nmodel was validated in five modalities including the hyperspectral, visible\nlight, synthetic aperture radar (SAR), infrared and low light to show its\nunified detection ability.\n","authors":["Jingtao Li","Xinyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.07511v1.pdf","comment":"Journal paper"},{"id":"http://arxiv.org/abs/2310.07506v1","updated":"2023-10-11T14:02:11Z","published":"2023-10-11T14:02:11Z","title":"Leveraging Hierarchical Feature Sharing for Efficient Dataset\n  Condensation","summary":"  Given a real-world dataset, data condensation (DC) aims to synthesize a\nsignificantly smaller dataset that captures the knowledge of this dataset for\nmodel training with high performance. Recent works propose to enhance DC with\ndata parameterization, which condenses data into parameterized data containers\nrather than pixel space. The intuition behind data parameterization is to\nencode shared features of images to avoid additional storage costs. In this\npaper, we recognize that images share common features in a hierarchical way due\nto the inherent hierarchical structure of the classification system, which is\noverlooked by current data parameterization methods. To better align DC with\nthis hierarchical nature and encourage more efficient information sharing\ninside data containers, we propose a novel data parameterization architecture,\nHierarchical Memory Network (HMN). HMN stores condensed data in a three-tier\nstructure, representing the dataset-level, class-level, and instance-level\nfeatures. Another helpful property of the hierarchical architecture is that HMN\nnaturally ensures good independence among images despite achieving information\nsharing. This enables instance-level pruning for HMN to reduce redundant\ninformation, thereby further minimizing redundancy and enhancing performance.\nWe evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and\nTiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results\nshow that our proposed method outperforms all baselines, even when trained with\na batch-based loss consuming less GPU memory.\n","authors":["Haizhong Zheng","Jiachen Sun","Shutong Wu","Bhavya Kailkhura","Zhuoqing Mao","Chaowei Xiao","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2310.07506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07497v1","updated":"2023-10-11T13:50:28Z","published":"2023-10-11T13:50:28Z","title":"Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT\n  Sensing","summary":"  In the domain of Federated Learning (FL) systems, recent cutting-edge methods\nheavily rely on ideal conditions convergence analysis. Specifically, these\napproaches assume that the training datasets on IoT devices possess similar\nattributes to the global data distribution. However, this approach fails to\ncapture the full spectrum of data characteristics in real-time sensing FL\nsystems. In order to overcome this limitation, we suggest a new approach system\nspecifically designed for IoT networks with real-time sensing capabilities. Our\napproach takes into account the generalization gap due to the user's data\nsampling process. By effectively controlling this sampling process, we can\nmitigate the overfitting issue and improve overall accuracy. In particular, We\nfirst formulate an optimization problem that harnesses the sampling process to\nconcurrently reduce overfitting while maximizing accuracy. In pursuit of this\nobjective, our surrogate optimization problem is adept at handling energy\nefficiency while optimizing the accuracy with high generalization. To solve the\noptimization problem with high complexity, we introduce an online reinforcement\nlearning algorithm, named Sample-driven Control for Federated Learning (SCFL)\nbuilt on the Soft Actor-Critic (A2C) framework. This enables the agent to\ndynamically adapt and find the global optima even in changing environments. By\nleveraging the capabilities of SCFL, our system offers a promising solution for\nresource allocation in FL systems with real-time sensing capabilities.\n","authors":["Minh Ngoc Luu","Minh-Duong Nguyen","Ebrahim Bedeer","Van Duc Nguyen","Dinh Thai Hoang","Diep N. Nguyen","Quoc-Viet Pham"],"pdf_url":"https://arxiv.org/pdf/2310.07497v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.07491v1","updated":"2023-10-11T13:39:04Z","published":"2023-10-11T13:39:04Z","title":"Model-based Clustering of Individuals' Ecological Momentary Assessment\n  Time-series Data for Improving Forecasting Performance","summary":"  Through Ecological Momentary Assessment (EMA) studies, a number of\ntime-series data is collected across multiple individuals, continuously\nmonitoring various items of emotional behavior. Such complex data is commonly\nanalyzed in an individual level, using personalized models. However, it is\nbelieved that additional information of similar individuals is likely to\nenhance these models leading to better individuals' description. Thus,\nclustering is investigated with an aim to group together the most similar\nindividuals, and subsequently use this information in group-based models in\norder to improve individuals' predictive performance. More specifically, two\nmodel-based clustering approaches are examined, where the first is using\nmodel-extracted parameters of personalized models, whereas the second is\noptimized on the model-based forecasting performance. Both methods are then\nanalyzed using intrinsic clustering evaluation measures (e.g. Silhouette\ncoefficients) as well as the performance of a downstream forecasting scheme,\nwhere each forecasting group-model is devoted to describe all individuals\nbelonging to one cluster. Among these, clustering based on performance shows\nthe best results, in terms of all examined evaluation measures. As another\nlevel of evaluation, those group-models' performance is compared to three\nbaseline scenarios, the personalized, the all-in-one group and the random\ngroup-based concept. According to this comparison, the superiority of\nclustering-based methods is again confirmed, indicating that the utilization of\ngroup-based information could be effectively enhance the overall performance of\nall individuals' data.\n","authors":["Mandani Ntekouli","Gerasimos Spanakis","Lourens Waldorp","Anne Roefs"],"pdf_url":"https://arxiv.org/pdf/2310.07491v1.pdf","comment":"17 pages, 7 figures, BNAIC/BeNeLearn 2023 (Joint International\n  Scientific Conferences on AI and Machine Learning)"},{"id":"http://arxiv.org/abs/2310.07488v1","updated":"2023-10-11T13:35:05Z","published":"2023-10-11T13:35:05Z","title":"KwaiYiiMath: Technical Report","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable abilities in handling a variety of natural language processing (NLP)\ndownstream tasks, even on mathematical tasks requiring multi-step reasoning. In\nthis report, we introduce the KwaiYiiMath which enhances the mathematical\nreasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)\nand Reinforced Learning from Human Feedback (RLHF), including on both English\nand Chinese mathematical tasks. Meanwhile, we also constructed a small-scale\nChinese primary school mathematics test set (named KMath), consisting of 188\nexamples to evaluate the correctness of the problem-solving process generated\nby the models. Empirical studies demonstrate that KwaiYiiMath can achieve\nstate-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with\nthe similar size models, respectively.\n","authors":["Jiayi Fu","Lei Lin","Xiaoyang Gao","Pengli Liu","Zhengzong Chen","Zhirui Yang","Shengnan Zhang","Xue Zheng","Yan Li","Yuliang Liu","Xucheng Ye","Yiqiao Liao","Chao Liao","Bin Chen","Chengru Song","Junchen Wan","Zijia Lin","Fuzheng Zhang","Zhongyuan Wang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2310.07488v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2310.07485v1","updated":"2023-10-11T13:32:04Z","published":"2023-10-11T13:32:04Z","title":"Nonlinear embeddings for conserving Hamiltonians and other quantities\n  with Neural Galerkin schemes","summary":"  This work focuses on the conservation of quantities such as Hamiltonians,\nmass, and momentum when solution fields of partial differential equations are\napproximated with nonlinear parametrizations such as deep networks. The\nproposed approach builds on Neural Galerkin schemes that are based on the\nDirac--Frenkel variational principle to train nonlinear parametrizations\nsequentially in time. We first show that only adding constraints that aim to\nconserve quantities in continuous time can be insufficient because the\nnonlinear dependence on the parameters implies that even quantities that are\nlinear in the solution fields become nonlinear in the parameters and thus are\nchallenging to discretize in time. Instead, we propose Neural Galerkin schemes\nthat compute at each time step an explicit embedding onto the manifold of\nnonlinearly parametrized solution fields to guarantee conservation of\nquantities. The embeddings can be combined with standard explicit and implicit\ntime integration schemes. Numerical experiments demonstrate that the proposed\napproach conserves quantities up to machine precision.\n","authors":["Paul Schwerdtner","Philipp Schulze","Jules Berman","Benjamin Peherstorfer"],"pdf_url":"https://arxiv.org/pdf/2310.07485v1.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.05869v2","updated":"2023-10-11T13:25:13Z","published":"2023-10-09T17:05:25Z","title":"HyperAttention: Long-context Attention in Near-Linear Time","summary":"  We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.\n","authors":["Insu Han","Rajesh Jayaram","Amin Karbasi","Vahab Mirrokni","David P. Woodruff","Amir Zandieh"],"pdf_url":"https://arxiv.org/pdf/2310.05869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07068v4","updated":"2023-10-11T13:09:46Z","published":"2022-07-14T17:16:45Z","title":"Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey","summary":"  This paper provides a comprehensive survey of bias mitigation methods for\nachieving fairness in Machine Learning (ML) models. We collect a total of 341\npublications concerning bias mitigation for ML classifiers. These methods can\nbe distinguished based on their intervention procedure (i.e., pre-processing,\nin-processing, post-processing) and the technique they apply. We investigate\nhow existing bias mitigation methods are evaluated in the literature. In\nparticular, we consider datasets, metrics and benchmarking. Based on the\ngathered insights (e.g., What is the most popular fairness metric? How many\ndatasets are used for evaluating bias mitigation methods?), we hope to support\npractitioners in making informed choices when developing and evaluating new\nbias mitigation methods.\n","authors":["Max Hort","Zhenpeng Chen","Jie M. Zhang","Mark Harman","Federica Sarro"],"pdf_url":"https://arxiv.org/pdf/2207.07068v4.pdf","comment":"52 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.07464v1","updated":"2023-10-11T13:05:33Z","published":"2023-10-11T13:05:33Z","title":"Deep Learning Predicts Biomarker Status and Discovers Related\n  Histomorphology Characteristics for Low-Grade Glioma","summary":"  Biomarker detection is an indispensable part in the diagnosis and treatment\nof low-grade glioma (LGG). However, current LGG biomarker detection methods\nrely on expensive and complex molecular genetic testing, for which\nprofessionals are required to analyze the results, and intra-rater variability\nis often reported. To overcome these challenges, we propose an interpretable\ndeep learning pipeline, a Multi-Biomarker Histomorphology Discoverer\n(Multi-Beholder) model based on the multiple instance learning (MIL) framework,\nto predict the status of five biomarkers in LGG using only hematoxylin and\neosin-stained whole slide images and slide-level biomarker status labels.\nSpecifically, by incorporating the one-class classification into the MIL\nframework, accurate instance pseudo-labeling is realized for instance-level\nsupervision, which greatly complements the slide-level labels and improves the\nbiomarker prediction performance. Multi-Beholder demonstrates superior\nprediction performance and generalizability for five LGG biomarkers\n(AUROC=0.6469-0.9735) in two cohorts (n=607) with diverse races and scanning\nprotocols. Moreover, the excellent interpretability of Multi-Beholder allows\nfor discovering the quantitative and qualitative correlations between biomarker\nstatus and histomorphology characteristics. Our pipeline not only provides a\nnovel approach for biomarker prediction, enhancing the applicability of\nmolecular treatments for LGG patients but also facilitates the discovery of new\nmechanisms in molecular functionality and LGG progression.\n","authors":["Zijie Fang","Yihan Liu","Yifeng Wang","Xiangyang Zhang","Yang Chen","Changjing Cai","Yiyang Lin","Ying Han","Zhi Wang","Shan Zeng","Hong Shen","Jun Tan","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07464v1.pdf","comment":"47 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07463v1","updated":"2023-10-11T13:05:28Z","published":"2023-10-11T13:05:28Z","title":"Uncovering ECG Changes during Healthy Aging using Explainable AI","summary":"  Cardiovascular diseases remain the leading global cause of mortality. This\nnecessitates a profound understanding of heart aging processes to diagnose\nconstraints in cardiovascular fitness. Traditionally, most of such insights\nhave been drawn from the analysis of electrocardiogram (ECG) feature changes of\nindividuals as they age. However, these features, while informative, may\npotentially obscure underlying data relationships. In this paper, we employ a\ndeep-learning model and a tree-based model to analyze ECG data from a robust\ndataset of healthy individuals across varying ages in both raw signals and ECG\nfeature format. Explainable AI techniques are then used to identify ECG\nfeatures or raw signal characteristics are most discriminative for\ndistinguishing between age groups. Our analysis with tree-based classifiers\nreveal age-related declines in inferred breathing rates and identifies notably\nhigh SDANN values as indicative of elderly individuals, distinguishing them\nfrom younger adults. Furthermore, the deep-learning model underscores the\npivotal role of the P-wave in age predictions across all age groups, suggesting\npotential changes in the distribution of different P-wave types with age. These\nfindings shed new light on age-related ECG changes, offering insights that\ntranscend traditional feature-based approaches.\n","authors":["Gabriel Ott","Yannik Schaubelt","Juan Miguel Lopez Alcaraz","Wilhelm Haverkamp","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2310.07463v1.pdf","comment":"10 pages, 8 figures, code available under\n  https://github.com/AI4HealthUOL/ECG-aging"},{"id":"http://arxiv.org/abs/2310.07461v1","updated":"2023-10-11T13:05:03Z","published":"2023-10-11T13:05:03Z","title":"Efficient machine-learning surrogates for large-scale geological carbon\n  and energy storage","summary":"  Geological carbon and energy storage are pivotal for achieving net-zero\ncarbon emissions and addressing climate change. However, they face\nuncertainties due to geological factors and operational limitations, resulting\nin possibilities of induced seismic events or groundwater contamination. To\novercome these challenges, we propose a specialized machine-learning (ML) model\nto manage extensive reservoir models efficiently.\n  While ML approaches hold promise for geological carbon storage, the\nsubstantial computational resources required for large-scale analysis are the\nobstacle. We've developed a method to reduce the training cost for deep neural\noperator models, using domain decomposition and a topology embedder to link\nspatio-temporal points. This approach allows accurate predictions within the\nmodel's domain, even for untrained data, enhancing ML efficiency for\nlarge-scale geological storage applications.\n","authors":["Teeratorn Kadeethum","Stephen J. Verzi","Hongkyu Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.07461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07446v1","updated":"2023-10-11T12:48:45Z","published":"2023-10-11T12:48:45Z","title":"ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting","summary":"  Time-series forecasting serves as a linchpin in a myriad of applications,\nspanning various domains. With the growth of deep learning, this arena has\nbifurcated into two salient branches: one focuses on crafting specific neural\narchitectures tailored for time series, and the other harnesses advanced deep\ngenerative models for probabilistic forecasting. While both branches have made\nsignificant progress, their differences across data scenarios, methodological\nfocuses, and decoding schemes pose profound, yet unexplored, research\nquestions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering\ntoolkit developed to synergize and compare these two distinct branches. Endowed\nwith a unified data module, a modularized model module, and a comprehensive\nevaluator module, ProbTS allows us to revisit and benchmark leading methods\nfrom both branches. The scrutiny with ProbTS highlights their distinct\ncharacteristics, relative strengths and weaknesses, and areas that need further\nexploration. Our analyses point to new avenues for research, aiming for more\neffective time-series forecasting.\n","authors":["Jiawen Zhang","Xumeng Wen","Shun Zheng","Jia Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.07446v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.07437v1","updated":"2023-10-11T12:40:07Z","published":"2023-10-11T12:40:07Z","title":"A Branched Deep Convolutional Network for Forecasting the Occurrence of\n  Hazes in Paris using Meteorological Maps with Different Characteristic\n  Spatial Scales","summary":"  A deep learning platform has been developed to forecast the occurrence of the\nlow visibility events or hazes. It is trained by using multi-decadal daily\nregional maps of various meteorological and hydrological variables as input\nfeatures and surface visibility observations as the targets. To better preserve\nthe characteristic spatial information of different input features for\ntraining, two branched architectures have recently been developed for the case\nof Paris hazes. These new architectures have improved the performance of the\nnetwork, producing reasonable scores in both validation and a blind forecasting\nevaluation using the data of 2021 and 2022 that have not been used in the\ntraining and validation.\n","authors":["Chien Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07435v1","updated":"2023-10-11T12:36:42Z","published":"2023-10-11T12:36:42Z","title":"Generalized Mixture Model for Extreme Events Forecasting in Time Series\n  Data","summary":"  Time Series Forecasting (TSF) is a widely researched topic with broad\napplications in weather forecasting, traffic control, and stock price\nprediction. Extreme values in time series often significantly impact human and\nnatural systems, but predicting them is challenging due to their rare\noccurrence. Statistical methods based on Extreme Value Theory (EVT) provide a\nsystematic approach to modeling the distribution of extremes, particularly the\nGeneralized Pareto (GP) distribution for modeling the distribution of\nexceedances beyond a threshold. To overcome the subpar performance of deep\nlearning in dealing with heavy-tailed data, we propose a novel framework to\nenhance the focus on extreme events. Specifically, we propose a Deep Extreme\nMixture Model with Autoencoder (DEMMA) for time series prediction. The model\ncomprises two main modules: 1) a generalized mixture distribution based on the\nHurdle model and a reparameterized GP distribution form independent of the\nextreme threshold, 2) an Autoencoder-based LSTM feature extractor and a\nquantile prediction module with a temporal attention mechanism. We demonstrate\nthe effectiveness of our approach on multiple real-world rainfall datasets.\n","authors":["Jincheng Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2310.07435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07434v1","updated":"2023-10-11T12:36:38Z","published":"2023-10-11T12:36:38Z","title":"HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator\n  Walker Assistance","summary":"  Rollator walkers allow people with physical limitations to increase their\nmobility and give them the confidence and independence to participate in\nsociety for longer. However, rollator walker users often have poor posture,\nleading to further health problems and, in the worst case, falls. Integrating\nsensors into rollator walker designs can help to address this problem and\nresults in a platform that allows several other interesting use cases. This\npaper briefly overviews existing systems and the current research directions\nand challenges in this field. We also present our early HealthWalk rollator\nwalker prototype for data collection with older people, rheumatism, multiple\nsclerosis and Parkinson patients, and individuals with visual impairments.\n","authors":["Ivanna Kramer","Kevin Weirauch","Sabine Bauer","Mark Oliver Mints","Peer Neubert"],"pdf_url":"https://arxiv.org/pdf/2310.07434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07433v1","updated":"2023-10-11T12:34:39Z","published":"2023-10-11T12:34:39Z","title":"Imitation Learning from Observation with Automatic Discount Scheduling","summary":"  Humans often acquire new skills through observation and imitation. For\nrobotic agents, learning from the plethora of unlabeled video demonstration\ndata available on the Internet necessitates imitating the expert without access\nto its action, presenting a challenge known as Imitation Learning from\nObservations (ILfO). A common approach to tackle ILfO problems is to convert\nthem into inverse reinforcement learning problems, utilizing a proxy reward\ncomputed from the agent's and the expert's observations. Nonetheless, we\nidentify that tasks characterized by a progress dependency property pose\nsignificant challenges for such approaches; in these tasks, the agent needs to\ninitially learn the expert's preceding behaviors before mastering the\nsubsequent ones. Our investigation reveals that the main cause is that the\nreward signals assigned to later steps hinder the learning of initial\nbehaviors. To address this challenge, we present a novel ILfO framework that\nenables the agent to master earlier behaviors before advancing to later ones.\nWe introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively\nalters the discount factor in reinforcement learning during the training phase,\nprioritizing earlier rewards initially and gradually engaging later rewards\nonly when the earlier behaviors have been mastered. Our experiments, conducted\non nine Meta-World tasks, demonstrate that our method significantly outperforms\nstate-of-the-art methods across all tasks, including those that are unsolvable\nby them.\n","authors":["Yuyang Liu","Weijun Dong","Yingdong Hu","Chuan Wen","Zhao-Heng Yin","Chongjie Zhang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.07433v1.pdf","comment":"Submitted to ICLR 2024"},{"id":"http://arxiv.org/abs/2310.07430v1","updated":"2023-10-11T12:32:13Z","published":"2023-10-11T12:32:13Z","title":"Non-backtracking Graph Neural Networks","summary":"  The celebrated message-passing updates for graph neural networks allow the\nrepresentation of large-scale graphs with local and computationally tractable\nupdates. However, the local updates suffer from backtracking, i.e., a message\nflows through the same edge twice and revisits the previously visited node.\nSince the number of message flows increases exponentially with the number of\nupdates, the redundancy in local updates prevents the graph neural network from\naccurately recognizing a particular message flow for downstream tasks. In this\nwork, we propose to resolve such a redundancy via the non-backtracking graph\nneural network (NBA-GNN) that updates a message without incorporating the\nmessage from the previously visited node. We further investigate how NBA-GNN\nalleviates the over-squashing of GNNs, and establish a connection between\nNBA-GNN and the impressive performance of non-backtracking updates for\nstochastic block model recovery. We empirically verify the effectiveness of our\nNBA-GNN on long-range graph benchmark and transductive node classification\nproblems.\n","authors":["Seonghyun Park","Narae Ryu","Gahee Kim","Dongyeop Woo","Se-Young Yun","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2310.07430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07427v1","updated":"2023-10-11T12:28:52Z","published":"2023-10-11T12:28:52Z","title":"Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n  and CNNs for Stock Return Predictions","summary":"  We propose a time series forecasting method named Quantum Gramian Angular\nField (QGAF). This approach merges the advantages of quantum computing\ntechnology with deep learning, aiming to enhance the precision of time series\nclassification and forecasting. We successfully transformed stock return time\nseries data into two-dimensional images suitable for Convolutional Neural\nNetwork (CNN) training by designing specific quantum circuits. Distinct from\nthe classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in\neliminating the need for data normalization and inverse cosine calculations,\nsimplifying the transformation process from time series data to two-dimensional\nimages. To validate the effectiveness of this method, we conducted experiments\non datasets from three major stock markets: the China A-share market, the Hong\nKong stock market, and the US stock market. Experimental results revealed that\ncompared to the classical GAF method, the QGAF approach significantly improved\ntime series prediction accuracy, reducing prediction errors by an average of\n25\\% for Mean Absolute Error (MAE) and 48\\% for Mean Squared Error (MSE). This\nresearch confirms the potential and promising prospects of integrating quantum\ncomputing with deep learning techniques in financial time series forecasting.\n","authors":["Zhengmeng Xu","Hai Lin"],"pdf_url":"https://arxiv.org/pdf/2310.07427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13078v2","updated":"2023-10-11T12:21:30Z","published":"2023-09-21T02:46:20Z","title":"LPML: LLM-Prompting Markup Language for Mathematical Reasoning","summary":"  In utilizing large language models (LLMs) for mathematical reasoning,\naddressing the errors in the reasoning and calculation present in the generated\ntext by LLMs is a crucial challenge. In this paper, we propose a novel\nframework that integrates the Chain-of-Thought (CoT) method with an external\ntool (Python REPL). We discovered that by prompting LLMs to generate structured\ntext in XML-like markup language, we could seamlessly integrate CoT and the\nexternal tool and control the undesired behaviors of LLMs. With our approach,\nLLMs can utilize Python computation to rectify errors within CoT. We applied\nour method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and\ndemonstrated that combining CoT and Python REPL through the markup language\nenhances the reasoning capability of LLMs. Our approach enables LLMs to write\nthe markup language and perform advanced mathematical reasoning using only\nzero-shot prompting.\n","authors":["Ryutaro Yamauchi","Sho Sonoda","Akiyoshi Sannai","Wataru Kumagai"],"pdf_url":"https://arxiv.org/pdf/2309.13078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09615v4","updated":"2023-10-11T12:09:35Z","published":"2022-05-19T15:13:00Z","title":"EXACT: How to Train Your Accuracy","summary":"  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n","authors":["Ivan Karpukhin","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2205.09615v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07418v1","updated":"2023-10-11T12:05:34Z","published":"2023-10-11T12:05:34Z","title":"Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules\n  and Training Stages","summary":"  Plasticity, the ability of a neural network to evolve with new data, is\ncrucial for high-performance and sample-efficient visual reinforcement learning\n(VRL). Although methods like resetting and regularization can potentially\nmitigate plasticity loss, the influences of various components within the VRL\nframework on the agent's plasticity are still poorly understood. In this work,\nwe conduct a systematic empirical exploration focusing on three primary\nunderexplored facets and derive the following insightful conclusions: (1) data\naugmentation is essential in maintaining plasticity; (2) the critic's\nplasticity loss serves as the principal bottleneck impeding efficient training;\nand (3) without timely intervention to recover critic's plasticity in the early\nstages, its loss becomes catastrophic. These insights suggest a novel strategy\nto address the high replay ratio (RR) dilemma, where exacerbated plasticity\nloss hinders the potential improvements of sample efficiency brought by\nincreased reuse frequency. Rather than setting a static RR for the entire\ntraining process, we propose Adaptive RR, which dynamically adjusts the RR\nbased on the critic's plasticity level. Extensive evaluations indicate that\nAdaptive RR not only avoids catastrophic plasticity loss in the early stages\nbut also benefits from more frequent reuse in later phases, resulting in\nsuperior sample efficiency.\n","authors":["Guozheng Ma","Lu Li","Sen Zhang","Zixuan Liu","Zhen Wang","Yixin Chen","Li Shen","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2310.07418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07417v1","updated":"2023-10-11T12:03:19Z","published":"2023-10-11T12:03:19Z","title":"What can knowledge graph alignment gain with Neuro-Symbolic learning\n  approaches?","summary":"  Knowledge Graphs (KG) are the backbone of many data-intensive applications\nsince they can represent data coupled with its meaning and context. Aligning\nKGs across different domains and providers is necessary to afford a fuller and\nintegrated representation. A severe limitation of current KG alignment (KGA)\nalgorithms is that they fail to articulate logical thinking and reasoning with\nlexical, structural, and semantic data learning. Deep learning models are\nincreasingly popular for KGA inspired by their good performance in other tasks,\nbut they suffer from limitations in explainability, reasoning, and data\nefficiency. Hybrid neurosymbolic learning models hold the promise of\nintegrating logical and data perspectives to produce high-quality alignments\nthat are explainable and support validation through human-centric approaches.\nThis paper examines the current state of the art in KGA and explores the\npotential for neurosymbolic integration, highlighting promising research\ndirections for combining these fields.\n","authors":["Pedro Giesteira Cotovio","Ernesto Jimenez-Ruiz","Catia Pesquita"],"pdf_url":"https://arxiv.org/pdf/2310.07417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07416v1","updated":"2023-10-11T12:01:52Z","published":"2023-10-11T12:01:52Z","title":"A Novel Voronoi-based Convolutional Neural Network Framework for Pushing\n  Person Detection in Crowd Videos","summary":"  Analyzing the microscopic dynamics of pushing behavior within crowds can\noffer valuable insights into crowd patterns and interactions. By identifying\ninstances of pushing in crowd videos, a deeper understanding of when, where,\nand why such behavior occurs can be achieved. This knowledge is crucial to\ncreating more effective crowd management strategies, optimizing crowd flow, and\nenhancing overall crowd experiences. However, manually identifying pushing\nbehavior at the microscopic level is challenging, and the existing automatic\napproaches cannot detect such microscopic behavior. Thus, this article\nintroduces a novel automatic framework for identifying pushing in videos of\ncrowds on a microscopic level. The framework comprises two main components: i)\nFeature extraction and ii) Video labeling. In the feature extraction component,\na new Voronoi-based method is developed for determining the local regions\nassociated with each person in the input video. Subsequently, these regions are\nfed into EfficientNetV1B0 Convolutional Neural Network to extract the deep\nfeatures of each person over time. In the second component, a combination of a\nfully connected layer with a Sigmoid activation function is employed to analyze\nthese deep features and annotate the individuals involved in pushing within the\nvideo. The framework is trained and evaluated on a new dataset created using\nsix real-world experiments, including their corresponding ground truths. The\nexperimental findings indicate that the suggested framework outperforms seven\nbaseline methods that are employed for comparative analysis purposes.\n","authors":["Ahmed Alia","Mohammed Maree","Mohcine Chraibi","Armin Seyfried"],"pdf_url":"https://arxiv.org/pdf/2310.07416v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2310.07402v1","updated":"2023-10-11T11:38:18Z","published":"2023-10-11T11:38:18Z","title":"NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series\n  Pretraining","summary":"  Recent research on time-series self-supervised models shows great promise in\nlearning semantic representations. However, it has been limited to small-scale\ndatasets, e.g., thousands of temporal sequences. In this work, we make key\ntechnical contributions that are tailored to the numerical properties of\ntime-series data and allow the model to scale to large datasets, e.g., millions\nof temporal sequences. We adopt the Transformer architecture by first\npartitioning the input into non-overlapping windows. Each window is then\ncharacterized by its normalized shape and two scalar values denoting the mean\nand standard deviation within each window. To embed scalar values that may\npossess arbitrary numerical scales to high-dimensional vectors, we propose a\nnumerically multi-scaled embedding module enumerating all possible scales for\nthe scalar values. The model undergoes pretraining using the proposed\nnumerically multi-scaled embedding with a simple contrastive objective on a\nlarge-scale dataset containing over a million sequences. We study its transfer\nperformance on a number of univariate and multivariate classification\nbenchmarks. Our method exhibits remarkable improvement against previous\nrepresentation learning approaches and establishes the new state of the art,\neven compared with domain-specific non-learning-based methods.\n","authors":["Chenguo Lin","Xumeng Wen","Wei Cao","Congrui Huang","Jiang Bian","Stephen Lin","Zhirong Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05682v2","updated":"2023-10-11T11:28:40Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07392v1","updated":"2023-10-11T11:20:35Z","published":"2023-10-11T11:20:35Z","title":"Deep Kernel and Image Quality Estimators for Optimizing Robotic\n  Ultrasound Controller using Bayesian Optimization","summary":"  Ultrasound is a commonly used medical imaging modality that requires expert\nsonographers to manually maneuver the ultrasound probe based on the acquired\nimage. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to\nthis manual procedure in order to reduce sonographers' workload. The key\nchallenge to A-RUS is optimizing the ultrasound image quality for the region of\ninterest across different patients. This requires knowledge of anatomy,\nrecognition of error sources and precise probe position, orientation and\npressure. Sample efficiency is important while optimizing these parameters\nassociated with the robotized probe controller. Bayesian Optimization (BO), a\nsample-efficient optimization framework, has recently been applied to optimize\nthe 2D motion of the probe. Nevertheless, further improvements are needed to\nimprove the sample efficiency for high-dimensional control of the probe. We aim\nto overcome this problem by using a neural network to learn a low-dimensional\nkernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained\nusing probe and image data acquired during the procedure. The two image quality\nestimators are proposed that use a deep convolution neural network and provide\nreal-time feedback to the BO. We validated our framework using these two\nfeedback functions on three urinary bladder phantoms. We obtained over 50%\nincrease in sample efficiency for 6D control of the robotized probe.\nFurthermore, our results indicate that this performance enhancement in BO is\nindependent of the specific training dataset, demonstrating inter-patient\nadaptability.\n","authors":["Deepak Raina","SH Chandrashekhara","Richard Voyles","Juan Wachs","Subir Kumar Saha"],"pdf_url":"https://arxiv.org/pdf/2310.07392v1.pdf","comment":"Accepted in IEEE International Symposium on Medical Robotics (ISMR)\n  2023"},{"id":"http://arxiv.org/abs/2310.07380v1","updated":"2023-10-11T10:55:14Z","published":"2023-10-11T10:55:14Z","title":"Histopathological Image Classification and Vulnerability Analysis using\n  Federated Learning","summary":"  Healthcare is one of the foremost applications of machine learning (ML).\nTraditionally, ML models are trained by central servers, which aggregate data\nfrom various distributed devices to forecast the results for newly generated\ndata. This is a major concern as models can access sensitive user information,\nwhich raises privacy concerns. A federated learning (FL) approach can help\naddress this issue: A global model sends its copy to all clients who train\nthese copies, and the clients send the updates (weights) back to it. Over time,\nthe global model improves and becomes more accurate. Data privacy is protected\nduring training, as it is conducted locally on the clients' devices.\n  However, the global model is susceptible to data poisoning. We develop a\nprivacy-preserving FL technique for a skin cancer dataset and show that the\nmodel is prone to data poisoning attacks. Ten clients train the model, but one\nof them intentionally introduces flipped labels as an attack. This reduces the\naccuracy of the global model. As the percentage of label flipping increases,\nthere is a noticeable decrease in accuracy. We use a stochastic gradient\ndescent optimization algorithm to find the most optimal accuracy for the model.\nAlthough FL can protect user privacy for healthcare diagnostics, it is also\nvulnerable to data poisoning, which must be addressed.\n","authors":["Sankalp Vyas","Amar Nath Patra","Raj Mani Shukla"],"pdf_url":"https://arxiv.org/pdf/2310.07380v1.pdf","comment":"Accepted in IEEE International Conference on Trust, Security and\n  Privacy in Computing and Communications (TrustCom)"},{"id":"http://arxiv.org/abs/2310.07379v1","updated":"2023-10-11T10:54:44Z","published":"2023-10-11T10:54:44Z","title":"Causal Unsupervised Semantic Segmentation","summary":"  Unsupervised semantic segmentation aims to achieve high-quality semantic\ngrouping without human-labeled annotations. With the advent of self-supervised\npre-training, various frameworks utilize the pre-trained features to train\nprediction heads for unsupervised dense prediction. However, a significant\nchallenge in this unsupervised setup is determining the appropriate level of\nclustering required for segmenting concepts. To address it, we propose a novel\nframework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages\ninsights from causal inference. Specifically, we bridge intervention-oriented\napproach (i.e., frontdoor adjustment) to define suitable two-step tasks for\nunsupervised prediction. The first step involves constructing a concept\nclusterbook as a mediator, which represents possible concept prototypes at\ndifferent levels of granularity in a discretized form. Then, the mediator\nestablishes an explicit link to the subsequent concept-wise self-supervised\nlearning for pixel-level grouping. Through extensive experiments and analyses\non various datasets, we corroborate the effectiveness of CAUSE and achieve\nstate-of-the-art performance in unsupervised semantic segmentation.\n","authors":["Junho Kim","Byung-Kwan Lee","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2310.07379v1.pdf","comment":"code available:\n  https://github.com/ByungKwanLee/Causal-Unsupervised-Segmentation"},{"id":"http://arxiv.org/abs/2305.08732v3","updated":"2023-10-11T10:51:12Z","published":"2023-05-15T15:47:09Z","title":"Knowledge Rumination for Pre-trained Language Models","summary":"  Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n","authors":["Yunzhi Yao","Peng Wang","Shengyu Mao","Chuanqi Tan","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.08732v3.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07371v1","updated":"2023-10-11T10:41:51Z","published":"2023-10-11T10:41:51Z","title":"Experimental quantum natural gradient optimization in photonics","summary":"  Variational quantum algorithms (VQAs) combining the advantages of\nparameterized quantum circuits and classical optimizers, promise practical\nquantum applications in the Noisy Intermediate-Scale Quantum era. The\nperformance of VQAs heavily depends on the optimization method. Compared with\ngradient-free and ordinary gradient descent methods, the quantum natural\ngradient (QNG), which mirrors the geometric structure of the parameter space,\ncan achieve faster convergence and avoid local minima more easily, thereby\nreducing the cost of circuit executions. We utilized a fully programmable\nphotonic chip to experimentally estimate the QNG in photonics for the first\ntime. We obtained the dissociation curve of the He-H$^+$ cation and achieved\nchemical accuracy, verifying the outperformance of QNG optimization on a\nphotonic device. Our work opens up a vista of utilizing QNG in photonics to\nimplement practical near-term quantum applications.\n","authors":["Yizhi Wang","Shichuan Xue","Yaxuan Wang","Jiangfang Ding","Weixu Shi","Dongyang Wang","Yong Liu","Yingwen Liu","Xiang Fu","Guangyao Huang","Anqi Huang","Mingtang Deng","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07370v1","updated":"2023-10-11T10:40:43Z","published":"2023-10-11T10:40:43Z","title":"Orthogonal Random Features: Explicit Forms and Sharp Inequalities","summary":"  Random features have been introduced to scale up kernel methods via\nrandomization techniques. In particular, random Fourier features and orthogonal\nrandom features were used to approximate the popular Gaussian kernel. The\nformer is performed by a random Gaussian matrix and leads exactly to the\nGaussian kernel after averaging. In this work, we analyze the bias and the\nvariance of the kernel approximation based on orthogonal random features which\nmakes use of Haar orthogonal matrices. We provide explicit expressions for\nthese quantities using normalized Bessel functions and derive sharp exponential\nbounds supporting the view that orthogonal random features are more informative\nthan random Fourier features.\n","authors":["Nizar Demni","Hachem Kadri"],"pdf_url":"https://arxiv.org/pdf/2310.07370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07367v1","updated":"2023-10-11T10:34:52Z","published":"2023-10-11T10:34:52Z","title":"Improved Analysis of Sparse Linear Regression in Local Differential\n  Privacy Model","summary":"  In this paper, we revisit the problem of sparse linear regression in the\nlocal differential privacy (LDP) model. Existing research in the\nnon-interactive and sequentially local models has focused on obtaining the\nlower bounds for the case where the underlying parameter is $1$-sparse, and\nextending such bounds to the more general $k$-sparse case has proven to be\nchallenging. Moreover, it is unclear whether efficient non-interactive LDP\n(NLDP) algorithms exist. To address these issues, we first consider the problem\nin the $\\epsilon$ non-interactive LDP model and provide a lower bound of\n$\\Omega(\\frac{\\sqrt{dk\\log d}}{\\sqrt{n}\\epsilon})$ on the $\\ell_2$-norm\nestimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is\nthe dimension of the space. We propose an innovative NLDP algorithm, the very\nfirst of its kind for the problem. As a remarkable outcome, this algorithm also\nyields a novel and highly efficient estimator as a valuable by-product. Our\nalgorithm achieves an upper bound of\n$\\tilde{O}({\\frac{d\\sqrt{k}}{\\sqrt{n}\\epsilon}})$ for the estimation error when\nthe data is sub-Gaussian, which can be further improved by a factor of\n$O(\\sqrt{d})$ if the server has additional public but unlabeled data. For the\nsequentially interactive LDP model, we show a similar lower bound of\n$\\Omega({\\frac{\\sqrt{dk}}{\\sqrt{n}\\epsilon}})$. As for the upper bound, we\nrectify a previous method and show that it is possible to achieve a bound of\n$\\tilde{O}(\\frac{k\\sqrt{d}}{\\sqrt{n}\\epsilon})$. Our findings reveal\nfundamental differences between the non-private case, central DP model, and\nlocal DP model in the sparse linear regression problem.\n","authors":["Liyang Zhu","Meng Ding","Vaneet Aggarwal","Jinhui Xu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07365v1","updated":"2023-10-11T10:30:49Z","published":"2023-10-11T10:30:49Z","title":"GraphControl: Adding Conditional Control to Universal Graph Pre-trained\n  Models for Graph Domain Transfer Learning","summary":"  Graph-structured data is ubiquitous in the world which models complex\nrelationships between objects, enabling various Web applications. Daily\ninfluxes of unlabeled graph data on the Web offer immense potential for these\napplications. Graph self-supervised algorithms have achieved significant\nsuccess in acquiring generic knowledge from abundant unlabeled graph data.\nThese pre-trained models can be applied to various downstream Web applications,\nsaving training time and improving downstream (target) performance. However,\ndifferent graphs, even across seemingly similar domains, can differ\nsignificantly in terms of attribute semantics, posing difficulties, if not\ninfeasibility, for transferring the pre-trained models to downstream tasks.\nConcretely speaking, for example, the additional task-specific node information\nin downstream tasks (specificity) is usually deliberately omitted so that the\npre-trained representation (transferability) can be leveraged. The trade-off as\nsuch is termed as \"transferability-specificity dilemma\" in this work. To\naddress this challenge, we introduce an innovative deployment module coined as\nGraphControl, motivated by ControlNet, to realize better graph domain transfer\nlearning. Specifically, by leveraging universal structural pre-trained models\nand GraphControl, we align the input space across various graphs and\nincorporate unique characteristics of target data as conditional inputs. These\nconditions will be progressively integrated into the model during fine-tuning\nor prompt tuning through ControlNet, facilitating personalized deployment.\nExtensive experiments show that our method significantly enhances the\nadaptability of pre-trained models on target attributed datasets, achieving\n1.4-3x performance gain. Furthermore, it outperforms training-from-scratch\nmethods on target data with a comparable margin and exhibits faster\nconvergence.\n","authors":["Yun Zhu","Yaoke Wang","Haizhou Shi","Zhenshuo Zhang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07365v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2307.02484v4","updated":"2023-10-11T10:27:40Z","published":"2023-07-05T17:58:21Z","title":"Elastic Decision Transformer","summary":"  This paper introduces Elastic Decision Transformer (EDT), a significant\nadvancement over the existing Decision Transformer (DT) and its variants.\nAlthough DT purports to generate an optimal trajectory, empirical evidence\nsuggests it struggles with trajectory stitching, a process involving the\ngeneration of an optimal or near-optimal trajectory from the best parts of a\nset of sub-optimal trajectories. The proposed EDT differentiates itself by\nfacilitating trajectory stitching during action inference at test time,\nachieved by adjusting the history length maintained in DT. Further, the EDT\noptimizes the trajectory by retaining a longer history when the previous\ntrajectory is optimal and a shorter one when it is sub-optimal, enabling it to\n\"stitch\" with a more optimal trajectory. Extensive experimentation demonstrates\nEDT's ability to bridge the performance gap between DT-based and Q\nLearning-based approaches. In particular, the EDT outperforms Q Learning-based\nmethods in a multi-task regime on the D4RL locomotion benchmark and Atari\ngames. Videos are available at: https://kristery.github.io/edt/\n","authors":["Yueh-Hua Wu","Xiaolong Wang","Masashi Hamaya"],"pdf_url":"https://arxiv.org/pdf/2307.02484v4.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2302.11509v2","updated":"2023-10-11T10:20:58Z","published":"2023-02-22T17:26:03Z","title":"Construction of Knowledge Graphs: State and Challenges","summary":"  With knowledge graphs (KGs) at the center of numerous applications such as\nrecommender systems and question answering, the need for generalized pipelines\nto construct and continuously update such KGs is increasing. While the\nindividual steps that are necessary to create KGs from unstructured (e.g. text)\nand structured data sources (e.g. databases) are mostly well-researched for\ntheir one-shot execution, their adoption for incremental KG updates and the\ninterplay of the individual steps have hardly been investigated in a systematic\nmanner so far. In this work, we first discuss the main graph models for KGs and\nintroduce the major requirement for future KG construction pipelines. Next, we\nprovide an overview of the necessary steps to build high-quality KGs, including\ncross-cutting topics such as metadata management, ontology development, and\nquality assurance. We then evaluate the state of the art of KG construction\nw.r.t the introduced requirements for specific popular KGs as well as some\nrecent tools and strategies for KG construction. Finally, we identify areas in\nneed of further research and improvement.\n","authors":["Marvin Hofer","Daniel Obraczka","Alieh Saeedi","Hanna Kpcke","Erhard Rahm"],"pdf_url":"https://arxiv.org/pdf/2302.11509v2.pdf","comment":"51 pages, 5 figures, 4 tables, 328 references"},{"id":"http://arxiv.org/abs/2310.07359v1","updated":"2023-10-11T10:17:41Z","published":"2023-10-11T10:17:41Z","title":"Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance\n  Images Using a Hybrid GAN-CNN Method","summary":"  Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive\ncycles of hypomania and depression. Since diagnosing BD relies on subjective\nbehavioral assessments over a long period, a solid diagnosis based on objective\ncriteria is not straightforward. The current study responded to the described\nobstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural\nMRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI\nsamples rather than conventional datasets such as functional MRI (fMRI),\nelectroencephalography (EEG), and behavioral symptoms while removing the data\ninsufficiency usually encountered when dealing with sMRI samples. The impact of\nvarious augmentation ratios is also tested using 5-fold cross-validation. Based\non the results, this study obtains an accuracy rate of 75.8%, a sensitivity of\n60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while\nutilizing less than 6% sample counts. Next, it is demonstrated that a 2- D\nlayer-based GAN generator can effectively reproduce complex 3D brain samples, a\nmore straightforward technique than manual image processing. Lastly, the\noptimum augmentation threshold for the current study using 172 sMRI samples is\n50%, showing the applicability of the described method for larger sMRI\ndatasets. In conclusion, it is established that data augmentation using GAN\nimproves the accuracy of the CNN classifier using sMRI samples, thus developing\nmore reliable decision support systems to assist practitioners in identifying\nBD patients more reliably and in a shorter period\n","authors":["Masood Hamed Saghayan","Mohammad Hossein Zolfagharnasab","Ali Khadem","Farzam Matinfar","Hassan Rashidi"],"pdf_url":"https://arxiv.org/pdf/2310.07359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07355v1","updated":"2023-10-11T10:12:43Z","published":"2023-10-11T10:12:43Z","title":"IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training","summary":"  In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.\n","authors":["Che Liu","Sibo Cheng","Miaojing Shi","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2310.07355v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.07351v1","updated":"2023-10-11T10:03:10Z","published":"2023-10-11T10:03:10Z","title":"Atom-Motif Contrastive Transformer for Molecular Property Prediction","summary":"  Recently, Graph Transformer (GT) models have been widely used in the task of\nMolecular Property Prediction (MPP) due to their high reliability in\ncharacterizing the latent relationship among graph nodes (i.e., the atoms in a\nmolecule). However, most existing GT-based methods usually explore the basic\ninteractions between pairwise atoms, and thus they fail to consider the\nimportant interactions among critical motifs (e.g., functional groups consisted\nof several atoms) of molecules. As motifs in a molecule are significant\npatterns that are of great importance for determining molecular properties\n(e.g., toxicity and solubility), overlooking motif interactions inevitably\nhinders the effectiveness of MPP. To address this issue, we propose a novel\nAtom-Motif Contrastive Transformer (AMCT), which not only explores the\natom-level interactions but also considers the motif-level interactions. Since\nthe representations of atoms and motifs for a given molecule are actually two\ndifferent views of the same instance, they are naturally aligned to generate\nthe self-supervisory signals for model training. Meanwhile, the same motif can\nexist in different molecules, and hence we also employ the contrastive loss to\nmaximize the representation agreement of identical motifs across different\nmolecules. Finally, in order to clearly identify the motifs that are critical\nin deciding the properties of each molecule, we further construct a\nproperty-aware attention mechanism into our learning framework. Our proposed\nAMCT is extensively evaluated on seven popular benchmark datasets, and both\nquantitative and qualitative results firmly demonstrate its effectiveness when\ncompared with the state-of-the-art methods.\n","authors":["Wentao Yu","Shuo Chen","Chen Gong","Gang Niu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2310.07351v1.pdf","comment":"submit to AAAI-24"},{"id":"http://arxiv.org/abs/2310.07347v1","updated":"2023-10-11T09:55:46Z","published":"2023-10-11T09:55:46Z","title":"Fast-ELECTRA for Efficient Pre-training","summary":"  ELECTRA pre-trains language models by detecting tokens in a sequence that\nhave been replaced by an auxiliary model. Although ELECTRA offers a significant\nboost in efficiency, its potential is constrained by the training cost brought\nby the auxiliary model. Notably, this model, which is jointly trained with the\nmain model, only serves to assist the training of the main model and is\ndiscarded post-training. This results in a substantial amount of training cost\nbeing expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which\nleverages an existing language model as the auxiliary model. To construct a\nlearning curriculum for the main model, we smooth its output distribution via\ntemperature scaling following a descending schedule. Our approach rivals the\nperformance of state-of-the-art ELECTRA-style pre-training methods, while\nsignificantly eliminating the computation and memory cost brought by the joint\ntraining of the auxiliary model. Our method also reduces the sensitivity to\nhyper-parameters and enhances the pre-training stability.\n","authors":["Chengyu Dong","Liyuan Liu","Hao Cheng","Jingbo Shang","Jianfeng Gao","Xiaodong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06841v2","updated":"2023-10-11T09:53:51Z","published":"2023-04-13T22:20:54Z","title":"Video alignment using unsupervised learning of local and global features","summary":"  In this paper, we tackle the problem of video alignment, the process of\nmatching the frames of a pair of videos containing similar actions. The main\nchallenge in video alignment is that accurate correspondence should be\nestablished despite the differences in the execution processes and appearances\nbetween the two videos. We introduce an unsupervised method for alignment that\nuses global and local features of the frames. In particular, we introduce\neffective features for each video frame using three machine vision tools:\nperson detection, pose estimation, and VGG network. Then, the features are\nprocessed and combined to construct a multidimensional time series that\nrepresents the video. The resulting time series are used to align videos of the\nsame actions using a novel version of dynamic time warping named Diagonalized\nDynamic Time Warping(DDTW). The main advantage of our approach is that no\ntraining is required, which makes it applicable for any new type of action\nwithout any need to collect training samples for it. For evaluation, we\nconsidered video synchronization and phase classification tasks on the Penn\naction dataset. Also, for an effective evaluation of the video synchronization\ntask, we present a new metric called Enclosed Area Error(EAE). The results show\nthat our method outperforms previous state-of-the-art methods, such as TCC, and\nother self-supervised and weakly supervised methods.\n","authors":["Niloufar Fakhfour","Mohammad ShahverdiKondori","Hoda Mohammadzade"],"pdf_url":"https://arxiv.org/pdf/2304.06841v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07338v1","updated":"2023-10-11T09:37:38Z","published":"2023-10-11T09:37:38Z","title":"Towards Foundation Models for Learning on Tabular Data","summary":"  Learning on tabular data underpins numerous real-world applications. Despite\nconsiderable efforts in developing effective learning models for tabular data,\ncurrent transferable tabular models remain in their infancy, limited by either\nthe lack of support for direct instruction following in new tasks or the\nneglect of acquiring foundational knowledge and capabilities from diverse\ntabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs)\nto overcome these limitations. TabFMs harness the potential of generative\ntabular learning, employing a pre-trained large language model (LLM) as the\nbase model and fine-tuning it using purpose-designed objectives on an extensive\nrange of tabular datasets. This approach endows TabFMs with a profound\nunderstanding and universal capabilities essential for learning on tabular\ndata. Our evaluations underscore TabFM's effectiveness: not only does it\nsignificantly excel in instruction-following tasks like zero-shot and\nin-context inference, but it also showcases performance that approaches, and in\ninstances, even transcends, the renowned yet mysterious closed-source LLMs like\nGPT-4. Furthermore, when fine-tuning with scarce data, our model achieves\nremarkable efficiency and maintains competitive performance with abundant\ntraining data. Finally, while our results are promising, we also delve into\nTabFM's limitations and potential opportunities, aiming to stimulate and\nexpedite future research on developing more potent TabFMs.\n","authors":["Han Zhang","Xumeng Wen","Shun Zheng","Wei Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.07338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09809v3","updated":"2023-10-11T09:34:21Z","published":"2022-10-18T12:28:37Z","title":"Analysis of Convolutions, Non-linearity and Depth in Graph Neural\n  Networks using Neural Tangent Kernel","summary":"  The fundamental principle of Graph Neural Networks (GNNs) is to exploit the\nstructural information of the data by aggregating the neighboring nodes using a\n`graph convolution' in conjunction with a suitable choice for the network\narchitecture, such as depth and activation functions. Therefore, understanding\nthe influence of each of the design choice on the network performance is\ncrucial. Convolutions based on graph Laplacian have emerged as the dominant\nchoice with the symmetric normalization of the adjacency matrix as the most\nwidely adopted one. However, some empirical studies show that row normalization\nof the adjacency matrix outperforms it in node classification. Despite the\nwidespread use of GNNs, there is no rigorous theoretical study on the\nrepresentation power of these convolutions, that could explain this behavior.\nSimilarly, the empirical observation of the linear GNNs performance being on\npar with non-linear ReLU GNNs lacks rigorous theory.\n  In this work, we theoretically analyze the influence of different aspects of\nthe GNN architecture using the Graph Neural Tangent Kernel in a semi-supervised\nnode classification setting. Under the population Degree Corrected Stochastic\nBlock Model, we prove that: (i) linear networks capture the class information\nas good as ReLU networks; (ii) row normalization preserves the underlying class\nstructure better than other convolutions; (iii) performance degrades with\nnetwork depth due to over-smoothing, but the loss in class information is the\nslowest in row normalization; (iv) skip connections retain the class\ninformation even at infinite depth, thereby eliminating over-smoothing. We\nfinally validate our theoretical findings numerically and on real datasets such\nas Cora and Citeseer.\n","authors":["Mahalakshmi Sabanayagam","Pascal Esser","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2210.09809v3.pdf","comment":"41 pages, 24 figures. Code available at\n  https://github.com/mahalakshmi-sabanayagam/NTK_GCN"},{"id":"http://arxiv.org/abs/2310.07335v1","updated":"2023-10-11T09:25:24Z","published":"2023-10-11T09:25:24Z","title":"Exploring Social Motion Latent Space and Human Awareness for Effective\n  Robot Navigation in Crowded Environments","summary":"  This work proposes a novel approach to social robot navigation by learning to\ngenerate robot controls from a social motion latent space. By leveraging this\nsocial motion latent space, the proposed method achieves significant\nimprovements in social navigation metrics such as success rate, navigation\ntime, and trajectory length while producing smoother (less jerk and angular\ndeviations) and more anticipatory trajectories. The superiority of the proposed\nmethod is demonstrated through comparison with baseline models in various\nscenarios. Additionally, the concept of humans' awareness towards the robot is\nintroduced into the social robot navigation framework, showing that\nincorporating human awareness leads to shorter and smoother trajectories owing\nto humans' ability to positively interact with the robot.\n","authors":["Junaid Ahmed Ansari","Satyajit Tourani","Gourav Kumar","Brojeshwar Bhowmick"],"pdf_url":"https://arxiv.org/pdf/2310.07335v1.pdf","comment":"Accepted at IROS 2023"},{"id":"http://arxiv.org/abs/2310.07325v1","updated":"2023-10-11T09:14:40Z","published":"2023-10-11T09:14:40Z","title":"An Adversarial Example for Direct Logit Attribution: Memory Management\n  in gelu-4l","summary":"  We provide concrete evidence for memory management in a 4-layer transformer.\nSpecifically, we identify clean-up behavior, in which model components\nconsistently remove the output of preceeding components during a forward pass.\nOur findings suggest that the interpretability technique Direct Logit\nAttribution provides misleading results. We show explicit examples where this\ntechnique is inaccurate, as it does not account for clean-up behavior.\n","authors":["James Dao","Yeu-Tong Lao","Can Rager","Jett Janiak"],"pdf_url":"https://arxiv.org/pdf/2310.07325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07323v1","updated":"2023-10-11T09:14:17Z","published":"2023-10-11T09:14:17Z","title":"Multichannel consecutive data cross-extraction with 1DCNN-attention for\n  diagnosis of power transformer","summary":"  Power transformer plays a critical role in grid infrastructure, and its\ndiagnosis is paramount for maintaining stable operation. However, the current\nmethods for transformer diagnosis focus on discrete dissolved gas analysis,\nneglecting deep feature extraction of multichannel consecutive data. The\nunutilized sequential data contains the significant temporal information\nreflecting the transformer condition. In light of this, the structure of\nmultichannel consecutive data cross-extraction (MCDC) is proposed in this\narticle in order to comprehensively exploit the intrinsic characteristic and\nevaluate the states of transformer. Moreover, for the better accommodation in\nscenario of transformer diagnosis, one dimensional convolution neural network\nattention (1DCNN-attention) mechanism is introduced and offers a more efficient\nsolution given the simplified spatial complexity. Finally, the effectiveness of\nMCDC and the superior generalization ability, compared with other algorithms,\nare validated in experiments conducted on a dataset collected from real\noperation cases of power transformer. Additionally, the better stability of\n1DCNN-attention has also been certified.\n","authors":["Wei Zheng","Guogang Zhang","Chenchen Zhao","Qianqian Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07321v1","updated":"2023-10-11T09:09:55Z","published":"2023-10-11T09:09:55Z","title":"On the Impact of Cross-Domain Data on German Language Models","summary":"  Traditionally, large language models have been either trained on general web\ncrawls or domain-specific data. However, recent successes of generative large\nlanguage models, have shed light on the benefits of cross-domain datasets. To\nexamine the significance of prioritizing data diversity over quality, we\npresent a German dataset comprising texts from five domains, along with another\ndataset aimed at containing high-quality data. Through training a series of\nmodels ranging between 122M and 750M parameters on both datasets, we conduct a\ncomprehensive benchmark on multiple downstream tasks. Our findings demonstrate\nthat the models trained on the cross-domain dataset outperform those trained on\nquality data alone, leading to improvements up to $4.45\\%$ over the previous\nstate-of-the-art. The models are available at\nhttps://huggingface.co/ikim-uk-essen\n","authors":["Amin Dada","Aokun Chen","Cheng Peng","Kaleb E Smith","Ahmad Idrissi-Yaghir","Constantin Marc Seibold","Jianning Li","Lars Heiliger","Christoph M. Friedrich","Daniel Truhn","Jan Egger","Jiang Bian","Jens Kleesiek","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07321v1.pdf","comment":"13 pages, 1 figure, accepted at Findings of the Association for\n  Computational Linguistics: EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07320v1","updated":"2023-10-11T09:09:50Z","published":"2023-10-11T09:09:50Z","title":"Byzantine-Resilient Decentralized Multi-Armed Bandits","summary":"  In decentralized cooperative multi-armed bandits (MAB), each agent observes a\ndistinct stream of rewards, and seeks to exchange information with others to\nselect a sequence of arms so as to minimize its regret. Agents in the\ncooperative setting can outperform a single agent running a MAB method such as\nUpper-Confidence Bound (UCB) independently. In this work, we study how to\nrecover such salient behavior when an unknown fraction of the agents can be\nByzantine, that is, communicate arbitrarily wrong information in the form of\nreward mean-estimates or confidence sets. This framework can be used to model\nattackers in computer networks, instigators of offensive content into\nrecommender systems, or manipulators of financial markets. Our key contribution\nis the development of a fully decentralized resilient upper confidence bound\n(UCB) algorithm that fuses an information mixing step among agents with a\ntruncation of inconsistent and extreme values. This truncation step enables us\nto establish that the performance of each normal agent is no worse than the\nclassic single-agent UCB1 algorithm in terms of regret, and more importantly,\nthe cumulative regret of all normal agents is strictly better than the\nnon-cooperative case, provided that each agent has at least 3f+1 neighbors\nwhere f is the maximum possible Byzantine agents in each agent's neighborhood.\nExtensions to time-varying neighbor graphs, and minimax lower bounds are\nfurther established on the achievable regret. Experiments corroborate the\nmerits of this framework in practice.\n","authors":["Jingxuan Zhu","Alec Koppel","Alvaro Velasquez","Ji Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07941v4","updated":"2023-10-11T09:07:24Z","published":"2022-03-15T14:25:44Z","title":"Reachability In Simple Neural Networks","summary":"  We investigate the complexity of the reachability problem for (deep) neural\nnetworks: does it compute valid output given some valid input? It was recently\nclaimed that the problem is NP-complete for general neural networks and\nspecifications over the input/output dimension given by conjunctions of linear\ninequalities. We recapitulate the proof and repair some flaws in the original\nupper and lower bound proofs. Motivated by the general result, we show that\nNP-hardness already holds for restricted classes of simple specifications and\nneural networks. Allowing for a single hidden layer and an output dimension of\none as well as neural networks with just one negative, zero and one positive\nweight or bias is sufficient to ensure NP-hardness. Additionally, we give a\nthorough discussion and outlook of possible extensions for this direction of\nresearch on neural network verification.\n","authors":["Marco Slzer","Martin Lange"],"pdf_url":"https://arxiv.org/pdf/2203.07941v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2108.13179"},{"id":"http://arxiv.org/abs/2104.05859v5","updated":"2023-10-11T09:07:01Z","published":"2021-04-12T23:14:41Z","title":"Rapid Exploration for Open-World Navigation with Latent Goal Models","summary":"  We describe a robotic learning system for autonomous exploration and\nnavigation in diverse, open-world environments. At the core of our method is a\nlearned latent variable model of distances and actions, along with a\nnon-parametric topological memory of images. We use an information bottleneck\nto regularize the learned policy, giving us (i) a compact visual representation\nof goals, (ii) improved generalization capabilities, and (iii) a mechanism for\nsampling feasible goals for exploration. Trained on a large offline dataset of\nprior experience, the model acquires a representation of visual goals that is\nrobust to task-irrelevant distractors. We demonstrate our method on a mobile\nground robot in open-world exploration scenarios. Given an image of a goal that\nis up to 80 meters away, our method leverages its representation to explore and\ndiscover the goal in under 20 minutes, even amidst previously-unseen obstacles\nand weather conditions. Please check out the project website for videos of our\nexperiments and information about the real-world dataset used at\nhttps://sites.google.com/view/recon-robot.\n","authors":["Dhruv Shah","Benjamin Eysenbach","Gregory Kahn","Nicholas Rhinehart","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2104.05859v5.pdf","comment":"Presented at 5th Annual Conference on Robot Learning (CoRL 2021),\n  London, UK as an Oral Talk. Project page and dataset release at\n  https://sites.google.com/view/recon-robot"},{"id":"http://arxiv.org/abs/2310.05052v2","updated":"2023-10-11T09:04:01Z","published":"2023-10-08T07:25:27Z","title":"Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan\n  Prediction across Diverse Conditions","summary":"  Battery life prediction holds significant practical value for battery\nresearch and development. Currently, many data-driven models rely on early\nelectrical signals from specific target batteries to predict their lifespan. A\ncommon shortfall is that most existing methods are developed based on specific\naging conditions, which not only limits their model's capability but also\ndiminishes their effectiveness in predicting degradation under varied\nconditions. As a result, these models often miss out on fully benefiting from\nthe rich historical data available under other conditions. Here, to address\nabove, we introduce an approach that explicitly captures differences between\nelectrical signals of a target battery and a reference battery, irrespective of\ntheir materials and aging conditions, to forecast the target battery life.\nThrough this inter-cell difference, we not only enhance the feature space but\nalso pave the way for a universal battery life prediction framework.\nRemarkably, our model that combines the inter- and intra-cell differences\nshines across diverse conditions, standing out in its efficiency and accuracy\nusing all accessible datasets. An essential application of our approach is its\ncapability to leverage data from older batteries effectively, enabling newer\nbatteries to capitalize on insights gained from past batteries. This work not\nonly enriches the battery data utilization strategy but also sets the stage for\nsmarter battery management system in the future.\n","authors":["Han Zhang","Yuqi Li","Shun Zheng","Ziheng Lu","Xiaofan Gui","Wei Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.05052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07313v1","updated":"2023-10-11T09:00:02Z","published":"2023-10-11T09:00:02Z","title":"Molecule-Edit Templates for Efficient and Accurate Retrosynthesis\n  Prediction","summary":"  Retrosynthesis involves determining a sequence of reactions to synthesize\ncomplex molecules from simpler precursors. As this poses a challenge in organic\nchemistry, machine learning has offered solutions, particularly for predicting\npossible reaction substrates for a given target molecule. These solutions\nmainly fall into template-based and template-free categories. The former is\nefficient but relies on a vast set of predefined reaction patterns, while the\nlatter, though more flexible, can be computationally intensive and less\ninterpretable. To address these issues, we introduce METRO (Molecule-Edit\nTemplates for RetrOsynthesis), a machine-learning model that predicts reactions\nusing minimal templates - simplified reaction patterns capturing only essential\nmolecular changes - reducing computational overhead and achieving\nstate-of-the-art results on standard benchmarks.\n","authors":["Mikoaj Sacha","Micha Sadowski","Piotr Kozakowski","Ruard van Workum","Stanisaw Jastrzbski"],"pdf_url":"https://arxiv.org/pdf/2310.07313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07312v1","updated":"2023-10-11T08:57:59Z","published":"2023-10-11T08:57:59Z","title":"WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models","summary":"  Innovative foundation models, such as GPT-3 and stable diffusion models, have\nmade a paradigm shift in the realm of artificial intelligence (AI) towards\ngenerative AI-based systems. In unison, from data communication and networking\nperspective, AI and machine learning (AI/ML) algorithms are envisioned to be\npervasively incorporated into the future generations of wireless communications\nsystems, highlighting the need for novel AI-native solutions for the emergent\ncommunication scenarios. In this article, we outline the applications of\ngenerative AI in wireless communication systems to lay the foundations for\nresearch in this field. Diffusion-based generative models, as the new\nstate-of-the-art paradigm of generative models, are introduced, and their\napplications in wireless communication systems are discussed. Two case studies\nare also presented to showcase how diffusion models can be exploited for the\ndevelopment of resilient AI-native communication systems. Specifically, we\npropose denoising diffusion probabilistic models (DDPM) for a wireless\ncommunication scheme with non-ideal transceivers, where 30% improvement is\nachieved in terms of bit error rate. As the second application, DDPMs are\nemployed at the transmitter to shape the constellation symbols, highlighting a\nrobust out-of-distribution performance. Finally, future directions and open\nissues for the development of generative AI-based wireless systems are\ndiscussed to promote future research endeavors towards wireless generative AI\n(WiGenAI).\n","authors":["Mehdi Letafati","Samad Ali","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2310.07312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01276v2","updated":"2023-10-11T08:57:34Z","published":"2023-06-02T05:34:01Z","title":"Enhancing Sample Efficiency in Black-box Combinatorial Optimization via\n  Symmetric Replay Training","summary":"  Black-box combinatorial optimization (black-box CO) is frequently encountered\nin various industrial fields, such as drug discovery or hardware design.\nDespite its widespread relevance, solving black-box CO problems is highly\nchallenging due to the vast combinatorial solution space and resource-intensive\nnature of black-box function evaluations. These inherent complexities induce\nsignificant constraints on the efficacy of existing deep reinforcement learning\n(DRL) methods when applied to practical problem settings. For efficient\nexploration with the limited availability of function evaluations, this paper\nintroduces a new generic method to enhance sample efficiency. We propose\nsymmetric replay training that leverages the high-reward samples and their\nunder-explored regions in the symmetric space. In replay training, the policy\nis trained to imitate the symmetric trajectories of these high-rewarded\nsamples. The proposed method is beneficial for the exploration of highly\nrewarded regions without the necessity for additional online interactions -\nfree. The experimental results show that our method consistently improves the\nsample efficiency of various DRL methods on real-world tasks, including\nmolecular optimization and hardware design.\n","authors":["Hyeonah Kim","Minsu Kim","Sungsoo Ahn","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2306.01276v2.pdf","comment":"18 pages (including 6 pages of the appendix)"},{"id":"http://arxiv.org/abs/2310.07306v1","updated":"2023-10-11T08:40:06Z","published":"2023-10-11T08:40:06Z","title":"SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification\n  Model","summary":"  This paper presents a Soft Labeling and Noisy Mixup-based open intent\nclassification model (SNOiC). Most of the previous works have used\nthreshold-based methods to identify open intents, which are prone to\noverfitting and may produce biased predictions. Additionally, the need for more\navailable data for an open intent class presents another limitation for these\nexisting models. SNOiC combines Soft Labeling and Noisy Mixup strategies to\nreduce the biasing and generate pseudo-data for open intent class. The\nexperimental results on four benchmark datasets show that the SNOiC model\nachieves a minimum and maximum performance of 68.72\\% and 94.71\\%,\nrespectively, in identifying open intents. Moreover, compared to\nstate-of-the-art models, the SNOiC model improves the performance of\nidentifying open intents by 0.93\\% (minimum) and 12.76\\% (maximum). The model's\nefficacy is further established by analyzing various parameters used in the\nproposed model. An ablation study is also conducted, which involves creating\nthree model variants to validate the effectiveness of the SNOiC model.\n","authors":["Aditi Kanwar","Aditi Seetha","Satyendra Singh Chouhan","Rajdeep Niyogi"],"pdf_url":"https://arxiv.org/pdf/2310.07306v1.pdf","comment":"9 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.00113v2","updated":"2023-10-11T08:38:28Z","published":"2023-09-29T20:01:11Z","title":"HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning","summary":"  Artificial neural networks suffer from catastrophic forgetting when they are\nsequentially trained on multiple tasks. To overcome this problem, there exist\nmany continual learning strategies. One of the most effective is the\nhypernetwork-based approach. The hypernetwork generates the weights of a target\nmodel based on the task's identity. The model's main limitation is that\nhypernetwork can produce completely different nests for each task.\nConsequently, each task is solved separately. The model does not use\ninformation from the network dedicated to previous tasks and practically\nproduces new architectures when it learns the subsequent tasks. To solve such a\nproblem, we use the lottery ticket hypothesis, which postulates the existence\nof sparse subnetworks, named winning tickets, that preserve the performance of\na full network. In the paper, we propose a method called HyperMask, which\ntrains a single network for all tasks. Hypernetwork produces semi-binary masks\nto obtain target subnetworks dedicated to new tasks. This solution inherits the\nability of the hypernetwork to adapt to new tasks with minimal forgetting.\nMoreover, due to the lottery ticket hypothesis, we can use a single network\nwith weighted subnets dedicated to each task.\n","authors":["Kamil Ksiek","Przemysaw Spurek"],"pdf_url":"https://arxiv.org/pdf/2310.00113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05672v2","updated":"2023-10-11T08:37:40Z","published":"2023-10-09T12:42:39Z","title":"Multi-timestep models for Model-based Reinforcement Learning","summary":"  In model-based reinforcement learning (MBRL), most algorithms rely on\nsimulating trajectories from one-step dynamics models learned on data. A\ncritical challenge of this approach is the compounding of one-step prediction\nerrors as length of the trajectory grows. In this paper we tackle this issue by\nusing a multi-timestep objective to train one-step models. Our objective is a\nweighted sum of a loss function (e.g., negative log-likelihood) at various\nfuture horizons. We explore and test a range of weights profiles. We find that\nexponentially decaying weights lead to models that significantly improve the\nlong-horizon R2 score. This improvement is particularly noticeable when the\nmodels were evaluated on noisy data. Finally, using a soft actor-critic (SAC)\nagent in pure batch reinforcement learning (RL) and iterated batch RL\nscenarios, we found that our multi-timestep models outperform or match standard\none-step models. This was especially evident in a noisy variant of the\nconsidered environment, highlighting the potential of our approach in\nreal-world applications.\n","authors":["Abdelhakim Benechehab","Giuseppe Paolo","Albert Thomas","Maurizio Filippone","Balzs Kgl"],"pdf_url":"https://arxiv.org/pdf/2310.05672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07298v1","updated":"2023-10-11T08:32:46Z","published":"2023-10-11T08:32:46Z","title":"Beyond Memorization: Violating Privacy Via Inference with Large Language\n  Models","summary":"  Current privacy research on large language models (LLMs) primarily focuses on\nthe issue of extracting memorized training data. At the same time, models'\ninference capabilities have increased drastically. This raises the key question\nof whether current LLMs could violate individuals' privacy by inferring\npersonal attributes from text given at inference time. In this work, we present\nthe first comprehensive study on the capabilities of pretrained LLMs to infer\npersonal attributes from text. We construct a dataset consisting of real Reddit\nprofiles, and show that current LLMs can infer a wide range of personal\nattributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and\n$95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time\n($240\\times$) required by humans. As people increasingly interact with\nLLM-powered chatbots across all aspects of life, we also explore the emerging\nthreat of privacy-invasive chatbots trying to extract personal information\nthrough seemingly benign questions. Finally, we show that common mitigations,\ni.e., text anonymization and model alignment, are currently ineffective at\nprotecting user privacy against LLM inference. Our findings highlight that\ncurrent LLMs can infer personal data at a previously unattainable scale. In the\nabsence of working defenses, we advocate for a broader discussion around LLM\nprivacy implications beyond memorization, striving for a wider privacy\nprotection.\n","authors":["Robin Staab","Mark Vero","Mislav Balunovi","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2310.07298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07297v1","updated":"2023-10-11T08:31:26Z","published":"2023-10-11T08:31:26Z","title":"Score Regularized Policy Optimization through Diffusion Behavior","summary":"  Recent developments in offline reinforcement learning have uncovered the\nimmense potential of diffusion modeling, which excels at representing\nheterogeneous behavior policies. However, sampling from diffusion policies is\nconsiderably slow because it necessitates tens to hundreds of iterative\ninference steps for one action. To address this issue, we propose to extract an\nefficient deterministic inference policy from critic models and pretrained\ndiffusion behavior models, leveraging the latter to directly regularize the\npolicy gradient with the behavior distribution's score function during\noptimization. Our method enjoys powerful generative capabilities of diffusion\nmodeling while completely circumventing the computationally intensive and\ntime-consuming diffusion sampling scheme, both during training and evaluation.\nExtensive results on D4RL tasks show that our method boosts action sampling\nspeed by more than 25 times compared with various leading diffusion-based\nmethods in locomotion tasks, while still maintaining state-of-the-art\nperformance.\n","authors":["Huayu Chen","Cheng Lu","Zhengyi Wang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07297v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.07276v1","updated":"2023-10-11T07:57:08Z","published":"2023-10-11T07:57:08Z","title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical\n  Knowledge and Natural Language Associations","summary":"  Recent advancements in biological research leverage the integration of\nmolecules, proteins, and natural language to enhance drug discovery. However,\ncurrent models exhibit several limitations, such as the generation of invalid\nmolecular SMILES, underutilization of contextual information, and equal\ntreatment of structured and unstructured knowledge. To address these issues, we\npropose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches\ncross-modal integration in biology with chemical knowledge and natural language\nassociations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular\nrepresentations and extracts knowledge from the surrounding context of\nbio-entities in unstructured biological literature. Furthermore,\n$\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,\nleading to more effective utilization of information. After fine-tuning, BioT5\nshows superior performance across a wide range of tasks, demonstrating its\nstrong capability of capturing underlying relations and properties of\nbio-entities. Our code is available at\n$\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n","authors":["Qizhi Pei","Wei Zhang","Jinhua Zhu","Kehan Wu","Kaiyuan Gao","Lijun Wu","Yingce Xia","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2310.07276v1.pdf","comment":"Empirical Methods in Natural Language Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2310.07269v1","updated":"2023-10-11T07:51:10Z","published":"2023-10-11T07:51:10Z","title":"Why Does Sharpness-Aware Minimization Generalize Better Than SGD?","summary":"  The challenge of overfitting, in which the model memorizes the training data\nand fails to generalize to test data, has become increasingly significant in\nthe training of large neural networks. To tackle this challenge,\nSharpness-Aware Minimization (SAM) has emerged as a promising training method,\nwhich can improve the generalization of neural networks even in the presence of\nlabel noise. However, a deep understanding of how SAM works, especially in the\nsetting of nonlinear neural networks and classification tasks, remains largely\nmissing. This paper fills this gap by demonstrating why SAM generalizes better\nthan Stochastic Gradient Descent (SGD) for a certain data model and two-layer\nconvolutional ReLU networks. The loss landscape of our studied problem is\nnonsmooth, thus current explanations for the success of SAM based on the\nHessian information are insufficient. Our result explains the benefits of SAM,\nparticularly its ability to prevent noise learning in the early stages, thereby\nfacilitating more effective learning of features. Experiments on both synthetic\nand real data corroborate our theory.\n","authors":["Zixiang Chen","Junkai Zhang","Yiwen Kou","Xiangning Chen","Cho-Jui Hsieh","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2310.07269v1.pdf","comment":"52 pages, 4 figures, 2 tables. In NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.07268v1","updated":"2023-10-11T07:50:51Z","published":"2023-10-11T07:50:51Z","title":"RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd\n  Intelligence","summary":"  Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated\nby state-of-the-art vehicular ad-hoc networks and artificial intelligence,\nvarious VCI applications come to place, e.g., collaborative sensing,\npositioning, and mapping. The collaborative property of VCI applications\ngenerally requires data to be shared among participants, thus forming\nnetwork-wide intelligence. How to fulfill this process without compromising\ndata privacy remains a challenging issue. Although federated learning (FL) is a\npromising tool to solve the problem, adapting conventional FL frameworks to VCI\nis nontrivial. First, the centralized model aggregation is unreliable in VCI\nbecause of the existence of stragglers with unfavorable channel conditions.\nSecond, existing FL schemes are vulnerable to Non-IID data, which is\nintensified by the data heterogeneity in VCI. This paper proposes a novel\nfederated learning framework called RaftFed to facilitate privacy-preserving\nVCI. The experimental results show that RaftFed performs better than baselines\nregarding communication overhead, model accuracy, and model convergence.\n","authors":["Changan Yang","Yaxing Chen","Yao Zhang","Helei Cui","Zhiwen Yu","Bin Guo","Zheng Yan","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07268v1.pdf","comment":"8 pages,8 figures"},{"id":"http://arxiv.org/abs/2304.03864v2","updated":"2023-10-11T07:50:28Z","published":"2023-04-07T23:25:48Z","title":"SGDP: A Stream-Graph Neural Network Based Data Prefetcher","summary":"  Data prefetching is important for storage system optimization and access\nperformance improvement. Traditional prefetchers work well for mining access\npatterns of sequential logical block address (LBA) but cannot handle complex\nnon-sequential patterns that commonly exist in real-world applications. The\nstate-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses.\nHowever, they do not adequately consider the spatial interdependencies between\nLBA deltas, which leads to limited performance and robustness. This paper\nproposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP).\nSpecifically, SGDP models LBA delta streams using a weighted directed graph\nstructure to represent interactive relations among LBA deltas and further\nextracts hybrid features by graph neural networks for data prefetching. We\nconduct extensive experiments on eight real-world datasets. Empirical results\nverify that SGDP outperforms the SOTA methods in terms of the hit ratio by\n6.21%, the effective prefetching ratio by 7.00%, and speeds up inference time\nby 3.13X on average. Besides, we generalize SGDP to different variants by\ndifferent stream constructions, further expanding its application scenarios and\ndemonstrating its robustness. SGDP offers a novel data prefetching solution and\nhas been verified in commercial hybrid storage systems in the experimental\nphase. Our codes and appendix are available at\nhttps://github.com/yyysjz1997/SGDP/.\n","authors":["Yiyuan Yang","Rongshang Li","Qiquan Shi","Xijun Li","Gang Hu","Xing Li","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2304.03864v2.pdf","comment":"Accepted by International Joint Conference on Neural Networks (IJCNN\n  2023)"},{"id":"http://arxiv.org/abs/2306.10347v2","updated":"2023-10-11T07:50:09Z","published":"2023-06-17T13:40:15Z","title":"DCdetector: Dual Attention Contrastive Representation Learning for Time\n  Series Anomaly Detection","summary":"  Time series anomaly detection is critical for a wide range of applications.\nIt aims to identify deviant samples from the normal sample distribution in time\nseries. The most fundamental challenge for this task is to learn a\nrepresentation map that enables effective discrimination of anomalies.\nReconstruction-based methods still dominate, but the representation learning\nwith anomalies might hurt the performance with its large abnormal loss. On the\nother hand, contrastive learning aims to find a representation that can clearly\ndistinguish any instance from the others, which can bring a more natural and\npromising representation for time series anomaly detection. In this paper, we\npropose DCdetector, a multi-scale dual attention contrastive representation\nlearning model. DCdetector utilizes a novel dual attention asymmetric design to\ncreate the permutated environment and pure contrastive loss to guide the\nlearning process, thus learning a permutation invariant representation with\nsuperior discrimination abilities. Extensive experiments show that DCdetector\nachieves state-of-the-art results on multiple time series anomaly detection\nbenchmark datasets. Code is publicly available at\nhttps://github.com/DAMO-DI-ML/KDD2023-DCdetector.\n","authors":["Yiyuan Yang","Chaoli Zhang","Tian Zhou","Qingsong Wen","Liang Sun"],"pdf_url":"https://arxiv.org/pdf/2306.10347v2.pdf","comment":"Accepted by ACM SIGKDD International Conference on Knowledge\n  Discovery & Data Mining (KDD 2023)"},{"id":"http://arxiv.org/abs/2305.00472v2","updated":"2023-10-11T07:47:07Z","published":"2023-04-30T13:10:56Z","title":"Efficient MILP Decomposition in Quantum Computing for ReLU Network\n  Robustness","summary":"  Emerging quantum computing technologies, such as Noisy Intermediate-Scale\nQuantum (NISQ) devices, offer potential advancements in solving mathematical\noptimization problems. However, limitations in qubit availability, noise, and\nerrors pose challenges for practical implementation. In this study, we examine\ntwo decomposition methods for Mixed-Integer Linear Programming (MILP) designed\nto reduce the original problem size and utilize available NISQ devices more\nefficiently. We concentrate on breaking down the original problem into smaller\nsubproblems, which are then solved iteratively using a combined\nquantum-classical hardware approach. We conduct a detailed analysis for the\ndecomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis,\nwe show that the number of qubits required to solve Benders is exponentially\nlarge in the worst-case, while remains constant for Dantzig-Wolfe.\nAdditionally, we leverage Dantzig-Wolfe decomposition on the use-case of\ncertifying the robustness of ReLU networks. Our experimental results\ndemonstrate that this approach can save up to 90\\% of qubits compared to\nexisting methods on quantum annealing and gate-based quantum computers.\n","authors":["Nicola Franco","Tom Wollschlger","Benedikt Poggel","Stephan Gnnemann","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2305.00472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07264v1","updated":"2023-10-11T07:40:46Z","published":"2023-10-11T07:40:46Z","title":"Classification of Dysarthria based on the Levels of Severity. A\n  Systematic Review","summary":"  Dysarthria is a neurological speech disorder that can significantly impact\naffected individuals' communication abilities and overall quality of life. The\naccurate and objective classification of dysarthria and the determination of\nits severity are crucial for effective therapeutic intervention. While\ntraditional assessments by speech-language pathologists (SLPs) are common, they\nare often subjective, time-consuming, and can vary between practitioners.\nEmerging machine learning-based models have shown the potential to provide a\nmore objective dysarthria assessment, enhancing diagnostic accuracy and\nreliability. This systematic review aims to comprehensively analyze current\nmethodologies for classifying dysarthria based on severity levels.\nSpecifically, this review will focus on determining the most effective set and\ntype of features that can be used for automatic patient classification and\nevaluating the best AI techniques for this purpose. We will systematically\nreview the literature on the automatic classification of dysarthria severity\nlevels. Sources of information will include electronic databases and grey\nliterature. Selection criteria will be established based on relevance to the\nresearch questions. Data extraction will include methodologies used, the type\nof features extracted for classification, and AI techniques employed. The\nfindings of this systematic review will contribute to the current understanding\nof dysarthria classification, inform future research, and support the\ndevelopment of improved diagnostic tools. The implications of these findings\ncould be significant in advancing patient care and improving therapeutic\noutcomes for individuals affected by dysarthria.\n","authors":["Afnan Al-Ali","Somaya Al-Maadeed","Moutaz Saleh","Rani Chinnappa Naidu","Zachariah C Alex","Prakash Ramachandran","Rajeev Khoodeeram","Rajesh Kumar M"],"pdf_url":"https://arxiv.org/pdf/2310.07264v1.pdf","comment":"no comments"},{"id":"http://arxiv.org/abs/2310.07261v1","updated":"2023-10-11T07:38:37Z","published":"2023-10-11T07:38:37Z","title":"Deep ReLU networks and high-order finite element methods II: Chebyshev\n  emulation","summary":"  Expression rates and stability in Sobolev norms of deep ReLU neural networks\n(NNs) in terms of the number of parameters defining the NN for continuous,\npiecewise polynomial functions, on arbitrary, finite partitions $\\mathcal{T}$\nof a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN\nsurrogates encoding the approximated functions in terms of Chebyshev polynomial\nexpansion coefficients are developed. Chebyshev coefficients can be computed\neasily from the values of the function in the Clenshaw--Curtis points using the\ninverse fast Fourier transform. Bounds on expression rates and stability that\nare superior to those of constructions based on ReLU NN emulations of monomials\nconsidered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation\nbounds are explicit in terms of the (arbitrary) partition of the interval, the\ntarget emulation accuracy and the polynomial degree in each element of the\npartition. ReLU NN emulation error estimates are provided for various classes\nof functions and norms, commonly encountered in numerical analysis. In\nparticular, we show exponential ReLU emulation rate bounds for analytic\nfunctions with point singularities and develop an interface between Chebfun\napproximations and constructive ReLU NN emulations.\n","authors":["Joost A. A. Opschoor","Christoph Schwab"],"pdf_url":"https://arxiv.org/pdf/2310.07261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.08786v6","updated":"2023-10-11T07:37:01Z","published":"2022-10-17T07:01:17Z","title":"Exposing Influence Campaigns in the Age of LLMs: A Behavioral-Based AI\n  Approach to Detecting State-Sponsored Trolls","summary":"  The detection of state-sponsored trolls operating in influence campaigns on\nsocial media is a critical and unsolved challenge for the research community,\nwhich has significant implications beyond the online realm. To address this\nchallenge, we propose a new AI-based solution that identifies troll accounts\nsolely through behavioral cues associated with their sequences of sharing\nactivity, encompassing both their actions and the feedback they receive from\nothers. Our approach does not incorporate any textual content shared and\nconsists of two steps: First, we leverage an LSTM-based classifier to determine\nwhether account sequences belong to a state-sponsored troll or an organic,\nlegitimate user. Second, we employ the classified sequences to calculate a\nmetric named the \"Troll Score\", quantifying the degree to which an account\nexhibits troll-like behavior. To assess the effectiveness of our method, we\nexamine its performance in the context of the 2016 Russian interference\ncampaign during the U.S. Presidential election. Our experiments yield\ncompelling results, demonstrating that our approach can identify account\nsequences with an AUC close to 99% and accurately differentiate between Russian\ntrolls and organic users with an AUC of 91%. Notably, our behavioral-based\napproach holds a significant advantage in the ever-evolving landscape, where\ntextual and linguistic properties can be easily mimicked by Large Language\nModels (LLMs): In contrast to existing language-based techniques, it relies on\nmore challenging-to-replicate behavioral cues, ensuring greater resilience in\nidentifying influence campaigns, especially given the potential increase in the\nusage of LLMs for generating inauthentic content. Finally, we assessed the\ngeneralizability of our solution to various entities driving different\ninformation operations and found promising results that will guide future\nresearch.\n","authors":["Fatima Ezzeddine","Luca Luceri","Omran Ayoub","Ihab Sbeity","Gianluca Nogara","Emilio Ferrara","Silvia Giordano"],"pdf_url":"https://arxiv.org/pdf/2210.08786v6.pdf","comment":"22"},{"id":"http://arxiv.org/abs/2306.01984v2","updated":"2023-10-11T07:35:27Z","published":"2023-06-03T02:46:31Z","title":"DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal\n  Forecasting","summary":"  While diffusion models can successfully generate data and make predictions,\nthey are predominantly designed for static images. We propose an approach for\nefficiently training diffusion models for probabilistic spatiotemporal\nforecasting, where generating stable and accurate rollout forecasts remains\nchallenging, Our method, DYffusion, leverages the temporal dynamics in the\ndata, directly coupling it with the diffusion steps in the model. We train a\nstochastic, time-conditioned interpolator and a forecaster network that mimic\nthe forward and reverse processes of standard diffusion models, respectively.\nDYffusion naturally facilitates multi-step and long-range forecasting, allowing\nfor highly flexible, continuous-time sampling trajectories and the ability to\ntrade-off performance with accelerated sampling at inference time. In addition,\nthe dynamics-informed diffusion process in DYffusion imposes a strong inductive\nbias and significantly improves computational efficiency compared to\ntraditional Gaussian noise-based diffusion models. Our approach performs\ncompetitively on probabilistic forecasting of complex dynamics in sea surface\ntemperatures, Navier-Stokes flows, and spring mesh systems.\n","authors":["Salva Rhling Cachay","Bo Zhao","Hailey Joren","Rose Yu"],"pdf_url":"https://arxiv.org/pdf/2306.01984v2.pdf","comment":"Accepted to NeurIPS 2023; Code is available at:\n  https://github.com/Rose-STL-Lab/dyffusion"},{"id":"http://arxiv.org/abs/2310.07253v1","updated":"2023-10-11T07:30:18Z","published":"2023-10-11T07:30:18Z","title":"ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction","summary":"  Obtaining accurate and valid information for drug molecules is a crucial and\nchallenging task. However, chemical knowledge and information have been\naccumulated over the past 100 years from various regions, laboratories, and\nexperimental purposes. Little has been explored in terms of the\nout-of-distribution (OOD) problem with noise and inconsistency, which may lead\nto weak robustness and unsatisfied performance. This study proposes a novel\nbenchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically\ndesigned for drug property prediction. ADMEOOD obtained 27 ADME (Absorption,\nDistribution, Metabolism, Excretion) drug properties from Chembl and relevant\nliterature. Additionally, it includes two kinds of OOD data shifts: Noise Shift\nand Concept Conflict Drift (CCD). Noise Shift responds to the noise level by\ncategorizing the environment into different confidence levels. On the other\nhand, CCD describes the data which has inconsistent label among the original\ndata. Finally, it tested on a variety of domain generalization models, and the\nexperimental results demonstrate the effectiveness of the proposed partition\nmethod in ADMEOOD: ADMEOOD demonstrates a significant difference performance\nbetween in-distribution and out-of-distribution data. Moreover, ERM (Empirical\nRisk Minimization) and other models exhibit distinct trends in performance\nacross different domains and measurement types.\n","authors":["Shuoying Wei","Xinlong Wen","Lida Zhu","Songquan Li","Rongbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07252v1","updated":"2023-10-11T07:30:01Z","published":"2023-10-11T07:30:01Z","title":"A Comparative Study of Pre-trained CNNs and GRU-Based Attention for\n  Image Caption Generation","summary":"  Image captioning is a challenging task involving generating a textual\ndescription for an image using computer vision and natural language processing\ntechniques. This paper proposes a deep neural framework for image caption\ngeneration using a GRU-based attention mechanism. Our approach employs multiple\npre-trained convolutional neural networks as the encoder to extract features\nfrom the image and a GRU-based language model as the decoder to generate\ndescriptive sentences. To improve performance, we integrate the Bahdanau\nattention model with the GRU decoder to enable learning to focus on specific\nimage parts. We evaluate our approach using the MSCOCO and Flickr30k datasets\nand show that it achieves competitive scores compared to state-of-the-art\nmethods. Our proposed framework can bridge the gap between computer vision and\nnatural language and can be extended to specific domains.\n","authors":["Rashid Khan","Bingding Huang","Haseeb Hassan","Asim Zaman","Zhongfu Ye"],"pdf_url":"https://arxiv.org/pdf/2310.07252v1.pdf","comment":"15pages, 10 figures, 5 tables. 2023 the 5th International Conference\n  on Robotics and Computer Vision (ICRCV 2023). arXiv admin note: substantial\n  text overlap with arXiv:2203.01594"},{"id":"http://arxiv.org/abs/2310.07250v1","updated":"2023-10-11T07:27:28Z","published":"2023-10-11T07:27:28Z","title":"Synthesizing Missing MRI Sequences from Available Modalities using\n  Generative Adversarial Networks in BraTS Dataset","summary":"  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic\nresonance imaging (MRI) plays a significant role in the diagnosis, treatment\nplanning, and follow-up of glioblastoma patients due to its non-invasive and\nradiation-free nature. The International Brain Tumor Segmentation (BraTS)\nchallenge has contributed to generating numerous AI algorithms to accurately\nand efficiently segment glioblastoma sub-compartments using four structural\n(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not\nalways be available. To address this issue, Generative Adversarial Networks\n(GANs) can be used to synthesize the missing MRI sequences. In this paper, we\nimplement and utilize an open-source GAN approach that takes any three MRI\nsequences as input to generate the missing fourth structural sequence. Our\nproposed approach is contributed to the community-driven generally nuanced deep\nlearning framework (GaNDLF) and demonstrates promising results in synthesizing\nhigh-quality and realistic MRI sequences, enabling clinicians to improve their\ndiagnostic capabilities and support the application of AI methods to brain\ntumor MRI quantification.\n","authors":["Ibrahim Ethem Hamamci"],"pdf_url":"https://arxiv.org/pdf/2310.07250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07245v1","updated":"2023-10-11T07:22:37Z","published":"2023-10-11T07:22:37Z","title":"Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs","summary":"  Visual crowd counting estimates the density of the crowd using deep learning\nmodels such as convolution neural networks (CNNs). The performance of the model\nheavily relies on the quality of the training data that constitutes crowd\nimages. In harsh weather such as fog, dust, and low light conditions, the\ninference performance may severely degrade on the noisy and blur images. In\nthis paper, we propose the use of Pix2Pix generative adversarial network (GAN)\nto first denoise the crowd images prior to passing them to the counting model.\nA Pix2Pix network is trained using synthetic noisy images generated from\noriginal crowd images and then the pretrained generator is then used in the\ninference engine to estimate the crowd density in unseen, noisy crowd images.\nThe performance is tested on JHU-Crowd dataset to validate the significance of\nthe proposed method particularly when high reliability and accuracy are\nrequired.\n","authors":["Muhammad Asif Khan","Hamid Menouar","Ridha Hamila"],"pdf_url":"https://arxiv.org/pdf/2310.07245v1.pdf","comment":"The paper has been accepted for presentation in IEEE 38th\n  International Conference on Image and Vision Computing New Zealand (IVCNZ\n  2023). The final manuscript can be accessed at ieeexplore"},{"id":"http://arxiv.org/abs/2310.07241v1","updated":"2023-10-11T07:13:16Z","published":"2023-10-11T07:13:16Z","title":"Surrogate modeling for stochastic crack growth processes in structural\n  health monitoring applications","summary":"  Fatigue crack growth is one of the most common types of deterioration in\nmetal structures with significant implications on their reliability. Recent\nadvances in Structural Health Monitoring (SHM) have motivated the use of\nstructural response data to predict future crack growth under uncertainty, in\norder to enable a transition towards predictive maintenance. Accurately\nrepresenting different sources of uncertainty in stochastic crack growth (SCG)\nprocesses is a non-trivial task. The present work builds on previous research\non physics-based SCG modeling under both material and load-related uncertainty.\nThe aim here is to construct computationally efficient, probabilistic surrogate\nmodels for SCG processes that successfully encode these different sources of\nuncertainty. An approach inspired by latent variable modeling is employed that\nutilizes Gaussian Process (GP) regression models to enable the surrogates to be\nused to generate prior distributions for different Bayesian SHM tasks as the\napplication of interest. Implementation is carried out in a numerical setting\nand model performance is assessed for two fundamental crack SHM problems;\nnamely crack length monitoring (damage quantification) and crack growth\nmonitoring (damage prognosis).\n","authors":["Nicholas E. Silionis","Konstantinos N. Anyfantis"],"pdf_url":"https://arxiv.org/pdf/2310.07241v1.pdf","comment":"20 pages, 9 figures. Preprint submitted to Elsevier journal"},{"id":"http://arxiv.org/abs/2310.07240v1","updated":"2023-10-11T07:08:20Z","published":"2023-10-11T07:08:20Z","title":"CacheGen: Fast Context Loading for Language Model Applications","summary":"  As large language models (LLMs) take on more complex tasks, their inputs\nincorporate longer contexts to respond to questions that require domain\nknowledge or user-specific conversational histories. Yet, using long contexts\nposes a challenge for responsive LLM systems, as nothing can be generated until\nall the contexts are fetched to and processed by the LLM. Existing systems\noptimize only the computation delay in context processing (e.g., by caching\nintermediate key-value features of the text context) but often cause longer\nnetwork delays in context fetching (e.g., key-value features consume orders of\nmagnitude larger bandwidth than the text context).\n  This paper presents CacheGen to minimize the delays in fetching and\nprocessing contexts for LLMs. CacheGen reduces the bandwidth needed for\ntransmitting long contexts' key-value (KV) features through a novel encoder\nthat compresses KV features into more compact bitstream representations. The\nencoder combines adaptive quantization with a tailored arithmetic coder, taking\nadvantage of the KV features' distributional properties, such as locality\nacross tokens. Furthermore, CacheGen minimizes the total delay in fetching and\nprocessing a context by using a controller that determines when to load the\ncontext as compressed KV features or raw text and picks the appropriate\ncompression level if loaded as KV features. We test CacheGen on three models of\nvarious sizes and three datasets of different context lengths. Compared to\nrecent methods that handle long contexts, CacheGen reduces bandwidth usage by\n3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x\nwhile maintaining similar LLM performance on various tasks as loading the text\ncontexts.\n","authors":["Yuhan Liu","Hanchen Li","Kuntai Du","Jiayi Yao","Yihua Cheng","Yuyang Huang","Shan Lu","Michael Maire","Henry Hoffmann","Ari Holtzman","Ganesh Ananthanarayanan","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07235v1","updated":"2023-10-11T06:53:05Z","published":"2023-10-11T06:53:05Z","title":"Are GATs Out of Balance?","summary":"  While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.\n","authors":["Nimrah Mustafa","Aleksandar Bojchevski","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2310.07235v1.pdf","comment":"24 pages. To be published in Advances in Neural Information\n  Processing Systems (NeurIPS), 2023"},{"id":"http://arxiv.org/abs/2310.07234v1","updated":"2023-10-11T06:51:46Z","published":"2023-10-11T06:51:46Z","title":"Hierarchical Decomposition of Prompt-Based Continual Learning:\n  Rethinking Obscured Sub-optimality","summary":"  Prompt-based continual learning is an emerging direction in leveraging\npre-trained knowledge for downstream continual learning, and has almost reached\nthe performance pinnacle under supervised pre-training. However, our empirical\nresearch reveals that the current strategies fall short of their full potential\nunder the more realistic self-supervised pre-training, which is essential for\nhandling vast quantities of unlabeled data in practice. This is largely due to\nthe difficulty of task-specific knowledge being incorporated into instructed\nrepresentations via prompt parameters and predicted by uninstructed\nrepresentations at test time. To overcome the exposed sub-optimality, we\nconduct a theoretical analysis of the continual learning objective in the\ncontext of pre-training, and decompose it into hierarchical components:\nwithin-task prediction, task-identity inference, and task-adaptive prediction.\nFollowing these empirical and theoretical insights, we propose Hierarchical\nDecomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes\nthe hierarchical components with an ensemble of task-specific prompts and\nstatistics of both uninstructed and instructed representations, further with\nthe coordination of a contrastive regularization strategy. Our extensive\nexperiments demonstrate the superior performance of HiDe-Prompt and its\nrobustness to pre-training paradigms in continual learning (e.g., up to 15.01%\nand 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code\nis available at \\url{https://github.com/thu-ml/HiDe-Prompt}.\n","authors":["Liyuan Wang","Jingyi Xie","Xingxing Zhang","Mingyi Huang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07234v1.pdf","comment":"23 pages, 20 figures, 11 tables, accepted by NeurIPS as a Spotlight"},{"id":"http://arxiv.org/abs/2212.12180v4","updated":"2023-10-11T06:49:08Z","published":"2022-12-23T07:42:56Z","title":"Autothrottle: A Practical Bi-Level Approach to Resource Management for\n  SLO-Targeted Microservices","summary":"  Achieving resource efficiency while preserving end-user experience is\nnon-trivial for cloud application operators. As cloud applications\nprogressively adopt microservices, resource managers are faced with two\ndistinct levels of system behavior: end-to-end application latency and\nper-service resource usage. Translating between the two levels, however, is\nchallenging because user requests traverse heterogeneous services that\ncollectively (but unevenly) contribute to the end-to-end latency. We present\nAutothrottle, a bi-level resource management framework for microservices with\nlatency SLOs (service-level objectives). It architecturally decouples\napplication SLO feedback from service resource control, and bridges them\nthrough the notion of performance targets. Specifically, an application-wide\nlearning-based controller is employed to periodically set performance targets\n-- expressed as CPU throttle ratios -- for per-service heuristic controllers to\nattain. We evaluate Autothrottle on three microservice applications, with\nworkload traces from production scenarios. Results show superior CPU savings,\nup to 26.21% over the best-performing baseline and up to 93.84% over all\nbaselines.\n","authors":["Zibo Wang","Pinghe Li","Chieh-Jan Mike Liang","Feng Wu","Francis Y. Yan"],"pdf_url":"https://arxiv.org/pdf/2212.12180v4.pdf","comment":"Accepted by USENIX NSDI '24"},{"id":"http://arxiv.org/abs/2310.07229v1","updated":"2023-10-11T06:36:23Z","published":"2023-10-11T06:36:23Z","title":"Self-supervised Pocket Pretraining via Protein Fragment-Surroundings\n  Alignment","summary":"  Pocket representations play a vital role in various biomedical applications,\nsuch as druggability estimation, ligand affinity prediction, and de novo drug\ndesign. While existing geometric features and pretrained representations have\ndemonstrated promising results, they usually treat pockets independent of\nligands, neglecting the fundamental interactions between them. However, the\nlimited pocket-ligand complex structures available in the PDB database (less\nthan 100 thousand non-redundant pairs) hampers large-scale pretraining\nendeavors for interaction modeling. To address this constraint, we propose a\nnovel pocket pretraining approach that leverages knowledge from high-resolution\natomic protein structures, assisted by highly effective pretrained small\nmolecule representations. By segmenting protein structures into drug-like\nfragments and their corresponding pockets, we obtain a reasonable simulation of\nligand-receptor interactions, resulting in the generation of over 5 million\ncomplexes. Subsequently, the pocket encoder is trained in a contrastive manner\nto align with the representation of pseudo-ligand furnished by some pretrained\nsmall molecule encoders. Our method, named ProFSA, achieves state-of-the-art\nperformance across various tasks, including pocket druggability prediction,\npocket matching, and ligand binding affinity prediction. Notably, ProFSA\nsurpasses other pretraining methods by a substantial margin. Moreover, our work\nopens up a new avenue for mitigating the scarcity of protein-ligand complex\ndata through the utilization of high-quality and diverse protein structure\ndatabases.\n","authors":["Bowen Gao","Yinjun Jia","Yuanle Mo","Yuyan Ni","Weiying Ma","Zhiming Ma","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2310.07229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12421v3","updated":"2023-10-11T06:32:32Z","published":"2022-11-11T02:14:28Z","title":"Data-Driven Network Neuroscience: On Data Collection and Benchmark","summary":"  This paper presents a comprehensive and quality collection of functional\nhuman brain \\emph{network} data for potential research in the intersection of\nneuroscience, machine learning, and graph analytics. Anatomical and functional\nMRI images have been used to understand the functional connectivity of the\nhuman brain and are particularly important in identifying underlying\nneurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism.\nRecently, the study of the brain in the form of brain networks using machine\nlearning and graph analytics has become increasingly popular, especially to\npredict the early onset of these conditions. A brain network, represented as a\ngraph, retains rich structural and positional information that traditional\nexamination methods are unable to capture. However, the lack of publicly\naccessible brain network data prevents researchers from data-driven\nexplorations. One of the main difficulties lies in the complicated\ndomain-specific preprocessing steps and the exhaustive computation required to\nconvert the data from MRI images into brain networks. We bridge this gap by\ncollecting a large amount of MRI images from public databases and a private\nsource, working with domain experts to make sensible design choices, and\npreprocessing the MRI images to produce a collection of brain network datasets.\nThe datasets originate from 6 different sources, cover 4 brain conditions, and\nconsist of a total of 2,702 subjects. We test our graph datasets on 12 machine\nlearning models to provide baselines and validate the data quality on a recent\ngraph analysis model. To lower the barrier to entry and promote the research in\nthis interdisciplinary field, we release our brain network data and complete\npreprocessing details including codes at\nhttps://doi.org/10.17608/k6.auckland.21397377 and\nhttps://figshare.com/s/fa33c10664ca08b022ce.\n","authors":["Jiaxing Xu","Yunhan Yang","David Tse Jung Huang","Sophi Shilpa Gururajapathy","Yiping Ke","Miao Qiao","Alan Wang","Haribalan Kumar","Josh McGeown","Eryn Kwon"],"pdf_url":"https://arxiv.org/pdf/2211.12421v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17382v2","updated":"2023-10-11T06:18:04Z","published":"2023-09-29T16:36:39Z","title":"Reason for Future, Act for Now: A Principled Framework for Autonomous\n  LLM Agents with Provable Sample Efficiency","summary":"  Large language models (LLMs) demonstrate impressive reasoning abilities, but\ntranslating reasoning into actions in the real world remains challenging. In\nparticular, it remains unclear how to complete a given task provably within a\nminimum number of interactions with the external environment, e.g., through an\ninternal mechanism of reasoning. To this end, we propose a principled framework\nwith provable regret guarantees to orchestrate reasoning and acting, which we\ncall \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design\na prompt template for reasoning that learns from the memory buffer and plans a\nfuture trajectory over a long horizon (\"reason for future\"). At each step, the\nLLM agent takes the initial action of the planned trajectory (\"act for now\"),\nstores the collected feedback in the memory buffer, and reinvokes the reasoning\nroutine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in\nBayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt\nLLMs to form an updated posterior of the unknown environment from the memory\nbuffer (learning) and generate an optimal trajectory for multiple future steps\nthat maximizes a value function (planning). The learning and planning\nsubroutines are performed in an \"in-context\" manner to emulate the actor-critic\nupdate for MDPs. Our theoretical analysis proves that the novel combination of\nlong-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In\nparticular, the regret bound highlights an intriguing interplay between the\nprior knowledge obtained through pretraining and the uncertainty reduction\nachieved by reasoning and acting. Our empirical validation shows that it\noutperforms various existing frameworks and achieves nearly perfect scores on a\nfew benchmarks.\n","authors":["Zhihan Liu","Hao Hu","Shenao Zhang","Hongyi Guo","Shuqi Ke","Boyi Liu","Zhaoran Wang"],"pdf_url":"https://arxiv.org/pdf/2309.17382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07223v1","updated":"2023-10-11T06:13:50Z","published":"2023-10-11T06:13:50Z","title":"Deep Learning for blind spectral unmixing of LULC classes with MODIS\n  multispectral time series and ancillary data","summary":"  Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)\ntypes. Spectral unmixing is a technique to extract information from mixed\npixels into their constituent LULC types and corresponding abundance fractions.\nTraditionally, solving this task has relied on either classical methods that\nrequire prior knowledge of endmembers or machine learning methods that avoid\nexplicit endmembers calculation, also known as blind spectral unmixing (BSU).\nMost BSU studies based on Deep Learning (DL) focus on one time-step\nhyperspectral data, yet its acquisition remains quite costly compared with\nmultispectral data. To our knowledge, here we provide the first study on BSU of\nLULC classes using multispectral time series data with DL models. We further\nboost the performance of a Long-Short Term Memory (LSTM)-based model by\nincorporating geographic plus topographic (geo-topographic) and climatic\nancillary information. Our experiments show that combining spectral-temporal\ninput data together with geo-topographic and climatic information substantially\nimproves the abundance estimation of LULC classes in mixed pixels. To carry out\nthis study, we built a new labeled dataset of the region of Andalusia (Spain)\nwith monthly multispectral time series of pixels for the year 2013 from MODIS\nat 460m resolution, for two hierarchical levels of LULC classes, named\nAndalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset\nprovides, at the pixel level, a multispectral time series plus ancillary\ninformation annotated with the abundance of each LULC class inside each pixel.\nThe dataset and code are available to the public.\n","authors":["Jos Rodrguez-Ortega","Rohaifa Khaldi","Domingo Alcaraz-Segura","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2310.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07221v1","updated":"2023-10-11T06:11:11Z","published":"2023-10-11T06:11:11Z","title":"Using Learnable Physics for Real-Time Exercise Form Recommendations","summary":"  Good posture and form are essential for safe and productive exercising. Even\nin gym settings, trainers may not be readily available for feedback.\nRehabilitation therapies and fitness workouts can thus benefit from recommender\nsystems that provide real-time evaluation. In this paper, we present an\nalgorithmic pipeline that can diagnose problems in exercise techniques and\noffer corrective recommendations, with high sensitivity and specificity in\nreal-time. We use MediaPipe for pose recognition, count repetitions using\npeak-prominence detection, and use a learnable physics simulator to track\nmotion evolution for each exercise. A test video is diagnosed based on\ndeviations from the prototypical learned motion using statistical learning. The\nsystem is evaluated on six full and upper body exercises. These real-time\nrecommendations, counseled via low-cost equipment like smartphones, will allow\nexercisers to rectify potential mistakes making self-practice feasible while\nreducing the risk of workout injuries.\n","authors":["Abhishek Jaiswal","Gautam Chauhan","Nisheeth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2310.07221v1.pdf","comment":"Accepted by ACM RecSys '23, 12 pages , 7 Figures"},{"id":"http://arxiv.org/abs/2310.07220v1","updated":"2023-10-11T06:10:07Z","published":"2023-10-11T06:10:07Z","title":"COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically\n  for Model-Based RL","summary":"  Dyna-style model-based reinforcement learning contains two phases: model\nrollouts to generate sample for policy learning and real environment\nexploration using current policy for dynamics model learning. However, due to\nthe complex real-world environment, it is inevitable to learn an imperfect\ndynamics model with model prediction error, which can further mislead policy\nlearning and result in sub-optimal solutions. In this paper, we propose\n$\\texttt{COPlanner}$, a planning-driven framework for model-based methods to\naddress the inaccurately learned dynamics model problem with conservative model\nrollouts and optimistic environment exploration. $\\texttt{COPlanner}$ leverages\nan uncertainty-aware policy-guided model predictive control (UP-MPC) component\nto plan for multi-step uncertainty estimation. This estimated uncertainty then\nserves as a penalty during model rollouts and as a bonus during real\nenvironment exploration respectively, to choose actions. Consequently,\n$\\texttt{COPlanner}$ can avoid model uncertain regions through conservative\nmodel rollouts, thereby alleviating the influence of model error.\nSimultaneously, it explores high-reward model uncertain regions to reduce model\nerror actively through optimistic real environment exploration.\n$\\texttt{COPlanner}$ is a plug-and-play framework that can be applied to any\ndyna-style model-based methods. Experimental results on a series of\nproprioceptive and visual continuous control tasks demonstrate that both sample\nefficiency and asymptotic performance of strong model-based methods are\nsignificantly improved combined with $\\texttt{COPlanner}$.\n","authors":["Xiyao Wang","Ruijie Zheng","Yanchao Sun","Ruonan Jia","Wichayaporn Wongkamjan","Huazhe Xu","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07220v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.07219v1","updated":"2023-10-11T06:09:48Z","published":"2023-10-11T06:09:48Z","title":"Improved Membership Inference Attacks Against Language Classification\n  Models","summary":"  Artificial intelligence systems are prevalent in everyday life, with use\ncases in retail, manufacturing, health, and many other fields. With the rise in\nAI adoption, associated risks have been identified, including privacy risks to\nthe people whose data was used to train models. Assessing the privacy risks of\nmachine learning models is crucial to enabling knowledgeable decisions on\nwhether to use, deploy, or share a model. A common approach to privacy risk\nassessment is to run one or more known attacks against the model and measure\ntheir success rate. We present a novel framework for running membership\ninference attacks against classification models. Our framework takes advantage\nof the ensemble method, generating many specialized attack models for different\nsubsets of the data. We show that this approach achieves higher accuracy than\neither a single attack model or an attack model per class label, both on\nclassical and language classification tasks.\n","authors":["Shlomit Shachor","Natalia Razinkov","Abigail Goldsteen"],"pdf_url":"https://arxiv.org/pdf/2310.07219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07217v1","updated":"2023-10-11T06:09:14Z","published":"2023-10-11T06:09:14Z","title":"Enhancing Neural Architecture Search with Multiple Hardware Constraints\n  for Deep Learning Model Deployment on Tiny IoT Devices","summary":"  The rapid proliferation of computing domains relying on Internet of Things\n(IoT) devices has created a pressing need for efficient and accurate\ndeep-learning (DL) models that can run on low-power devices. However,\ntraditional DL models tend to be too complex and computationally intensive for\ntypical IoT end-nodes. To address this challenge, Neural Architecture Search\n(NAS) has emerged as a popular design automation technique for co-optimizing\nthe accuracy and complexity of deep neural networks. Nevertheless, existing NAS\ntechniques require many iterations to produce a network that adheres to\nspecific hardware constraints, such as the maximum memory available on the\nhardware or the maximum latency allowed by the target application. In this\nwork, we propose a novel approach to incorporate multiple constraints into\nso-called Differentiable NAS optimization methods, which allows the generation,\nin a single shot, of a model that respects user-defined constraints on both\nmemory and latency in a time comparable to a single standard training. The\nproposed approach is evaluated on five IoT-relevant benchmarks, including the\nMLPerf Tiny suite and Tiny ImageNet, demonstrating that, with a single search,\nit is possible to reduce memory and latency by 87.4% and 54.2%, respectively\n(as defined by our targets), while ensuring non-inferior accuracy on\nstate-of-the-art hand-tuned deep neural networks for TinyML.\n","authors":["Alessio Burrello","Matteo Risso","Beatrice Alessandra Motetti","Enrico Macii","Luca Benini","Daniele Jahier Pagliari"],"pdf_url":"https://arxiv.org/pdf/2310.07217v1.pdf","comment":"Accepted for publication at the IEEE Transactions on Emerging Topics\n  in Computing"},{"id":"http://arxiv.org/abs/2310.07216v1","updated":"2023-10-11T06:04:40Z","published":"2023-10-11T06:04:40Z","title":"Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion\n  Processes","summary":"  Learning the distribution of data on Riemannian manifolds is crucial for\nmodeling data from non-Euclidean space, which is required by many applications\nfrom diverse scientific fields. Yet, existing generative models on manifolds\nsuffer from expensive divergence computation or rely on approximations of heat\nkernel. These limitations restrict their applicability to simple geometries and\nhinder scalability to high dimensions. In this work, we introduce the\nRiemannian Diffusion Mixture, a principled framework for building a generative\nprocess on manifolds as a mixture of endpoint-conditioned diffusion processes\ninstead of relying on the denoising approach of previous diffusion models, for\nwhich the generative process is characterized by its drift guiding toward the\nmost probable endpoint with respect to the geometry of the manifold. We further\npropose a simple yet efficient training objective for learning the mixture\nprocess, that is readily applicable to general manifolds. Our method\noutperforms previous generative models on various manifolds while scaling to\nhigh dimensions and requires a dramatically reduced number of in-training\nsimulation steps for general manifolds.\n","authors":["Jaehyeong Jo","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.07216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04895v2","updated":"2023-10-11T05:59:53Z","published":"2023-10-07T18:47:17Z","title":"Cell Tracking-by-detection using Elliptical Bounding Boxes","summary":"  Cell detection and tracking are paramount for bio-analysis. Recent approaches\nrely on the tracking-by-model evolution paradigm, which usually consists of\ntraining end-to-end deep learning models to detect and track the cells on the\nframes with promising results. However, such methods require extensive amounts\nof annotated data, which is time-consuming to obtain and often requires\nspecialized annotators. This work proposes a new approach based on the\nclassical tracking-by-detection paradigm that alleviates the requirement of\nannotated data. More precisely, it approximates the cell shapes as oriented\nellipses and then uses generic-purpose oriented object detectors to identify\nthe cells in each frame. We then rely on a global data association algorithm\nthat explores temporal cell similarity using probability distance metrics,\nconsidering that the ellipses relate to two-dimensional Gaussian distributions.\nOur results show that our method can achieve detection and tracking results\ncompetitively with state-of-the-art techniques that require considerably more\nextensive data annotation. Our code is available at:\nhttps://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.\n","authors":["Lucas N. Kirsten","Cludio R. Jung"],"pdf_url":"https://arxiv.org/pdf/2310.04895v2.pdf","comment":"Paper under review on IEEE/ACM Transactions on Computational Biology\n  and Bioinformatics"},{"id":"http://arxiv.org/abs/2310.07211v1","updated":"2023-10-11T05:55:20Z","published":"2023-10-11T05:55:20Z","title":"Bridging the Gap between Newton-Raphson Method and Regularized Policy\n  Iteration","summary":"  Regularization is one of the most important techniques in reinforcement\nlearning algorithms. The well-known soft actor-critic algorithm is a special\ncase of regularized policy iteration where the regularizer is chosen as Shannon\nentropy. Despite some empirical success of regularized policy iteration, its\ntheoretical underpinnings remain unclear. This paper proves that regularized\npolicy iteration is strictly equivalent to the standard Newton-Raphson method\nin the condition of smoothing out Bellman equation with strongly convex\nfunctions. This equivalence lays the foundation of a unified analysis for both\nglobal and local convergence behaviors of regularized policy iteration. We\nprove that regularized policy iteration has global linear convergence with the\nrate being $\\gamma$ (discount factor). Furthermore, this algorithm converges\nquadratically once it enters a local region around the optimal value. We also\nshow that a modified version of regularized policy iteration, i.e., with\nfinite-step policy evaluation, is equivalent to inexact Newton method where the\nNewton iteration formula is solved with truncated iterations. We prove that the\nassociated algorithm achieves an asymptotic linear convergence rate of\n$\\gamma^M$ in which $M$ denotes the number of steps carried out in policy\nevaluation. Our results take a solid step towards a better understanding of the\nconvergence properties of regularized policy iteration algorithms.\n","authors":["Zeyang Li","Chuxiong Hu","Yunan Wang","Guojian Zhan","Jie Li","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2310.07211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07207v1","updated":"2023-10-11T05:34:46Z","published":"2023-10-11T05:34:46Z","title":"Robust Safe Reinforcement Learning under Adversarial Disturbances","summary":"  Safety is a primary concern when applying reinforcement learning to\nreal-world control tasks, especially in the presence of external disturbances.\nHowever, existing safe reinforcement learning algorithms rarely account for\nexternal disturbances, limiting their applicability and robustness in practice.\nTo address this challenge, this paper proposes a robust safe reinforcement\nlearning framework that tackles worst-case disturbances. First, this paper\npresents a policy iteration scheme to solve for the robust invariant set, i.e.,\na subset of the safe set, where persistent safety is only possible for states\nwithin. The key idea is to establish a two-player zero-sum game by leveraging\nthe safety value function in Hamilton-Jacobi reachability analysis, in which\nthe protagonist (i.e., control inputs) aims to maintain safety and the\nadversary (i.e., external disturbances) tries to break down safety. This paper\nproves that the proposed policy iteration algorithm converges monotonically to\nthe maximal robust invariant set. Second, this paper integrates the proposed\npolicy iteration scheme into a constrained reinforcement learning algorithm\nthat simultaneously synthesizes the robust invariant set and uses it for\nconstrained policy optimization. This algorithm tackles both optimality and\nsafety, i.e., learning a policy that attains high rewards while maintaining\nsafety under worst-case disturbances. Experiments on classic control tasks show\nthat the proposed method achieves zero constraint violation with learned\nworst-case adversarial disturbances, while other baseline algorithms violate\nthe safety constraints substantially. Our proposed method also attains\ncomparable performance as the baselines even in the absence of the adversary.\n","authors":["Zeyang Li","Chuxiong Hu","Shengbo Eben Li","Jia Cheng","Yunan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07204v1","updated":"2023-10-11T05:32:29Z","published":"2023-10-11T05:32:29Z","title":"State of the Art on Diffusion Models for Visual Computing","summary":"  The field of visual computing is rapidly advancing due to the emergence of\ngenerative artificial intelligence (AI), which unlocks unprecedented\ncapabilities for the generation, editing, and reconstruction of images, videos,\nand 3D scenes. In these domains, diffusion models are the generative AI\narchitecture of choice. Within the last year alone, the literature on\ndiffusion-based tools and applications has seen exponential growth and relevant\npapers are published across the computer graphics, computer vision, and AI\ncommunities with new works appearing daily on arXiv. This rapid growth of the\nfield makes it difficult to keep up with all recent developments. The goal of\nthis state-of-the-art report (STAR) is to introduce the basic mathematical\nconcepts of diffusion models, implementation details and design choices of the\npopular Stable Diffusion model, as well as overview important aspects of these\ngenerative AI tools, including personalization, conditioning, inversion, among\nothers. Moreover, we give a comprehensive overview of the rapidly growing\nliterature on diffusion-based generation and editing, categorized by the type\nof generated medium, including 2D images, videos, 3D objects, locomotion, and\n4D scenes. Finally, we discuss available datasets, metrics, open challenges,\nand social implications. This STAR provides an intuitive starting point to\nexplore this exciting topic for researchers, artists, and practitioners alike.\n","authors":["Ryan Po","Wang Yifan","Vladislav Golyanik","Kfir Aberman","Jonathan T. Barron","Amit H. Bermano","Eric Ryan Chan","Tali Dekel","Aleksander Holynski","Angjoo Kanazawa","C. Karen Liu","Lingjie Liu","Ben Mildenhall","Matthias Niener","Bjrn Ommer","Christian Theobalt","Peter Wonka","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2310.07204v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.07517v1","updated":"2023-10-11T14:15:25Z","published":"2023-10-11T14:15:25Z","title":"CM-PIE: Cross-modal perception for interactive-enhanced audio-visual\n  video parsing","summary":"  Audio-visual video parsing is the task of categorizing a video at the segment\nlevel with weak labels, and predicting them as audible or visible events.\nRecent methods for this task leverage the attention mechanism to capture the\nsemantic correlations among the whole video across the audio-visual modalities.\nHowever, these approaches have overlooked the importance of individual segments\nwithin a video and the relationship among them, and tend to rely on a single\nmodality when learning features. In this paper, we propose a novel\ninteractive-enhanced cross-modal perception method~(CM-PIE), which can learn\nfine-grained features by applying a segment-based attention module.\nFurthermore, a cross-modal aggregation block is introduced to jointly optimize\nthe semantic representation of audio and visual signals by enhancing\ninter-modal interactions. The experimental results show that our model offers\nimproved parsing performance on the Look, Listen, and Parse dataset compared to\nother methods.\n","authors":["Yaru Chen","Ruohao Guo","Xubo Liu","Peipei Wu","Guangyao Li","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07517v1.pdf","comment":"5 pages, 3 figures, 15 references"},{"id":"http://arxiv.org/abs/2304.11161v2","updated":"2023-10-11T13:29:23Z","published":"2023-04-02T16:03:44Z","title":"altiro3D: Scene representation from single image and novel view\n  synthesis","summary":"  We introduce altiro3D, a free extended library developed to represent reality\nstarting from a given original RGB image or flat video. It allows to generate a\nlight-field (or Native) image or video and get a realistic 3D experience. To\nsynthesize N-number of virtual images and add them sequentially into a Quilt\ncollage, we apply MiDaS models for the monocular depth estimation, simple\nOpenCV and Telea inpainting techniques to map all pixels, and implement a\n'Fast' algorithm to handle 3D projection camera and scene transformations along\nN-viewpoints. We use the degree of depth to move proportionally the pixels,\nassuming the original image to be at the center of all the viewpoints. altiro3D\ncan also be used with DIBR algorithm to compute intermediate snapshots from a\nequivalent 'Real (slower)' camera with N-geometric viewpoints, which requires\nto calibrate a priori several intrinsic and extrinsic camera parameters. We\nadopt a pixel- and device-based Lookup Table to optimize computing time. The\nmultiple viewpoints and video generated from a single image or frame can be\ndisplayed in a free-view LCD display.\n","authors":["E. Canessa","L. Tenze"],"pdf_url":"https://arxiv.org/pdf/2304.11161v2.pdf","comment":"In press (2023) Springer International Journal of Information\n  Technology (IJIT) 10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.07376v1","updated":"2023-10-11T10:50:15Z","published":"2023-10-11T10:50:15Z","title":"Point Cloud Denoising and Outlier Detection with Local Geometric\n  Structure by Dynamic Graph CNN","summary":"  The digitalization of society is rapidly developing toward the realization of\nthe digital twin and metaverse. In particular, point clouds are attracting\nattention as a media format for 3D space. Point cloud data is contaminated with\nnoise and outliers due to measurement errors. Therefore, denoising and outlier\ndetection are necessary for point cloud processing. Among them, PointCleanNet\nis an effective method for point cloud denoising and outlier detection.\nHowever, it does not consider the local geometric structure of the patch. We\nsolve this problem by applying two types of graph convolutional layer designed\nbased on the Dynamic Graph CNN. Experimental results show that the proposed\nmethods outperform the conventional method in AUPR, which indicates outlier\ndetection accuracy, and Chamfer Distance, which indicates denoising accuracy.\n","authors":["Kosuke Nakayama","Hiroto Fukuta","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2310.07376v1.pdf","comment":"2023 IEEE 12th Global Conference on Consumer Electronics (GCCE 2023)"},{"id":"http://arxiv.org/abs/2310.07287v1","updated":"2023-10-11T08:18:51Z","published":"2023-10-11T08:18:51Z","title":"Interactive Interior Design Recommendation via Coarse-to-fine Multimodal\n  Reinforcement Learning","summary":"  Personalized interior decoration design often incurs high labor costs. Recent\nefforts in developing intelligent interior design systems have focused on\ngenerating textual requirement-based decoration designs while neglecting the\nproblem of how to mine homeowner's hidden preferences and choose the proper\ninitial design. To fill this gap, we propose an Interactive Interior Design\nRecommendation System (IIDRS) based on reinforcement learning (RL). IIDRS aims\nto find an ideal plan by interacting with the user, who provides feedback on\nthe gap between the recommended plan and their ideal one. To improve\ndecision-making efficiency and effectiveness in large decoration spaces, we\npropose a Decoration Recommendation Coarse-to-Fine Policy Network (DecorRCFN).\nAdditionally, to enhance generalization in online scenarios, we propose an\nobject-aware feedback generation method that augments model training with\ndiversified and dynamic textual feedback. Extensive experiments on a real-world\ndataset demonstrate our method outperforms traditional methods by a large\nmargin in terms of recommendation accuracy. Further user studies demonstrate\nthat our method reaches higher real-world user satisfaction than baseline\nmethods.\n","authors":["He Zhang","Ying Sun","Weiyu Guo","Yafei Liu","Haonan Lu","Xiaodong Lin","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.07287v1.pdf","comment":"Accepted by ACM International Conference on Multimedia'23. 9 pages, 7\n  figures"},{"id":"http://arxiv.org/abs/2310.07236v1","updated":"2023-10-11T06:56:08Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07236v1.pdf","comment":"Project Page: https://adamesh.github.io"},{"id":"http://arxiv.org/abs/2310.04673v3","updated":"2023-10-11T02:55:54Z","published":"2023-10-07T03:17:59Z","title":"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","summary":"  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks. However, there has\nbeen limited research on applying similar frameworks to audio tasks. Previously\nproposed large language models for audio tasks either lack sufficient\nquantitative evaluations, or are limited to tasks for recognizing and\nunderstanding audio content, or significantly underperform existing\nstate-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified\nGPT model for audio recognition, understanding, and generation. LauraGPT is a\nversatile language model that can process both audio and text inputs and\ngenerate outputs in either modalities. It can perform a wide range of tasks\nrelated to content, semantics, paralinguistics, and audio-signal analysis. Some\nof its noteworthy tasks include automatic speech recognition, speech-to-text\ntranslation, text-to-speech synthesis, machine translation, speech enhancement,\nautomated audio captioning, speech emotion recognition, and spoken language\nunderstanding. To achieve this goal, we use a combination of continuous and\ndiscrete features for audio. We encode input audio into continuous\nrepresentations using an audio encoder and decode output audio from discrete\ncodec codes. We then fine-tune a large decoder-only Transformer-based language\nmodel on multiple audio-to-text, text-to-audio, audio-to-audio, and\ntext-to-text tasks using a supervised multitask learning approach. Extensive\nexperiments show that LauraGPT achieves competitive or superior performance\ncompared to existing SOTA models on various audio processing benchmarks.\n","authors":["Jiaming Wang","Zhihao Du","Qian Chen","Yunfei Chu","Zhifu Gao","Zerui Li","Kai Hu","Xiaohuan Zhou","Jin Xu","Ziyang Ma","Wen Wang","Siqi Zheng","Chang Zhou","Zhijie Yan","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04673v3.pdf","comment":"10 pages, under review"},{"id":"http://arxiv.org/abs/2310.07121v1","updated":"2023-10-11T01:51:19Z","published":"2023-10-11T01:51:19Z","title":"Motion Vector-Domain Video Steganalysis Exploiting Skipped Macroblocks","summary":"  Video steganography has the potential to be used to convey illegal\ninformation, and video steganalysis is a vital tool to detect the presence of\nthis illicit act. Currently, all the motion vector (MV)-based video\nsteganalysis algorithms extract feature sets directly on the MVs, but ignoring\nthe steganograhic operation may perturb the statistics distribution of other\nvideo encoding elements, such as the skipped macroblocks (no direct MVs). This\npaper proposes a novel 11-dimensional feature set to detect MV-based video\nsteganography based on the above observation. The proposed feature is extracted\nbased on the skipped macroblocks by recompression calibration. Specifically,\nthe feature consists of two components. The first is the probability\ndistribution of motion vector prediction (MVP) difference, and the second is\nthe probability distribution of partition state transfer. Extensive experiments\non different conditions demonstrate that the proposed feature set achieves good\ndetection accuracy, especially in lower embedding capacity. In addition, the\nloss of detection performance caused by recompression calibration using\nmismatched quantization parameters (QP) is within the acceptable range, so the\nproposed method can be used in practical scenarios.\n","authors":["Jun Li","Minqing Zhang","Ke Niu","Yingnan Zhang","Xiaoyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07121v1.pdf","comment":null}]},"2023-10-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.08582v1","updated":"2023-10-12T17:59:50Z","published":"2023-10-12T17:59:50Z","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models","summary":"  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections. Project page:\nhttps://tree-planner.github.io/\n","authors":["Mengkang Hu","Yao Mu","Xinmiao Yu","Mingyu Ding","Shiguang Wu","Wenqi Shao","Qiguang Chen","Bin Wang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08577v1","updated":"2023-10-12T17:59:30Z","published":"2023-10-12T17:59:30Z","title":"Visual Data-Type Understanding does not emerge from Scaling\n  Vision-Language Models","summary":"  Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of \\textit{Visual Data-Type Identification}, a basic perceptual\nskill with implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual \\textit{data-types}, spanning four broad categories. An extensive\nzero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a\nnuanced performance landscape. While VLMs are reasonably good at identifying\ncertain stylistic \\textit{data-types}, such as cartoons and sketches, they\nstruggle with simpler \\textit{data-types} arising from basic manipulations like\nimage rotations or additive noise. Our findings reveal that (i) model scaling\nalone yields marginal gains for contrastively-trained models like CLIP, and\n(ii) there is a pronounced drop in performance for the largest\nauto-regressively trained VLMs like OpenFlamingo. This finding points to a\nblind spot in current frontier VLMs: they excel in recognizing semantic content\nbut fail to acquire an understanding of visual \\textit{data-types} through\nscaling. By analyzing the pre-training distributions of these models and\nincorporating \\textit{data-type} information into the captions during\nfine-tuning, we achieve a significant enhancement in performance. By exploring\nthis previously uncharted task, we aim to set the stage for further advancing\nVLMs to equip them with visual data-type understanding. Code and datasets are\nreleased \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.\n","authors":["Vishaal Udandarao","Max F. Burg","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2310.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08566v1","updated":"2023-10-12T17:55:02Z","published":"2023-10-12T17:55:02Z","title":"Transformers as Decision Makers: Provable In-Context Reinforcement\n  Learning via Supervised Pretraining","summary":"  Large transformer models pretrained on offline reinforcement learning\ndatasets have demonstrated remarkable in-context reinforcement learning (ICRL)\ncapabilities, where they can make good decisions when prompted with interaction\ntrajectories from unseen environments. However, when and how transformers can\nbe trained to perform ICRL have not been theoretically well-understood. In\nparticular, it is unclear which reinforcement-learning algorithms transformers\ncan perform in context, and how distribution mismatch in offline training data\naffects the learned algorithms. This paper provides a theoretical framework\nthat analyzes supervised pretraining for ICRL. This includes two recently\nproposed training methods -- algorithm distillation and decision-pretrained\ntransformers. First, assuming model realizability, we prove the\nsupervised-pretrained transformer will imitate the conditional expectation of\nthe expert algorithm given the observed trajectory. The generalization error\nwill scale with model capacity and a distribution divergence factor between the\nexpert and offline algorithms. Second, we show transformers with ReLU attention\ncan efficiently approximate near-optimal online reinforcement learning\nalgorithms like LinUCB and Thompson sampling for stochastic linear bandits, and\nUCB-VI for tabular Markov decision processes. This provides the first\nquantitative analysis of the ICRL capabilities of transformers pretrained from\noffline trajectories.\n","authors":["Licong Lin","Yu Bai","Song Mei"],"pdf_url":"https://arxiv.org/pdf/2310.08566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08559v1","updated":"2023-10-12T17:51:10Z","published":"2023-10-12T17:51:10Z","title":"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of\n  Language Models with Hypothesis Refinement","summary":"  The ability to derive underlying principles from a handful of observations\nand then generalize to novel situations -- known as inductive reasoning -- is\ncentral to human intelligence. Prior work suggests that language models (LMs)\noften fall short on inductive reasoning, despite achieving impressive success\non research benchmarks. In this work, we conduct a systematic study of the\ninductive reasoning capabilities of LMs through iterative hypothesis\nrefinement, a technique that more closely mirrors the human inductive process\nthan standard input-output prompting. Iterative hypothesis refinement employs a\nthree-step process: proposing, selecting, and refining hypotheses in the form\nof textual rules. By examining the intermediate rules, we observe that LMs are\nphenomenal hypothesis proposers (i.e., generating candidate rules), and when\ncoupled with a (task-specific) symbolic interpreter that is able to\nsystematically filter the proposed set of rules, this hybrid approach achieves\nstrong results across inductive reasoning benchmarks that require inducing\ncausal relations, language-like instructions, and symbolic concepts. However,\nthey also behave as puzzling inductive reasoners, showing notable performance\ngaps in rule induction (i.e., identifying plausible rules) and rule application\n(i.e., applying proposed rules to instances), suggesting that LMs are proposing\nhypotheses without being able to actually apply the rules. Through empirical\nand human analyses, we further reveal several discrepancies between the\ninductive reasoning processes of LMs and humans, shedding light on both the\npotentials and limitations of using LMs in inductive reasoning tasks.\n","authors":["Linlu Qiu","Liwei Jiang","Ximing Lu","Melanie Sclar","Valentina Pyatkin","Chandra Bhagavatula","Bailin Wang","Yoon Kim","Yejin Choi","Nouha Dziri","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2310.08559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11606v2","updated":"2023-10-12T17:50:38Z","published":"2023-08-22T17:53:55Z","title":"StoryBench: A Multifaceted Benchmark for Continuous Story Visualization","summary":"  Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n","authors":["Emanuele Bugliarello","Hernan Moraldo","Ruben Villegas","Mohammad Babaeizadeh","Mohammad Taghi Saffar","Han Zhang","Dumitru Erhan","Vittorio Ferrari","Pieter-Jan Kindermans","Paul Voigtlaender"],"pdf_url":"https://arxiv.org/pdf/2308.11606v2.pdf","comment":"NeurIPS D&B 2023"},{"id":"http://arxiv.org/abs/2310.00737v2","updated":"2023-10-12T17:39:04Z","published":"2023-10-01T17:25:56Z","title":"GenAI Against Humanity: Nefarious Applications of Generative Artificial\n  Intelligence and Large Language Models","summary":"  Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare marvels of technology; celebrated for their prowess in natural language\nprocessing and multimodal content generation, they promise a transformative\nfuture. But as with all powerful tools, they come with their shadows. Picture\nliving in a world where deepfakes are indistinguishable from reality, where\nsynthetic identities orchestrate malicious campaigns, and where targeted\nmisinformation or scams are crafted with unparalleled precision. Welcome to the\ndarker side of GenAI applications. This article is not just a journey through\nthe meanders of potential misuse of GenAI and LLMs, but also a call to\nrecognize the urgency of the challenges ahead. As we navigate the seas of\nmisinformation campaigns, malicious content generation, and the eerie creation\nof sophisticated malware, we'll uncover the societal implications that ripple\nthrough the GenAI revolution we are witnessing. From AI-powered botnets on\nsocial media platforms to the unnerving potential of AI to generate fabricated\nidentities, or alibis made of synthetic realities, the stakes have never been\nhigher. The lines between the virtual and the real worlds are blurring, and the\nconsequences of potential GenAI's nefarious applications impact us all. This\narticle serves both as a synthesis of rigorous research presented on the risks\nof GenAI and misuse of LLMs and as a thought-provoking vision of the different\ntypes of harmful GenAI applications we might encounter in the near future, and\nsome ways we can prepare for them.\n","authors":["Emilio Ferrara"],"pdf_url":"https://arxiv.org/pdf/2310.00737v2.pdf","comment":"Submitted to CACM (Viewpoint)"},{"id":"http://arxiv.org/abs/2306.07951v2","updated":"2023-10-12T17:34:12Z","published":"2023-06-13T17:48:27Z","title":"Questioning the Survey Responses of Large Language Models","summary":"  As large language models increase in capability, researchers have started to\nconduct surveys of all kinds on these models with varying scientific\nmotivations. In this work, we examine what we can learn from language models'\nsurvey responses on the basis of the well-established American Community Survey\n(ACS) by the U.S. Census Bureau. Using a de-facto standard multiple-choice\nprompting technique and evaluating 40 different language models, hundreds of\nthousands of times each on questions from the ACS, we systematically establish\ntwo dominant patterns. First, models have significant position and labeling\nbiases, for example, towards survey responses labeled with the letter \"A\".\nSecond, when adjusting for labeling biases through randomized answer ordering,\nmodels across the board trend towards uniformly random survey responses. In\nfact, binary classifiers can almost perfectly differentiate between models'\nresponses to the ACS and the responses of the US census. Taken together, our\nfindings suggest caution in treating survey responses from language models as\nequivalent to those of human populations at present time.\n","authors":["Ricardo Dominguez-Olmedo","Moritz Hardt","Celestine Mendler-Dnner"],"pdf_url":"https://arxiv.org/pdf/2306.07951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08540v1","updated":"2023-10-12T17:32:09Z","published":"2023-10-12T17:32:09Z","title":"Do pretrained Transformers Really Learn In-context by Gradient Descent?","summary":"  Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?\nSeveral recent works draw analogies between the dynamics of GD and the emergent\nbehavior of ICL in large language models. However, these works make assumptions\nfar from the realistic natural language setting in which language models are\ntrained. Such discrepancies between theory and practice, therefore, necessitate\nfurther investigation to validate their applicability.\n  We start by highlighting the weaknesses in prior works that construct\nTransformer weights to simulate gradient descent. Their experiments with\ntraining Transformers on ICL objective, inconsistencies in the order\nsensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity\nto parameter changes are some examples of a mismatch from the real-world\nsetting.\n  Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural\nsetting. We conduct comprehensive empirical analyses on language models\npretrained on natural data (LLaMa-7B). Our comparisons on various performance\nmetrics highlight the inconsistent behavior of ICL and GD as a function of\nvarious factors such as datasets, models, and number of demonstrations. We\nobserve that ICL and GD adapt the output distribution of language models\ndifferently. These results indicate that the equivalence between ICL and GD is\nan open hypothesis, requires nuanced considerations and calls for further\nstudies.\n","authors":["Lingfeng Shen","Aayush Mishra","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2310.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05173v2","updated":"2023-10-12T17:25:44Z","published":"2023-09-11T00:02:05Z","title":"DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n","authors":["Zhengxiang Shi","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2309.05173v2.pdf","comment":"Code is available at https://github.com/ZhengxiangShi/DePT"},{"id":"http://arxiv.org/abs/2310.08535v1","updated":"2023-10-12T17:24:15Z","published":"2023-10-12T17:24:15Z","title":"Formally Specifying the High-Level Behavior of LLM-Based Agents","summary":"  LLM-based agents have recently emerged as promising tools for solving\nchallenging problems without the need for task-specific finetuned models that\ncan be expensive to procure. Currently, the design and implementation of such\nagents is ad hoc, as the wide variety of tasks that LLM-based agents may be\napplied to naturally means there can be no one-size-fits-all approach to agent\ndesign. In this work we aim to alleviate the difficulty of designing and\nimplementing new agents by proposing a minimalistic, high-level generation\nframework that simplifies the process of building agents. The framework we\nintroduce allows the user to specify desired agent behaviors in Linear Temporal\nLogic (LTL). The declarative LTL specification is then used to construct a\nconstrained decoder that guarantees the LLM will produce an output exhibiting\nthe desired behavior. By designing our framework in this way, we obtain several\nbenefits, including the ability to enforce complex agent behavior, the ability\nto formally validate prompt examples, and the ability to seamlessly incorporate\ncontent-focused logical constraints into generation. In particular, our\ndeclarative approach, in which the desired behavior is simply described without\nconcern for how it should be implemented or enforced, enables rapid design,\nimplementation and experimentation with different LLM-based agents. We\ndemonstrate how the proposed framework can be used to implement recent\nLLM-based agents, and show how the guardrails our approach provides can lead to\nimprovements in agent performance. In addition, we release our code for general\nuse.\n","authors":["Maxwell Crouse","Ibrahim Abdelaziz","Kinjal Basu","Soham Dan","Sadhana Kumaravel","Achille Fokoue","Pavan Kapanipathi","Luis Lastras"],"pdf_url":"https://arxiv.org/pdf/2310.08535v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2302.07863v4","updated":"2023-10-12T17:23:56Z","published":"2023-02-15T18:55:29Z","title":"Speculative Decoding with Big Little Decoder","summary":"  The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment and makes them prohibitively expensive for various real-time\napplications. The inference latency is further exacerbated by autoregressive\ngenerative tasks, as models need to run iteratively to generate tokens\nsequentially without leveraging token-level parallelization. To address this,\nwe propose Big Little Decoder (BiLD), a framework that can improve inference\nefficiency and latency for a wide range of text generation applications. The\nBiLD framework contains two models with different sizes that collaboratively\ngenerate text. The small model runs autoregressively to generate text with a\nlow inference cost, and the large model is only invoked occasionally to refine\nthe small model's inaccurate predictions in a non-autoregressive manner. To\ncoordinate the small and large models, BiLD introduces two simple yet effective\npolicies: (1) the fallback policy that determines when to hand control over to\nthe large model; and (2) the rollback policy that determines when the large\nmodel needs to correct the small model's inaccurate predictions. To evaluate\nour framework across different tasks and models, we apply BiLD to various text\ngeneration scenarios encompassing machine translation on IWSLT 2017 De-En and\nWMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4\nGPU, our framework achieves a speedup of up to 2.12x speedup with minimal\ngeneration quality degradation. Furthermore, our framework is fully\nplug-and-play and can be applied without any modifications in the training\nprocess or model architecture. Our code is open-sourced\n","authors":["Sehoon Kim","Karttikeya Mangalam","Suhong Moon","Jitendra Malik","Michael W. Mahoney","Amir Gholami","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2302.07863v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.08523v1","updated":"2023-10-12T17:17:27Z","published":"2023-10-12T17:17:27Z","title":"LLM-augmented Preference Learning from Natural Language","summary":"  Finding preferences expressed in natural language is an important but\nchallenging task. State-of-the-art(SotA) methods leverage transformer-based\nmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graph\nattention networks. Since Large Language Models (LLMs) are equipped to deal\nwith larger context lengths and have much larger model sizes than the\ntransformer-based model, we investigate their ability to classify comparative\ntext directly. This work aims to serve as a first step towards using LLMs for\nthe CPC task. We design and conduct a set of experiments that format the\nclassification task into an input prompt for the LLM and a methodology to get a\nfixed-format response that can be automatically evaluated. Comparing\nperformances with existing methods, we see that pre-trained LLMs are able to\noutperform the previous SotA models with no fine-tuning involved. Our results\nshow that the LLMs can consistently outperform the SotA when the target text is\nlarge -- i.e. composed of multiple sentences --, and are still comparable to\nthe SotA performance in shorter text. We also find that few-shot learning\nyields better performance than zero-shot learning.\n","authors":["Inwon Kang","Sikai Ruan","Tyler Ho","Jui-Chien Lin","Farhad Mohsin","Oshani Seneviratne","Lirong Xia"],"pdf_url":"https://arxiv.org/pdf/2310.08523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07580v2","updated":"2023-10-12T17:07:29Z","published":"2022-04-15T13:02:33Z","title":"mGPT: Few-Shot Learners Go Multilingual","summary":"  Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.\n","authors":["Oleh Shliazhko","Alena Fenogenova","Maria Tikhonova","Vladislav Mikhailov","Anastasia Kozlova","Tatiana Shavrina"],"pdf_url":"https://arxiv.org/pdf/2204.07580v2.pdf","comment":"Accepted for publication at Transactions of the Association for\n  Computational Linguistics (TACL) To be presented at the Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2310.08511v1","updated":"2023-10-12T17:06:19Z","published":"2023-10-12T17:06:19Z","title":"HoneyBee: Progressive Instruction Finetuning of Large Language Models\n  for Materials Science","summary":"  We propose an instruction-based process for trustworthy data curation in\nmaterials science (MatSci-Instruct), which we then apply to finetune a\nLLaMa-based language model targeted for materials science (HoneyBee).\nMatSci-Instruct helps alleviate the scarcity of relevant, high-quality\nmaterials science textual data available in the open literature, and HoneyBee\nis the first billion-parameter language model specialized to materials science.\nIn MatSci-Instruct we improve the trustworthiness of generated data by\nprompting multiple commercially available large language models for generation\nwith an Instructor module (e.g. Chat-GPT) and verification from an independent\nVerifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of\nmultiple tasks and measure the quality of our dataset along multiple\ndimensions, including accuracy against known facts, relevance to materials\nscience, as well as completeness and reasonableness of the data. Moreover, we\niteratively generate more targeted instructions and instruction-data in a\nfinetuning-evaluation-feedback loop leading to progressively better performance\nfor our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark\nshows HoneyBee's outperformance of existing language models on materials\nscience tasks and iterative improvement in successive stages of\ninstruction-data refinement. We study the quality of HoneyBee's language\nmodeling through automatic evaluation and analyze case studies to further\nunderstand the model's capabilities and limitations. Our code and relevant\ndatasets are publicly available at\n\\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.\n","authors":["Yu Song","Santiago Miret","Huan Zhang","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08496v1","updated":"2023-10-12T16:55:44Z","published":"2023-10-12T16:55:44Z","title":"The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and\n  POS","summary":"  Automatic analysis for modern Chinese has greatly improved the accuracy of\ntext mining in related fields, but the study of ancient Chinese is still\nrelatively rare. Ancient text division and lexical annotation are important\nparts of classical literature comprehension, and previous studies have tried to\nconstruct auxiliary dictionary and other fused knowledge to improve the\nperformance. In this paper, we propose a framework for ancient Chinese Word\nSegmentation and Part-of-Speech Tagging that makes a twofold effort: on the one\nhand, we try to capture the wordhood semantics; on the other hand, we\nre-predict the uncertain samples of baseline model by introducing external\nknowledge. The performance of our architecture outperforms pre-trained BERT\nwith CRF and existing tools such as Jiayan.\n","authors":["Pengyu Wang","Zhichen Ren"],"pdf_url":"https://arxiv.org/pdf/2310.08496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08491v1","updated":"2023-10-12T16:50:08Z","published":"2023-10-12T16:50:08Z","title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language\n  Models","summary":"  Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://github.com/kaistAI/Prometheus.\n","authors":["Seungone Kim","Jamin Shin","Yejin Cho","Joel Jang","Shayne Longpre","Hwaran Lee","Sangdoo Yun","Seongjin Shin","Sungdong Kim","James Thorne","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2310.08491v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2310.08487v1","updated":"2023-10-12T16:46:58Z","published":"2023-10-12T16:46:58Z","title":"GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language\n  Models","summary":"  While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.\n","authors":["Yuanchun Shen","Ruotong Liao","Zhen Han","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.08487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08483v1","updated":"2023-10-12T16:42:53Z","published":"2023-10-12T16:42:53Z","title":"Understanding the Humans Behind Online Misinformation: An Observational\n  Study Through the Lens of the COVID-19 Pandemic","summary":"  The proliferation of online misinformation has emerged as one of the biggest\nthreats to society. Considerable efforts have focused on building\nmisinformation detection models, still the perils of misinformation remain\nabound. Mitigating online misinformation and its ramifications requires a\nholistic approach that encompasses not only an understanding of its intricate\nlandscape in relation to the complex issue and topic-rich information ecosystem\nonline, but also the psychological drivers of individuals behind it. Adopting a\ntime series analytic technique and robust causal inference-based design, we\nconduct a large-scale observational study analyzing over 32 million COVID-19\ntweets and 16 million historical timeline tweets. We focus on understanding the\nbehavior and psychology of users disseminating misinformation during COVID-19\nand its relationship with the historical inclinations towards sharing\nmisinformation on Non-COVID topics before the pandemic. Our analysis\nunderscores the intricacies inherent to cross-topic misinformation, and\nhighlights that users' historical inclination toward sharing misinformation is\npositively associated with their present behavior pertaining to misinformation\nsharing on emergent topics and beyond. This work may serve as a valuable\nfoundation for designing user-centric inoculation strategies and\necologically-grounded agile interventions for effectively tackling online\nmisinformation.\n","authors":["Mohit Chandra","Anush Mattapalli","Munmun De Choudhury"],"pdf_url":"https://arxiv.org/pdf/2310.08483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08475v1","updated":"2023-10-12T16:32:44Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights\\footnote{Code\nand dataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08461v1","updated":"2023-10-12T16:21:04Z","published":"2023-10-12T16:21:04Z","title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation","summary":"  Speculative decoding (SD) accelerates large language model inference by\nemploying a faster draft model for generating multiple tokens, which are then\nverified in parallel by the larger target model, resulting in the text\ngenerated according to the target model distribution. However, identifying a\ncompact draft model that is well-aligned with the target model is challenging.\nTo tackle this issue, we propose DistillSpec that uses knowledge distillation\nto better align the draft model with the target model, before applying SD.\nDistillSpec makes two key design choices, which we demonstrate via systematic\nstudy to be crucial to improving the draft and target alignment: utilizing\non-policy data generation from the draft model, and tailoring the divergence\nfunction to the task and decoding strategy. Notably, DistillSpec yields\nimpressive 10 - 45% speedups over standard SD on a range of standard\nbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combine\nDistillSpec with lossy SD to achieve fine-grained control over the latency vs.\ntask performance trade-off. Finally, in practical scenarios with models of\nvarying sizes, first using distillation to boost the performance of the target\nmodel and then applying DistillSpec to train a well-aligned draft model can\nreduce decoding latency by 6-10x with minimal performance drop, compared to\nstandard decoding without distillation.\n","authors":["Yongchao Zhou","Kaifeng Lyu","Ankit Singh Rawat","Aditya Krishna Menon","Afshin Rostamizadeh","Sanjiv Kumar","Jean-Franois Kagy","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2310.08461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15363v3","updated":"2023-10-12T16:12:57Z","published":"2022-11-28T14:38:45Z","title":"On the Security Vulnerabilities of Text-to-SQL Models","summary":"  Although it has been demonstrated that Natural Language Processing (NLP)\nalgorithms are vulnerable to deliberate attacks, the question of whether such\nweaknesses can lead to software security threats is under-explored. To bridge\nthis gap, we conducted vulnerability tests on Text-to-SQL systems that are\ncommonly used to create natural language interfaces to databases. We showed\nthat the Text-to-SQL modules within six commercial applications can be\nmanipulated to produce malicious code, potentially leading to data breaches and\nDenial of Service attacks. This is the first demonstration that NLP models can\nbe exploited as attack vectors in the wild. In addition, experiments using four\nopen-source language models verified that straightforward backdoor attacks on\nText-to-SQL systems achieve a 100% success rate without affecting their\nperformance. The aim of this work is to draw the community's attention to\npotential software security issues associated with NLP algorithms and encourage\nexploration of methods to mitigate against them.\n","authors":["Xutan Peng","Yipeng Zhang","Jingfeng Yang","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2211.15363v3.pdf","comment":"ISSRE 2023: Best Paper Candidate"},{"id":"http://arxiv.org/abs/2305.14259v3","updated":"2023-10-12T16:10:51Z","published":"2023-05-23T17:12:08Z","title":"Learning to Generate Novel Scientific Directions with Contextualized\n  Literature-based Discovery","summary":"  Literature-Based Discovery (LBD) aims to discover new scientific knowledge by\nmining papers and generating hypotheses. Standard LBD is limited to predicting\npairwise relations between discrete concepts (e.g., drug-disease links), and\nignores critical contexts like experimental settings (e.g., a specific patient\npopulation where a drug is evaluated) and background motivations (e.g., to find\ndrugs without specific side effects). We address these limitations with a novel\nformulation of contextualized-LBD (C-LBD): generating scientific hypotheses in\nnatural language, while grounding them in a context that controls the\nhypothesis search space. We present a modeling framework using retrieval of\n``inspirations'' from past scientific papers. Our evaluations reveal that GPT-4\ntends to generate ideas with overall low technical depth and novelty, while our\ninspiration prompting approaches partially mitigate this issue. Our work\nrepresents a first step toward building language models that generate new ideas\nderived from scientific literature.\n","authors":["Qingyun Wang","Doug Downey","Heng Ji","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2305.14259v3.pdf","comment":"24 pages. Code and resource is available at\n  https://github.com/EagleW/CLBD"},{"id":"http://arxiv.org/abs/2307.07864v2","updated":"2023-10-12T16:06:19Z","published":"2023-07-15T18:25:56Z","title":"CIDER: Context sensitive sentiment analysis for short-form text","summary":"  Researchers commonly perform sentiment analysis on large collections of short\ntexts like tweets, Reddit posts or newspaper headlines that are all focused on\na specific topic, theme or event. Usually, general purpose sentiment analysis\nmethods are used which perform well on average but miss the variation in\nmeaning that happens across different contexts, for example, the word \"active\"\nhas a very different intention and valence in the phrase \"active lifestyle\"\nversus \"active volcano\". This work presents a new approach, CIDER (Context\nInformed Dictionary and sEntiment Reasoner), which performs context sensitive\nsentiment analysis, where the valence of sentiment laden terms is inferred from\nthe whole corpus before being used to score the individual texts. In this paper\nwe detail the CIDER algorithm and demonstrate that it outperforms\nstate-of-the-art generalist sentiment analysis on a large collection of tweets\nabout the weather. We have made our implementation of CIDER available as a\npython package: https://pypi.org/project/ciderpolarity/.\n","authors":["James C. Young","Rudy Arthur","Hywel T. P. Williams"],"pdf_url":"https://arxiv.org/pdf/2307.07864v2.pdf","comment":"12 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2310.04472v2","updated":"2023-10-12T15:57:04Z","published":"2023-10-06T04:48:48Z","title":"Effective Slogan Generation with Noise Perturbation","summary":"  Slogans play a crucial role in building the brand's identity of the firm. A\nslogan is expected to reflect firm's vision and brand's value propositions in\nmemorable and likeable ways. Automating the generation of slogans with such\ncharacteristics is challenging. Previous studies developted and tested slogan\ngeneration with syntactic control and summarization models which are not\ncapable of generating distinctive slogans. We introduce a a novel apporach that\nleverages pre-trained transformer T5 model with noise perturbation on newly\nproposed 1:N matching pair dataset. This approach serves as a contributing\nfator in generting distinctive and coherent slogans. Turthermore, the proposed\napproach incorporates descriptions about the firm and brand into the generation\nof slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine\nSimilarity metrics and also assess them with human subjects in terms of\nslogan's distinctiveness, coherence, and fluency. The results demonstrate that\nour approach yields better performance than baseline models and other\ntransformer-based models.\n","authors":["Jongeun Kim","MinChung Kim","Taehwan Kim"],"pdf_url":"https://arxiv.org/pdf/2310.04472v2.pdf","comment":"Accepted in CIKM 2023 short paper\n  https://github.com/joannekim0420/SloganGeneration"},{"id":"http://arxiv.org/abs/2310.08433v1","updated":"2023-10-12T15:56:24Z","published":"2023-10-12T15:56:24Z","title":"A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative\n  Writing","summary":"  We evaluate a range of recent LLMs on English creative writing, a challenging\nand complex task that requires imagination, coherence, and style. We use a\ndifficult, open-ended scenario chosen to avoid training data reuse: an epic\nnarration of a single combat between Ignatius J. Reilly, the protagonist of the\nPulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl,\na prehistoric flying reptile. We ask several LLMs and humans to write such a\nstory and conduct a human evalution involving various criteria such as fluency,\ncoherence, originality, humor, and style. Our results show that some\nstate-of-the-art commercial LLMs match or slightly outperform our writers in\nmost dimensions; whereas open-source LLMs lag behind. Humans retain an edge in\ncreativity, while humor shows a binary divide between LLMs that can handle it\ncomparably to humans and those that fail at it. We discuss the implications and\nlimitations of our study and suggest directions for future research.\n","authors":["Carlos Gmez-Rodrguez","Paul Williams"],"pdf_url":"https://arxiv.org/pdf/2310.08433v1.pdf","comment":"Accepted for publication in Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2302.11713v4","updated":"2023-10-12T15:30:41Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v4.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2310.08395v1","updated":"2023-10-12T15:08:14Z","published":"2023-10-12T15:08:14Z","title":"Prompting Large Language Models with Chain-of-Thought for Few-Shot\n  Knowledge Base Question Generation","summary":"  The task of Question Generation over Knowledge Bases (KBQG) aims to convert a\nlogical form into a natural language question. For the sake of expensive cost\nof large-scale question annotation, the methods of KBQG under low-resource\nscenarios urgently need to be developed. However, current methods heavily rely\non annotated data for fine-tuning, which is not well-suited for few-shot\nquestion generation. The emergence of Large Language Models (LLMs) has shown\ntheir impressive generalization ability in few-shot tasks. Inspired by\nChain-of-Thought (CoT) prompting, which is an in-context learning strategy for\nreasoning, we formulate KBQG task as a reasoning problem, where the generation\nof a complete question is splitted into a series of sub-question generation.\nOur proposed prompting method KQG-CoT first retrieves supportive logical forms\nfrom the unlabeled data pool taking account of the characteristics of the\nlogical form. Then, we write a prompt to explicit the reasoning chain of\ngenerating complicated questions based on the selected demonstrations. To\nfurther ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the\nlogical forms by their complexity. We conduct extensive experiments over three\npublic KBQG datasets. The results demonstrate that our prompting method\nconsistently outperforms other prompting baselines on the evaluated datasets.\nRemarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of\nthe PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,\nMETEOR, and ROUGE-L, respectively.\n","authors":["Yuanyuan Liang","Jianing Wang","Hanlun Zhu","Lei Wang","Weining Qian","Yunshi Lan"],"pdf_url":"https://arxiv.org/pdf/2310.08395v1.pdf","comment":"Accepted by EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.08394v1","updated":"2023-10-12T15:07:11Z","published":"2023-10-12T15:07:11Z","title":"Towards Better Evaluation of Instruction-Following: A Case-Study in\n  Summarization","summary":"  Despite recent advances, evaluating how well large language models (LLMs)\nfollow user instructions remains an open problem. While evaluation methods of\nlanguage models have seen a rise in prompt-based approaches, limited work on\nthe correctness of these methods has been conducted. In this work, we perform a\nmeta-evaluation of a variety of metrics to quantify how accurately they measure\nthe instruction-following abilities of LLMs. Our investigation is performed on\ngrounded query-based summarization by collecting a new short-form, real-world\ndataset riSum, containing $300$ document-instruction pairs with $3$ answers\neach. All $900$ answers are rated by $3$ human annotators. Using riSum, we\nanalyze agreement between evaluation methods and human judgment. Finally, we\npropose new LLM-based reference-free evaluation methods that improve upon\nestablished baselines and perform on-par with costly reference-based metrics\nwhich require high-quality summaries.\n","authors":["Ondrej Skopek","Rahul Aralikatte","Sian Gooding","Victor Carbune"],"pdf_url":"https://arxiv.org/pdf/2310.08394v1.pdf","comment":"Accepted to CoNLL 2023"},{"id":"http://arxiv.org/abs/2310.08383v1","updated":"2023-10-12T14:57:24Z","published":"2023-10-12T14:57:24Z","title":"Reconstructing Materials Tetrahedron: Challenges in Materials\n  Information Extraction","summary":"  Discovery of new materials has a documented history of propelling human\nprogress for centuries and more. The behaviour of a material is a function of\nits composition, structure, and properties, which further depend on its\nprocessing and testing conditions. Recent developments in deep learning and\nnatural language processing have enabled information extraction at scale from\npublished literature such as peer-reviewed publications, books, and patents.\nHowever, this information is spread in multiple formats, such as tables, text,\nand images, and with little or no uniformity in reporting style giving rise to\nseveral machine learning challenges. Here, we discuss, quantify, and document\nthese outstanding challenges in automated information extraction (IE) from\nmaterials science literature towards the creation of a large materials science\nknowledge base. Specifically, we focus on IE from text and tables and outline\nseveral challenges with examples. We hope the present work inspires researchers\nto address the challenges in a coherent fashion, providing to fillip to IE for\nthe materials knowledge base.\n","authors":["Kausik Hira","Mohd Zaki","Dhruvil Sheth"," Mausam","N M Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2310.08383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08372v1","updated":"2023-10-12T14:44:05Z","published":"2023-10-12T14:44:05Z","title":"Improving Factual Consistency for Knowledge-Grounded Dialogue Systems\n  via Knowledge Enhancement and Alignment","summary":"  Pretrained language models (PLMs) based knowledge-grounded dialogue systems\nare prone to generate responses that are factually inconsistent with the\nprovided knowledge source. In such inconsistent responses, the dialogue models\nfail to accurately express the external knowledge they rely upon. Inspired by\nprevious work which identified that feed-forward networks (FFNs) within\nTransformers are responsible for factual knowledge expressions, we investigate\ntwo methods to efficiently improve the factual expression capability {of FFNs}\nby knowledge enhancement and alignment respectively. We first propose\n\\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers\nto enhance factual knowledge expressions} given the specific patterns of\nknowledge-grounded dialogue inputs. Additionally, we apply the reinforcement\nlearning for factual consistency (RLFC) method to implicitly adjust FFNs'\nexpressions in responses by aligning with gold knowledge for the factual\nconsistency preference. To comprehensively assess the factual consistency and\ndialogue quality of responses, we employ extensive automatic measures and human\nevaluations including sophisticated fine-grained NLI-based metrics.\nExperimental results on WoW and CMU\\_DoG datasets demonstrate that our methods\nefficiently enhance the ability of the FFN module to convey factual knowledge,\nvalidating the efficacy of improving factual consistency for knowledge-grounded\ndialogue systems.\n","authors":["Boyang Xue","Weichao Wang","Hongru Wang","Fei Mi","Rui Wang","Yasheng Wang","Lifeng Shang","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2310.08372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08367v1","updated":"2023-10-12T14:38:25Z","published":"2023-10-12T14:38:25Z","title":"MCU: A Task-centric Framework for Open-ended Agent Evaluation in\n  Minecraft","summary":"  To pursue the goal of creating an open-ended agent in Minecraft, an\nopen-ended game environment with unlimited possibilities, this paper introduces\na task-centric framework named MCU for Minecraft agent evaluation. The MCU\nframework leverages the concept of atom tasks as fundamental building blocks,\nenabling the generation of diverse or even arbitrary tasks. Within the MCU\nframework, each task is measured with six distinct difficulty scores (time\nconsumption, operational effort, planning complexity, intricacy, creativity,\nnovelty). These scores offer a multi-dimensional assessment of a task from\ndifferent angles, and thus can reveal an agent's capability on specific facets.\nThe difficulty scores also serve as the feature of each task, which creates a\nmeaningful task space and unveils the relationship between tasks. For efficient\nevaluation of Minecraft agents employing the MCU framework, we maintain a\nunified benchmark, namely SkillForge, which comprises representative tasks with\ndiverse categories and difficulty distribution. We also provide convenient\nfilters for users to select tasks to assess specific capabilities of agents. We\nshow that MCU has the high expressivity to cover all tasks used in recent\nliterature on Minecraft agent, and underscores the need for advancements in\nareas such as creativity, precise control, and out-of-distribution\ngeneralization under the goal of open-ended Minecraft agent development.\n","authors":["Haowei Lin","Zihao Wang","Jianzhu Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03128v2","updated":"2023-10-12T14:37:55Z","published":"2023-10-04T19:39:26Z","title":"MetaTool Benchmark for Large Language Models: Deciding Whether to Use\n  Tools and Which to Use","summary":"  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving nine popular\nLLMs and find that the majority of them still struggle to effectively select\ntools, highlighting the existing gaps between LLMs and genuine intelligent\nagents. However, through the error analysis, we found there is still\nsignificant room for improvement. Finally, we conclude with insights for tool\ndevelopers that follow ChatGPT to provide detailed descriptions that can\nenhance the tool selection performance of LLMs.\n","authors":["Yue Huang","Jiawen Shi","Yuan Li","Chenrui Fan","Siyuan Wu","Qihui Zhang","Yixin Liu","Pan Zhou","Yao Wan","Neil Zhenqiang Gong","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2310.03128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08365v1","updated":"2023-10-12T14:36:13Z","published":"2023-10-12T14:36:13Z","title":"From Large Language Models to Knowledge Graphs for Biomarker Discovery\n  in Cancer","summary":"  Domain experts often rely on up-to-date knowledge for apprehending and\ndisseminating specific biological processes that help them design strategies to\ndevelop prevention and therapeutic decision-making. A challenging scenario for\nartificial intelligence (AI) is using biomedical data (e.g., texts, imaging,\nomics, and clinical) to provide diagnosis and treatment recommendations for\ncancerous conditions. Data and knowledge about cancer, drugs, genes, proteins,\nand their mechanism is spread across structured (knowledge bases (KBs)) and\nunstructured (e.g., scientific articles) sources. A large-scale knowledge graph\n(KG) can be constructed by integrating these data, followed by extracting facts\nabout semantically interrelated entities and relations. Such KGs not only allow\nexploration and question answering (QA) but also allow domain experts to deduce\nnew knowledge. However, exploring and querying large-scale KGs is tedious for\nnon-domain users due to a lack of understanding of the underlying data assets\nand semantic technologies. In this paper, we develop a domain KG to leverage\ncancer-specific biomarker discovery and interactive QA. For this, a domain\nontology called OncoNet Ontology (ONO) is developed to enable semantic\nreasoning for validating gene-disease relations. The KG is then enriched by\nharmonizing the ONO, controlled vocabularies, and additional biomedical\nconcepts from scientific articles by employing BioBERT- and SciBERT-based\ninformation extraction (IE) methods. Further, since the biomedical domain is\nevolving, where new findings often replace old ones, without employing\nup-to-date findings, there is a high chance an AI system exhibits concept drift\nwhile providing diagnosis and treatment. Therefore, we finetuned the KG using\nlarge language models (LLMs) based on more recent articles and KBs that might\nnot have been seen by the named entity recognition models.\n","authors":["Md. Rezaul Karim","Lina Molinas Comet","Md Shajalal","Oya Beyan","Dietrich Rebholz-Schuhmann","Stefan Decker"],"pdf_url":"https://arxiv.org/pdf/2310.08365v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04737"},{"id":"http://arxiv.org/abs/2304.11657v2","updated":"2023-10-12T13:57:58Z","published":"2023-04-23T13:54:39Z","title":"Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in\n  Large Language Models","summary":"  Large language models (LLMs) can achieve highly effective performance on\nvarious reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting as demonstrations. However, the reasoning chains of demonstrations\ngenerated by LLMs are prone to errors, which can subsequently lead to incorrect\nreasoning during inference. Furthermore, inappropriate exemplars (overly\nsimplistic or complex), can affect overall performance among varying levels of\ndifficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts\nPrompting), an iterative bootstrapping approach for selecting exemplars and\ngenerating reasoning chains. By utilizing iterative bootstrapping, our approach\nenables LLMs to autonomously rectify errors, resulting in more precise and\ncomprehensive reasoning chains. Simultaneously, our approach selects\nchallenging yet answerable questions accompanied by reasoning chains as\nexemplars with a moderate level of difficulty, which enhances the LLMs'\ngeneralizability across varying levels of difficulty. Experimental results\nindicate that Iter-CoT exhibits superiority, achieving competitive performance\nacross three distinct reasoning tasks on ten datasets.\n","authors":["Jiashuo Sun","Yi Luo","Yeyun Gong","Chen Lin","Yelong Shen","Jian Guo","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2304.11657v2.pdf","comment":"28 pages, 10 figures, 21 tables"},{"id":"http://arxiv.org/abs/2309.16396v3","updated":"2023-10-12T13:39:39Z","published":"2023-09-28T12:43:32Z","title":"A Comprehensive Survey of Document-level Relation Extraction (2016-2023)","summary":"  Document-level relation extraction (DocRE) is an active area of research in\nnatural language processing (NLP) concerned with identifying and extracting\nrelationships between entities beyond sentence boundaries. Compared to the more\ntraditional sentence-level relation extraction, DocRE provides a broader\ncontext for analysis and is more challenging because it involves identifying\nrelationships that may span multiple sentences or paragraphs. This task has\ngained increased interest as a viable solution to build and populate knowledge\nbases automatically from unstructured large-scale documents (e.g., scientific\npapers, legal contracts, or news articles), in order to have a better\nunderstanding of relationships between entities. This paper aims to provide a\ncomprehensive overview of recent advances in this field, highlighting its\ndifferent applications in comparison to sentence-level relation extraction.\n","authors":["Julien Delaunay","Hanh Thi Hong Tran","Carlos-Emiliano Gonzlez-Gallardo","Georgeta Bordea","Nicolas Sidere","Antoine Doucet"],"pdf_url":"https://arxiv.org/pdf/2309.16396v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v1","updated":"2023-10-12T13:33:04Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08309v1","updated":"2023-10-12T13:15:11Z","published":"2023-10-12T13:15:11Z","title":"Not All Demonstration Examples are Equally Beneficial: Reweighting\n  Demonstration Examples for In-Context Learning","summary":"  Large Language Models (LLMs) have recently gained the In-Context Learning\n(ICL) ability with the models scaling up, allowing them to quickly adapt to\ndownstream tasks with only a few demonstration examples prepended in the input\nsequence. Nonetheless, the current practice of ICL treats all demonstration\nexamples equally, which still warrants improvement, as the quality of examples\nis usually uneven. In this paper, we investigate how to determine approximately\noptimal weights for demonstration examples and how to apply them during ICL. To\nassess the quality of weights in the absence of additional validation data, we\ndesign a masked self-prediction (MSP) score that exhibits a strong correlation\nwith the final ICL performance. To expedite the weight-searching process, we\ndiscretize the continuous weight space and adopt beam search. With\napproximately optimal weights obtained, we further propose two strategies to\napply them to demonstrations at different model positions. Experimental results\non 8 text classification tasks show that our approach outperforms conventional\nICL by a large margin. Our code are publicly available at\nhttps:github.com/Zhe-Young/WICL.\n","authors":["Zhe Yang","Damai Dai","Peiyi Wang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2310.08309v1.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08298v1","updated":"2023-10-12T13:02:34Z","published":"2023-10-12T13:02:34Z","title":"MProto: Multi-Prototype Network with Denoised Optimal Transport for\n  Distantly Supervised Named Entity Recognition","summary":"  Distantly supervised named entity recognition (DS-NER) aims to locate entity\nmentions and classify their types with only knowledge bases or gazetteers and\nunlabeled corpus. However, distant annotations are noisy and degrade the\nperformance of NER models. In this paper, we propose a noise-robust prototype\nnetwork named MProto for the DS-NER task. Different from previous\nprototype-based NER methods, MProto represents each entity type with multiple\nprototypes to characterize the intra-class variance among entity\nrepresentations. To optimize the classifier, each token should be assigned an\nappropriate ground-truth prototype and we consider such token-prototype\nassignment as an optimal transport (OT) problem. Furthermore, to mitigate the\nnoise from incomplete labeling, we propose a novel denoised optimal transport\n(DOT) algorithm. Specifically, we utilize the assignment result between Other\nclass tokens and all prototypes to distinguish unlabeled entity tokens from\ntrue negatives. Experiments on several DS-NER benchmarks demonstrate that our\nMProto achieves state-of-the-art performance. The source code is now available\non Github.\n","authors":["Shuhui Wu","Yongliang Shen","Zeqi Tan","Wenqi Ren","Jietian Guo","Shiliang Pu","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2310.08298v1.pdf","comment":"Accepted to EMNLP-2023, camera ready version"},{"id":"http://arxiv.org/abs/2310.08291v1","updated":"2023-10-12T12:52:46Z","published":"2023-10-12T12:52:46Z","title":"Expanding the Vocabulary of BERT for Knowledge Base Construction","summary":"  Knowledge base construction entails acquiring structured information to\ncreate a knowledge base of factual and relational data, facilitating question\nanswering, information retrieval, and semantic understanding. The challenge\ncalled \"Knowledge Base Construction from Pretrained Language Models\" at\nInternational Semantic Web Conference 2023 defines tasks focused on\nconstructing knowledge base using language model. Our focus was on Track 1 of\nthe challenge, where the parameters are constrained to a maximum of 1 billion,\nand the inclusion of entity descriptions within the prompt is prohibited.\n  Although the masked language model offers sufficient flexibility to extend\nits vocabulary, it is not inherently designed for multi-token prediction. To\naddress this, we present Vocabulary Expandable BERT for knowledge base\nconstruction, which expand the language model's vocabulary while preserving\nsemantic embeddings for newly added words. We adopt task-specific\nre-pre-training on masked language model to further enhance the language model.\n  Through experimentation, the results show the effectiveness of our\napproaches. Our framework achieves F1 score of 0.323 on the hidden test set and\n0.362 on the validation set, both data set is provided by the challenge.\nNotably, our framework adopts a lightweight language model (BERT-base, 0.13\nbillion parameters) and surpasses the model using prompts directly on large\nlanguage model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode\nachieves comparable performances as Re-pretrain. This research advances\nlanguage understanding models by enabling the direct embedding of multi-token\nentities, signifying a substantial step forward in link prediction task in\nknowledge graph and metadata completion in data management.\n","authors":["Dong Yang","Xu Wang","Remzi Celebi"],"pdf_url":"https://arxiv.org/pdf/2310.08291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08280v1","updated":"2023-10-12T12:36:46Z","published":"2023-10-12T12:36:46Z","title":"Optimizing Odia Braille Literacy: The Influence of Speed on Error\n  Reduction and Enhanced Comprehension","summary":"  This study aims to conduct an extensive detailed analysis of the Odia Braille\nreading comprehension among students with visual disability. Specifically, the\nstudy explores their reading speed and hand or finger movements. The study also\naims to investigate any comprehension difficulties and reading errors they may\nencounter. Six students from the 9th and 10th grades, aged between 14 and 16,\nparticipated in the study. We observed participants hand movements to\nunderstand how reading errors were connected to hand movement and identify the\nstudents reading difficulties. We also evaluated the participants Odia Braille\nreading skills, including their reading speed (in words per minute), errors,\nand comprehension. The average speed of Odia Braille reader is 17.64wpm.\nAccording to the study, there was a noticeable correlation between reading\nspeed and reading errors. As reading speed decreased, the number of reading\nerrors tended to increase. Moreover, the study established a link between\nreduced Braille reading errors and improved reading comprehension. In contrast,\nthe study found that better comprehension was associated with increased reading\nspeed. The researchers concluded with some interesting findings about preferred\nBraille reading patterns. These findings have important theoretical,\ndevelopmental, and methodological implications for instruction.\n","authors":["Monnie Parida","Manjira Sinha","Anupam Basu","Pabitra Mitra"],"pdf_url":"https://arxiv.org/pdf/2310.08280v1.pdf","comment":"4 Pages, Paper accepted in Diversity and Inclusion track at\n  CODS-COMAD 2024"},{"id":"http://arxiv.org/abs/2310.08279v1","updated":"2023-10-12T12:31:23Z","published":"2023-10-12T12:31:23Z","title":"CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large\n  Language Models","summary":"  Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce\nand infer missing connections within knowledge graphs. Text-based approaches,\nlike SimKGC, have outperformed graph embedding methods, showcasing the promise\nof inductive KGC. However, the efficacy of text-based methods hinges on the\nquality of entity textual descriptions. In this paper, we identify the key\nissue of whether large language models (LLMs) can generate effective text. To\nmitigate hallucination in LLM-generated text in this paper, we introduce a\nconstraint-based prompt that utilizes the entity and its textual description as\ncontextual constraints to enhance data quality. Our Constrained-Prompt\nKnowledge Graph Completion (CP-KGC) method demonstrates effective inference\nunder low resource computing conditions and surpasses prior results on the\nWN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC\ntasks and provides new directions for future research.\n","authors":["Rui Yang","Li Fang","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12053v3","updated":"2023-10-12T12:06:37Z","published":"2023-09-21T13:20:13Z","title":"AceGPT, Localizing Large Language Models in Arabic","summary":"  This paper is devoted to the development of a localized Large Language Model\n(LLM) specifically for Arabic, a language imbued with unique cultural\ncharacteristics inadequately addressed by current mainstream models.\nSignificant concerns emerge when addressing cultural sensitivity and local\nvalues. To address this, the paper proposes a comprehensive solution that\nincludes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)\nutilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside\nReinforcement Learning with AI Feedback (RLAIF) employing a reward model\nattuned to local culture and values. The goal is to cultivate culturally\ncognizant and value-aligned Arabic LLMs capable of accommodating the diverse,\napplication-specific needs of Arabic-speaking communities. Comprehensive\nevaluations reveal that the resulting model, dubbed 'AceGPT', sets the\nstate-of-the-art standard for open Arabic LLMs across various benchmarks,\nincluding the instruction-following benchmark (i.e., Arabic Vicuna-80 and\nArabic AlpacaEval), knowledge benchmark (i.e., Arabic MMLU and EXAMs), and the\nnewly introduced Arabic Cultural and Value Alignment benchmark. Notably, AceGPT\noutperforms Turbo in the popular Vicuna-80 benchmark when evaluated with GPT-4,\ndespite the benchmark's limited scale. Codes, data, and models are in\nhttps://github.com/FreedomIntelligence/AceGPT.\n","authors":["Huang Huang","Fei Yu","Jianqing Zhu","Xuening Sun","Hao Cheng","Dingjie Song","Zhihong Chen","Abdulmohsen Alharthi","Bang An","Ziche Liu","Zhiyi Zhang","Junying Chen","Jianquan Li","Benyou Wang","Lian Zhang","Ruoyu Sun","Xiang Wan","Haizhou Li","Jinchao Xu"],"pdf_url":"https://arxiv.org/pdf/2309.12053v3.pdf","comment":"https://github.com/FreedomIntelligence/AceGPT"},{"id":"http://arxiv.org/abs/2310.08256v1","updated":"2023-10-12T12:01:32Z","published":"2023-10-12T12:01:32Z","title":"Impact of Co-occurrence on Factual Knowledge of Large Language Models","summary":"  Large language models (LLMs) often make factually incorrect responses despite\ntheir success in various applications. In this paper, we hypothesize that\nrelying heavily on simple co-occurrence statistics of the pre-training corpora\nis one of the main factors that cause factual errors. Our results reveal that\nLLMs are vulnerable to the co-occurrence bias, defined as preferring frequently\nco-occurred words over the correct answer. Consequently, LLMs struggle to\nrecall facts whose subject and object rarely co-occur in the pre-training\ndataset although they are seen during finetuning. We show that co-occurrence\nbias remains despite scaling up model sizes or finetuning. Therefore, we\nsuggest finetuning on a debiased dataset to mitigate the bias by filtering out\nbiased samples whose subject-object co-occurrence count is high. Although\ndebiased finetuning allows LLMs to memorize rare facts in the training set, it\nis not effective in recalling rare facts unseen during finetuning. Further\nresearch in mitigation will help build reliable language models by preventing\npotential errors. The code is available at\n\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.\n","authors":["Cheongwoong Kang","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2310.08256v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2309.02092v3","updated":"2023-10-12T11:46:23Z","published":"2023-09-05T09:56:29Z","title":"Where are We in Event-centric Emotion Analysis? Bridging Emotion Role\n  Labeling and Appraisal-based Approaches","summary":"  The term emotion analysis in text subsumes various natural language\nprocessing tasks which have in common the goal to enable computers to\nunderstand emotions. Most popular is emotion classification in which one or\nmultiple emotions are assigned to a predefined textual unit. While such setting\nis appropriate for identifying the reader's or author's emotion, emotion role\nlabeling adds the perspective of mentioned entities and extracts text spans\nthat correspond to the emotion cause. The underlying emotion theories agree on\none important point; that an emotion is caused by some internal or external\nevent and comprises several subcomponents, including the subjective feeling and\na cognitive evaluation. We therefore argue that emotions and events are related\nin two ways. (1) Emotions are events; and this perspective is the fundament in\nnatural language processing for emotion role labeling. (2) Emotions are caused\nby events; a perspective that is made explicit with research how to incorporate\npsychological appraisal theories in NLP models to interpret events. These two\nresearch directions, role labeling and (event-focused) emotion classification,\nhave by and large been tackled separately. In this paper, we contextualize both\nperspectives and discuss open research questions.\n","authors":["Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2309.02092v3.pdf","comment":"accepted to the Big Picture Workshop\n  (https://bigpictureworkshop.com/)"},{"id":"http://arxiv.org/abs/2305.15020v2","updated":"2023-10-12T11:45:56Z","published":"2023-05-24T11:00:33Z","title":"An Efficient Multilingual Language Model Compression through Vocabulary\n  Trimming","summary":"  Multilingual language model (LM) have become a powerful tool in NLP\nespecially for non-English languages. Nevertheless, model parameters of\nmultilingual LMs remain large due to the larger embedding matrix of the\nvocabulary covering tokens in different languages. On the contrary, monolingual\nLMs can be trained in a target language with the language-specific vocabulary\nonly, but this requires a large budget and availability of reliable corpora to\nachieve a high-quality LM from scratch. In this paper, we propose\nvocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a\ntarget language by deleting irrelevant tokens from its vocabulary. In theory,\nVT can compress any existing multilingual LM to build monolingual LMs in any\nlanguage covered by the multilingual LM. In our experiments, we show that VT\ncan retain the original performance of the multilingual LM, while being smaller\nin size (in general around 50% of the original vocabulary size is enough) than\nthe original multilingual LM. The evaluation is performed over four NLP tasks\n(two generative and two classification tasks) among four widely used\nmultilingual LMs in seven languages. Finally, we show that this methodology can\nkeep the best of both monolingual and multilingual worlds by keeping a small\nsize as monolingual models without the need for specifically retraining them,\nand even limiting potentially harmful social biases.\n","authors":["Asahi Ushio","Yi Zhou","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2305.15020v2.pdf","comment":"EMNLP 2023 findings"},{"id":"http://arxiv.org/abs/2304.00457v3","updated":"2023-10-12T11:35:35Z","published":"2023-04-02T05:47:09Z","title":"LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the dataset, which often covers an\nentire field. This field-based evaluation, is problematic with respect to\ntransparency and model improvement. A stratified evaluation could instead\nreveal subfields, where hallucinations are more likely to occur and thus help\nto better assess LLMs' risks and guide their further development. To support\nsuch stratified evaluations, we propose LLMMaps as a novel visualization\ntechnique that enables users to evaluate LLMs' performance with respect to Q&A\ndatasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities\nin different subfields, by transforming Q&A datasets as well as LLM responses\ninto an internal knowledge structure. An extension for comparative\nvisualization furthermore, allows for the detailed comparison of multiple LLMs.\nTo assess LLMMaps we use them to conduct a comparative analysis of several\nstate-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as\nwell as two qualitative user evaluations. All necessary source code and data\nfor generating LLMMaps to be used in scientific publications and elsewhere is\navailable on GitHub: https://github.com/viscom-ulm/LLMMaps\n","authors":["Patrik Puchert","Poonam Poonam","Christian van Onzenoodt","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2304.00457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08240v1","updated":"2023-10-12T11:35:24Z","published":"2023-10-12T11:35:24Z","title":"Who Said That? Benchmarking Social Media AI Detection","summary":"  AI-generated text has proliferated across various online platforms, offering\nboth transformative prospects and posing significant risks related to\nmisinformation and manipulation. Addressing these challenges, this paper\nintroduces SAID (Social media AI Detection), a novel benchmark developed to\nassess AI-text detection models' capabilities in real social media platforms.\nIt incorporates real AI-generate text from popular social media platforms like\nZhihu and Quora. Unlike existing benchmarks, SAID deals with content that\nreflects the sophisticated strategies employed by real AI users on the Internet\nwhich may evade detection or gain visibility, providing a more realistic and\nchallenging evaluation landscape. A notable finding of our study, based on the\nZhihu dataset, reveals that annotators can distinguish between AI-generated and\nhuman-generated texts with an average accuracy rate of 96.5%. This finding\nnecessitates a re-evaluation of human capability in recognizing AI-generated\ntext in today's widely AI-influenced environment. Furthermore, we present a new\nuser-oriented AI-text detection challenge focusing on the practicality and\neffectiveness of identifying AI-generated text based on user information and\nmultiple responses. The experimental results demonstrate that conducting\ndetection tasks on actual social media platforms proves to be more challenging\ncompared to traditional simulated AI-text detection, resulting in a decreased\naccuracy. On the other hand, user-oriented AI-generated text detection\nsignificantly improve the accuracy of detection.\n","authors":["Wanyun Cui","Linqiu Zhang","Qianle Wang","Shuyang Cai"],"pdf_url":"https://arxiv.org/pdf/2310.08240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08232v1","updated":"2023-10-12T11:25:46Z","published":"2023-10-12T11:25:46Z","title":"Language Models are Universal Embedders","summary":"  In the large language model (LLM) revolution, embedding is a key component of\nvarious systems. For example, it is used to retrieve knowledge or memories for\nLLMs, to build content moderation filters, etc. As such cases span from English\nto other natural or programming languages, from retrieval to classification and\nbeyond, it is desirable to build a unified embedding model rather than\ndedicated ones for each scenario. In this work, we make an initial step towards\nthis goal, demonstrating that multiple languages (both natural and programming)\npre-trained transformer decoders can embed universally when finetuned on\nlimited English data. We provide a comprehensive practice with thorough\nevaluations. On English MTEB, our models achieve competitive performance on\ndifferent embedding tasks by minimal training data. On other benchmarks, such\nas multilingual classification and code search, our models (without any\nsupervision) perform comparably to, or even surpass heavily supervised\nbaselines and/or APIs. These results provide evidence of a promising path\ntowards building powerful unified embedders that can be applied across tasks\nand languages.\n","authors":["Xin Zhang","Zehan Li","Yanzhao Zhang","Dingkun Long","Pengjun Xie","Meishan Zhang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08232v1.pdf","comment":"13 pages, in progress"},{"id":"http://arxiv.org/abs/2310.08225v1","updated":"2023-10-12T11:17:40Z","published":"2023-10-12T11:17:40Z","title":"Fast Word Error Rate Estimation Using Self-Supervised Representations\n  For Speech And Text","summary":"  The quality of automatic speech recognition (ASR) is typically measured by\nword error rate (WER). WER estimation is a task aiming to predict the WER of an\nASR system, given a speech utterance and a transcription. This task has gained\nincreasing attention while advanced ASR systems are trained on large amounts of\ndata. In this case, WER estimation becomes necessary in many scenarios, for\nexample, selecting training data with unknown transcription quality or\nestimating the testing performance of an ASR system without ground truth\ntranscriptions. Facing large amounts of data, the computation efficiency of a\nWER estimator becomes essential in practical applications. However, previous\nworks usually did not consider it as a priority. In this paper, a Fast WER\nestimator (Fe-WER) using self-supervised learning representation (SSLR) is\nintroduced. The estimator is built upon SSLR aggregated by average pooling. The\nresults show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%\nand 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and\nPearson correlation coefficient, respectively. Moreover, the estimation\nweighted by duration was 10.43% when the target was 10.88%. Lastly, the\ninference speed was about 4x in terms of a real-time factor.\n","authors":["Chanho Park","Chengsong Lu","Mingjie Chen","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2310.08225v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.08221v1","updated":"2023-10-12T11:11:54Z","published":"2023-10-12T11:11:54Z","title":"SimCKP: Simple Contrastive Learning of Keyphrase Representations","summary":"  Keyphrase generation (KG) aims to generate a set of summarizing words or\nphrases given a source document, while keyphrase extraction (KE) aims to\nidentify them from the text. Because the search space is much smaller in KE, it\nis often combined with KG to predict keyphrases that may or may not exist in\nthe corresponding document. However, current unified approaches adopt sequence\nlabeling and maximization-based generation that primarily operate at a token\nlevel, falling short in observing and scoring keyphrases as a whole. In this\nwork, we propose SimCKP, a simple contrastive learning framework that consists\nof two stages: 1) An extractor-generator that extracts keyphrases by learning\ncontext-aware phrase-level representations in a contrastive manner while also\ngenerating keyphrases that do not appear in the document; 2) A reranker that\nadapts scores for each generated phrase by likewise aligning their\nrepresentations with the corresponding document. Experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our proposed\napproach, which outperforms the state-of-the-art models by a significant\nmargin.\n","authors":["Minseok Choi","Chaeheon Gwak","Seho Kim","Si Hyeong Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2310.08221v1.pdf","comment":"Accepted to Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.16292v2","updated":"2023-10-12T11:11:47Z","published":"2023-09-28T09:41:35Z","title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large\n  Language Models","summary":"  Recent advancements in autonomous driving have relied on data-driven\napproaches, which are widely adopted but face challenges including dataset\nbias, overfitting, and uninterpretability. Drawing inspiration from the\nknowledge-driven nature of human driving, we explore the question of how to\ninstill similar capabilities into autonomous driving systems and summarize a\nparadigm that integrates an interactive environment, a driver agent, as well as\na memory component to address this question. Leveraging large language models\nwith emergent abilities, we propose the DiLu framework, which combines a\nReasoning and a Reflection module to enable the system to perform\ndecision-making based on common-sense knowledge and evolve continuously.\nExtensive experiments prove DiLu's capability to accumulate experience and\ndemonstrate a significant advantage in generalization ability over\nreinforcement learning-based methods. Moreover, DiLu is able to directly\nacquire experiences from real-world datasets which highlights its potential to\nbe deployed on practical autonomous driving systems. To the best of our\nknowledge, we are the first to instill knowledge-driven capability into\nautonomous driving systems from the perspective of how humans drive.\n","authors":["Licheng Wen","Daocheng Fu","Xin Li","Xinyu Cai","Tao Ma","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2309.16292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08187v1","updated":"2023-10-12T10:26:26Z","published":"2023-10-12T10:26:26Z","title":"Visual Question Generation in Bengali","summary":"  The task of Visual Question Generation (VQG) is to generate human-like\nquestions relevant to the given image. As VQG is an emerging research field,\nexisting works tend to focus only on resource-rich language such as English due\nto the availability of datasets. In this paper, we propose the first Bengali\nVisual Question Generation task and develop a novel transformer-based\nencoder-decoder architecture that generates questions in Bengali when given an\nimage. We propose multiple variants of models - (i) image-only: baseline model\nof generating questions from images without additional information, (ii)\nimage-category and image-answer-category: guided VQG where we condition the\nmodel to generate questions based on the answer and the category of expected\nquestion. These models are trained and evaluated on the translated VQAv2.0\ndataset. Our quantitative and qualitative results establish the first state of\nthe art models for VQG task in Bengali and demonstrate that our models are\ncapable of generating grammatically correct and relevant questions. Our\nquantitative results show that our image-cat model achieves a BLUE-1 score of\n33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants.\nWe also perform a human evaluation to assess the quality of the generation\ntasks. Human evaluation suggests that image-cat model is capable of generating\ngoal-driven and attribute-specific questions and also stays relevant to the\ncorresponding image.\n","authors":["Mahmud Hasan","Labiba Islam","Jannatul Ferdous Ruma","Tasmiah Tahsin Mayeesha","Rashedur M. Rahman"],"pdf_url":"https://arxiv.org/pdf/2310.08187v1.pdf","comment":"19 pages including references, 4 figures and 3 tables. Accepted in\n  the Proceedings of the Workshop on Multimodal, Multilingual Natural Language\n  Generation and Multilingual WebNLG Challenge (MM-NLG 2023)"},{"id":"http://arxiv.org/abs/2310.08185v1","updated":"2023-10-12T10:21:37Z","published":"2023-10-12T10:21:37Z","title":"EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form\n  Narrative Text Generation","summary":"  Plan-and-Write is a common hierarchical approach in long-form narrative text\ngeneration, which first creates a plan to guide the narrative writing.\nFollowing this approach, several studies rely on simply prompting large\nlanguage models for planning, which often yields suboptimal results. In this\npaper, we propose a new framework called Evaluation-guided Iterative Plan\nExtraction for long-form narrative text generation (EIPE-text), which extracts\nplans from the corpus of narratives and utilizes the extracted plans to\nconstruct a better planner. EIPE-text has three stages: plan extraction,\nlearning, and inference. In the plan extraction stage, it iteratively extracts\nand improves plans from the narrative corpus and constructs a plan corpus. We\npropose a question answer (QA) based evaluation mechanism to automatically\nevaluate the plans and generate detailed plan refinement instructions to guide\nthe iterative improvement. In the learning stage, we build a better planner by\nfine-tuning with the plan corpus or in-context learning with examples in the\nplan corpus. Finally, we leverage a hierarchical approach to generate long-form\nnarratives. We evaluate the effectiveness of EIPE-text in the domains of novels\nand storytelling. Both GPT-4-based evaluations and human evaluations\ndemonstrate that our method can generate more coherent and relevant long-form\nnarratives. Our code will be released in the future.\n","authors":["Wang You","Wenshan Wu","Yaobo Liang","Shaoguang Mao","Chenfei Wu","Maosong Cao","Yuzhe Cai","Yiduo Guo","Yan Xia","Furu Wei","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2310.08185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08172v1","updated":"2023-10-12T09:55:45Z","published":"2023-10-12T09:55:45Z","title":"Exploring the Cognitive Knowledge Structure of Large Language Models: An\n  Educational Diagnostic Assessment Approach","summary":"  Large Language Models (LLMs) have not only exhibited exceptional performance\nacross various tasks, but also demonstrated sparks of intelligence. Recent\nstudies have focused on assessing their capabilities on human exams and\nrevealed their impressive competence in different domains. However, cognitive\nresearch on the overall knowledge structure of LLMs is still lacking. In this\npaper, based on educational diagnostic assessment method, we conduct an\nevaluation using MoocRadar, a meticulously annotated human test dataset based\non Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain\ninsights of their cognitive capabilities. This research emphasizes the\nsignificance of investigating LLMs' knowledge and understanding the disparate\ncognitive patterns of LLMs. By shedding light on models' knowledge, researchers\ncan advance development and utilization of LLMs in a more informed and\neffective manner.\n","authors":["Zheyuan Zhang","Jifan Yu","Juanzi Li","Lei Hou"],"pdf_url":"https://arxiv.org/pdf/2310.08172v1.pdf","comment":"Findings of EMNLP 2023 (Short Paper)"},{"id":"http://arxiv.org/abs/2310.08170v1","updated":"2023-10-12T09:49:10Z","published":"2023-10-12T09:49:10Z","title":"Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for\n  Sentence Simplification","summary":"  Automatic evaluation for sentence simplification remains a challenging\nproblem. Most popular evaluation metrics require multiple high-quality\nreferences -- something not readily available for simplification -- which makes\nit difficult to test performance on unseen domains. Furthermore, most existing\nmetrics conflate simplicity with correlated attributes such as fluency or\nmeaning preservation. We propose a new learned evaluation metric (SLE) which\nfocuses on simplicity, outperforming almost all existing metrics in terms of\ncorrelation with human judgements.\n","authors":["Liam Cripwell","Jol Legrand","Claire Gardent"],"pdf_url":"https://arxiv.org/pdf/2310.08170v1.pdf","comment":"Accepted to EMNLP 2023 (Main Conference)"},{"id":"http://arxiv.org/abs/2310.08167v1","updated":"2023-10-12T09:41:22Z","published":"2023-10-12T09:41:22Z","title":"Multiclass Classification of Policy Documents with Large Language Models","summary":"  Classifying policy documents into policy issue topics has been a long-time\neffort in political science and communication disciplines. Efforts to automate\ntext classification processes for social science research purposes have so far\nachieved remarkable results, but there is still a large room for progress. In\nthis work, we test the prediction performance of an alternative strategy, which\nrequires human involvement much less than full manual coding. We use the GPT\n3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned\nLarge Language Models (LLM), to classify congressional bills and congressional\nhearings into Comparative Agendas Project's 21 major policy issue topics. We\npropose three use-case scenarios and estimate overall accuracies ranging from\n%58-83 depending on scenario and GPT model employed. The three scenarios aims\nat minimal, moderate, and major human interference, respectively. Overall, our\nresults point towards the insufficiency of complete reliance on GPT with\nminimal human intervention, an increasing accuracy along with the human effort\nexerted, and a surprisingly high accuracy achieved in the most humanly\ndemanding use-case. However, the superior use-case achieved the %83 accuracy on\nthe %65 of the data in which the two models agreed, suggesting that a similar\napproach to ours can be relatively easily implemented and allow for mostly\nautomated coding of a majority of a given dataset. This could free up resources\nallowing manual human coding of the remaining %35 of the data to achieve an\noverall higher level of accuracy while reducing costs significantly.\n","authors":["Erkan Gunes","Christoffer Koch Florczak"],"pdf_url":"https://arxiv.org/pdf/2310.08167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08166v1","updated":"2023-10-12T09:39:17Z","published":"2023-10-12T09:39:17Z","title":"Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task\n  Instruction Tuning","summary":"  Recent advancements enlarge the capabilities of large language models (LLMs)\nin zero-shot image-to-text generation and understanding by integrating\nmulti-modal inputs. However, such success is typically limited to English\nscenarios due to the lack of large-scale and high-quality non-English\nmulti-modal resources, making it extremely difficult to establish competitive\ncounterparts in other languages. In this paper, we introduce the Ziya-VL\nseries, a set of bilingual large-scale vision-language models (LVLMs) designed\nto incorporate visual semantics into LLM for multi-modal dialogue. Composed of\nZiya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from\nBLIP-2, further exploring the assistance of optimization schemes such as\ninstruction tuning, multi-stage training and low-rank adaptation module for\nvisual-language alignment. In addition, we stimulate the understanding ability\nof GPT-4 in multi-modal scenarios, translating our gathered English image-text\ndatasets into Chinese and generating instruction-response through the\nin-context learning method. The experiment results demonstrate that compared to\nthe existing LVLMs, Ziya-VL achieves competitive performance across a wide\nrange of English-only tasks including zero-shot image-text retrieval, image\ncaptioning, and visual question answering. The evaluation leaderboard accessed\nby GPT-4 also indicates that our models possess satisfactory image-text\nunderstanding and generation capabilities in Chinese multi-modal scenario\ndialogues. Code, demo and models are available at\n~\\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.\n","authors":["Junyu Lu","Dixiang Zhang","Xiaojun Wu","Xinyu Gao","Ruyi Gan","Jiaxing Zhang","Yan Song","Pingjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08152v1","updated":"2023-10-12T09:18:19Z","published":"2023-10-12T09:18:19Z","title":"Context Compression for Auto-regressive Transformers with Sentinel\n  Tokens","summary":"  The quadratic complexity of the attention module makes it gradually become\nthe bulk of compute in Transformer-based LLMs during generation. Moreover, the\nexcessive key-value cache that arises when dealing with long inputs also brings\nsevere issues on memory footprint and inference latency. In this work, we\npropose a plug-and-play approach that is able to incrementally compress the\nintermediate activation of a specified span of tokens into compact ones,\nthereby reducing both memory and computational cost when processing subsequent\ncontext. Experiments on both in-domain language modeling and zero-shot\nopen-ended document generation demonstrate the advantage of our approach over\nsparse attention baselines in terms of fluency, n-gram matching, and semantic\nsimilarity. At last, we comprehensively profile the benefit of context\ncompression on improving the system throughout. Code is available at\nhttps://github.com/DRSY/KV_Compression.\n","authors":["Siyu Ren","Qi Jia","Kenny Q. Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.08152v1.pdf","comment":"To appear at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05199v2","updated":"2023-10-12T09:04:07Z","published":"2023-10-08T15:14:39Z","title":"Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning\n  from Human Feedback","summary":"  Reinforcement learning from human feedback serves as a crucial bridge,\naligning large language models with human and societal values. This alignment\nrequires a vast corpus of human feedback to learn a reward model, which is\nsubsequently used to finetune language models. However, we have identified that\nthe reward model often finds shortcuts to bypass its intended objectives,\nmisleadingly assuming that humans prefer longer responses. The emergence of\nlength bias often induces the model to favor longer outputs, yet it doesn't\nequate to an increase in helpful information within these outputs. In this\npaper, we propose an innovative solution, applying the Product-of-Experts (PoE)\ntechnique to separate reward modeling from the influence of sequence length. In\nour framework, the main expert concentrates on understanding human intents,\nwhile the biased expert targets the identification and capture of length bias.\nTo further enhance the learning of bias, we introduce perturbations into the\nbias-focused expert, disrupting the flow of semantic information. Experimental\nresults validate the effectiveness of our approach, indicating that language\nmodel performance is improved, irrespective of sequence length.\n","authors":["Wei Shen","Rui Zheng","Wenyu Zhan","Jun Zhao","Shihan Dou","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11534v4","updated":"2023-10-12T08:50:19Z","published":"2023-08-21T06:51:56Z","title":"PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator","summary":"  The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, due to challenges\nin gathering conversations involving human participation, current endeavors\nlike Baize and UltraChat aim to automatically generate conversational data.\nThey primarily rely on ChatGPT conducting roleplay to simulate human behaviors\nbased on instructions rather than genuine learning from humans, resulting in\nlimited scope, diminished diversity, and an absence of genuine multi-round\nconversational dynamics. To address the above issues, we target human questions\nextracted from genuine human-machine conversations as a learning goal and train\na user simulator called `Socratic' to produce a high-quality human-centric\nsynthetic conversation dataset. Subsequently, this dataset was used to train\nour assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline\nmodels in both Vicuna-Bench and MT-Bench by pairwise comparison when\nconsidering equivalent training set sizes, and manual evaluation also shows\nthat our model is highly competitive. Impressively, when fine-tuned with the\nlatest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models\n(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in\nAlpaca-Eval benchmark, it ranks second among 7B models, even beating some\nlarger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth\nanalysis demonstrates the scalability and transferability of our approach. The\ncode is available at https://github.com/FreedomIntelligence/PlatoLM.\n","authors":["Chuyi Kong","Yaxin Fan","Xiang Wan","Feng Jiang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11534v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08132v1","updated":"2023-10-12T08:45:21Z","published":"2023-10-12T08:45:21Z","title":"On the Relevance of Phoneme Duration Variability of Synthesized Training\n  Data for Automatic Speech Recognition","summary":"  Synthetic data generated by text-to-speech (TTS) systems can be used to\nimprove automatic speech recognition (ASR) systems in low-resource or domain\nmismatch tasks. It has been shown that TTS-generated outputs still do not have\nthe same qualities as real data. In this work we focus on the temporal\nstructure of synthetic data and its relation to ASR training. By using a novel\noracle setup we show how much the degradation of synthetic data quality is\ninfluenced by duration modeling in non-autoregressive (NAR) TTS. To get\nreference phoneme durations we use two common alignment methods, a hidden\nMarkov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist\ntemporal classification (CTC) aligner. Using a simple algorithm based on random\nwalks we shift phoneme duration distributions of the TTS system closer to real\ndurations, resulting in an improvement of an ASR system using synthetic data in\na semi-supervised setting.\n","authors":["Nick Rossenbach","Benedikt Hilmes","Ralf Schlter"],"pdf_url":"https://arxiv.org/pdf/2310.08132v1.pdf","comment":"To appear at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.08130v1","updated":"2023-10-12T08:38:12Z","published":"2023-10-12T08:38:12Z","title":"Fine-grained Conversational Decoding via Isotropic and Proximal Search","summary":"  General-purpose text decoding approaches are usually adopted for dialogue\nresponse generation. Although the quality of the generated responses can be\nimproved with dialogue-specific encoding methods, conversational decoding\nmethods are still under-explored. Inspired by \\citet{wu2023learning} that a\ngood dialogue feature space should follow the rules of locality and isotropy,\nwe present a fine-grained conversational decoding method, termed\n\\textit{isotropic and proximal search (IPS)}. Our method is designed to\ngenerate the semantic-concentrated response, while still maintaining\ninformativeness and discrimination against the context. Experiments show that\nour approach outperforms existing decoding strategies in the dialogue field\nacross both automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our approach.\n","authors":["Yuxuan Yao","Han Wu","Qiling Xu","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2310.08130v1.pdf","comment":"To appear in EMNLP 2024"},{"id":"http://arxiv.org/abs/2310.08123v1","updated":"2023-10-12T08:24:15Z","published":"2023-10-12T08:24:15Z","title":"Who Wrote it and Why? Prompting Large-Language Models for Authorship\n  Verification","summary":"  Authorship verification (AV) is a fundamental task in natural language\nprocessing (NLP) and computational linguistics, with applications in forensic\nanalysis, plagiarism detection, and identification of deceptive content.\nExisting AV techniques, including traditional stylometric and deep learning\napproaches, face limitations in terms of data requirements and lack of\nexplainability. To address these limitations, this paper proposes PromptAV, a\nnovel technique that leverages Large-Language Models (LLMs) for AV by providing\nstep-by-step stylometric explanation prompts. PromptAV outperforms\nstate-of-the-art baselines, operates effectively with limited training data,\nand enhances interpretability through intuitive explanations, showcasing its\npotential as an effective and interpretable solution for the AV task.\n","authors":["Chia-Yu Hung","Zhiqiang Hu","Yujia Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2310.08123v1.pdf","comment":"7 pages,1 figure"},{"id":"http://arxiv.org/abs/2310.08104v1","updated":"2023-10-12T08:00:25Z","published":"2023-10-12T08:00:25Z","title":"Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and\n  Textually Described Voices","summary":"  Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.\n","authors":["Matthew Baas","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2310.08104v1.pdf","comment":"11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023"},{"id":"http://arxiv.org/abs/2310.01917v2","updated":"2023-10-12T07:59:56Z","published":"2023-10-03T09:46:02Z","title":"Hierarchical Evaluation Framework: Best Practices for Human Evaluation","summary":"  Human evaluation plays a crucial role in Natural Language Processing (NLP) as\nit assesses the quality and relevance of developed systems, thereby\nfacilitating their enhancement. However, the absence of widely accepted human\nevaluation metrics in NLP hampers fair comparisons among different systems and\nthe establishment of universal assessment standards. Through an extensive\nanalysis of existing literature on human evaluation metrics, we identified\nseveral gaps in NLP evaluation methodologies. These gaps served as motivation\nfor developing our own hierarchical evaluation framework. The proposed\nframework offers notable advantages, particularly in providing a more\ncomprehensive representation of the NLP system's performance. We applied this\nframework to evaluate the developed Machine Reading Comprehension system, which\nwas utilized within a human-AI symbiosis model. The results highlighted the\nassociations between the quality of inputs and outputs, underscoring the\nnecessity to evaluate both components rather than solely focusing on outputs.\nIn future work, we will investigate the potential time-saving benefits of our\nproposed framework for evaluators assessing NLP systems.\n","authors":["Iva Bojic","Jessica Chen","Si Yuan Chang","Qi Chwen Ong","Shafiq Joty","Josip Car"],"pdf_url":"https://arxiv.org/pdf/2310.01917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07282v2","updated":"2023-10-12T07:53:53Z","published":"2023-10-11T08:16:35Z","title":"An Analysis on Large Language Models in Healthcare: A Case Study of\n  BioBERT","summary":"  This paper conducts a comprehensive investigation into applying large\nlanguage models, particularly on BioBERT, in healthcare. It begins with\nthoroughly examining previous natural language processing (NLP) approaches in\nhealthcare, shedding light on the limitations and challenges these methods\nface. Following that, this research explores the path that led to the\nincorporation of BioBERT into healthcare applications, highlighting its\nsuitability for addressing the specific requirements of tasks related to\nbiomedical text mining. The analysis outlines a systematic methodology for\nfine-tuning BioBERT to meet the unique needs of the healthcare domain. This\napproach includes various components, including the gathering of data from a\nwide range of healthcare sources, data annotation for tasks like identifying\nmedical entities and categorizing them, and the application of specialized\npreprocessing techniques tailored to handle the complexities found in\nbiomedical texts. Additionally, the paper covers aspects related to model\nevaluation, with a focus on healthcare benchmarks and functions like processing\nof natural language in biomedical, question-answering, clinical document\nclassification, and medical entity recognition. It explores techniques to\nimprove the model's interpretability and validates its performance compared to\nexisting healthcare-focused language models. The paper thoroughly examines\nethical considerations, particularly patient privacy and data security. It\nhighlights the benefits of incorporating BioBERT into healthcare contexts,\nincluding enhanced clinical decision support and more efficient information\nretrieval. Nevertheless, it acknowledges the impediments and complexities of\nthis integration, encompassing concerns regarding data privacy, transparency,\nresource-intensive requirements, and the necessity for model customization to\nalign with diverse healthcare domains.\n","authors":["Shyni Sharaf","V. S. Anoop"],"pdf_url":"https://arxiv.org/pdf/2310.07282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08102v1","updated":"2023-10-12T07:52:19Z","published":"2023-10-12T07:52:19Z","title":"QASiNa: Religious Domain Question Answering using Sirah Nabawiyah","summary":"  Nowadays, Question Answering (QA) tasks receive significant research focus,\nparticularly with the development of Large Language Model (LLM) such as Chat\nGPT [1]. LLM can be applied to various domains, but it contradicts the\nprinciples of information transmission when applied to the Islamic domain. In\nIslam we strictly regulates the sources of information and who can give\ninterpretations or tafseer for that sources [2]. The approach used by LLM to\ngenerate answers based on its own interpretation is similar to the concept of\ntafseer, LLM is neither an Islamic expert nor a human which is not permitted in\nIslam. Indonesia is the country with the largest Islamic believer population in\nthe world [3]. With the high influence of LLM, we need to make evaluation of\nLLM in religious domain. Currently, there is only few religious QA dataset\navailable and none of them using Sirah Nabawiyah especially in Indonesian\nLanguage. In this paper, we propose the Question Answering Sirah Nabawiyah\n(QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in\nIndonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5],\nand IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0\n[7]. XLM-R model returned the best performance on QASiNa with EM of 61.20,\nF1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance\nwith Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and\nF1-Score with higher Substring Match, the gap of EM and Substring Match get\nwider in GPT-4. The experiment indicate that Chat GPT tends to give excessive\ninterpretations as evidenced by its higher Substring Match scores compared to\nEM and F1-Score, even after providing instruction and context. This concludes\nChat GPT is unsuitable for question answering task in religious domain\nespecially for Islamic religion.\n","authors":["Muhammad Razif Rizqullah","Ayu Purwarianti","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2310.08102v1.pdf","comment":"6 Pages. In Proceeding of 10th International Conference on Advanced\n  Informatics: Concepts, Theory and Applications (ICAICTA 2023)"},{"id":"http://arxiv.org/abs/2310.08101v1","updated":"2023-10-12T07:51:43Z","published":"2023-10-12T07:51:43Z","title":"Promptor: A Conversational and Autonomous Prompt Generation Agent for\n  Intelligent Text Entry Techniques","summary":"  Text entry is an essential task in our day-to-day digital interactions.\nNumerous intelligent features have been developed to streamline this process,\nmaking text entry more effective, efficient, and fluid. These improvements\ninclude sentence prediction and user personalization. However, as deep\nlearning-based language models become the norm for these advanced features, the\nnecessity for data collection and model fine-tuning increases. These challenges\ncan be mitigated by harnessing the in-context learning capability of large\nlanguage models such as GPT-3.5. This unique feature allows the language model\nto acquire new skills through prompts, eliminating the need for data collection\nand fine-tuning. Consequently, large language models can learn various text\nprediction techniques. We initially showed that, for a sentence prediction\ntask, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is\ncomparable with a fine-tuned GPT-3.5 model, with the latter two methods\nrequiring costly data collection, fine-tuning and post-processing. However, the\ntask of prompting large language models to specialize in specific text\nprediction tasks can be challenging, particularly for designers without\nexpertise in prompt engineering. To address this, we introduce Promptor, a\nconversational prompt generation agent designed to engage proactively with\ndesigners. Promptor can automatically generate complex prompts tailored to meet\nspecific needs, thus offering a solution to this challenge. We conducted a user\nstudy involving 24 participants creating prompts for three intelligent text\nentry tasks, half of the participants used Promptor while the other half\ndesigned prompts themselves. The results show that Promptor-designed prompts\nresult in a 35% increase in similarity and 22% in coherence over those by\ndesigners.\n","authors":["Junxiao Shen","John J. Dudley","Jingyao Zheng","Bill Byrne","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2310.08101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08099v1","updated":"2023-10-12T07:48:50Z","published":"2023-10-12T07:48:50Z","title":"ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using\n  Natural Language Processing","summary":"  Climate change's impact on human health poses unprecedented and diverse\nchallenges. Unless proactive measures based on solid evidence are implemented,\nthese threats will likely escalate and continue to endanger human well-being.\nThe escalating advancements in information and communication technologies have\nfacilitated the widespread availability and utilization of social media\nplatforms. Individuals utilize platforms such as Twitter and Facebook to\nexpress their opinions, thoughts, and critiques on diverse subjects,\nencompassing the pressing issue of climate change. The proliferation of climate\nchange-related content on social media necessitates comprehensive analysis to\nglean meaningful insights. This paper employs natural language processing (NLP)\ntechniques to analyze climate change discourse and quantify the sentiment of\nclimate change-related tweets. We use ClimateBERT, a pretrained model\nfine-tuned specifically for the climate change domain. The objective is to\ndiscern the sentiment individuals express and uncover patterns in public\nopinion concerning climate change. Analyzing tweet sentiments allows a deeper\ncomprehension of public perceptions, concerns, and emotions about this critical\nglobal challenge. The findings from this experiment unearth valuable insights\ninto public sentiment and the entities associated with climate change\ndiscourse. Policymakers, researchers, and organizations can leverage such\nanalyses to understand public perceptions, identify influential actors, and\ndevise informed strategies to address climate change challenges.\n","authors":["Ajay Krishnan T. K.","V. S. Anoop"],"pdf_url":"https://arxiv.org/pdf/2310.08099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08085v1","updated":"2023-10-12T07:17:17Z","published":"2023-10-12T07:17:17Z","title":"Low-Resource Clickbait Spoiling for Indonesian via Question Answering","summary":"  Clickbait spoiling aims to generate a short text to satisfy the curiosity\ninduced by a clickbait post. As it is a newly introduced task, the dataset is\nonly available in English so far. Our contributions include the construction of\nmanually labeled clickbait spoiling corpus in Indonesian and an evaluation on\nusing cross-lingual zero-shot question answering-based models to tackle\nclikcbait spoiling for low-resource language like Indonesian. We utilize\nselection of multilingual language models. The experimental results suggest\nthat XLM-RoBERTa (large) model outperforms other models for phrase and passage\nspoilers, meanwhile, mDeBERTa (base) model outperforms other models for\nmultipart spoilers.\n","authors":["Ni Putu Intan Maharani","Ayu Purwarianti","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2310.08085v1.pdf","comment":"Accepted in ICAICTA 2023 (10th International Conference on Advanced\n  Informatics: Concepts, Theory and Applications)"},{"id":"http://arxiv.org/abs/2310.08078v1","updated":"2023-10-12T06:59:10Z","published":"2023-10-12T06:59:10Z","title":"To token or not to token: A Comparative Study of Text Representations\n  for Cross-Lingual Transfer","summary":"  Choosing an appropriate tokenization scheme is often a bottleneck in\nlow-resource cross-lingual transfer. To understand the downstream implications\nof text representation choices, we perform a comparative analysis on language\nmodels having diverse text representation modalities including 2\nsegmentation-based models (\\texttt{BERT}, \\texttt{mBERT}), 1 image-based model\n(\\texttt{PIXEL}), and 1 character-level model (\\texttt{CANINE}). First, we\npropose a scoring Language Quotient (LQ) metric capable of providing a weighted\nrepresentation of both zero-shot and few-shot evaluation combined. Utilizing\nthis metric, we perform experiments comprising 19 source languages and 133\ntarget languages on three tasks (POS tagging, Dependency parsing, and NER). Our\nanalysis reveals that image-based models excel in cross-lingual transfer when\nlanguages are closely related and share visually similar scripts. However, for\ntasks biased toward word meaning (POS, NER), segmentation-based models prove to\nbe superior. Furthermore, in dependency parsing tasks where word relationships\nplay a crucial role, models with their character-level focus, outperform\nothers. Finally, we propose a recommendation scheme based on our findings to\nguide model selection according to task and language requirements.\n","authors":["Md Mushfiqur Rahman","Fardin Ahsan Sakib","Fahim Faisal","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.08078v1.pdf","comment":"Accepted at 3RD MULTILINGUAL REPRESENTATION LEARNING (MRL) WORKSHOP,\n  2023"},{"id":"http://arxiv.org/abs/2310.08072v1","updated":"2023-10-12T06:46:07Z","published":"2023-10-12T06:46:07Z","title":"Training Generative Question-Answering on Synthetic Data Obtained from\n  an Instruct-tuned Mo","summary":"  This paper presents a simple and cost-effective method for synthesizing data\nto train question-answering systems. For training, fine-tuning GPT models is a\ncommon practice in resource-rich languages like English, however, it becomes\nchallenging for non-English languages due to the scarcity of sufficient\nquestion-answer (QA) pairs. Existing approaches use question and answer\ngenerators trained on human-authored QA pairs, which involves substantial human\nexpenses. In contrast, we use an instruct-tuned model to generate QA pairs in a\nzero-shot or few-shot manner. We conduct experiments to compare various\nstrategies for obtaining QA pairs from the instruct-tuned model. The results\ndemonstrate that a model trained on our proposed synthetic data achieves\ncomparable performance to a model trained on manually curated datasets, without\nincurring human costs.\n","authors":["Kosuke Takahashi","Takahiro Omi","Kosuke Arima","Tatsuya Ishigaki"],"pdf_url":"https://arxiv.org/pdf/2310.08072v1.pdf","comment":"PACLIC 2023 short paper, 4 pages (6 pages including references), 4\n  figures"},{"id":"http://arxiv.org/abs/2305.07375v4","updated":"2023-10-12T06:42:25Z","published":"2023-05-12T10:54:13Z","title":"Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation","summary":"  Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\nexplainer. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\nThe code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .\n","authors":["Jinglong Gao","Xiao Ding","Bing Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2305.07375v4.pdf","comment":"Accepted to Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08069v1","updated":"2023-10-12T06:32:42Z","published":"2023-10-12T06:32:42Z","title":"Rethinking Negative Pairs in Code Search","summary":"  Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.\n","authors":["Haochen Li","Xin Zhou","Luu Anh Tuan","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2310.08069v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.14069v2","updated":"2023-10-12T06:20:42Z","published":"2023-05-23T13:48:32Z","title":"Evaluating Factual Consistency of Summaries with Large Language Models","summary":"  Detecting factual errors in summaries has been an important and challenging\nsubject in summarization research. Inspired by the emergent ability of large\nlanguage models (LLMs), we explore evaluating factual consistency of summaries\nby directly prompting LLMs. We present a comprehensive empirical study to\nassess the ability of LLMs as factual consistency evaluators, which consists of\n(1) analyzing different LLMs such as the GPT model series and Flan-T5; (2)\ninvestigating a variety of prompting methods including vanilla prompting,\nchain-of-thought prompting, and a sentence-by-sentence prompting method to\ntackle long summaries; and (3) evaluating on diverse summaries generated by\nmultiple summarization systems, ranging from pre-transformer methods to SOTA\npretrained models. Our experiments demonstrate that prompting LLMs is able to\noutperform the previous best factuality systems in all settings, by up to 12.2\nabsolute points in terms of the binary classification accuracy on inconsistency\ndetection.\n","authors":["Shiqi Chen","Siyang Gao","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2305.14069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08041v1","updated":"2023-10-12T05:25:49Z","published":"2023-10-12T05:25:49Z","title":"QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large\n  Language Models","summary":"  Large Language Models (LLMs) excel in NLP, but their demands hinder their\nwidespread deployment. While Quantization-Aware Training (QAT) offers a\nsolution, its extensive training costs make Post-Training Quantization (PTQ) a\nmore practical approach for LLMs. In existing studies, activation outliers in\nparticular channels are identified as the bottleneck to PTQ accuracy. They\npropose to transform the magnitudes from activations to weights, which however\noffers limited alleviation or suffers from unstable gradients, resulting in a\nsevere performance drop at low-bitwidth. In this paper, we propose QLLM, an\naccurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM\nintroduces an adaptive channel reassembly technique that reallocates the\nmagnitude of outliers to other channels, thereby mitigating their impact on the\nquantization range. This is achieved by channel disassembly and channel\nassembly, which first breaks down the outlier channels into several\nsub-channels to ensure a more balanced distribution of activation magnitudes.\nThen similar channels are merged to maintain the original channel number for\nefficiency. Additionally, an adaptive strategy is designed to autonomously\ndetermine the optimal number of sub-channels for channel disassembly. To\nfurther compensate for the performance loss caused by quantization, we propose\nan efficient tuning method that only learns a small number of low-rank weights\nwhile freezing the pre-trained quantized model. After training, these low-rank\nparameters can be fused into the frozen weights without affecting inference.\nExtensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate\nquantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B\nwithin 10 hours on a single A100-80G GPU, outperforming the previous\nstate-of-the-art method by 7.89% on the average accuracy across five zero-shot\ntasks.\n","authors":["Jing Liu","Ruihao Gong","Xiuying Wei","Zhiwei Dong","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.08041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08027v1","updated":"2023-10-12T04:14:28Z","published":"2023-10-12T04:14:28Z","title":"Exploring Large Language Models for Multi-Modal Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.\n","authors":["Yi Dai","Hao Lang","Kaisheng Zeng","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.08027v1.pdf","comment":"EMNLP2023 Findings Long Paper"},{"id":"http://arxiv.org/abs/2309.10691v2","updated":"2023-10-12T04:07:56Z","published":"2023-09-19T15:25:42Z","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback","summary":"  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n","authors":["Xingyao Wang","Zihan Wang","Jiateng Liu","Yangyi Chen","Lifan Yuan","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.10691v2.pdf","comment":"Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench"},{"id":"http://arxiv.org/abs/2310.08017v1","updated":"2023-10-12T03:33:06Z","published":"2023-10-12T03:33:06Z","title":"Harnessing Large Language Models' Empathetic Response Generation\n  Capabilities for Online Mental Health Counselling Support","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious information-seeking and reasoning tasks. These computational systems\ndrive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also\ncarry substantial promise in meeting the growing demands of mental health care,\nalbeit relatively unexplored. As such, this study sought to examine LLMs'\ncapability to generate empathetic responses in conversations that emulate those\nin a mental health counselling setting. We selected five LLMs: version 3.5 and\nversion 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways\nLanguage Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple\ninstructional prompt, these models responded to utterances derived from the\nEmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we\ncompared their responses to those from traditional response generation dialogue\nsystems, which were fine-tuned on the ED dataset, along with human-generated\nresponses. Notably, we discovered that responses from the LLMs were remarkably\nmore empathetic in most scenarios. We position our findings in light of\ncatapulting advancements in creating empathetic conversational systems.\n","authors":["Siyuan Brandon Loh","Aravind Sesagiri Raamkumar"],"pdf_url":"https://arxiv.org/pdf/2310.08017v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2310.07644v2","updated":"2023-10-12T03:32:32Z","published":"2023-10-11T16:40:57Z","title":"Rethinking the BERT-like Pretraining for DNA Sequences","summary":"  With the success of large-scale pretraining in NLP, there is an increasing\ntrend of applying it to the domain of life sciences. In particular, pretraining\nmethods based on DNA sequences have garnered growing attention due to their\npotential to capture generic information about genes. However, existing\npretraining methods for DNA sequences largely rely on direct adoptions of BERT\npretraining from NLP, lacking a comprehensive understanding and a specifically\ntailored approach. To address this research gap, we first conducted a series of\nexploratory experiments and gained several insightful observations: 1) In the\nfine-tuning phase of downstream tasks, when using K-mer overlapping\ntokenization instead of K-mer non-overlapping tokenization, both overlapping\nand non-overlapping pretraining weights show consistent performance\nimprovement.2) During the pre-training process, using K-mer overlapping\ntokenization quickly produces clear K-mer embeddings and reduces the loss to a\nvery low level, while using K-mer non-overlapping tokenization results in less\ndistinct embeddings and continuously decreases the loss. 3) Using overlapping\ntokenization causes the self-attention in the intermediate layers of\npre-trained models to tend to overly focus on certain tokens, reflecting that\nthese layers are not adequately optimized. In summary, overlapping tokenization\ncan benefit the fine-tuning of downstream tasks but leads to inadequate\npretraining with fast convergence. To unleash the pretraining potential, we\nintroduce a novel approach called RandomMask, which gradually increases the\ntask difficulty of BERT-like pretraining by continuously expanding its mask\nboundary, forcing the model to learn more knowledge. RandomMask is simple but\neffective, achieving top-tier performance across 26 datasets of 28 datasets\nspanning 7 downstream tasks.\n","authors":["Chaoqi Liang","Weiqiang Bai","Lifeng Qiao","Yuchen Ren","Jianle Sun","Peng Ye","Hongliang Yan","Xinzhu Ma","Wangmeng Zuo","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.07644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v2","updated":"2023-10-12T03:23:40Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02285v2","updated":"2023-10-12T03:05:36Z","published":"2023-09-05T14:45:27Z","title":"PromptTTS 2: Describing and Generating Voices with Text Prompt","summary":"  Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.\n","authors":["Yichong Leng","Zhifang Guo","Kai Shen","Xu Tan","Zeqian Ju","Yanqing Liu","Yufei Liu","Dongchao Yang","Leying Zhang","Kaitao Song","Lei He","Xiang-Yang Li","Sheng Zhao","Tao Qin","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2309.02285v2.pdf","comment":"Demo page: https://speechresearch.github.io/prompttts2"},{"id":"http://arxiv.org/abs/2310.04948v2","updated":"2023-10-12T02:43:13Z","published":"2023-10-08T00:02:25Z","title":"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series\n  Forecasting","summary":"  The past decade has witnessed significant advances in time series modeling\nwith deep learning. While achieving state-of-the-art results, the\nbest-performing architectures vary highly across applications and domains.\nMeanwhile, for natural language processing, the Generative Pre-trained\nTransformer (GPT) has demonstrated impressive performance via training one\ngeneral-purpose model across various textual datasets. It is intriguing to\nexplore whether GPT-type architectures can be effective for time series,\ncapturing the intrinsic dynamic attributes and leading to significant accuracy\nimprovements. In this paper, we propose a novel framework, TEMPO, that can\neffectively learn time series representations. We focus on utilizing two\nessential inductive biases of the time series task for pre-trained models: (i)\ndecomposition of the complex interaction between trend, seasonal and residual\ncomponents; and (ii) introducing the selection-based prompts to facilitate\ndistribution adaptation in non-stationary time series. TEMPO expands the\ncapability for dynamically modeling real-world temporal phenomena from data\nwithin diverse domains. Our experiments demonstrate the superior performance of\nTEMPO over state-of-the-art methods on a number of time series benchmark\ndatasets. This performance gain is observed not only in standard supervised\nlearning settings but also in scenarios involving previously unseen datasets as\nwell as in scenarios with multi-modal inputs. This compelling finding\nhighlights TEMPO's potential to constitute a foundational model-building\nframework.\n","authors":["Defu Cao","Furong Jia","Sercan O Arik","Tomas Pfister","Yixiang Zheng","Wen Ye","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.04948v2.pdf","comment":"35 pages, 20 figures, 17 tables"},{"id":"http://arxiv.org/abs/2302.12461v2","updated":"2023-10-12T02:29:09Z","published":"2023-02-24T05:26:08Z","title":"Analyzing And Editing Inner Mechanisms Of Backdoored Language Models","summary":"  Poisoning of data sets is a potential security threat to large language\nmodels that can lead to backdoored models. A description of the internal\nmechanisms of backdoored language models and how they process trigger inputs,\ne.g., when switching to toxic language, has yet to be found. In this work, we\nstudy the internal representations of transformer-based backdoored language\nmodels and determine early-layer MLP modules as most important for the backdoor\nmechanism in combination with the initial embedding projection. We use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements that reduce the MLP module outputs to essentials for the backdoor\nmechanism. To this end, we introduce PCP ablation, where we replace transformer\nmodules with low-rank matrices based on the principal components of their\nactivations. We demonstrate our results on backdoored toy, backdoored large,\nand non-backdoored open-source models. We show that we can improve the backdoor\nrobustness of large language models by locally constraining individual modules\nduring fine-tuning on potentially poisonous data sets.\n  Trigger warning: Offensive language.\n","authors":["Max Lamparth","Anka Reuel"],"pdf_url":"https://arxiv.org/pdf/2302.12461v2.pdf","comment":"included new experimental results and addressed reviewer feedback"},{"id":"http://arxiv.org/abs/2310.07284v2","updated":"2023-10-12T01:40:37Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) to extract useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process, or complement the pre-registered cues. Our\nexperimental results demonstrate competitive performance when only text-based\ncues are presented, the effectiveness of using input text as a task selector,\nand a new state-of-the-art when combining text-based cues with pre-registered\ncues. To our knowledge, this is the first study to successfully incorporate\nLLMs to guide target speaker extraction, which can be a cornerstone for\ncocktail party problem research.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v2.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2310.07968v1","updated":"2023-10-12T01:17:56Z","published":"2023-10-12T01:17:56Z","title":"Think, Act, and Ask: Open-World Interactive Personalized Robot\n  Navigation","summary":"  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards\nopen-vocabulary objects in unknown environments. The existing works of ZSON\nmainly focus on following individual instructions to find generic object\nclasses, neglecting the utilization of natural language interaction and the\ncomplexities of identifying user-specific objects. To address these\nlimitations, we introduce Zero-shot Interactive Personalized Object Navigation\n(ZIPON), where robots need to navigate to personalized goal objects while\nengaging in conversations with users. To solve ZIPON, we propose a new\nframework termed Open-woRld Interactive persOnalized Navigation (ORION), which\nuses Large Language Models (LLMs) to make sequential decisions to manipulate\ndifferent modules for perception, navigation and communication. Experimental\nresults show that the performance of interactive agents that can leverage user\nfeedback exhibits significant improvement. However, obtaining a good balance\nbetween task completion and the efficiency of navigation and interaction\nremains challenging for all methods. We further provide more findings on the\nimpact of diverse user feedback forms on the agents' performance.\n","authors":["Yinpei Dai","Run Peng","Sikai Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2310.07968v1.pdf","comment":"Video available at https://www.youtube.com/watch?v=QW6rMHVpxUY"},{"id":"http://arxiv.org/abs/2310.07659v2","updated":"2023-10-12T01:08:39Z","published":"2023-10-11T17:00:29Z","title":"Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for\n  Knowledge-Grounded Dialogue","summary":"  Accurate knowledge selection is critical in knowledge-grounded dialogue\nsystems. Towards a closer look at it, we offer a novel perspective to organize\nexisting literature, i.e., knowledge selection coupled with, after, and before\ngeneration. We focus on the third under-explored category of study, which can\nnot only select knowledge accurately in advance, but has the advantage to\nreduce the learning, adjustment, and interpretation burden of subsequent\nresponse generation models, especially LLMs. We propose GATE, a\ngenerator-agnostic knowledge selection method, to prepare knowledge for\nsubsequent response generation models by selecting context-related knowledge\namong different knowledge structures and variable knowledge requirements.\nExperimental results demonstrate the superiority of GATE, and indicate that\nknowledge selection before generation is a lightweight yet effective way to\nfacilitate LLMs (e.g., ChatGPT) to generate more informative responses.\n","authors":["Lang Qin","Yao Zhang","Hongru Liang","Jun Wang","Zhenglu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07659v2.pdf","comment":"Accepted by EMNLP2023 main conference"},{"id":"http://arxiv.org/abs/2310.01889v3","updated":"2023-10-12T01:00:09Z","published":"2023-10-03T08:44:50Z","title":"Ring Attention with Blockwise Transformers for Near-Infinite Context","summary":"  Transformers have emerged as the architecture of choice for many\nstate-of-the-art AI models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands imposed by Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving extended sequences or long-term dependencies. We present a\ndistinct approach, Ring Attention, which leverages blockwise computation of\nself-attention to distribute long sequences across multiple devices while\noverlapping the communication of key-value blocks with the computation of\nblockwise attention. Ring Attention enables training and inference of sequences\nthat are up to device count times longer than those of prior memory-efficient\nTransformers, effectively eliminating the memory constraints imposed by\nindividual devices. Extensive experiments on language modeling tasks\ndemonstrate the effectiveness of Ring Attention in allowing large sequence\ninput size and improving performance.\n","authors":["Hao Liu","Matei Zaharia","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2310.01889v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07962v1","updated":"2023-10-12T00:57:32Z","published":"2023-10-12T00:57:32Z","title":"Clustering of Spell Variations for Proper Nouns Transliterated from the\n  other languages","summary":"  One of the prominent problems with processing and operating on text data is\nthe non uniformity of it. Due to the change in the dialects and languages, the\ncaliber of translation is low. This creates a unique problem while using NLP in\ntext data; which is the spell variation arising from the inconsistent\ntranslations and transliterations. This problem can also be further aggravated\nby the human error arising from the various ways to write a Proper Noun from an\nIndian language into its English equivalent. Translating proper nouns\noriginating from Indian languages can be complicated as some proper nouns are\nalso used as common nouns which might be taken literally. Applications of NLP\nthat require addresses, names and other proper nouns face this problem\nfrequently. We propose a method to cluster these spell variations for proper\nnouns using ML techniques and mathematical similarity equations. We aimed to\nuse Affinity Propagation to determine relative similarity between the tokens.\nThe results are augmented by filtering the token-variation pair by a similarity\nthreshold. We were able to reduce the spell variations by a considerable\namount. This application can significantly reduce the amount of human\nannotation efforts needed for data cleansing and formatting.\n","authors":["Prathamesh Pawar"],"pdf_url":"https://arxiv.org/pdf/2310.07962v1.pdf","comment":"3 pages, published Airial Conference 2023"},{"id":"http://arxiv.org/abs/2310.07957v1","updated":"2023-10-12T00:50:24Z","published":"2023-10-12T00:50:24Z","title":"A New Approach Towards Autoformalization","summary":"  Verifying mathematical proofs is difficult, but can be automated with the\nassistance of a computer. Autoformalization is the task of automatically\ntranslating natural language mathematics into a formal language that can be\nverified by a program. This is a challenging task, and especially for\nhigher-level mathematics found in research papers. Research paper mathematics\nrequires large amounts of background and context. In this paper, we propose an\navenue towards tackling autoformalization for research-level mathematics, by\nbreaking the task into easier and more approachable subtasks: unlinked\nformalization (formalization with unlinked definitions and theorems), entity\nlinking (linking to the proper theorems and definitions), and finally adjusting\ntypes so it passes the type checker. In addition, we present arXiv2Formal, a\nbenchmark dataset for unlinked formalization consisting of 50 theorems\nformalized for the Lean theorem prover sampled from papers on arXiv.org. We\nwelcome any contributions from the community to future versions of this\ndataset.\n","authors":["Nilay Patel","Jeffrey Flanigan","Rahul Saha"],"pdf_url":"https://arxiv.org/pdf/2310.07957v1.pdf","comment":"Under review at MATHAI 2023 @ NeurIPS 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.08587v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Is Generalized Dynamic Novel View Synthesis from Monocular Videos\n  Possible Today?","summary":"  Rendering scenes observed in a monocular video from novel viewpoints is a\nchallenging problem. For static scenes the community has studied both\nscene-specific optimization techniques, which optimize on every test scene, and\ngeneralized techniques, which only run a deep net forward pass on a test scene.\nIn contrast, for dynamic scenes, scene-specific optimization techniques exist,\nbut, to our best knowledge, there is currently no generalized method for\ndynamic novel view synthesis from a given monocular video. To answer whether\ngeneralized dynamic novel view synthesis from monocular videos is possible\ntoday, we establish an analysis framework based on existing techniques and work\ntoward the generalized approach. We find a pseudo-generalized process without\nscene-specific appearance optimization is possible, but geometrically and\ntemporally consistent depth estimates are needed. Despite no scene-specific\nappearance optimization, the pseudo-generalized approach improves upon some\nscene-specific methods.\n","authors":["Xiaoming Zhao","Alex Colburn","Fangchang Ma","Miguel Angel Bautista","Joshua M. Susskind","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2310.08587v1.pdf","comment":"Project page: https://xiaoming-zhao.github.io/projects/pgdvs"},{"id":"http://arxiv.org/abs/2310.08588v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback","summary":"  Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.\n","authors":["Jingkang Yang","Yuhao Dong","Shuai Liu","Bo Li","Ziyue Wang","Chencheng Jiang","Haoran Tan","Jiamu Kang","Yuanhan Zhang","Kaiyang Zhou","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08588v1.pdf","comment":"Project Page: https://choiszt.github.io/Octopus/, Codebase:\n  https://github.com/dongyh20/Octopus"},{"id":"http://arxiv.org/abs/2310.08585v1","updated":"2023-10-12T17:59:57Z","published":"2023-10-12T17:59:57Z","title":"Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic\n  Scenes","summary":"  This paper aims to tackle the challenge of dynamic view synthesis from\nmulti-view videos. The key observation is that while previous grid-based\nmethods offer consistent rendering, they fall short in capturing appearance\ndetails of a complex dynamic scene, a domain where multi-view image-based\nrendering methods demonstrate the opposite properties. To combine the best of\ntwo worlds, we introduce Im4D, a hybrid scene representation that consists of a\ngrid-based geometry representation and a multi-view image-based appearance\nrepresentation. Specifically, the dynamic geometry is encoded as a 4D density\nfunction composed of spatiotemporal feature planes and a small MLP network,\nwhich globally models the scene structure and facilitates the rendering\nconsistency. We represent the scene appearance by the original multi-view\nvideos and a network that learns to predict the color of a 3D point from image\nfeatures, instead of memorizing detailed appearance totally with networks,\nthereby naturally making the learning of networks easier. Our method is\nevaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,\nNHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D\nexhibits state-of-the-art performance in rendering quality and can be trained\nefficiently, while realizing real-time rendering with a speed of 79.8 FPS for\n512x512 images, on a single RTX 3090 GPU.\n","authors":["Haotong Lin","Sida Peng","Zhen Xu","Tao Xie","Xingyi He","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08585v1.pdf","comment":"SIGGRAPH Asia 2023; Project page: https://zju3dv.github.io/im4d"},{"id":"http://arxiv.org/abs/2310.08586v1","updated":"2023-10-12T17:59:57Z","published":"2023-10-12T17:59:57Z","title":"PonderV2: Pave the Way for 3D Foundataion Model with A Universal\n  Pre-training Paradigm","summary":"  In contrast to numerous NLP and 2D computer vision foundational models, the\nlearning of a robust and highly generalized 3D foundational model poses\nconsiderably greater challenges. This is primarily due to the inherent data\nvariability and the diversity of downstream tasks. In this paper, we introduce\na comprehensive 3D pre-training framework designed to facilitate the\nacquisition of efficient 3D representations, thereby establishing a pathway to\n3D foundational models. Motivated by the fact that informative 3D features\nshould be able to encode rich geometry and appearance cues that can be utilized\nto render realistic images, we propose a novel universal paradigm to learn\npoint cloud representations by differentiable neural rendering, serving as a\nbridge between 3D and 2D worlds. We train a point cloud encoder within a\ndevised volumetric neural renderer by comparing the rendered images with the\nreal images. Notably, our approach demonstrates the seamless integration of the\nlearned 3D encoder into diverse downstream tasks. These tasks encompass not\nonly high-level challenges such as 3D detection and segmentation but also\nlow-level objectives like 3D reconstruction and image synthesis, spanning both\nindoor and outdoor scenarios. Besides, we also illustrate the capability of\npre-training a 2D backbone using the proposed universal methodology, surpassing\nconventional pre-training methods by a large margin. For the first time,\n\\sexyname achieves state-of-the-art performance on 11 indoor and outdoor\nbenchmarks. The consistent improvements in various settings imply the\neffectiveness of the proposed method. Code and models will be made available at\nhttps://github.com/Pointcept/Pointcept.\n","authors":["Haoyi Zhu","Honghui Yang","Xiaoyang Wu","Di Huang","Sha Zhang","Xianglong He","Tong He","Hengshuang Zhao","Chunhua Shen","Yu Qiao","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.08586v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.00157"},{"id":"http://arxiv.org/abs/2310.08584v1","updated":"2023-10-12T17:59:55Z","published":"2023-10-12T17:59:55Z","title":"Is ImageNet worth 1 video? Learning strong image encoders from 1 long\n  unlabelled video","summary":"  Self-supervised learning has unlocked the potential of scaling up pretraining\nto billions of images, since annotation is unnecessary. But are we making the\nbest use of data? How more economical can we be? In this work, we attempt to\nanswer this question by making two contributions. First, we investigate\nfirst-person videos and introduce a \"Walking Tours\" dataset. These videos are\nhigh-resolution, hours-long, captured in a single uninterrupted take, depicting\na large number of objects and actions with natural scene transitions. They are\nunlabeled and uncurated, thus realistic for self-supervision and comparable\nwith human learning.\n  Second, we introduce a novel self-supervised image pretraining method\ntailored for learning from continuous videos. Existing methods typically adapt\nimage-based pretraining approaches to incorporate more frames. Instead, we\nadvocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\nleads to attention maps that Discover and tRAck objects over time in an\nend-to-end manner, using transformer cross-attention. We derive multiple views\nfrom the tracks and use them in a classical self-supervised distillation loss.\nUsing our novel approach, a single Walking Tours video remarkably becomes a\nstrong competitor to ImageNet for several image and video downstream tasks.\n","authors":["Shashanka Venkataramanan","Mamshad Nayeem Rizve","Joo Carreira","Yuki M. Asano","Yannis Avrithis"],"pdf_url":"https://arxiv.org/pdf/2310.08584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08581v1","updated":"2023-10-12T17:59:41Z","published":"2023-10-12T17:59:41Z","title":"Universal Visual Decomposer: Long-Horizon Manipulation Made Easy","summary":"  Real-world robotic tasks stretch over extended horizons and encompass\nmultiple stages. Learning long-horizon manipulation tasks, however, is a\nlong-standing challenge, and demands decomposing the overarching task into\nseveral manageable subtasks to facilitate policy learning and generalization to\nunseen tasks. Prior task decomposition methods require task-specific knowledge,\nare computationally intensive, and cannot readily be applied to new tasks. To\naddress these shortcomings, we propose Universal Visual Decomposer (UVD), an\noff-the-shelf task decomposition method for visual long horizon manipulation\nusing pre-trained visual representations designed for robotic control. At a\nhigh level, UVD discovers subgoals by detecting phase shifts in the embedding\nspace of the pre-trained representation. Operating purely on visual\ndemonstrations without auxiliary information, UVD can effectively extract\nvisual subgoals embedded in the videos, while incurring zero additional\ntraining cost on top of standard visuomotor policy training. Goal-conditioned\npolicies learned with UVD-discovered subgoals exhibit significantly improved\ncompositional generalization at test time to unseen tasks. Furthermore,\nUVD-discovered subgoals can be used to construct goal-based reward shaping that\njump-starts temporally extended exploration for reinforcement learning. We\nextensively evaluate UVD on both simulation and real-world tasks, and in all\ncases, UVD substantially outperforms baselines across imitation and\nreinforcement learning settings on in-domain and out-of-domain task sequences\nalike, validating the clear advantage of automated visual task decomposition\nwithin the simple, compact UVD framework.\n","authors":["Zichen Zhang","Yunshuang Li","Osbert Bastani","Abhishek Gupta","Dinesh Jayaraman","Yecheng Jason Ma","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2310.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08580v1","updated":"2023-10-12T17:59:38Z","published":"2023-10-12T17:59:38Z","title":"OmniControl: Control Any Joint at Any Time for Human Motion Generation","summary":"  We present a novel approach named OmniControl for incorporating flexible\nspatial control signals into a text-conditioned human motion generation model\nbased on the diffusion process. Unlike previous methods that can only control\nthe pelvis trajectory, OmniControl can incorporate flexible spatial control\nsignals over different joints at different times with only one model.\nSpecifically, we propose analytic spatial guidance that ensures the generated\nmotion can tightly conform to the input control signals. At the same time,\nrealism guidance is introduced to refine all the joints to generate more\ncoherent motion. Both the spatial and realism guidance are essential and they\nare highly complementary for balancing control accuracy and motion realism. By\ncombining them, OmniControl generates motions that are realistic, coherent, and\nconsistent with the spatial constraints. Experiments on HumanML3D and KIT-ML\ndatasets show that OmniControl not only achieves significant improvement over\nstate-of-the-art methods on pelvis control but also shows promising results\nwhen incorporating the constraints over other joints.\n","authors":["Yiming Xie","Varun Jampani","Lei Zhong","Deqing Sun","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.08580v1.pdf","comment":"Project page: https://neu-vi.github.io/omnicontrol/"},{"id":"http://arxiv.org/abs/2310.08579v1","updated":"2023-10-12T17:59:34Z","published":"2023-10-12T17:59:34Z","title":"HyperHuman: Hyper-Realistic Human Generation with Latent Structural\n  Diffusion","summary":"  Despite significant advances in large-scale text-to-image models, achieving\nhyper-realistic human image generation remains a desirable yet unsolved task.\nExisting models like Stable Diffusion and DALL-E 2 tend to generate human\nimages with incoherent parts or unnatural poses. To tackle these challenges,\nour key insight is that human image is inherently structural over multiple\ngranularities, from the coarse-level body skeleton to fine-grained spatial\ngeometry. Therefore, capturing such correlations between the explicit\nappearance and latent structure in one model is essential to generate coherent\nand natural human images. To this end, we propose a unified framework,\nHyperHuman, that generates in-the-wild human images of high realism and diverse\nlayouts. Specifically, 1) we first build a large-scale human-centric dataset,\nnamed HumanVerse, which consists of 340M images with comprehensive annotations\nlike human pose, depth, and surface normal. 2) Next, we propose a Latent\nStructural Diffusion Model that simultaneously denoises the depth and surface\nnormal along with the synthesized RGB image. Our model enforces the joint\nlearning of image appearance, spatial relationship, and geometry in a unified\nnetwork, where each branch in the model complements to each other with both\nstructural awareness and textural richness. 3) Finally, to further boost the\nvisual quality, we propose a Structure-Guided Refiner to compose the predicted\nconditions for more detailed generation of higher resolution. Extensive\nexperiments demonstrate that our framework yields the state-of-the-art\nperformance, generating hyper-realistic human images under diverse scenarios.\nProject Page: https://snap-research.github.io/HyperHuman/\n","authors":["Xian Liu","Jian Ren","Aliaksandr Siarohin","Ivan Skorokhodov","Yanyu Li","Dahua Lin","Xihui Liu","Ziwei Liu","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2310.08579v1.pdf","comment":"Project Page: https://snap-research.github.io/HyperHuman/"},{"id":"http://arxiv.org/abs/2310.08577v1","updated":"2023-10-12T17:59:30Z","published":"2023-10-12T17:59:30Z","title":"Visual Data-Type Understanding does not emerge from Scaling\n  Vision-Language Models","summary":"  Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of \\textit{Visual Data-Type Identification}, a basic perceptual\nskill with implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual \\textit{data-types}, spanning four broad categories. An extensive\nzero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a\nnuanced performance landscape. While VLMs are reasonably good at identifying\ncertain stylistic \\textit{data-types}, such as cartoons and sketches, they\nstruggle with simpler \\textit{data-types} arising from basic manipulations like\nimage rotations or additive noise. Our findings reveal that (i) model scaling\nalone yields marginal gains for contrastively-trained models like CLIP, and\n(ii) there is a pronounced drop in performance for the largest\nauto-regressively trained VLMs like OpenFlamingo. This finding points to a\nblind spot in current frontier VLMs: they excel in recognizing semantic content\nbut fail to acquire an understanding of visual \\textit{data-types} through\nscaling. By analyzing the pre-training distributions of these models and\nincorporating \\textit{data-type} information into the captions during\nfine-tuning, we achieve a significant enhancement in performance. By exploring\nthis previously uncharted task, we aim to set the stage for further advancing\nVLMs to equip them with visual data-type understanding. Code and datasets are\nreleased \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.\n","authors":["Vishaal Udandarao","Max F. Burg","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2310.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08576v1","updated":"2023-10-12T17:59:23Z","published":"2023-10-12T17:59:23Z","title":"Learning to Act from Actionless Videos through Dense Correspondences","summary":"  In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.\n","authors":["Po-Chen Ko","Jiayuan Mao","Yilun Du","Shao-Hua Sun","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2310.08576v1.pdf","comment":"Project page: https://flow-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2112.09726v2","updated":"2023-10-12T17:57:51Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Cristbal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v2.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: https://soundify.cc"},{"id":"http://arxiv.org/abs/2308.11606v2","updated":"2023-10-12T17:50:38Z","published":"2023-08-22T17:53:55Z","title":"StoryBench: A Multifaceted Benchmark for Continuous Story Visualization","summary":"  Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n","authors":["Emanuele Bugliarello","Hernan Moraldo","Ruben Villegas","Mohammad Babaeizadeh","Mohammad Taghi Saffar","Han Zhang","Dumitru Erhan","Vittorio Ferrari","Pieter-Jan Kindermans","Paul Voigtlaender"],"pdf_url":"https://arxiv.org/pdf/2308.11606v2.pdf","comment":"NeurIPS D&B 2023"},{"id":"http://arxiv.org/abs/2310.08541v1","updated":"2023-10-12T17:34:20Z","published":"2023-10-12T17:34:20Z","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic\n  Image Design and Generation","summary":"  We introduce ``Idea to Image,'' a system that enables multimodal iterative\nself-refinement with GPT-4V(ision) for automatic image design and generation.\nHumans can quickly identify the characteristics of different text-to-image\n(T2I) models via iterative explorations. This enables them to efficiently\nconvert their high-level generation ideas into effective T2I prompts that can\nproduce good images. We investigate if systems based on large multimodal models\n(LMMs) can develop analogous multimodal self-refinement abilities that enable\nexploring unknown models or environments via self-refining tries. Idea2Img\ncyclically generates revised T2I prompts to synthesize draft images, and\nprovides directional feedback for prompt revision, both conditioned on its\nmemory of the probed T2I model's characteristics. The iterative self-refinement\nbrings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img\ncan process input ideas with interleaved image-text sequences, follow ideas\nwith design instructions, and generate images of better semantic and visual\nqualities. The user preference study validates the efficacy of multimodal\niterative self-refinement on automatic image design and generation.\n","authors":["Zhengyuan Yang","Jianfeng Wang","Linjie Li","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08541v1.pdf","comment":"Project page at https://idea2img.github.io/"},{"id":"http://arxiv.org/abs/2310.08538v1","updated":"2023-10-12T17:28:06Z","published":"2023-10-12T17:28:06Z","title":"Image2PCI -- A Multitask Learning Framework for Estimating Pavement\n  Condition Indices Directly from Images","summary":"  The Pavement Condition Index (PCI) is a widely used metric for evaluating\npavement performance based on the type, extent and severity of distresses\ndetected on a pavement surface. In recent times, significant progress has been\nmade in utilizing deep-learning approaches to automate PCI estimation process.\nHowever, the current approaches rely on at least two separate models to\nestimate PCI values -- one model dedicated to determining the type and extent\nand another for estimating their severity. This approach presents several\nchallenges, including complexities, high computational resource demands, and\nmaintenance burdens that necessitate careful consideration and resolution. To\novercome these challenges, the current study develops a unified multi-tasking\nmodel that predicts the PCI directly from a top-down pavement image. The\nproposed architecture is a multi-task model composed of one encoder for feature\nextraction and four decoders to handle specific tasks: two detection heads, one\nsegmentation head and one PCI estimation head. By multitasking, we are able to\nextract features from the detection and segmentation heads for automatically\nestimating the PCI directly from the images. The model performs very well on\nour benchmarked and open pavement distress dataset that is annotated for\nmultitask learning (the first of its kind). To our best knowledge, this is the\nfirst work that can estimate PCI directly from an image at real time speeds\nwhile maintaining excellent accuracy on all related tasks for crack detection\nand segmentation.\n","authors":["Neema Jakisa Owor","Hang Du","Abdulateef Daud","Armstrong Aboah","Yaw Adu-Gyamfi"],"pdf_url":"https://arxiv.org/pdf/2310.08538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08537v1","updated":"2023-10-12T17:26:16Z","published":"2023-10-12T17:26:16Z","title":"XAI Benchmark for Visual Explanation","summary":"  The rise of deep learning algorithms has led to significant advancements in\ncomputer vision tasks, but their \"black box\" nature has raised concerns\nregarding interpretability. Explainable AI (XAI) has emerged as a critical area\nof research aiming to open this \"black box\", and shed light on the\ndecision-making process of AI models. Visual explanations, as a subset of\nExplainable Artificial Intelligence (XAI), provide intuitive insights into the\ndecision-making processes of AI models handling visual data by highlighting\ninfluential areas in an input image. Despite extensive research conducted on\nvisual explanations, most evaluations are model-centered since the availability\nof corresponding real-world datasets with ground truth explanations is scarce\nin the context of image data. To bridge this gap, we introduce an XAI Benchmark\ncomprising a dataset collection from diverse topics that provide both class\nlabels and corresponding explanation annotations for images. We have processed\ndata from diverse domains to align with our unified visual explanation\nframework. We introduce a comprehensive Visual Explanation pipeline, which\nintegrates data loading, preprocessing, experimental setup, and model\nevaluation processes. This structure enables researchers to conduct fair\ncomparisons of various visual explanation techniques. In addition, we provide a\ncomprehensive review of over 10 evaluation methods for visual explanation to\nassist researchers in effectively utilizing our dataset collection. To further\nassess the performance of existing visual explanation methods, we conduct\nexperiments on selected datasets using various model-centered and ground\ntruth-centered evaluation metrics. We envision this benchmark could facilitate\nthe advancement of visual explanation models. The XAI dataset collection and\neasy-to-use code for evaluation are publicly accessible at\nhttps://xaidataset.github.io.\n","authors":["Yifei Zhang","Siyi Gu","James Song","Bo Pan","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05173v2","updated":"2023-10-12T17:25:44Z","published":"2023-09-11T00:02:05Z","title":"DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n","authors":["Zhengxiang Shi","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2309.05173v2.pdf","comment":"Code is available at https://github.com/ZhengxiangShi/DePT"},{"id":"http://arxiv.org/abs/2307.03293v2","updated":"2023-10-12T17:25:26Z","published":"2023-07-06T21:08:03Z","title":"CheXmask: a large-scale dataset of anatomical segmentation masks for\n  multi-center chest x-ray images","summary":"  The development of successful artificial intelligence models for chest X-ray\nanalysis relies on large, diverse datasets with high-quality annotations. While\nseveral databases of chest X-ray images have been released, most include\ndisease diagnosis labels but lack detailed pixel-level anatomical segmentation\nlabels. To address this gap, we introduce an extensive chest X-ray multi-center\nsegmentation dataset with uniform and fine-grain anatomical annotations for\nimages coming from six well-known publicly available databases: CANDID-PTX,\nChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in\n676,803 segmentation masks. Our methodology utilizes the HybridGNet model to\nensure consistent and high-quality segmentations across all datasets. Rigorous\nvalidation, including expert physician evaluation and automatic quality\ncontrol, was conducted to validate the resulting masks. Additionally, we\nprovide individualized quality indices per mask and an overall quality\nestimation per dataset. This dataset serves as a valuable resource for the\nbroader scientific community, streamlining the development and assessment of\ninnovative methodologies in chest X-ray analysis. The CheXmask dataset is\npublicly available at:\nhttps://physionet.org/content/chexmask-cxr-segmentation-data/\n","authors":["Nicols Gaggion","Candelaria Mosquera","Lucas Mansilla","Martina Aineseder","Diego H. Milone","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2307.03293v2.pdf","comment":"The CheXmask dataset is publicly available at\n  https://physionet.org/content/chexmask-cxr-segmentation-data/"},{"id":"http://arxiv.org/abs/2310.08534v1","updated":"2023-10-12T17:24:05Z","published":"2023-10-12T17:24:05Z","title":"Animating Street View","summary":"  We present a system that automatically brings street view imagery to life by\npopulating it with naturally behaving, animated pedestrians and vehicles. Our\napproach is to remove existing people and vehicles from the input image, insert\nmoving objects with proper scale, angle, motion, and appearance, plan paths and\ntraffic behavior, as well as render the scene with plausible occlusion and\nshadowing effects. The system achieves these by reconstructing the still image\nstreet scene, simulating crowd behavior, and rendering with consistent\nlighting, visibility, occlusions, and shadows. We demonstrate results on a\ndiverse range of street scenes including regular still images and panoramas.\n","authors":["Mengyi Shan","Brian Curless","Ira Kemelmacher-Shlizerman","Steve Seitz"],"pdf_url":"https://arxiv.org/pdf/2310.08534v1.pdf","comment":"SIGGRAPH Asia 2023 Conference Track"},{"id":"http://arxiv.org/abs/2310.08530v1","updated":"2023-10-12T17:22:58Z","published":"2023-10-12T17:22:58Z","title":"UniPose: Detecting Any Keypoints","summary":"  This work proposes a unified framework called UniPose to detect keypoints of\nany articulated (e.g., human and animal), rigid, and soft objects via visual or\ntextual prompts for fine-grained vision understanding and manipulation.\nKeypoint is a structure-aware, pixel-level, and compact representation of any\nobject, especially articulated objects. Existing fine-grained promptable tasks\nmainly focus on object instance detection and segmentation but often fail to\nidentify fine-grained granularity and structured information of image and\ninstance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint\ndetection is still under-explored. To bridge the gap, we make the first attempt\nto develop an end-to-end prompt-based keypoint detection framework called\nUniPose to detect keypoints of any objects. As keypoint detection tasks are\nunified in this framework, we can leverage 13 keypoint detection datasets with\n338 keypoints across 1,237 categories over 400K instances to train a generic\nkeypoint detection model. UniPose can effectively align text-to-keypoint and\nimage-to-keypoint due to the mutual enhancement of textual and visual prompts\nbased on the cross-modality contrastive learning optimization objectives. Our\nexperimental results show that UniPose has strong fine-grained localization and\ngeneralization abilities across image styles, categories, and poses. Based on\nUniPose as a generalist keypoint detector, we hope it could serve fine-grained\nvisual perception, understanding, and generation.\n","authors":["Jie Yang","Ailing Zeng","Ruimao Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08529v1","updated":"2023-10-12T17:22:24Z","published":"2023-10-12T17:22:24Z","title":"GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with\n  Point Cloud Priors","summary":"  In recent times, the generation of 3D assets from text prompts has shown\nimpressive results. Both 2D and 3D diffusion models can generate decent 3D\nobjects based on prompts. 3D diffusion models have good 3D consistency, but\ntheir quality and generalization are limited as trainable 3D data is expensive\nand hard to obtain. 2D diffusion models enjoy strong abilities of\ngeneralization and fine generation, but the 3D consistency is hard to\nguarantee. This paper attempts to bridge the power from the two types of\ndiffusion models via the recent explicit and efficient 3D Gaussian splatting\nrepresentation. A fast 3D generation framework, named as \\name, is proposed,\nwhere the 3D diffusion model provides point cloud priors for initialization and\nthe 2D diffusion model enriches the geometry and appearance. Operations of\nnoisy point growing and color perturbation are introduced to enhance the\ninitialized Gaussians. Our \\name can generate a high-quality 3D instance within\n25 minutes on one GPU, much faster than previous methods, while the generated\ninstances can be directly rendered in real time. Demos and code are available\nat https://taoranyi.com/gaussiandreamer/.\n","authors":["Taoran Yi","Jiemin Fang","Guanjun Wu","Lingxi Xie","Xiaopeng Zhang","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08529v1.pdf","comment":"Work in progress. Project page: https://taoranyi.com/gaussiandreamer/"},{"id":"http://arxiv.org/abs/2310.08528v1","updated":"2023-10-12T17:21:41Z","published":"2023-10-12T17:21:41Z","title":"4D Gaussian Splatting for Real-Time Dynamic Scene Rendering","summary":"  Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting\n(4D-GS) to achieve real-time dynamic scene rendering while also enjoying high\ntraining and storage efficiency. An efficient deformation field is constructed\nto model both Gaussian motions and shape deformations. Different adjacent\nGaussians are connected via a HexPlane to produce more accurate position and\nshape deformations. Our 4D-GS method achieves real-time rendering under high\nresolutions, 70 FPS at a 800$\\times$800 resolution on an RTX 3090 GPU, while\nmaintaining comparable or higher quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps://guanjunwu.github.io/4dgs/.\n","authors":["Guanjun Wu","Taoran Yi","Jiemin Fang","Lingxi Xie","Xiaopeng Zhang","Wei Wei","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08528v1.pdf","comment":"Work in progress. Project page: https://guanjunwu.github.io/4dgs/"},{"id":"http://arxiv.org/abs/2310.08501v1","updated":"2023-10-12T16:59:50Z","published":"2023-10-12T16:59:50Z","title":"Unsupervised Learning of Object-Centric Embeddings for Cell Instance\n  Segmentation in Microscopy Images","summary":"  Segmentation of objects in microscopy images is required for many biomedical\napplications. We introduce object-centric embeddings (OCEs), which embed image\npatches such that the spatial offsets between patches cropped from the same\nobject are preserved. Those learnt embeddings can be used to delineate\nindividual objects and thus obtain instance segmentations. Here, we show\ntheoretically that, under assumptions commonly found in microscopy images, OCEs\ncan be learnt through a self-supervised task that predicts the spatial offset\nbetween image patches. Together, this forms an unsupervised cell instance\nsegmentation method which we evaluate on nine diverse large-scale microscopy\ndatasets. Segmentations obtained with our method lead to substantially improved\nresults, compared to state-of-the-art baselines on six out of nine datasets,\nand perform on par on the remaining three datasets. If ground-truth annotations\nare available, our method serves as an excellent starting point for supervised\ntraining, reducing the required amount of ground-truth needed by one order of\nmagnitude, thus substantially increasing the practical applicability of our\nmethod. Source code is available at https://github.com/funkelab/cellulus.\n","authors":["Steffen Wolf","Manan Lalit","Henry Westmacott","Katie McDole","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2310.08501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v2","updated":"2023-10-12T16:42:55Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Moun Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.08475v1","updated":"2023-10-12T16:32:44Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights\\footnote{Code\nand dataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08465v1","updated":"2023-10-12T16:26:18Z","published":"2023-10-12T16:26:18Z","title":"MotionDirector: Motion Customization of Text-to-Video Diffusion Models","summary":"  Large-scale pre-trained diffusion models have exhibited remarkable\ncapabilities in diverse video generations. Given a set of video clips of the\nsame motion concept, the task of Motion Customization is to adapt existing\ntext-to-video diffusion models to generate videos with this motion. For\nexample, generating a video with a car moving in a prescribed manner under\nspecific camera movements to make a movie, or a video illustrating how a bear\nwould lift weights to inspire creators. Adaptation methods have been developed\nfor customizing appearance like subject or style, yet unexplored for motion. It\nis straightforward to extend mainstream adaption methods for motion\ncustomization, including full model tuning, parameter-efficient tuning of\nadditional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept\nlearned by these methods is often coupled with the limited appearances in the\ntraining videos, making it difficult to generalize the customized motion to\nother appearances. To overcome this challenge, we propose MotionDirector, with\na dual-path LoRAs architecture to decouple the learning of appearance and\nmotion. Further, we design a novel appearance-debiased temporal loss to\nmitigate the influence of appearance on the temporal training objective.\nExperimental results show the proposed method can generate videos of diverse\nappearances for the customized motions. Our method also supports various\ndownstream applications, such as the mixing of different videos with their\nappearance and motion respectively, and animating a single image with\ncustomized motions. Our code and model weights will be released.\n","authors":["Rui Zhao","Yuchao Gu","Jay Zhangjie Wu","David Junhao Zhang","Jiawei Liu","Weijia Wu","Jussi Keppo","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2310.08465v1.pdf","comment":"Project Page: https://showlab.github.io/MotionDirector/"},{"id":"http://arxiv.org/abs/2310.08451v1","updated":"2023-10-12T16:11:13Z","published":"2023-10-12T16:11:13Z","title":"Proving the Potential of Skeleton Based Action Recognition to Automate\n  the Analysis of Manual Processes","summary":"  In manufacturing sectors such as textiles and electronics, manual processes\nare a fundamental part of production. The analysis and monitoring of the\nprocesses is necessary for efficient production design. Traditional methods for\nanalyzing manual processes are complex, expensive, and inflexible. Compared to\nestablished approaches such as Methods-Time-Measurement (MTM), machine learning\n(ML) methods promise: Higher flexibility, self-sufficient & permanent use,\nlower costs. In this work, based on a video stream, the current motion class in\na manual assembly process is detected. With information on the current motion,\nKey-Performance-Indicators (KPIs) can be derived easily. A skeleton-based\naction recognition approach is taken, as this field recently shows major\nsuccess in machine vision tasks. For skeleton-based action recognition in\nmanual assembly, no sufficient pre-work could be found. Therefore, a ML\npipeline is developed, to enable extensive research on different (pre-)\nprocessing methods and neural nets. Suitable well generalizing approaches are\nfound, proving the potential of ML to enhance analyzation of manual processes.\nModels detect the current motion, performed by an operator in manual assembly,\nbut the results can be transferred to all kinds of manual processes.\n","authors":["Marlin Berger","Frederik Cloppenburg","Jens Eufinger","Thomas Gries"],"pdf_url":"https://arxiv.org/pdf/2310.08451v1.pdf","comment":"16 pages, 6 figures. Find peer-reviewed version in Proceedings of\n  IntelliSys 2023"},{"id":"http://arxiv.org/abs/2310.08442v1","updated":"2023-10-12T16:04:41Z","published":"2023-10-12T16:04:41Z","title":"Debias the Training of Diffusion Models","summary":"  Diffusion models have demonstrated compelling generation quality by\noptimizing the variational lower bound through a simple denoising score\nmatching loss. In this paper, we provide theoretical evidence that the\nprevailing practice of using a constant loss weight strategy in diffusion\nmodels leads to biased estimation during the training phase. Simply optimizing\nthe denoising network to predict Gaussian noise with constant weighting may\nhinder precise estimations of original images. To address the issue, we propose\nan elegant and effective weighting strategy grounded in the theoretically\nunbiased principle. Moreover, we conduct a comprehensive and systematic\nexploration to dissect the inherent bias problem deriving from constant\nweighting loss from the perspectives of its existence, impact and reasons.\nThese analyses are expected to advance our understanding and demystify the\ninner workings of diffusion models. Through empirical evaluation, we\ndemonstrate that our proposed debiased estimation method significantly enhances\nsample quality without the reliance on complex techniques, and exhibits\nimproved efficiency compared to the baseline method both in training and\nsampling processes.\n","authors":["Hu Yu","Li Shen","Jie Huang","Man Zhou","Hongsheng Li","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08442v1.pdf","comment":"University of Science and Technology of China, Alibaba Group, The\n  Chinese University of Hong Kong"},{"id":"http://arxiv.org/abs/2310.08430v1","updated":"2023-10-12T15:53:47Z","published":"2023-10-12T15:53:47Z","title":"Assessing of Soil Erosion Risk Through Geoinformation Sciences and\n  Remote Sensing -- A Review","summary":"  During past decades a marked manifestation of widespread erosion phenomena\nwas studied worldwide. Global conservation community has launched campaigns at\nlocal, regional and continental level in developing countries for preservation\nof soil resources in order not only to stop or mitigate human impact on nature\nbut also to improve life in rural areas introducing new approaches for soil\ncultivation. After the adoption of Sustainable Development Goals of UNs and\nlaunching several world initiatives such as the Land Degradation Neutrality\n(LDN) the world came to realize the very importance of the soil resources on\nwhich the biosphere relies for its existence. The main goal of the chapter is\nto review different types and structures erosion models as well as their\napplications. Several methods using spatial analysis capabilities of geographic\ninformation systems (GIS) are in operation for soil erosion risk assessment,\nsuch as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss\nEquation (RUSLE) in operation worldwide and in the USA and MESALES model. These\nand more models are being discussed in the present work alongside more\nexperimental models and methods for assessing soil erosion risk such as\nArtificial Intelligence (AI), Machine and Deep Learning, etc. At the end of\nthis work, a prospectus for the future development of soil erosion risk\nassessment is drawn.\n","authors":["Lachezar Filchev","Vasil Kolev"],"pdf_url":"https://arxiv.org/pdf/2310.08430v1.pdf","comment":"Chapter 21 (pages 54)"},{"id":"http://arxiv.org/abs/2310.08429v1","updated":"2023-10-12T15:53:24Z","published":"2023-10-12T15:53:24Z","title":"Revisiting Data Augmentation for Rotational Invariance in Convolutional\n  Neural Networks","summary":"  Convolutional Neural Networks (CNN) offer state of the art performance in\nvarious computer vision tasks. Many of those tasks require different subtypes\nof affine invariances (scale, rotational, translational) to image\ntransformations. Convolutional layers are translation equivariant by design,\nbut in their basic form lack invariances. In this work we investigate how best\nto include rotational invariance in a CNN for image classification. Our\nexperiments show that networks trained with data augmentation alone can\nclassify rotated images nearly as well as in the normal unrotated case; this\nincrease in representational power comes only at the cost of training time. We\nalso compare data augmentation versus two modified CNN models for achieving\nrotational invariance or equivariance, Spatial Transformer Networks and Group\nEquivariant CNNs, finding no significant accuracy increase with these\nspecialized methods. In the case of data augmented networks, we also analyze\nwhich layers help the network to encode the rotational invariance, which is\nimportant for understanding its limitations and how to best retrain a network\nwith data augmentation to achieve invariance to rotation.\n","authors":["Facundo Manuel Quiroga","Franco Ronchetti","Laura Lanzarini","Aurelio Fernandez-Bariviera"],"pdf_url":"https://arxiv.org/pdf/2310.08429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.05188v3","updated":"2023-10-12T15:43:01Z","published":"2022-05-10T21:55:26Z","title":"On Scale Space Radon Transform, Properties and Application in CT Image\n  Reconstruction","summary":"  Since the Radon transform (RT) consists in a line integral function, some\nmodeling assumptions are made on Computed Tomography (CT) system, making image\nreconstruction analytical methods, such as Filtered Backprojection (FBP),\nsensitive to artifacts and noise. In the other hand, recently, a new integral\ntransform, called Scale Space Radon Transform (SSRT), is introduced where, RT\nis a particular case. Thanks to its interesting properties, such as good scale\nspace behavior, the SSRT has known number of new applications. In this paper,\nwith the aim to improve the reconstructed image quality for these methods, we\npropose to model the X-ray beam with the Scale Space Radon Transform (SSRT)\nwhere, the assumptions done on the physical dimensions of the CT system\nelements reflect better the reality. After depicting the basic properties and\nthe inversion of SSRT, the FBP algorithm is used to reconstruct the image from\nthe SSRT sinogram where the RT spectrum used in FBP is replaced by SSRT and the\nGaussian kernel, expressed in their frequency domain. PSNR and SSIM, as quality\nmeasures, are used to compare RT and SSRT-based image reconstruction on\nShepp-Logan head and anthropomorphic abdominal phantoms. The first findings\nshow that the SSRT-based method outperforms the methods based on RT,\nespecially, when the number of projections is reduced, making it more\nappropriate for applications requiring low-dose radiation, such as medical\nX-ray CT. While SSRT-FBP and RT-FBP have utmost the same runtime, the\nexperiments show that SSRT-FBP is more robust to Poisson-Gaussian noise\ncorrupting CT data.\n","authors":["Nafaa Nacereddine","Djemel Ziou","Aicha Baya Goumeidane"],"pdf_url":"https://arxiv.org/pdf/2205.05188v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08421v1","updated":"2023-10-12T15:42:17Z","published":"2023-10-12T15:42:17Z","title":"\"SegLoc\": Study on Novel Visual Self-supervised Learning Scheme (Segment\n  Localization) Tailored for Dense Prediction Tasks of Security Inspection\n  X-ray Images","summary":"  Lately, remarkable advancements of artificial intelligence have been\nattributed to the integration of self-supervised learning scheme. Despite\nimpressive achievements within NLP, yet SSL in computer vision has not been\nable to stay on track comparatively. Recently, integration of contrastive\nlearning on top of existing SSL models has established considerable progress in\ncomputer vision through which visual SSL models have outperformed their\nsupervised counterparts. Nevertheless, most of these improvements were limited\nto classification tasks, and also, few works have been dedicated to evaluation\nof SSL models in real-world scenarios of computer vision, while the majority of\nworks are centered around datasets containing class-wise portrait images, most\nnotably, ImageNet. Consequently, in this work, we have considered dense\nprediction task of semantic segmentation in security inspection x-ray images to\nevaluate our proposed model Segmentation Localization. Based upon the model\nInstance Localization, our model SegLoc has managed to address one of the most\nchallenging downsides of contrastive learning, i.e., false negative pairs of\nquery embeddings. In order to do so, in contrast to baseline model InsLoc, our\npretraining dataset is synthesized by cropping, transforming, then pasting\nalready labeled segments from an available labeled dataset, foregrounds, onto\ninstances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray\ndatasets are considered as labeled and unlabeled datasets, respectively.\nMoreover, we fully harness labels by avoiding false negative pairs through\nimplementing the idea, one queue per class, in MoCo-v2 whereby negative pairs\ncorresponding to each query are extracted from its corresponding queue within\nthe memory bank. Our approach has outperformed random initialization by 3% to\n6%, while having underperformed supervised initialization.\n","authors":["Shervin Halat","Mohammad Rahmati","Ehsan Nazerfard"],"pdf_url":"https://arxiv.org/pdf/2310.08421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08420v1","updated":"2023-10-12T15:39:54Z","published":"2023-10-12T15:39:54Z","title":"Visual Attention-Prompted Prediction and Learning","summary":"  Explanation(attention)-guided learning is a method that enhances a model's\npredictive power by incorporating human understanding during the training\nphase. While attention-guided learning has shown promising results, it often\ninvolves time-consuming and computationally expensive model retraining. To\naddress this issue, we introduce the attention-prompted prediction technique,\nwhich enables direct prediction guided by the attention prompt without the need\nfor model retraining. However, this approach presents several challenges,\nincluding: 1) How to incorporate the visual attention prompt into the model's\ndecision-making process and leverage it for future predictions even in the\nabsence of a prompt? and 2) How to handle the incomplete information from the\nvisual attention prompt? To tackle these challenges, we propose a novel\nframework called Visual Attention-Prompted Prediction and Learning, which\nseamlessly integrates visual attention prompts into the model's decision-making\nprocess and adapts to images both with and without attention prompts for\nprediction. To address the incomplete information of the visual attention\nprompt, we introduce a perturbation-based attention map modification method.\nAdditionally, we propose an optimization-based mask aggregation method with a\nnew weight learning function for adaptive perturbed annotation aggregation in\nthe attention map modification process. Our overall framework is designed to\nlearn in an attention-prompt guided multi-task manner to enhance future\npredictions even for samples without attention prompts and trained in an\nalternating manner for better convergence. Extensive experiments conducted on\ntwo datasets demonstrate the effectiveness of our proposed framework in\nenhancing predictions for samples, both with and without provided prompts.\n","authors":["Yifei Zhang","Siyi Gu","Bo Pan","Guangji Bai","Xiaofeng Yang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11713v4","updated":"2023-10-12T15:30:41Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v4.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2310.08398v1","updated":"2023-10-12T15:09:12Z","published":"2023-10-12T15:09:12Z","title":"Towards Design and Development of an ArUco Markers-Based Quantitative\n  Surface Tactile Sensor","summary":"  In this paper, with the goal of quantifying the qualitative image outputs of\na Vision-based Tactile Sensor (VTS), we present the design, fabrication, and\ncharacterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).\nQS-TS directly estimates the sensor's gel layer deformation in real-time\nenabling safe and autonomous tactile manipulation and servoing of delicate\nobjects using robotic manipulators. The core of the proposed sensor is the\nutilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner\nbinary patterns and a broad black border, called ArUco Markers. Each ArUco\nmarker can provide real-time camera pose estimation that, in our design, is\nused as a quantitative measure for obtaining deformation of the QS-TS gel\nlayer. Moreover, thanks to the use of ArUco markers, we propose a unique\nfabrication procedure that mitigates various challenges associated with the\nfabrication of the existing marker-based VTSs and offers an intuitive and\nless-arduous method for the construction of the VTS. Remarkably, the proposed\nfabrication facilitates the integration and adherence of markers with the gel\nlayer to robustly and reliably obtain a quantitative measure of deformation in\nreal-time regardless of the orientation of ArUco Markers. The performance and\nefficacy of the proposed QS-TS in estimating the deformation of the sensor's\ngel layer were experimentally evaluated and verified. Results demonstrate the\nphenomenal performance of the QS-TS in estimating the deformation of the gel\nlayer with a relative error of <5%.\n","authors":["Ozdemir Can Kara","Charles Everson","Farshid Alambeigi"],"pdf_url":"https://arxiv.org/pdf/2310.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02621v2","updated":"2023-10-12T15:05:15Z","published":"2023-04-05T17:43:57Z","title":"High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation","summary":"  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset show that our proposed add-on is well-suited for\nlarge-scale settings. Our code is available at https://github.com/arvijj/hfpl.\n","authors":["Arvi Jonnarth","Yushan Zhang","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2304.02621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04946v2","updated":"2023-10-12T15:04:30Z","published":"2023-09-10T06:33:17Z","title":"Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation","summary":"  Audio-driven talking-head synthesis is a popular research topic for virtual\nhuman-related applications. However, the inflexibility and inefficiency of\nexisting methods, which necessitate expensive end-to-end training to transfer\nemotions from guidance videos to talking-head predictions, are significant\nlimitations. In this work, we propose the Emotional Adaptation for Audio-driven\nTalking-head (EAT) method, which transforms emotion-agnostic talking-head\nmodels into emotion-controllable ones in a cost-effective and efficient manner\nthrough parameter-efficient adaptations. Our approach utilizes a pretrained\nemotion-agnostic talking-head transformer and introduces three lightweight\nadaptations (the Deep Emotional Prompts, Emotional Deformation Network, and\nEmotional Adaptation Module) from different perspectives to enable precise and\nrealistic emotion controls. Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including LRW\nand MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable\ngeneralization ability, even in scenarios where emotional training videos are\nscarce or nonexistent. Project website: https://yuangan.github.io/eat/\n","authors":["Yuan Gan","Zongxin Yang","Xihang Yue","Lingyun Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2309.04946v2.pdf","comment":"Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/"},{"id":"http://arxiv.org/abs/2310.08390v1","updated":"2023-10-12T15:00:06Z","published":"2023-10-12T15:00:06Z","title":"Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric\n  Learning","summary":"  Metric learning plays a critical role in training image retrieval and\nclassification. It is also a key algorithm in representation learning, e.g.,\nfor feature learning and its alignment in metric space. Hyperbolic embedding\nhas been recently developed, compared to the conventional Euclidean embedding\nin most of the previously developed models, and can be more effective in\nrepresenting the hierarchical data structure. Second, uncertainty\nestimation/measurement is a long-lasting challenge in artificial intelligence.\nSuccessful uncertainty estimation can improve a machine learning model's\nperformance, robustness, and security. In Hyperbolic space, uncertainty\nmeasurement is at least with equivalent, if not more, critical importance. In\nthis paper, we develop a Hyperbolic image embedding with uncertainty-aware\nmetric learning for image retrieval. We call our method Hyp-UML: Hyperbolic\nUncertainty-aware Metric Learning. Our contribution are threefold: we propose\nan image embedding algorithm based on Hyperbolic space, with their\ncorresponding uncertainty value; we propose two types of uncertainty-aware\nmetric learning, for the popular Contrastive learning and conventional\nmargin-based metric learning, respectively. We perform extensive experimental\nvalidations to prove that the proposed algorithm can achieve state-of-the-art\nresults among related methods. The comprehensive ablation study validates the\neffectiveness of each component of the proposed algorithm.\n","authors":["Shiyang Yan","Zongxuan Liu","Lin Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08387v1","updated":"2023-10-12T14:59:22Z","published":"2023-10-12T14:59:22Z","title":"MeanAP-Guided Reinforced Active Learning for Object Detection","summary":"  Active learning presents a promising avenue for training high-performance\nmodels with minimal labeled data, achieved by judiciously selecting the most\ninformative instances to label and incorporating them into the task learner.\nDespite notable advancements in active learning for image recognition, metrics\ndevised or learned to gauge the information gain of data, crucial for query\nstrategy design, do not consistently align with task model performance metrics,\nsuch as Mean Average Precision (MeanAP) in object detection tasks. This paper\nintroduces MeanAP-Guided Reinforced Active Learning for Object Detection\n(MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task\nmodel to devise a sampling strategy employing a reinforcement learning-based\nsampling agent. Built upon LSTM architecture, the agent efficiently explores\nand selects subsequent training instances, and optimizes the process through\npolicy gradient with MeanAP serving as reward. Recognizing the time-intensive\nnature of MeanAP computation at each step, we propose fast look-up tables to\nexpedite agent training. We assess MAGRAL's efficacy across popular benchmarks,\nPASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical\nfindings substantiate MAGRAL's superiority over recent state-of-the-art\nmethods, showcasing substantial performance gains. MAGRAL establishes a robust\nbaseline for reinforced active object detection, signifying its potential in\nadvancing the field.\n","authors":["Zhixuan Liang","Xingyu Zeng","Rui Zhao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08381v1","updated":"2023-10-12T14:55:31Z","published":"2023-10-12T14:55:31Z","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","summary":"  Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach\nto adapting pre-trained vision models to solve various downstream\nimage-classification tasks. However, there has hitherto been little systematic\nstudy of the design space of VP and no clear benchmark for evaluating its\nperformance. To bridge this gap, we propose AutoVP, an end-to-end expandable\nframework for automating VP design choices, along with 12 downstream\nimage-classification tasks that can serve as a holistic VP-performance\nbenchmark. Our design space covers 1) the joint optimization of the prompts; 2)\nthe selection of pre-trained models, including image classifiers and text-image\nencoders; and 3) model output mapping strategies, including nonparametric and\ntrainable label mapping. Our extensive experimental results show that AutoVP\noutperforms the best-known current VP methods by a substantial margin, having\nup to 6.7% improvement in accuracy; and attains a maximum performance increase\nof 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold\ncontribution: serving both as an efficient tool for hyperparameter tuning on VP\ndesign choices, and as a comprehensive benchmark that can reasonably be\nexpected to accelerate VP's development. The source code is available at\nhttps://github.com/IBM/AutoVP.\n","authors":["Hsi-Ai Tsao","Lei Hsiung","Pin-Yu Chen","Sijia Liu","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2310.08381v1.pdf","comment":"Preprint. The code is available at https://github.com/IBM/AutoVP"},{"id":"http://arxiv.org/abs/2310.08371v1","updated":"2023-10-12T14:40:24Z","published":"2023-10-12T14:40:24Z","title":"Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN","summary":"  A lot of progress has been made in the last years on using Generative\nAdversarial Networks (GAN) to create realistic images. However, to be able\nreconstruct images or to generate images using real data as input, an Encoder\nis needed that reverses the mapping from the GAN's latent space to image space.\nThis means that three networks are needed: an Encoder, a Decoder (called\nGenerator in a normal GAN) and a Discriminator. These three networks can be\ntrained from scratch simultaneously (Adversarially Learned Inference), or\nalternatively an Encoder network can be trained that maps images into the\nlatent space of a \\textit{pretrained} GAN model (Inverse GAN). In the latter\ncase, the networks are trained consecutively, so the Encoder has to make do\nwith whatever model the Decoder learned during GAN training. Training three\nnetworks simultaneously is more unstable and therefore more challenging, but it\nis possible that the Encoder and Decoder benefit from interacting with each\nother during training. We compare the two different approaches and discuss\nwhether it is worth the extra effort to train all three networks\nsimultaneously.\n","authors":["Una M. Kelly","Meike Nauta","Lu Liu","Luuk J. Spreeuwers","Raymond N. J. Veldhuis"],"pdf_url":"https://arxiv.org/pdf/2310.08371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08370v1","updated":"2023-10-12T14:39:58Z","published":"2023-10-12T14:39:58Z","title":"UniPAD: A Universal Pre-training Paradigm for Autonomous Driving","summary":"  In the context of autonomous driving, the significance of effective feature\nlearning is widely acknowledged. While conventional 3D self-supervised\npre-training methods have shown widespread success, most methods follow the\nideas originally designed for 2D images. In this paper, we present UniPAD, a\nnovel self-supervised learning paradigm applying 3D volumetric differentiable\nrendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction\nof continuous 3D shape structures and the intricate appearance characteristics\nof their 2D projections. The flexibility of our method enables seamless\nintegration into both 2D and 3D frameworks, enabling a more holistic\ncomprehension of the scenes. We manifest the feasibility and effectiveness of\nUniPAD by conducting extensive experiments on various downstream 3D tasks. Our\nmethod significantly improves lidar-, camera-, and lidar-camera-based baseline\nby 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline\nachieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic\nsegmentation on the nuScenes validation set, achieving state-of-the-art results\nin comparison with previous methods. The code will be available at\nhttps://github.com/Nightmare-n/UniPAD.\n","authors":["Honghui Yang","Sha Zhang","Di Huang","Xiaoyang Wu","Haoyi Zhu","Tong He","Shixiang Tang","Hengshuang Zhao","Qibo Qiu","Binbin Lin","Xiaofei He","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08368v1","updated":"2023-10-12T14:38:52Z","published":"2023-10-12T14:38:52Z","title":"Mapping Memes to Words for Multimodal Hateful Meme Classification","summary":"  Multimodal image-text memes are prevalent on the internet, serving as a\nunique form of communication that combines visual and textual elements to\nconvey humor, ideas, or emotions. However, some memes take a malicious turn,\npromoting hateful content and perpetuating discrimination. Detecting hateful\nmemes within this multimodal context is a challenging task that requires\nunderstanding the intertwined meaning of text and images. In this work, we\naddress this issue by proposing a novel approach named ISSUES for multimodal\nhateful meme classification. ISSUES leverages a pre-trained CLIP\nvision-language model and the textual inversion technique to effectively\ncapture the multimodal semantic content of the memes. The experiments show that\nour method achieves state-of-the-art results on the Hateful Memes Challenge and\nHarMeme datasets. The code and the pre-trained models are publicly available at\nhttps://github.com/miccunifi/ISSUES.\n","authors":["Giovanni Burbi","Alberto Baldrati","Lorenzo Agnolucci","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2310.08368v1.pdf","comment":"ICCV2023 CLVL Workshop"},{"id":"http://arxiv.org/abs/2310.08367v1","updated":"2023-10-12T14:38:25Z","published":"2023-10-12T14:38:25Z","title":"MCU: A Task-centric Framework for Open-ended Agent Evaluation in\n  Minecraft","summary":"  To pursue the goal of creating an open-ended agent in Minecraft, an\nopen-ended game environment with unlimited possibilities, this paper introduces\na task-centric framework named MCU for Minecraft agent evaluation. The MCU\nframework leverages the concept of atom tasks as fundamental building blocks,\nenabling the generation of diverse or even arbitrary tasks. Within the MCU\nframework, each task is measured with six distinct difficulty scores (time\nconsumption, operational effort, planning complexity, intricacy, creativity,\nnovelty). These scores offer a multi-dimensional assessment of a task from\ndifferent angles, and thus can reveal an agent's capability on specific facets.\nThe difficulty scores also serve as the feature of each task, which creates a\nmeaningful task space and unveils the relationship between tasks. For efficient\nevaluation of Minecraft agents employing the MCU framework, we maintain a\nunified benchmark, namely SkillForge, which comprises representative tasks with\ndiverse categories and difficulty distribution. We also provide convenient\nfilters for users to select tasks to assess specific capabilities of agents. We\nshow that MCU has the high expressivity to cover all tasks used in recent\nliterature on Minecraft agent, and underscores the need for advancements in\nareas such as creativity, precise control, and out-of-distribution\ngeneralization under the goal of open-ended Minecraft agent development.\n","authors":["Haowei Lin","Zihao Wang","Jianzhu Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04099v2","updated":"2023-10-12T14:18:52Z","published":"2023-10-06T09:01:15Z","title":"ClusVPR: Efficient Visual Place Recognition with Clustering-based\n  Weighted Transformer","summary":"  Visual place recognition (VPR) is a highly challenging task that has a wide\nrange of applications, including robot navigation and self-driving vehicles.\nVPR is particularly difficult due to the presence of duplicate regions and the\nlack of attention to small objects in complex scenes, resulting in recognition\ndeviations. In this paper, we present ClusVPR, a novel approach that tackles\nthe specific issues of redundant information in duplicate regions and\nrepresentations of small objects. Different from existing methods that rely on\nConvolutional Neural Networks (CNNs) for feature map generation, ClusVPR\nintroduces a unique paradigm called Clustering-based Weighted Transformer\nNetwork (CWTNet). CWTNet leverages the power of clustering-based weighted\nfeature maps and integrates global dependencies to effectively address visual\ndeviations encountered in large-scale VPR problems. We also introduce the\noptimized-VLAD (OptLAD) layer that significantly reduces the number of\nparameters and enhances model efficiency. This layer is specifically designed\nto aggregate the information obtained from scale-wise image patches.\nAdditionally, our pyramid self-supervised strategy focuses on extracting\nrepresentative and diverse information from scale-wise image patches instead of\nentire images, which is crucial for capturing representative and diverse\ninformation in VPR. Extensive experiments on four VPR datasets show our model's\nsuperior performance compared to existing models while being less complex.\n","authors":["Yifan Xu","Pourya Shamsolmoali","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08339v1","updated":"2023-10-12T13:57:32Z","published":"2023-10-12T13:57:32Z","title":"A Generic Software Framework for Distributed Topological Analysis\n  Pipelines","summary":"  This system paper presents a software framework for the support of\ntopological analysis pipelines in a distributed-memory model. While several\nrecent papers introduced topology-based approaches for distributed-memory\nenvironments, these were reporting experiments obtained with tailored,\nmono-algorithm implementations. In contrast, we describe in this paper a\ngeneral-purpose, generic framework for topological analysis pipelines, i.e. a\nsequence of topological algorithms interacting together, possibly on distinct\nnumbers of processes. Specifically, we instantiated our framework with the MPI\nmodel, within the Topology ToolKit (TTK). While developing this framework, we\nfaced several algorithmic and software engineering challenges, which we\ndocument in this paper. We provide a taxonomy for the distributed-memory\ntopological algorithms supported by TTK, depending on their communication needs\nand provide examples of hybrid MPI+thread parallelizations. Detailed\nperformance analyses show that parallel efficiencies range from $20\\%$ to\n$80\\%$ (depending on the algorithms), and that the MPI-specific preconditioning\nintroduced by our framework induces a negligible computation time overhead. We\nillustrate the new distributed-memory capabilities of TTK with an example of\nadvanced analysis pipeline, combining multiple algorithms, run on the largest\npublicly available dataset we have found (120 billion vertices) on a standard\ncluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a\nroadmap for the completion of TTK's MPI extension, along with generic\nrecommendations for each algorithm communication category.\n","authors":["Eve Le Guillou","Michael Will","Pierre Guillou","Jonas Lukasczyk","Pierre Fortin","Christoph Garth","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2310.08339v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2206.02136v3","updated":"2023-10-12T13:55:06Z","published":"2022-06-05T09:39:12Z","title":"LDRNet: Enabling Real-time Document Localization on Mobile Devices","summary":"  While Identity Document Verification (IDV) technology on mobile devices\nbecomes ubiquitous in modern business operations, the risk of identity theft\nand fraud is increasing. The identity document holder is normally required to\nparticipate in an online video interview to circumvent impostors. However, the\ncurrent IDV process depends on an additional human workforce to support online\nstep-by-step guidance which is inefficient and expensive. The performance of\nexisting AI-based approaches cannot meet the real-time and lightweight demands\nof mobile devices. In this paper, we address those challenges by designing an\nedge intelligence-assisted approach for real-time IDV. Aiming at improving the\nresponsiveness of the IDV process, we propose a new document localization model\nfor mobile devices, LDRNet, to Localize the identity Document in Real-time. On\nthe basis of a lightweight backbone network, we build three prediction branches\nfor LDRNet, the corner points prediction, the line borders prediction and the\ndocument classification. We design novel supplementary targets, the\nequal-division points, and use a new loss function named Line Loss, to improve\nthe speed and accuracy of our approach. In addition to the IDV process, LDRNet\nis an efficient and reliable document localization alternative for all kinds of\nmobile applications. As a matter of proof, we compare the performance of LDRNet\nwith other popular approaches on localizing general document datasets. The\nexperimental results show that LDRNet runs at a speed up to 790 FPS which is\n47x faster, while still achieving comparable Jaccard Index(JI) in single-model\nand single-scale tests.\n","authors":["Han Wu","Holland Qian","Huaming Wu","Aad van Moorsel"],"pdf_url":"https://arxiv.org/pdf/2206.02136v3.pdf","comment":"ECML-PKDD 2022 https://doi.org/10.1007/978-3-031-23618-1_42"},{"id":"http://arxiv.org/abs/2310.08332v1","updated":"2023-10-12T13:46:36Z","published":"2023-10-12T13:46:36Z","title":"Real-Time Neural BRDF with Spherically Distributed Primitives","summary":"  We propose a novel compact and efficient neural BRDF offering highly\nversatile material representation, yet with very-light memory and neural\ncomputation consumption towards achieving real-time rendering. The results in\nFigure 1, rendered at full HD resolution on a current desktop machine, show\nthat our system achieves real-time rendering with a wide variety of\nappearances, which is approached by the following two designs. On the one hand,\nnoting that bidirectional reflectance is distributed in a very sparse\nhigh-dimensional subspace, we propose to project the BRDF into two\nlow-dimensional components, i.e., two hemisphere feature-grids for incoming and\noutgoing directions, respectively. On the other hand, learnable neural\nreflectance primitives are distributed on our highly-tailored spherical surface\ngrid, which offer informative features for each component and alleviate the\nconventional heavy feature learning network to a much smaller one, leading to\nvery fast evaluation. These primitives are centrally stored in a codebook and\ncan be shared across multiple grids and even across materials, based on the\nlow-cost indices stored in material-specific spherical surface grids. Our\nneural BRDF, which is agnostic to the material, provides a unified framework\nthat can represent a variety of materials in consistent manner. Comprehensive\nexperimental results on measured BRDF compression, Monte Carlo simulated BRDF\nacceleration, and extension to spatially varying effect demonstrate the\nsuperior quality and generalizability achieved by the proposed scheme.\n","authors":["Yishun Dou","Zhong Zheng","Qiaoqiao Jin","Bingbing Ni","Yugang Chen","Junxiang Ke"],"pdf_url":"https://arxiv.org/pdf/2310.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08326v1","updated":"2023-10-12T13:42:49Z","published":"2023-10-12T13:42:49Z","title":"NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence\n  Understanding","summary":"  Understanding 4D point cloud sequences online is of significant practical\nvalue in various scenarios such as VR/AR, robotics, and autonomous driving. The\nkey goal is to continuously analyze the geometry and dynamics of a 3D scene as\nunstructured and redundant point cloud sequences arrive. And the main challenge\nis to effectively model the long-term history while keeping computational costs\nmanageable. To tackle these challenges, we introduce a generic online 4D\nperception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that\ncan be adapted to existing 4D backbones, significantly enhancing their online\nperception capabilities for both indoor and outdoor scenarios. To efficiently\ncapture the redundant 4D history, we propose a neural scene model that\nfactorizes geometry and motion information by constructing geometry tokens\nseparately storing geometry and motion features. Exploiting the history becomes\nas straightforward as querying the neural scene model. As the sequence\nprogresses, the neural scene model dynamically deforms to align with new\nobservations, effectively providing the historical context and updating itself\nwith the new observations. By employing token representation, NSM4D also\nexhibits robustness to low-level sensor noise and maintains a compact size\nthrough a geometric sampling scheme. We integrate NSM4D with state-of-the-art\n4D perception backbones, demonstrating significant improvements on various\nonline perception benchmarks in indoor and outdoor settings. Notably, we\nachieve a 9.6% accuracy improvement for HOI4D online action segmentation and a\n3.4% mIoU improvement for SemanticKITTI online semantic segmentation.\nFurthermore, we show that NSM4D inherently offers excellent scalability to\nlonger sequences beyond the training set, which is crucial for real-world\napplications.\n","authors":["Yuhao Dong","Zhuoyang Zhang","Yunze Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2310.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v1","updated":"2023-10-12T13:33:04Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08316v1","updated":"2023-10-12T13:27:21Z","published":"2023-10-12T13:27:21Z","title":"Extended target tracking utilizing machine-learning software -- with\n  applications to animal classification","summary":"  This paper considers the problem of detecting and tracking objects in a\nsequence of images. The problem is formulated in a filtering framework, using\nthe output of object-detection algorithms as measurements. An extension to the\nfiltering formulation is proposed that incorporates class information from the\nprevious frame to robustify the classification, even if the object-detection\nalgorithm outputs an incorrect prediction. Further, the properties of the\nobject-detection algorithm are exploited to quantify the uncertainty of the\nbounding box detection in each frame. The complete filtering method is\nevaluated on camera trap images of the four large Swedish carnivores, bear,\nlynx, wolf, and wolverine. The experiments show that the class tracking\nformulation leads to a more robust classification.\n","authors":["Magnus Malmstrm","Anton Kullberg","Isaac Skog","Daniel Axehill","Fredrik Gustafsson"],"pdf_url":"https://arxiv.org/pdf/2310.08316v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.08312v1","updated":"2023-10-12T13:20:17Z","published":"2023-10-12T13:20:17Z","title":"GePSAn: Generative Procedure Step Anticipation in Cooking Videos","summary":"  We study the problem of future step anticipation in procedural videos. Given\na video of an ongoing procedural activity, we predict a plausible next\nprocedure step described in rich natural language. While most previous work\nfocus on the problem of data scarcity in procedural video datasets, another\ncore challenge of future anticipation is how to account for multiple plausible\nfuture realizations in natural settings. This problem has been largely\noverlooked in previous work. To address this challenge, we frame future step\nprediction as modelling the distribution of all possible candidates for the\nnext step. Specifically, we design a generative model that takes a series of\nvideo clips as input, and generates multiple plausible and diverse candidates\n(in natural language) for the next step. Following previous work, we side-step\nthe video annotation scarcity by pretraining our model on a large text-based\ncorpus of procedural activities, and then transfer the model to the video\ndomain. Our experiments, both in textual and video domains, show that our model\ncaptures diversity in the next step prediction and generates multiple plausible\nfuture predictions. Moreover, our model establishes new state-of-the-art\nresults on YouCookII, where it outperforms existing baselines on the next step\nanticipation. Finally, we also show that our model can successfully transfer\nfrom text to the video domain zero-shot, ie, without fine-tuning or adaptation,\nand produces good-quality future step predictions from video.\n","authors":["Mohamed Ashraf Abdelsalam","Samrudhdhi B. Rangrej","Isma Hadji","Nikita Dvornik","Konstantinos G. Derpanis","Afsaneh Fazly"],"pdf_url":"https://arxiv.org/pdf/2310.08312v1.pdf","comment":"published at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.08304v1","updated":"2023-10-12T13:11:38Z","published":"2023-10-12T13:11:38Z","title":"CHIP: Contrastive Hierarchical Image Pretraining","summary":"  Few-shot object classification is the task of classifying objects in an image\nwith limited number of examples as supervision. We propose a one-shot/few-shot\nclassification model that can classify an object of any unseen class into a\nrelatively general category in an hierarchically based classification. Our\nmodel uses a three-level hierarchical contrastive loss based ResNet152\nclassifier for classifying an object based on its features extracted from Image\nembedding, not used during the training phase. For our experimentation, we have\nused a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal\nclasses for training our model and created our own dataset of unseen classes\nfor evaluating our trained model. Our model provides satisfactory results in\nclassifying the unknown objects into a generic category which has been later\ndiscussed in greater detail.\n","authors":["Arpit Mittal","Harshil Jhaveri","Swapnil Mallick","Abhishek Ajmera"],"pdf_url":"https://arxiv.org/pdf/2310.08304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08303v1","updated":"2023-10-12T13:09:40Z","published":"2023-10-12T13:09:40Z","title":"Multimodal Variational Auto-encoder based Audio-Visual Segmentation","summary":"  We propose an Explicit Conditional Multimodal Variational Auto-Encoder\n(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources\nin the video sequence. Existing AVS methods focus on implicit feature fusion\nstrategies, where models are trained to fit the discrete samples in the\ndataset. With a limited and less diverse dataset, the resulting performance is\nusually unsatisfactory. In contrast, we address this problem from an effective\nrepresentation learning perspective, aiming to model the contribution of each\nmodality explicitly. Specifically, we find that audio contains critical\ncategory information of the sound producers, and visual data provides candidate\nsound producer(s). Their shared information corresponds to the target sound\nproducer(s) shown in the visual data. In this case, cross-modal shared\nrepresentation learning is especially important for AVS. To achieve this, our\nECMVAE factorizes the representations of each modality with a modality-shared\nrepresentation and a modality-specific representation. An orthogonality\nconstraint is applied between the shared and specific representations to\nmaintain the exclusive attribute of the factorized latent code. Further, a\nmutual information maximization regularizer is introduced to achieve extensive\nexploration of each modality. Quantitative and qualitative evaluations on the\nAVSBench demonstrate the effectiveness of our approach, leading to a new\nstate-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging\nMS3 subset for multiple sound source segmentation.\n","authors":["Yuxin Mao","Jing Zhang","Mochu Xiang","Yiran Zhong","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2310.08303v1.pdf","comment":"Accepted by ICCV2023,Project\n  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)"},{"id":"http://arxiv.org/abs/2308.12435v2","updated":"2023-10-12T12:57:55Z","published":"2023-08-23T21:36:35Z","title":"Characterising representation dynamics in recurrent neural networks for\n  object recognition","summary":"  Recurrent neural networks (RNNs) have yielded promising results for both\nrecognizing objects in challenging conditions and modeling aspects of primate\nvision. However, the representational dynamics of recurrent computations remain\npoorly understood, especially in large-scale visual models. Here, we studied\nsuch dynamics in RNNs trained for object classification on MiniEcoset, a novel\nsubset of ecoset. We report two main insights. First, upon inference,\nrepresentations continued to evolve after correct classification, suggesting a\nlack of the notion of being ``done with classification''. Second, focusing on\n``readout zones'' as a way to characterize the activation trajectories, we\nobserve that misclassified representations exhibit activation patterns with\nlower L2 norm, and are positioned more peripherally in the readout zones. Such\narrangements help the misclassified representations move into the correct zones\nas time progresses. Our findings generalize to networks with lateral and\ntop-down connections, and include both additive and multiplicative interactions\nwith the bottom-up sweep. The results therefore contribute to a general\nunderstanding of RNN dynamics in naturalistic tasks. We hope that the analysis\nframework will aid future investigations of other types of RNNs, including\nunderstanding of representational dynamics in primate vision.\n","authors":["Sushrut Thorat","Adrien Doerig","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2308.12435v2.pdf","comment":"8 pages, 7 figures; revision of our Conference on Cognitive\n  Computational Neuroscience (CCN) 2023 paper"},{"id":"http://arxiv.org/abs/2202.09348v2","updated":"2023-10-12T12:56:28Z","published":"2022-02-18T18:36:01Z","title":"A Machine Learning Paradigm for Studying Pictorial Realism: Are\n  Constable's Clouds More Real than His Contemporaries?","summary":"  The British landscape painter John Constable is considered foundational for\nthe Realist movement in 19th-century European painting. Constable's painted\nskies, in particular, were seen as remarkably accurate by his contemporaries,\nan impression shared by many viewers today. Yet, assessing the accuracy of\nrealist paintings like Constable's is subjective or intuitive, even for\nprofessional art historians, making it difficult to say with certainty what set\nConstable's skies apart from those of his contemporaries. Our goal is to\ncontribute to a more objective understanding of Constable's realism. We propose\na new machine-learning-based paradigm for studying pictorial realism in an\nexplainable way. Our framework assesses realism by measuring the similarity\nbetween clouds painted by artists noted for their skies, like Constable, and\nphotographs of clouds. The experimental results of cloud classification show\nthat Constable approximates more consistently than his contemporaries the\nformal features of actual clouds in his paintings. The study, as a novel\ninterdisciplinary approach that combines computer vision and machine learning,\nmeteorology, and art history, is a springboard for broader and deeper analyses\nof pictorial realism.\n","authors":["Zhuomin Zhang","Elizabeth C. Mansfield","Jia Li","John Russell","George S. Young","Catherine Adams","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2202.09348v2.pdf","comment":"Supplementary materials are available from the authors or\n  http://wang.ist.psu.edu"},{"id":"http://arxiv.org/abs/2303.07189v3","updated":"2023-10-12T12:47:22Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  We aim to optimize the binary detection of Chronic Obstructive Pulmonary\nDisease (COPD) based on emphysema presence in the lung with convolutional\nneural networks (CNN) by exploring manually adjusted versus automated\nwindow-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT\nimages (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with\nCOPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and\npreprocessed. For each image, intensity values were manually clipped to the\nemphysema window setting and a baseline 'full-range' window setting.\nClass-balanced train, validation, and test sets contained 3,392, 1,114, and\n2,688 images. The network backbone was optimized by comparing various CNN\narchitectures. Furthermore, automated WSO was implemented by adding a\ncustomized layer to the model. The image-level area under the Receiver\nOperating Characteristics curve (AUC) [lower, upper limit 95% confidence] was\nutilized to compare model variations. Repeated inference (n=7) on the test set\nshowed that the DenseNet was the most efficient backbone and achieved a mean\nAUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually\nadjusted to the emphysema window, the DenseNet model predicted COPD with a mean\nAUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an\noptimal window in the proximity of the emphysema window setting was learned\nautomatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of\nCOPD with DenseNet models was improved by WSO of CT data to the emphysema\nwindow setting range.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Johannes Thalhammer","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01601v3","updated":"2023-10-12T12:43:39Z","published":"2023-04-04T07:43:56Z","title":"Primitive Simultaneous Optimization of Similarity Metrics for Image\n  Registration","summary":"  Even though simultaneous optimization of similarity metrics is a standard\nprocedure in the field of semantic segmentation, surprisingly, this is much\nless established for image registration. To help closing this gap in the\nliterature, we investigate in a complex multi-modal 3D setting whether\nsimultaneous optimization of registration metrics, here implemented by means of\nprimitive summation, can benefit image registration. We evaluate two\nchallenging datasets containing collections of pre- to post-operative and pre-\nto intra-operative MR images of glioma. Employing the proposed optimization, we\ndemonstrate improved registration accuracy in terms of TRE on expert\nneuroradiologists' landmark annotations.\n","authors":["Diana Waldmannstetter","Benedikt Wiestler","Julian Schwarting","Ivan Ezhov","Marie Metz","Spyridon Bakas","Bhakti Baheti","Satrajit Chakrabarty","Daniel Rueckert","Jan S. Kirschke","Rolf A. Heckemann","Marie Piraud","Bjoern H. Menze","Florian Kofler"],"pdf_url":"https://arxiv.org/pdf/2304.01601v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08276v1","updated":"2023-10-12T12:28:47Z","published":"2023-10-12T12:28:47Z","title":"Direction-Oriented Visual-semantic Embedding Model for Remote Sensing\n  Image-text Retrieval","summary":"  Image-text retrieval has developed rapidly in recent years. However, it is\nstill a challenge in remote sensing due to visual-semantic imbalance, which\nleads to incorrect matching of non-semantic visual and textual features. To\nsolve this problem, we propose a novel Direction-Oriented Visual-semantic\nEmbedding Model (DOVE) to mine the relationship between vision and language.\nConcretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the\ndistance between the final visual and textual embeddings in the latent semantic\nspace, oriented by regional visual features. Meanwhile, a lightweight Digging\nText Genome Assistant (DTGA) is designed to expand the range of tractable\ntextual representation and enhance global word-level semantic connections using\nless attention operations. Ultimately, we exploit a global visual-semantic\nconstraint to reduce single visual dependency and serve as an external\nconstraint for the final visual and textual representations. The effectiveness\nand superiority of our method are verified by extensive experiments including\nparameter evaluation, quantitative comparison, ablation studies and visual\nanalysis, on two benchmark datasets, RSICD and RSITMD.\n","authors":["Qing Ma","Jiancheng Pan","Cong Bai"],"pdf_url":"https://arxiv.org/pdf/2310.08276v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.08261v1","updated":"2023-10-12T12:06:31Z","published":"2023-10-12T12:06:31Z","title":"GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for\n  Multi-Modal 3D Object Detection","summary":"  LiDAR and cameras are complementary sensors for 3D object detection in\nautonomous driving. However, it is challenging to explore the unnatural\ninteraction between point clouds and images, and the critical factor is how to\nconduct feature alignment of heterogeneous modalities. Currently, many methods\nachieve feature alignment by projection calibration only, without considering\nthe problem of coordinate conversion accuracy errors between sensors, leading\nto sub-optimal performance. In this paper, we present GraphAlign, a more\naccurate feature alignment strategy for 3D object detection by graph matching.\nSpecifically, we fuse image features from a semantic segmentation encoder in\nthe image branch and point cloud features from a 3D Sparse CNN in the LiDAR\nbranch. To save computation, we construct the nearest neighbor relationship by\ncalculating Euclidean distance within the subspaces that are divided into the\npoint cloud features. Through the projection calibration between the image and\npoint cloud, we project the nearest neighbors of point cloud features onto the\nimage features. Then by matching the nearest neighbors with a single point\ncloud to multiple images, we search for a more appropriate feature alignment.\nIn addition, we provide a self-attention module to enhance the weights of\nsignificant relations to fine-tune the feature alignment between heterogeneous\nmodalities. Extensive experiments on nuScenes benchmark demonstrate the\neffectiveness and efficiency of our GraphAlign.\n","authors":["Ziying Song","Haiyue Wei","Lin Bai","Lei Yang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2310.08261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08259v1","updated":"2023-10-12T12:05:51Z","published":"2023-10-12T12:05:51Z","title":"Invisible Threats: Backdoor Attack in OCR Systems","summary":"  Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.\n","authors":["Mauro Conti","Nicola Farronato","Stefanos Koffas","Luca Pajola","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2310.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04825v2","updated":"2023-10-12T12:05:15Z","published":"2023-10-07T14:29:57Z","title":"Comparative study of multi-person tracking methods","summary":"  This paper presents a study of two tracking algorithms (SORT~\\cite{7533003}\nand Tracktor++~\\cite{2019}) that were ranked first positions on the MOT\nChallenge leaderboard (The MOTChallenge web page: https://motchallenge.net ).\nThe purpose of this study is to discover the techniques used and to provide\nuseful insights about these algorithms in the tracking pipeline that could\nimprove the performance of MOT tracking algorithms. To this end, we adopted the\npopular tracking-by-detection approach. We trained our own Pedestrian Detection\nmodel using the MOT17Det dataset (MOT17Det :\nhttps://motchallenge.net/data/MOT17Det/ ). We also used a re-identification\nmodel trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ )\nfor Tracktor++ to reduce the false re-identification alarms. We then present\nexperimental results which shows that Tracktor++ is a better multi-person\ntracking algorithm than SORT. We also performed ablation studies to discover\nthe contribution of re-identification(RE-ID) network and motion to the results\nof Tracktor++. We finally conclude by providing some recommendations for future\nresearch.\n","authors":["Denis Mbey Akola"],"pdf_url":"https://arxiv.org/pdf/2310.04825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04829v2","updated":"2023-10-12T12:04:16Z","published":"2023-10-07T14:38:16Z","title":"How to effectively train an ensemble of Faster R-CNN object detectors to\n  quantify uncertainty","summary":"  This paper presents a new approach for training two-stage object detection\nensemble models, more specifically, Faster R-CNN models to estimate\nuncertainty. We propose training one Region Proposal\nNetwork(RPN)~\\cite{https://doi.org/10.48550/arxiv.1506.01497} and multiple Fast\nR-CNN prediction heads is all you need to build a robust deep ensemble network\nfor estimating uncertainty in object detection. We present this approach and\nprovide experiments to show that this approach is much faster than the naive\nmethod of fully training all $n$ models in an ensemble. We also estimate the\nuncertainty by measuring this ensemble model's Expected Calibration Error\n(ECE). We then further compare the performance of this model with that of\nGaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted\nbounding box coordinates. The source code is released at\n\\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}\n","authors":["Denis Mbey Akola","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.04829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19443v2","updated":"2023-10-12T12:02:34Z","published":"2023-05-30T22:34:48Z","title":"OWAdapt: An adaptive loss function for deep learning using OWA operators","summary":"  In this paper, we propose a fuzzy adaptive loss function for enhancing deep\nlearning performance in classification tasks. Specifically, we redefine the\ncross-entropy loss to effectively address class-level noise conditions,\nincluding the challenging problem of class imbalance. Our approach introduces\naggregation operators, leveraging the power of fuzzy logic to improve\nclassification accuracy. The rationale behind our proposed method lies in the\niterative up-weighting of class-level components within the loss function,\nfocusing on those with larger errors. To achieve this, we employ the ordered\nweighted average (OWA) operator and combine it with an adaptive scheme for\ngradient-based learning. Through extensive experimentation, our method\noutperforms other commonly used loss functions, such as the standard\ncross-entropy or focal loss, across various binary and multiclass\nclassification tasks. Furthermore, we explore the influence of hyperparameters\nassociated with the OWA operators and present a default configuration that\nperforms well across different experimental settings.\n","authors":["Sebastin Maldonado","Carla Vairetti","Katherine Jara","Miguel Carrasco","Julio Lpez"],"pdf_url":"https://arxiv.org/pdf/2305.19443v2.pdf","comment":"15 pages, 1 figure, published"},{"id":"http://arxiv.org/abs/2310.08255v1","updated":"2023-10-12T11:59:54Z","published":"2023-10-12T11:59:54Z","title":"Distilling from Vision-Language Models for Improved OOD Generalization\n  in Vision Tasks","summary":"  Vision-Language Models (VLMs) such as CLIP are trained on large amounts of\nimage-text pairs, resulting in remarkable generalization across several data\ndistributions. The prohibitively expensive training and data\ncollection/curation costs of these models make them valuable Intellectual\nProperty (IP) for organizations. This motivates a vendor-client paradigm, where\na vendor trains a large-scale VLM and grants only input-output access to\nclients on a pay-per-query basis in a black-box setting. The client aims to\nminimize inference cost by distilling the VLM to a student model using the\nlimited available task-specific data, and further deploying this student model\nin the downstream application. While naive distillation largely improves the\nIn-Domain (ID) accuracy of the student, it fails to transfer the superior\nout-of-distribution (OOD) generalization of the VLM teacher using the limited\navailable labeled images. To mitigate this, we propose Vision-Language to\nVision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and\nlanguage modalities of the teacher model with the vision modality of a\npre-trained student model, and further distills the aligned VLM embeddings to\nthe student. This maximally retains the pre-trained features of the student,\nwhile also incorporating the rich representations of the VLM image encoder and\nthe superior generalization of the text embeddings. The proposed approach\nachieves state-of-the-art results on the standard Domain Generalization\nbenchmarks in a black-box teacher setting, and also when weights of the VLM are\naccessible.\n","authors":["Sravanti Addepalli","Ashish Ramayee Asokan","Lakshay Sharma","R. Venkatesh Babu"],"pdf_url":"https://arxiv.org/pdf/2310.08255v1.pdf","comment":"Code is available at https://github.com/val-iisc/VL2V-ADiP.git"},{"id":"http://arxiv.org/abs/2309.06188v2","updated":"2023-10-12T11:51:21Z","published":"2023-09-12T12:54:12Z","title":"Computer Vision Pipeline for Automated Antarctic Krill Analysis","summary":"  British Antarctic Survey (BAS) researchers launch annual expeditions to the\nAntarctic in order to estimate Antarctic Krill biomass and assess the change\nfrom previous years. These comparisons provide insight into the effects of the\ncurrent environment on this key component of the marine food chain. In this\nwork we have developed tools for automating the data collection and analysis\nprocess, using web-based image annotation tools and deep learning image\nclassification and regression models. We achieve highly accurate krill instance\nsegmentation results with an average 77.28% AP score, as well as separate\nmaturity stage and length estimation of krill specimens with 62.99% accuracy\nand a 1.98mm length error respectively.\n","authors":["Mazvydas Gudelis","Michal Mackiewicz","Julie Bremner","Sophie Fielding"],"pdf_url":"https://arxiv.org/pdf/2309.06188v2.pdf","comment":"Accepted to MVEO @ BMVC 2023"},{"id":"http://arxiv.org/abs/2308.03998v4","updated":"2023-10-12T11:49:34Z","published":"2023-08-08T02:28:48Z","title":"Real-time Strawberry Detection Based on Improved YOLOv5s Architecture\n  for Robotic Harvesting in open-field environment","summary":"  This study proposed a YOLOv5-based custom object detection model to detect\nstrawberries in an outdoor environment. The original architecture of the\nYOLOv5s was modified by replacing the C3 module with the C2f module in the\nbackbone network, which provided a better feature gradient flow. Secondly, the\nSpatial Pyramid Pooling Fast in the final layer of the backbone network of\nYOLOv5s was combined with Cross Stage Partial Net to improve the generalization\nability over the strawberry dataset in this study. The proposed architecture\nwas named YOLOv5s-Straw. The RGB images dataset of the strawberry canopy with\nthree maturity classes (immature, nearly mature, and mature) was collected in\nopen-field environment and augmented through a series of operations including\nbrightness reduction, brightness increase, and noise adding. To verify the\nsuperiority of the proposed method for strawberry detection in open-field\nenvironment, four competitive detection models (YOLOv3-tiny, YOLOv5s,\nYOLOv5s-C2f, and YOLOv8s) were trained, and tested under the same computational\nenvironment and compared with YOLOv5s-Straw. The results showed that the\nhighest mean average precision of 80.3% was achieved using the proposed\narchitecture whereas the same was achieved with YOLOv3-tiny, YOLOv5s,\nYOLOv5s-C2f, and YOLOv8s were 73.4%, 77.8%, 79.8%, 79.3%, respectively.\nSpecifically, the average precision of YOLOv5s-Straw was 82.1% in the immature\nclass, 73.5% in the nearly mature class, and 86.6% in the mature class, which\nwere 2.3% and 3.7%, respectively, higher than that of the latest YOLOv8s. The\nmodel included 8.6*10^6 network parameters with an inference speed of 18ms per\nimage while the inference speed of YOLOv8s had a slower inference speed of\n21.0ms and heavy parameters of 11.1*10^6, which indicates that the proposed\nmodel is fast enough for real time strawberry detection and localization for\nthe robotic picking.\n","authors":["Zixuan He","Salik Ram Khanal","Xin Zhang","Manoj Karkee","Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.03998v4.pdf","comment":"20 pages; 15 figures"},{"id":"http://arxiv.org/abs/2310.08230v1","updated":"2023-10-12T11:23:07Z","published":"2023-10-12T11:23:07Z","title":"Fast Discrete Optimisation for Geometrically Consistent 3D Shape\n  Matching","summary":"  In this work we propose to combine the advantages of learning-based and\ncombinatorial formalisms for 3D shape matching. While learning-based shape\nmatching solutions lead to state-of-the-art matching performance, they do not\nensure geometric consistency, so that obtained matchings are locally unsmooth.\nOn the contrary, axiomatic methods allow to take geometric consistency into\naccount by explicitly constraining the space of valid matchings. However,\nexisting axiomatic formalisms are impractical since they do not scale to\npractically relevant problem sizes, or they require user input for the\ninitialisation of non-convex optimisation problems. In this work we aim to\nclose this gap by proposing a novel combinatorial solver that combines a unique\nset of favourable properties: our approach is (i) initialisation free, (ii)\nmassively parallelisable powered by a quasi-Newton method, (iii) provides\noptimality gaps, and (iv) delivers decreased runtime and globally optimal\nresults for many instances.\n","authors":["Paul Roetzer","Ahmed Abbas","Dongliang Cao","Florian Bernard","Paul Swoboda"],"pdf_url":"https://arxiv.org/pdf/2310.08230v1.pdf","comment":"Paul Roetzer and Ahmed Abbas contributed equally"},{"id":"http://arxiv.org/abs/2310.08222v1","updated":"2023-10-12T11:14:27Z","published":"2023-10-12T11:14:27Z","title":"Structural analysis of Hindi online handwritten characters for character\n  recognition","summary":"  Direction properties of online strokes are used to analyze them in terms of\nhomogeneous regions or sub-strokes with points satisfying common geometric\nproperties. Such sub-strokes are called sub-units. These properties are used to\nextract sub-units from Hindi ideal online characters. These properties along\nwith some heuristics are used to extract sub-units from Hindi online\nhandwritten characters.\\\\ A method is developed to extract point stroke,\nclockwise curve stroke, counter-clockwise curve stroke and loop stroke segments\nas sub-units from Hindi online handwritten characters. These extracted\nsub-units are close in structure to the sub-units of the corresponding Hindi\nonline ideal characters.\\\\ Importance of local representation of online\nhandwritten characters in terms of sub-units is assessed by training a\nclassifier with sub-unit level local and character level global features\nextracted from characters for character recognition. The classifier has the\nrecognition accuracy of 93.5\\% on the testing set. This accuracy is the highest\nwhen compared with that of the classifiers trained only with global features\nextracted from characters in the same training set and evaluated on the same\ntesting set.\\\\ Sub-unit extraction algorithm and the sub-unit based character\nclassifier are tested on Hindi online handwritten character dataset. This\ndataset consists of samples from 96 different characters. There are 12832 and\n2821 samples in the training and testing sets, respectively.\n","authors":["Anand Sharma","A. G. Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2310.08222v1.pdf","comment":"34 pages, 36 jpg figures"},{"id":"http://arxiv.org/abs/2309.00848v2","updated":"2023-10-12T11:11:23Z","published":"2023-09-02T07:17:43Z","title":"Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach","summary":"  This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using\nthe YOLOv8 model and innovative post-processing techniques. We tackle\nchallenges unique to the complex Bengali script by employing data augmentation\nfor model robustness. After meticulous validation set evaluation, we fine-tune\nour approach on the complete dataset, leading to a two-stage prediction\nstrategy for accurate element segmentation. Our ensemble model, combined with\npost-processing, outperforms individual base architectures, addressing issues\nidentified in the BaDLAD dataset. By leveraging this approach, we aim to\nadvance Bengali document analysis, contributing to improved OCR and document\ncomprehension and BaDLAD serves as a foundational resource for this endeavor,\naiding future research in the field. Furthermore, our experiments provided key\ninsights to incorporate new strategies into the established solution.\n","authors":["Nazmus Sakib Ahmed","Saad Sakib Noor","Ashraful Islam Shanto Sikder","Abhijit Paul"],"pdf_url":"https://arxiv.org/pdf/2309.00848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08217v1","updated":"2023-10-12T11:05:34Z","published":"2023-10-12T11:05:34Z","title":"TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge\n  Retention and Promotion","summary":"  Continual learning (CL) has remained a persistent challenge for deep neural\nnetworks due to catastrophic forgetting (CF) of previously learned tasks.\nSeveral techniques such as weight regularization, experience rehearsal, and\nparameter isolation have been proposed to alleviate CF. Despite their relative\nsuccess, these research directions have predominantly remained orthogonal and\nsuffer from several shortcomings, while missing out on the advantages of\ncompeting strategies. On the contrary, the brain continually learns,\naccommodates, and transfers knowledge across tasks by simultaneously leveraging\nseveral neurophysiological processes, including neurogenesis, active\nforgetting, neuromodulation, metaplasticity, experience rehearsal, and\ncontext-dependent gating, rarely resulting in CF. Inspired by how the brain\nexploits multiple mechanisms concurrently, we propose TriRE, a novel CL\nparadigm that encompasses retaining the most prominent neurons for each task,\nrevising and solidifying the extracted knowledge of current and past tasks, and\nactively promoting less active neurons for subsequent tasks through rewinding\nand relearning. Across CL settings, TriRE significantly reduces task\ninterference and surpasses different CL approaches considered in isolation.\n","authors":["Preetha Vijayan","Prashant Bhat","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2310.08217v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.08206v1","updated":"2023-10-12T10:51:23Z","published":"2023-10-12T10:51:23Z","title":"Long-Tailed Classification Based on Coarse-Grained Leading Forest and\n  Multi-Center Loss","summary":"  Long-tailed(LT) classification is an unavoidable and challenging problem in\nthe real world. Most of the existing long-tailed classification methods focus\nonly on solving the inter-class imbalance in which there are more samples in\nthe head class than in the tail class, while ignoring the intra-lass imbalance\nin which the number of samples of the head attribute within the same class is\nmuch larger than the number of samples of the tail attribute. The deviation in\nthe model is caused by both of these factors, and due to the fact that\nattributes are implicit in most datasets and the combination of attributes is\nvery complex, the intra-class imbalance is more difficult to handle. For this\npurpose, we proposed a long-tailed classification framework, known as\n\\textbf{\\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest\n(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint\nsolution model by means of invariant feature learning. In this method, we\ndesigned an unsupervised learning method, i.e., CLF, to better characterize the\ndistribution of attributes within a class. Depending on the distribution of\nattributes, we can flexibly construct sampling strategies suitable for\ndifferent environments. In addition, we introduce a new metric learning loss\n(MCL), which aims to gradually eliminate confusing attributes during the\nfeature learning process. More importantly, this approach does not depend on a\nspecific model structure and can be integrated with existing LT methods as an\nindependent component. We have conducted extensive experiments and our approach\nhas state-of-the-art performance in both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT, and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n","authors":["Jinye Yang","Ji Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08206v1.pdf","comment":"This is another research work to apply leading tree structure along\n  with deep learning architecture"},{"id":"http://arxiv.org/abs/2310.08204v1","updated":"2023-10-12T10:50:21Z","published":"2023-10-12T10:50:21Z","title":"Lifelong Audio-video Masked Autoencoder with Forget-robust Localized\n  Alignments","summary":"  We present a lifelong audio-video masked autoencoder that continually learns\nthe multimodal representations from a video stream containing audio-video\npairs, while its distribution continually shifts over time. Specifically, we\npropose two novel ideas to tackle the problem: (1) Localized Alignment: We\nintroduce a small trainable multimodal encoder that predicts the audio and\nvideo tokens that are well-aligned with each other. This allows the model to\nlearn only the highly correlated audiovisual patches with accurate multimodal\nrelationships. (2) Forget-robust multimodal patch selection: We compare the\nrelative importance of each audio-video patch between the current and past data\npair to mitigate unintended drift of the previously learned audio-video\nrepresentations. Our proposed method, FLAVA (Forget-robust Localized\nAudio-Video Alignment), therefore, captures the complex relationships between\nthe audio and video modalities during training on a sequence of pre-training\ntasks while alleviating the forgetting of learned audiovisual correlations. Our\nexperiments validate that FLAVA outperforms the state-of-the-art continual\nlearning methods on several benchmark datasets under continual audio-video\nrepresentation learning scenarios.\n","authors":["Jaewoo Lee","Jaehong Yoon","Wonjae Kim","Yunji Kim","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.08204v1.pdf","comment":"Preprint, project page: https://g-jwlee.github.io/FLAVA/"},{"id":"http://arxiv.org/abs/2310.05969v2","updated":"2023-10-12T10:26:17Z","published":"2023-09-28T07:57:03Z","title":"Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning\n  Approach","summary":"  Reading and interpreting chest X-ray images is one of the most radiologist's\nroutines. However, it still can be challenging, even for the most experienced\nones. Therefore, we proposed a multi-model deep learning-based automated chest\nX-ray report generator system designed to assist radiologists in their work.\nThe basic idea of the proposed system is by utilizing multi\nbinary-classification models for detecting multi abnormalities, with each model\nresponsible for detecting one abnormality, in a single image. In this study, we\nlimited the radiology abnormalities detection to only cardiomegaly, lung\neffusion, and consolidation. The system generates a radiology report by\nperforming the following three steps: image pre-processing, utilizing deep\nlearning models to detect abnormalities, and producing a report. The aim of the\nimage pre-processing step is to standardize the input by scaling it to 128x128\npixels and slicing it into three segments, which covers the upper, lower, and\nmiddle parts of the lung. After pre-processing, each corresponding model\nclassifies the image, resulting in a 0 (zero) for no abnormality detected and a\n1 (one) for the presence of an abnormality. The prediction outputs of each\nmodel are then concatenated to form a 'result code'. The 'result code' is used\nto construct a report by selecting the appropriate pre-determined sentence for\neach detected abnormality in the report generation step. The proposed system is\nexpected to reduce the workload of radiologists and increase the accuracy of\nchest X-ray diagnosis.\n","authors":["Arief Purnama Muharram","Hollyana Puteri Haryono","Abassi Haji Juma","Ira Puspasari","Nugraha Priya Utama"],"pdf_url":"https://arxiv.org/pdf/2310.05969v2.pdf","comment":"Presented in the 2023 IEEE International Conference on Data and\n  Software Engineering (ICoDSE 2023)"},{"id":"http://arxiv.org/abs/2309.13336v2","updated":"2023-10-12T10:24:42Z","published":"2023-09-23T10:58:08Z","title":"FedDrive v2: an Analysis of the Impact of Label Skewness in Federated\n  Semantic Segmentation for Autonomous Driving","summary":"  We propose FedDrive v2, an extension of the Federated Learning benchmark for\nSemantic Segmentation in Autonomous Driving. While the first version aims at\nstudying the effect of domain shift of the visual features across clients, in\nthis work, we focus on the distribution skewness of the labels. We propose six\nnew federated scenarios to investigate how label skewness affects the\nperformance of segmentation models and compare it with the effect of domain\nshift. Finally, we study the impact of using the domain information during\ntesting. Official website: https://feddrive.github.io\n","authors":["Eros Fan","Marco Ciccone","Barbara Caputo"],"pdf_url":"https://arxiv.org/pdf/2309.13336v2.pdf","comment":"5th Italian Conference on Robotics and Intelligent Machines (I-RIM)\n  2023"},{"id":"http://arxiv.org/abs/2310.08182v1","updated":"2023-10-12T10:17:40Z","published":"2023-10-12T10:17:40Z","title":"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness\n  Evaluation","summary":"  The lack of standardized robustness metrics and the widespread reliance on\nnumerous unrelated benchmark datasets for testing have created a gap between\nacademically validated robust models and their often problematic practical\nadoption. To address this, we introduce XIMAGENET-12, an explainable benchmark\ndataset with over 200K images and 15,600 manual semantic annotations. Covering\n12 categories from ImageNet to represent objects commonly encountered in\npractical life and simulating six diverse scenarios, including overexposure,\nblurring, color changing, etc., we further propose a novel robustness criterion\nthat extends beyond model generation ability assessment. This benchmark\ndataset, along with related code, is available at\nhttps://sites.google.com/view/ximagenet-12/home. Researchers and practitioners\ncan leverage this resource to evaluate the robustness of their visual models\nunder challenging conditions and ultimately benefit from the demands of\npractical computer vision systems.\n","authors":["Qiang Li","Dan Zhang","Shengzhao Lei","Xun Zhao","Shuyan Li","Porawit Kamnoedboon","WeiWei Li"],"pdf_url":"https://arxiv.org/pdf/2310.08182v1.pdf","comment":"UnderSubmission"},{"id":"http://arxiv.org/abs/2310.07449v2","updated":"2023-10-12T10:14:39Z","published":"2023-10-11T12:51:16Z","title":"PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction","summary":"  Neural surface reconstruction is sensitive to the camera pose noise, even if\nstate-of-the-art pose estimators like COLMAP or ARKit are used. More\nimportantly, existing Pose-NeRF joint optimisation methods have struggled to\nimprove pose accuracy in challenging real-world scenarios. To overcome the\nchallenges, we introduce the pose residual field (\\textbf{PoRF}), a novel\nimplicit representation that uses an MLP for regressing pose updates. This is\nmore robust than the conventional pose parameter optimisation due to parameter\nsharing that leverages global information over the entire sequence.\nFurthermore, we propose an epipolar geometry loss to enhance the supervision\nthat leverages the correspondences exported from COLMAP results without the\nextra computational overhead. Our method yields promising results. On the DTU\ndataset, we reduce the rotation error by 78\\% for COLMAP poses, leading to the\ndecreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the\nMobileBrick dataset that contains casually captured unbounded 360-degree\nvideos, our method refines ARKit poses and improves the reconstruction F1 score\nfrom 69.18 to 75.67, outperforming that with the dataset provided ground-truth\npose (75.14). These achievements demonstrate the efficacy of our approach in\nrefining camera poses and improving the accuracy of neural surface\nreconstruction in real-world scenarios.\n","authors":["Jia-Wang Bian","Wenjing Bian","Victor Adrian Prisacariu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2310.07449v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.08177v1","updated":"2023-10-12T10:03:25Z","published":"2023-10-12T10:03:25Z","title":"Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization","summary":"  Evaluating the adversarial robustness of machine learning models using\ngradient-based attacks is challenging. In this work, we show that\nhyperparameter optimization can improve fast minimum-norm attacks by automating\nthe selection of the loss function, the optimizer and the step-size scheduler,\nalong with the corresponding hyperparameters. Our extensive evaluation\ninvolving several robust models demonstrates the improved efficacy of fast\nminimum-norm attacks when hyper-up with hyperparameter optimization. We release\nour open-source code at https://github.com/pralab/HO-FMN.\n","authors":["Giuseppe Floris","Raffaele Mura","Luca Scionis","Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08177v1.pdf","comment":"Accepted at ESANN23"},{"id":"http://arxiv.org/abs/2310.08165v1","updated":"2023-10-12T09:37:56Z","published":"2023-10-12T09:37:56Z","title":"COVID-19 Detection Using Swin Transformer Approach from Computed\n  Tomography Images","summary":"  The accurate and efficient diagnosis of COVID-19 is of paramount importance,\nparticularly in the context of large-scale medical imaging datasets. In this\npreprint paper, we propose a novel approach for COVID-19 diagnosis using CT\nimages that leverages the power of Swin Transformer models, state-of-the-art\nsolutions in computer vision tasks. Our method includes a systematic approach\nfor patient-level predictions, where individual CT slices are classified as\nCOVID-19 or non-COVID, and the patient's overall diagnosis is determined\nthrough majority voting. The application of the Swin Transformer in this\ncontext results in patient-level predictions that demonstrate exceptional\ndiagnostic accuracy. In terms of evaluation metrics, our approach consistently\noutperforms the baseline, as well as numerous competing methods, showcasing its\neffectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model\nexceeds the baseline and offers a robust solution for accurate diagnosis.\n","authors":["Kenan Morani"],"pdf_url":"https://arxiv.org/pdf/2310.08165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17046v2","updated":"2023-10-12T09:22:10Z","published":"2023-06-29T15:43:06Z","title":"Spiking Denoising Diffusion Probabilistic Models","summary":"  Spiking neural networks (SNNs) have ultra-low energy consumption and high\nbiological plausibility due to their binary and bio-driven nature compared with\nartificial neural networks (ANNs). While previous research has primarily\nfocused on enhancing the performance of SNNs in classification tasks, the\ngenerative potential of SNNs remains relatively unexplored. In our paper, we\nput forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new\nclass of SNN-based generative models that achieve high sample quality. To fully\nexploit the energy efficiency of SNNs, we propose a purely Spiking U-Net\narchitecture, which achieves comparable performance to its ANN counterpart\nusing only 4 time steps, resulting in significantly reduced energy consumption.\nExtensive experimental results reveal that our approach achieves\nstate-of-the-art on the generative tasks and substantially outperforms other\nSNN-based generative models, achieving up to $12\\times$ and $6\\times$\nimprovement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we\npropose a threshold-guided strategy that can further improve the performances\nby 16.7% in a training-free manner. The SDDPM symbolizes a significant\nadvancement in the field of SNN generation, injecting new perspectives and\npotential avenues of exploration.\n","authors":["Jiahang Cao","Ziqing Wang","Hanzhong Guo","Hao Cheng","Qiang Zhang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2306.17046v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2207.09339v3","updated":"2023-10-12T09:13:37Z","published":"2022-07-19T15:49:35Z","title":"Vision Transformers: From Semantic Segmentation to Dense Prediction","summary":"  The emergence of vision transformers (ViTs) in image classification has\nshifted the methodologies for visual representation learning. In particular,\nViTs learn visual representation at full receptive field per layer across all\nthe image patches, in comparison to the increasing receptive fields of CNNs\nacross layers and other alternatives (e.g., large kernels and atrous\nconvolution). In this work, for the first time we explore the global context\nlearning potentials of ViTs for dense visual prediction (e.g., semantic\nsegmentation). Our motivation is that through learning global context at full\nreceptive field layer by layer, ViTs may capture stronger long-range dependency\ninformation, critical for dense prediction tasks. We first demonstrate that\nencoding an image as a sequence of patches, a vanilla ViT without local\nconvolution and resolution reduction can yield stronger visual representation\nfor semantic segmentation. For example, our model, termed as SEgmentation\nTRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the\ntest leaderboard on the day of submission) and Pascal Context (55.83% mIoU),\nand performs competitively on Cityscapes. For tackling general dense visual\nprediction tasks in a cost-effective manner, we further formulate a family of\nHierarchical Local-Global (HLG) Transformers, characterized by local attention\nwithin windows and global-attention across windows in a pyramidal architecture.\nExtensive experiments show that our methods achieve appealing performance on a\nvariety of dense prediction tasks (e.g., object detection and instance\nsegmentation and semantic segmentation) as well as image classification. Our\ncode and models are available at https://github.com/fudan-zvg/SETR.\n","authors":["Li Zhang","Jiachen Lu","Sixiao Zheng","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2207.09339v3.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840"},{"id":"http://arxiv.org/abs/2301.12082v3","updated":"2023-10-12T09:01:04Z","published":"2023-01-28T03:58:32Z","title":"Pushing the Limits of Fewshot Anomaly Detection in Industry Vision:\n  Graphcore","summary":"  In the area of fewshot anomaly detection (FSAD), efficient visual feature\nplays an essential role in memory bank M-based methods. However, these methods\ndo not account for the relationship between the visual feature and its rotated\nvisual feature, drastically limiting the anomaly detection performance. To push\nthe limits, we reveal that rotation-invariant feature property has a\nsignificant impact in industrial-based FSAD. Specifically, we utilize graph\nrepresentation in FSAD and provide a novel visual isometric invariant feature\n(VIIF) as anomaly measurement feature. As a result, VIIF can robustly improve\nthe anomaly discriminating ability and can further reduce the size of redundant\nfeatures stored in M by a large amount. Besides, we provide a novel model\nGraphCore via VIIFs that can fast implement unsupervised FSAD training and can\nimprove the performance of anomaly detection. A comprehensive evaluation is\nprovided for comparing GraphCore and other SOTA anomaly detection models under\nour proposed fewshot anomaly detection setting, which shows GraphCore can\nincrease average AUC by 5.8%, 4.1%, 3.4%, and 1.6% on MVTec AD and by 25.5%,\n22.0%, 16.9%, and 14.1% on MPDD for 1, 2, 4, and 8-shot cases, respectively.\n","authors":["Guoyang Xie","Jinbao Wang","Jiaqi Liu","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.12082v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02081v2","updated":"2023-10-12T09:00:34Z","published":"2022-10-05T08:19:16Z","title":"Locate before Answering: Answer Guided Question Localization for Video\n  Question Answering","summary":"  Video question answering (VideoQA) is an essential task in vision-language\nunderstanding, which has attracted numerous research attention recently.\nNevertheless, existing works mostly achieve promising performances on short\nvideos of duration within 15 seconds. For VideoQA on minute-level long-term\nvideos, those methods are likely to fail because of lacking the ability to deal\nwith noise and redundancy caused by scene changes and multiple actions in the\nvideo. Considering the fact that the question often remains concentrated in a\nshort temporal range, we propose to first locate the question to a segment in\nthe video and then infer the answer using the located segment only. Under this\nscheme, we propose \"Locate before Answering\" (LocAns), a novel approach that\nintegrates a question locator and an answer predictor into an end-to-end model.\nDuring the training phase, the available answer label not only serves as the\nsupervision signal of the answer predictor, but also is used to generate pseudo\ntemporal labels for the question locator. Moreover, we design a decoupled\nalternative training strategy to update the two modules separately. In the\nexperiments, LocAns achieves state-of-the-art performance on two modern\nlong-term VideoQA datasets NExT-QA and ActivityNet-QA, and its qualitative\nexamples show the reliable performance of the question localization.\n","authors":["Tianwen Qian","Ran Cui","Jingjing Chen","Pai Peng","Xiaowei Guo","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2210.02081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08143v1","updated":"2023-10-12T08:58:01Z","published":"2023-10-12T08:58:01Z","title":"A Deep Learning Framework for Spatiotemporal Ultrasound Localization\n  Microscopy","summary":"  Ultrasound Localization Microscopy can resolve the microvascular bed down to\na few micrometers. To achieve such performance microbubble contrast agents must\nperfuse the entire microvascular network. Microbubbles are then located\nindividually and tracked over time to sample individual vessels, typically over\nhundreds of thousands of images. To overcome the fundamental limit of\ndiffraction and achieve a dense reconstruction of the network, low microbubble\nconcentrations must be used, which lead to acquisitions lasting several\nminutes. Conventional processing pipelines are currently unable to deal with\ninterference from multiple nearby microbubbles, further reducing achievable\nconcentrations. This work overcomes this problem by proposing a Deep Learning\napproach to recover dense vascular networks from ultrasound acquisitions with\nhigh microbubble concentrations. A realistic mouse brain microvascular network,\nsegmented from 2-photon microscopy, was used to train a three-dimensional\nconvolutional neural network based on a V-net architecture. Ultrasound data\nsets from multiple microbubbles flowing through the microvascular network were\nsimulated and used as ground truth to train the 3D CNN to track microbubbles.\nThe 3D-CNN approach was validated in silico using a subset of the data and in\nvivo on a rat brain acquisition. In silico, the CNN reconstructed vascular\nnetworks with higher precision (81%) than a conventional ULM framework (70%).\nIn vivo, the CNN could resolve micro vessels as small as 10 $\\mu$m with an\nincrease in resolution when compared against a conventional approach.\n","authors":["Lo Milecki","Jonathan Pore","Hatim Belgharbi","Chlo Bourquin","Rafat Damseh","Patrick Delafontaine-Martel","Frdric Lesage","Maxime Gasse","Jean Provost"],"pdf_url":"https://arxiv.org/pdf/2310.08143v1.pdf","comment":"Copyright 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.08142v1","updated":"2023-10-12T08:57:33Z","published":"2023-10-12T08:57:33Z","title":"Fine-Grained Annotation for Face Anti-Spoofing","summary":"  Face anti-spoofing plays a critical role in safeguarding facial recognition\nsystems against presentation attacks. While existing deep learning methods show\npromising results, they still suffer from the lack of fine-grained annotations,\nwhich lead models to learn task-irrelevant or unfaithful features. In this\npaper, we propose a fine-grained annotation method for face anti-spoofing.\nSpecifically, we first leverage the Segment Anything Model (SAM) to obtain\npixel-wise segmentation masks by utilizing face landmarks as point prompts. The\nface landmarks provide segmentation semantics, which segments the face into\nregions. We then adopt these regions as masks and assemble them into three\nseparate annotation maps: spoof, living, and background maps. Finally, we\ncombine three separate maps into a three-channel map as annotations for model\ntraining. Furthermore, we introduce the Multi-Channel Region Exchange\nAugmentation (MCREA) to diversify training data and reduce overfitting.\nExperimental results demonstrate that our method outperforms existing\nstate-of-the-art approaches in both intra-dataset and cross-dataset\nevaluations.\n","authors":["Xu Chen","Yunde Jia","Yuwei Wu"],"pdf_url":"https://arxiv.org/pdf/2310.08142v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.08139v1","updated":"2023-10-12T08:55:10Z","published":"2023-10-12T08:55:10Z","title":"DualAug: Exploiting Additional Heavy Augmentation with OOD Data\n  Rejection","summary":"  Data augmentation is a dominant method for reducing model overfitting and\nimproving generalization. Most existing data augmentation methods tend to find\na compromise in augmenting the data, \\textit{i.e.}, increasing the amplitude of\naugmentation carefully to avoid degrading some data too much and doing harm to\nthe model performance. We delve into the relationship between data augmentation\nand model performance, revealing that the performance drop with heavy\naugmentation comes from the presence of out-of-distribution (OOD) data.\nNonetheless, as the same data transformation has different effects for\ndifferent training samples, even for heavy augmentation, there remains part of\nin-distribution data which is beneficial to model training. Based on the\nobservation, we propose a novel data augmentation method, named\n\\textbf{DualAug}, to keep the augmentation in distribution as much as possible\nat a reasonable time and computational cost. We design a data mixing strategy\nto fuse augmented data from both the basic- and the heavy-augmentation\nbranches. Extensive experiments on supervised image classification benchmarks\nshow that DualAug improve various automated data augmentation method. Moreover,\nthe experiments on semi-supervised learning and contrastive self-supervised\nlearning demonstrate that our DualAug can also improve related method. Code is\navailable at\n\\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.\n","authors":["Zehao Wang","Yiwen Guo","Qizhang Li","Guanglei Yang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2310.08139v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2301.11514v5","updated":"2023-10-12T08:49:31Z","published":"2023-01-27T03:18:09Z","title":"Deep Industrial Image Anomaly Detection: A Survey","summary":"  The recent rapid development of deep learning has laid a milestone in\nindustrial Image Anomaly Detection (IAD). In this paper, we provide a\ncomprehensive review of deep learning-based image anomaly detection techniques,\nfrom the perspectives of neural network architectures, levels of supervision,\nloss functions, metrics and datasets. In addition, we extract the new setting\nfrom industrial manufacturing and review the current IAD approaches under our\nproposed our new setting. Moreover, we highlight several opening challenges for\nimage anomaly detection. The merits and downsides of representative network\narchitectures under varying supervision are discussed. Finally, we summarize\nthe research findings and point out future research directions. More resources\nare available at\nhttps://github.com/M-3LAB/awesome-industrial-anomaly-detection.\n","authors":["Jiaqi Liu","Guoyang Xie","Jinbao Wang","Shangnian Li","Chengjie Wang","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.11514v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13359v3","updated":"2023-10-12T08:47:19Z","published":"2023-01-31T01:24:45Z","title":"IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing","summary":"  Image anomaly detection (IAD) is an urgent issue that needs to be addressed\nin modern industrial manufacturing (IM). Recently, many advanced algorithms\nhave been released, but their performance varies greatly due to non-uniformed\nsettings. That is, researchers find it difficult to analyze because they are\ndesigned for different or specific cases in IM. To eliminate this problem, we\nfirst propose a uniform IAD setting to systematically assess the effectiveness\nof these algorithms, mainly considering three aspects of supervision level\n(unsupervised, fully supervised), learning paradigm (few-shot, continual, noisy\nlabel), and efficiency (memory usage, inference speed). Then, we skillfully\nconstruct a comprehensive image anomaly detection benchmark (IM-IAD), which\nincludes 19 algorithms on 7 major datasets with the same setting. Our extensive\nexperiments (17,017 total) provide new insights into the redesign or selection\nof the IAD algorithm under uniform conditions. Importantly, the proposed IM-IAD\npresents feasible challenges and future directions for further work. We believe\nthat this work can have a significant impact on the IAD field. To foster\nreproducibility and accessibility, the source code of IM-IAD is uploaded on the\nwebsite, https://github.com/M-3LAB/IM-IAD.\n","authors":["Guoyang Xie","Jinbao Wang","Jiaqi Liu","Jiayi Lyu","Yong Liu","Chengjie Wang","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.13359v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08129v1","updated":"2023-10-12T08:36:25Z","published":"2023-10-12T08:36:25Z","title":"Tailored Visions: Enhancing Text-to-Image Generation with Personalized\n  Prompt Rewriting","summary":"  We propose a novel perspective of viewing large pretrained models as search\nengines, thereby enabling the repurposing of techniques previously used to\nenhance search engine performance. As an illustration, we employ a personalized\nquery rewriting technique in the realm of text-to-image generation. Despite\nsignificant progress in the field, it is still challenging to create\npersonalized visual representations that align closely with the desires and\npreferences of individual users. This process requires users to articulate\ntheir ideas in words that are both comprehensible to the models and accurately\ncapture their vision, posing difficulties for many users. In this paper, we\ntackle this challenge by leveraging historical user interactions with the\nsystem to enhance user prompts. We propose a novel approach that involves\nrewriting user prompts based a new large-scale text-to-image dataset with over\n300k prompts from 3115 users. Our rewriting model enhances the expressiveness\nand alignment of user prompts with their intended visual outputs. Experimental\nresults demonstrate the superiority of our methods over baseline approaches, as\nevidenced in our new offline evaluation method and online tests. Our approach\nopens up exciting possibilities of applying more search engine techniques to\nbuild truly personalized large pretrained models.\n","authors":["Zijie Chen","Lichao Zhang","Fangsheng Weng","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2310.08129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03198v5","updated":"2023-10-12T08:31:50Z","published":"2023-04-06T16:21:56Z","title":"RFAConv: Innovating Spatial Attention and Standard Convolutional\n  Operation","summary":"  Spatial attention has been widely used to improve the performance of\nconvolutional neural networks. However, it has certain limitations. In this\npaper, we propose a new perspective on the effectiveness of spatial attention,\nwhich is that the spatial attention mechanism essentially solves the problem of\nconvolutional kernel parameter sharing. However, the information contained in\nthe attention map generated by spatial attention is not sufficient for\nlarge-size convolutional kernels. Therefore, we propose a novel attention\nmechanism called Receptive-Field Attention (RFA). Existing spatial attention,\nsuch as Convolutional Block Attention Module (CBAM) and Coordinated Attention\n(CA) focus only on spatial features, which does not fully address the problem\nof convolutional kernel parameter sharing. In contrast, RFA not only focuses on\nthe receptive-field spatial feature but also provides effective attention\nweights for large-size convolutional kernels. The Receptive-Field Attention\nconvolutional operation (RFAConv), developed by RFA, represents a new approach\nto replace the standard convolution operation. It offers nearly negligible\nincrement of computational cost and parameters, while significantly improving\nnetwork performance. We conducted a series of experiments on ImageNet-1k, COCO,\nand VOC datasets to demonstrate the superiority of our approach. Of particular\nimportance, we believe that it is time to shift focus from spatial features to\nreceptive-field spatial features for current spatial attention mechanisms. In\nthis way, we can further improve network performance and achieve even better\nresults. The code and pre-trained models for the relevant tasks can be found at\nhttps://github.com/Liuchen1997/RFAConv.\n","authors":["Xin Zhang","Chen Liu","Degang Yang","Tingting Song","Yichen Ye","Ke Li","Yingze Song"],"pdf_url":"https://arxiv.org/pdf/2304.03198v5.pdf","comment":"12 pages, 11figures"},{"id":"http://arxiv.org/abs/2310.08117v1","updated":"2023-10-12T08:21:17Z","published":"2023-10-12T08:21:17Z","title":"DUSA: Decoupled Unsupervised Sim2Real Adaptation for\n  Vehicle-to-Everything Collaborative Perception","summary":"  Vehicle-to-Everything (V2X) collaborative perception is crucial for\nautonomous driving. However, achieving high-precision V2X perception requires a\nsignificant amount of annotated real-world data, which can always be expensive\nand hard to acquire. Simulated data have raised much attention since they can\nbe massively produced at an extremely low cost. Nevertheless, the significant\ndomain gap between simulated and real-world data, including differences in\nsensor type, reflectance patterns, and road surroundings, often leads to poor\nperformance of models trained on simulated data when evaluated on real-world\ndata. In addition, there remains a domain gap between real-world collaborative\nagents, e.g. different types of sensors may be installed on autonomous vehicles\nand roadside infrastructures with different extrinsics, further increasing the\ndifficulty of sim2real generalization. To take full advantage of simulated\ndata, we present a new unsupervised sim2real domain adaptation method for V2X\ncollaborative detection named Decoupled Unsupervised Sim2Real Adaptation\n(DUSA). Our new method decouples the V2X collaborative sim2real domain\nadaptation problem into two sub-problems: sim2real adaptation and inter-agent\nadaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real\nAdapter (LSA) module to adaptively aggregate features from critical locations\nof the feature map and align the features between simulated data and real-world\ndata via a sim/real discriminator on the aggregated global feature. For\ninter-agent adaptation, we further devise a Confidence-aware Inter-agent\nAdapter (CIA) module to align the fine-grained features from heterogeneous\nagents under the guidance of agent-wise confidence maps. Experiments\ndemonstrate the effectiveness of the proposed DUSA approach on unsupervised\nsim2real adaptation from the simulated V2XSet dataset to the real-world\nDAIR-V2X-C dataset.\n","authors":["Xianghao Kong","Wentao Jiang","Jinrang Jia","Yifeng Shi","Runsheng Xu","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08117v1.pdf","comment":"ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.08116v1","updated":"2023-10-12T08:17:57Z","published":"2023-10-12T08:17:57Z","title":"Multimodal Active Measurement for Human Mesh Recovery in Close Proximity","summary":"  For safe and sophisticated physical human-robot interactions (pHRI), a robot\nneeds to estimate the accurate body pose or mesh of the target person. However,\nin these pHRI scenarios, the robot cannot fully observe the target person's\nbody with equipped cameras because the target person is usually close to the\nrobot. This leads to severe truncation and occlusions, and results in poor\naccuracy of human pose estimation. For better accuracy of human pose estimation\nor mesh recovery on this limited information from cameras, we propose an active\nmeasurement and sensor fusion framework of the equipped cameras and other\nsensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are\nobtained attendantly through pHRI without additional costs. These sensor\nmeasurements are sparse but reliable and informative cues for human mesh\nrecovery. In our active measurement process, camera viewpoints and sensor\nplacements are optimized based on the uncertainty of the estimated pose, which\nis closely related to the truncated or occluded areas. In our sensor fusion\nprocess, we fuse the sensor measurements to the camera-based estimated pose by\nminimizing the distance between the estimated mesh and measured positions. Our\nmethod is agnostic to robot configurations. Experiments were conducted using\nthe Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch\nsensor on the robot arm. Our proposed method demonstrated the superiority in\nthe human pose estimation accuracy on the quantitative comparison. Furthermore,\nour proposed method reliably estimated the pose of the target person in\npractical settings such as target people occluded by a blanket and standing aid\nwith the robot arm.\n","authors":["Takahiro Maeda","Keisuke Takeshita","Kazuhito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2310.08116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14108v3","updated":"2023-10-12T08:05:18Z","published":"2022-11-25T13:50:00Z","title":"3DDesigner: Towards Photorealistic 3D Object Generation and Editing with\n  Text-guided Diffusion Models","summary":"  Text-guided diffusion models have shown superior performance in image/video\ngeneration and editing. While few explorations have been performed in 3D\nscenarios. In this paper, we discuss three fundamental and interesting problems\non this topic. First, we equip text-guided diffusion models to achieve\n3D-consistent generation. Specifically, we integrate a NeRF-like neural field\nto generate low-resolution coarse results for a given camera view. Such results\ncan provide 3D priors as condition information for the following diffusion\nprocess. During denoising diffusion, we further enhance the 3D consistency by\nmodeling cross-view correspondences with a novel two-stream (corresponding to\ntwo different views) asynchronous diffusion process. Second, we study 3D local\nediting and propose a two-step solution that can generate 360-degree\nmanipulated results by editing an object from a single view. Step 1, we propose\nto perform 2D local editing by blending the predicted noises. Step 2, we\nconduct a noise-to-text inversion process that maps 2D blended noises into the\nview-independent text embedding space. Once the corresponding text embedding is\nobtained, 360-degree images can be generated. Last but not least, we extend our\nmodel to perform one-shot novel view synthesis by fine-tuning on a single\nimage, firstly showing the potential of leveraging text guidance for novel view\nsynthesis. Extensive experiments and various applications show the prowess of\nour 3DDesigner. The project page is available at\nhttps://3ddesigner-diffusion.github.io/.\n","authors":["Gang Li","Heliang Zheng","Chaoyue Wang","Chang Li","Changwen Zheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2211.14108v3.pdf","comment":"Submitted to IJCV"},{"id":"http://arxiv.org/abs/2310.00847v2","updated":"2023-10-12T08:04:14Z","published":"2023-10-02T02:01:00Z","title":"Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?","summary":"  Out-of-distribution (OOD) detection is critical for safety-sensitive machine\nlearning applications and has been extensively studied, yielding a plethora of\nmethods developed in the literature. However, most studies for OOD detection\ndid not use pre-trained models and trained a backbone from scratch. In recent\nyears, transferring knowledge from large pre-trained models to downstream tasks\nby lightweight tuning has become mainstream for training in-distribution (ID)\nclassifiers. To bridge the gap between the practice of OOD detection and\ncurrent classifiers, the unique and crucial problem is that the samples whose\ninformation networks know often come as OOD input. We consider that such data\nmay significantly affect the performance of large pre-trained networks because\nthe discriminability of these OOD data depends on the pre-training algorithm.\nHere, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,\nwe aim to reveal the effect of PT-OOD on the OOD detection performance of\npre-trained networks from the perspective of pre-training algorithms. To\nachieve this, we explore the PT-OOD detection performance of supervised and\nself-supervised pre-training algorithms with linear-probing tuning, the most\ncommon efficient tuning method. Through our experiments and analysis, we find\nthat the low linear separability of PT-OOD in the feature space heavily\ndegrades the PT-OOD detection performance, and self-supervised models are more\nvulnerable to PT-OOD than supervised pre-trained models, even with\nstate-of-the-art detection methods. To solve this vulnerability, we further\npropose a unique solution to large-scale pre-trained models: Leveraging\npowerful instance-by-instance discriminative representations of pre-trained\nmodels and detecting OOD in the feature space independent of the ID decision\nboundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.\n","authors":["Atsuyuki Miyai","Qing Yu","Go Irie","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2310.00847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07522v2","updated":"2023-10-12T08:02:18Z","published":"2023-10-11T14:19:05Z","title":"S4C: Self-Supervised Semantic Scene Completion with Neural Fields","summary":"  3D semantic scene understanding is a fundamental challenge in computer\nvision. It enables mobile agents to autonomously plan and navigate arbitrary\nenvironments. SSC formalizes this challenge as jointly estimating dense\ngeometry and semantic information from sparse observations of a scene. Current\nmethods for SSC are generally trained on 3D ground truth based on aggregated\nLiDAR scans. This process relies on special sensors and annotation by hand\nwhich are costly and do not scale well. To overcome this issue, our work\npresents the first self-supervised approach to SSC called S4C that does not\nrely on 3D ground truth data. Our proposed method can reconstruct a scene from\na single image and only relies on videos and pseudo segmentation ground truth\ngenerated from off-the-shelf image segmentation network during training. Unlike\nexisting methods, which use discrete voxel grids, we represent scenes as\nimplicit semantic fields. This formulation allows querying any point within the\ncamera frustum for occupancy and semantic class. Our architecture is trained\nthrough rendering-based self-supervised losses. Nonetheless, our method\nachieves performance close to fully supervised state-of-the-art methods.\nAdditionally, our method demonstrates strong generalization capabilities and\ncan synthesize accurate segmentation maps for far away viewpoints.\n","authors":["Adrian Hayler","Felix Wimbauer","Dominik Muhle","Christian Rupprecht","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.07522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08106v1","updated":"2023-10-12T08:01:11Z","published":"2023-10-12T08:01:11Z","title":"Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing\n  Label Bias in Foundation Models","summary":"  Foundation models like CLIP allow zero-shot transfer on various tasks without\nadditional training data. Yet, the zero-shot performance is less competitive\nthan a fully supervised one. Thus, to enhance the performance, fine-tuning and\nensembling are also commonly adopted to better fit the downstream tasks.\nHowever, we argue that such prior work has overlooked the inherent biases in\nfoundation models. Due to the highly imbalanced Web-scale training set, these\nfoundation models are inevitably skewed toward frequent semantics, and thus the\nsubsequent fine-tuning or ensembling is still biased. In this study, we\nsystematically examine the biases in foundation models and demonstrate the\nefficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that\nbias estimation in foundation models is challenging, as most pre-train data\ncannot be explicitly accessed like in traditional long-tailed classification\ntasks. To this end, GLA has an optimization-based bias estimation approach for\ndebiasing foundation models. As our work resolves a fundamental flaw in the\npre-training, the proposed GLA demonstrates significant improvements across a\ndiverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large\naverage improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on\nlong-tailed classification. Codes are in \\url{https://github.com/BeierZhu/GLA}.\n","authors":["Beier Zhu","Kaihua Tang","Qianru Sun","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08106v1.pdf","comment":"Accepted by NeurIPS2023"},{"id":"http://arxiv.org/abs/2212.09408v3","updated":"2023-10-12T07:55:38Z","published":"2022-12-19T12:40:13Z","title":"Universal Object Detection with Large Vision Model","summary":"  Over the past few years, there has been growing interest in developing a\nbroad, universal, and general-purpose computer vision system. Such systems have\nthe potential to address a wide range of vision tasks simultaneously, without\nbeing limited to specific problems or data domains. This universality is\ncrucial for practical, real-world computer vision applications. In this study,\nour focus is on a specific challenge: the large-scale, multi-domain universal\nobject detection problem, which contributes to the broader goal of achieving a\nuniversal vision system. This problem presents several intricate challenges,\nincluding cross-dataset category label duplication, label conflicts, and the\nnecessity to handle hierarchical taxonomies. To address these challenges, we\nintroduce our approach to label handling, hierarchy-aware loss design, and\nresource-efficient model training utilizing a pre-trained large vision model.\nOur method has demonstrated remarkable performance, securing a prestigious\nsecond-place ranking in the object detection track of the Robust Vision\nChallenge 2022 (RVC 2022) on a million-scale cross-dataset object detection\nbenchmark. We believe that our comprehensive study will serve as a valuable\nreference and offer an alternative approach for addressing similar challenges\nwithin the computer vision community. The source code for our work is openly\navailable at https://github.com/linfeng93/Large-UniDet.\n","authors":["Feng Lin","Wenze Hu","Yaowei Wang","Yonghong Tian","Guangming Lu","Fanglin Chen","Yong Xu","Xiaoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2212.09408v3.pdf","comment":"Accepted by International Journal of Computer Vision (IJCV). The 2nd\n  place in the object detection track of the Robust Vision Challenge (RVC 2022)"},{"id":"http://arxiv.org/abs/2309.15505v2","updated":"2023-10-12T07:55:05Z","published":"2023-09-27T09:13:40Z","title":"Finite Scalar Quantization: VQ-VAE Made Simple","summary":"  We propose to replace vector quantization (VQ) in the latent representation\nof VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where\nwe project the VAE representation down to a few dimensions (typically less than\n10). Each dimension is quantized to a small set of fixed values, leading to an\n(implicit) codebook given by the product of these sets. By appropriately\nchoosing the number of dimensions and values each dimension can take, we obtain\nthe same codebook size as in VQ. On top of such discrete representations, we\ncan train the same models that have been trained on VQ-VAE representations. For\nexample, autoregressive and masked transformer models for image generation,\nmultimodal generation, and dense prediction computer vision tasks. Concretely,\nwe employ FSQ with MaskGIT for image generation, and with UViM for depth\nestimation, colorization, and panoptic segmentation. Despite the much simpler\ndesign of FSQ, we obtain competitive performance in all these tasks. We\nemphasize that FSQ does not suffer from codebook collapse and does not need the\ncomplex machinery employed in VQ (commitment losses, codebook reseeding, code\nsplitting, entropy penalties, etc.) to learn expressive discrete\nrepresentations.\n","authors":["Fabian Mentzer","David Minnen","Eirikur Agustsson","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2309.15505v2.pdf","comment":"Code:\n  https://github.com/google-research/google-research/tree/master/fsq"},{"id":"http://arxiv.org/abs/2310.08094v1","updated":"2023-10-12T07:40:39Z","published":"2023-10-12T07:40:39Z","title":"SingleInsert: Inserting New Concepts from a Single Image into\n  Text-to-Image Models for Flexible Editing","summary":"  Recent progress in text-to-image (T2I) models enables high-quality image\ngeneration with flexible textual control. To utilize the abundant visual priors\nin the off-the-shelf T2I models, a series of methods try to invert an image to\nproper embedding that aligns with the semantic space of the T2I model. However,\nthese image-to-text (I2T) inversion methods typically need multiple source\nimages containing the same concept or struggle with the imbalance between\nediting flexibility and visual fidelity. In this work, we point out that the\ncritical problem lies in the foreground-background entanglement when learning\nan intended concept, and propose a simple and effective baseline for\nsingle-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage\nscheme. In the first stage, we regulate the learned embedding to concentrate on\nthe foreground area without being associated with the irrelevant background. In\nthe second stage, we finetune the T2I model for better visual resemblance and\ndevise a semantic loss to prevent the language drift problem. With the proposed\ntechniques, SingleInsert excels in single concept generation with high visual\nfidelity while allowing flexible editing. Additionally, SingleInsert can\nperform single-image novel view synthesis and multiple concepts composition\nwithout requiring joint training. To facilitate evaluation, we design an\nediting prompt list and introduce a metric named Editing Success Rate (ESR) for\nquantitative assessment of editing flexibility. Our project page is:\nhttps://jarrentwu1031.github.io/SingleInsert-web/\n","authors":["Zijie Wu","Chaohui Yu","Zhen Zhu","Fan Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2310.08094v1.pdf","comment":"Project page: https://jarrentwu1031.github.io/SingleInsert-web/"},{"id":"http://arxiv.org/abs/2310.08092v1","updated":"2023-10-12T07:38:28Z","published":"2023-10-12T07:38:28Z","title":"Consistent123: Improve Consistency for One Image to 3D Object Synthesis","summary":"  Large image diffusion models enable novel view synthesis with high quality\nand excellent zero-shot capability. However, such models based on\nimage-to-image translation have no guarantee of view consistency, limiting the\nperformance for downstream tasks like 3D reconstruction and image-to-3D\ngeneration. To empower consistency, we propose Consistent123 to synthesize\nnovel views simultaneously by incorporating additional cross-view attention\nlayers and the shared self-attention mechanism. The proposed attention\nmechanism improves the interaction across all synthesized views, as well as the\nalignment between the condition view and novel views. In the sampling stage,\nsuch architecture supports simultaneously generating an arbitrary number of\nviews while training at a fixed length. We also introduce a progressive\nclassifier-free guidance strategy to achieve the trade-off between texture and\ngeometry for synthesized object views. Qualitative and quantitative experiments\nshow that Consistent123 outperforms baselines in view consistency by a large\nmargin. Furthermore, we demonstrate a significant improvement of Consistent123\non varying downstream tasks, showing its great potential in the 3D generation\nfield. The project page is available at consistent-123.github.io.\n","authors":["Haohan Weng","Tianyu Yang","Jianan Wang","Yu Li","Tong Zhang","C. L. Philip Chen","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08092v1.pdf","comment":"For more qualitative results, please see\n  https://consistent-123.github.io/"},{"id":"http://arxiv.org/abs/2212.00564v2","updated":"2023-10-12T07:21:23Z","published":"2022-12-01T15:11:21Z","title":"Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion","summary":"  Point clouds captured by scanning devices are often incomplete due to\nocclusion. To overcome this limitation, point cloud completion methods have\nbeen developed to predict the complete shape of an object based on its partial\ninput. These methods can be broadly classified as supervised or unsupervised.\nHowever, both categories require a large number of 3D complete point clouds,\nwhich may be difficult to capture. In this paper, we propose Cross-PCC, an\nunsupervised point cloud completion method without requiring any 3D complete\npoint clouds. We only utilize 2D images of the complete objects, which are\neasier to capture than 3D complete and clean point clouds. Specifically, to\ntake advantage of the complementary information from 2D images, we use a\nsingle-view RGB image to extract 2D features and design a fusion module to fuse\nthe 2D and 3D features extracted from the partial point cloud. To guide the\nshape of predicted point clouds, we project the predicted points of the object\nto the 2D plane and use the foreground pixels of its silhouette maps to\nconstrain the position of the projected points. To reduce the outliers of the\npredicted point clouds, we propose a view calibrator to move the points\nprojected to the background into the foreground by the single-view silhouette\nimage. To the best of our knowledge, our approach is the first point cloud\ncompletion method that does not require any 3D supervision. The experimental\nresults of our method are superior to those of the state-of-the-art\nunsupervised methods by a large margin. Moreover, our method even achieves\ncomparable performance to some supervised methods. We will make the source code\npublicly available at https://github.com/ltwu6/cross-pcc.\n","authors":["Lintai Wu","Qijian Zhang","Junhui Hou","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2212.00564v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.08084v1","updated":"2023-10-12T07:17:14Z","published":"2023-10-12T07:17:14Z","title":"Volumetric Medical Image Segmentation via Scribble Annotations and Shape\n  Priors","summary":"  Recently, weakly-supervised image segmentation using weak annotations like\nscribbles has gained great attention in computer vision and medical image\nanalysis, since such annotations are much easier to obtain compared to\ntime-consuming and labor-intensive labeling at the pixel/voxel level. However,\ndue to a lack of structure supervision on regions of interest (ROIs), existing\nscribble-based methods suffer from poor boundary localization. Furthermore,\nmost current methods are designed for 2D image segmentation, which do not fully\nleverage the volumetric information if directly applied to each image slice. In\nthis paper, we propose a scribble-based volumetric image segmentation,\nScribble2D5, which tackles 3D anisotropic image segmentation and aims to its\nimprove boundary prediction. To achieve this, we augment a 2.5D attention UNet\nwith a proposed label propagation module to extend semantic information from\nscribbles and use a combination of static and active boundary prediction to\nlearn ROI's boundary and regularize its shape. Also, we propose an optional\nadd-on component, which incorporates the shape prior information from unpaired\nsegmentation masks to further improve model accuracy. Extensive experiments on\nthree public datasets and one private dataset demonstrate our Scribble2D5\nachieves state-of-the-art performance on volumetric image segmentation using\nscribbles and shape prior if available.\n","authors":["Qiuhui Chen","Haiying Lyu","Xinyue Hu","Yong Lu","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2310.08084v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2205.06779"},{"id":"http://arxiv.org/abs/2310.08082v1","updated":"2023-10-12T07:12:20Z","published":"2023-10-12T07:12:20Z","title":"Jointly Optimized Global-Local Visual Localization of UAVs","summary":"  Navigation and localization of UAVs present a challenge when global\nnavigation satellite systems (GNSS) are disrupted and unreliable. Traditional\ntechniques, such as simultaneous localization and mapping (SLAM) and visual\nodometry (VO), exhibit certain limitations in furnishing absolute coordinates\nand mitigating error accumulation. Existing visual localization methods achieve\nautonomous visual localization without error accumulation by matching with\northo satellite images. However, doing so cannot guarantee real-time\nperformance due to the complex matching process. To address these challenges,\nwe propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL\nnetwork is a two-stage visual localization approach, combining a large-scale\nretrieval module that finds similar regions with the UAV flight scene, and a\nfine-grained matching module that localizes the precise UAV coordinate,\nenabling real-time and precise localization. The training process is jointly\noptimized in an end-to-end manner to further enhance the model capability.\nExperiments on six UAV flight scenes encompassing both texture-rich and\ntexture-sparse regions demonstrate the ability of our model to achieve the\nreal-time precise localization requirements of UAVs. Particularly, our method\nachieves a localization error of only 2.39 meters in 0.48 seconds in a village\nscene with sparse texture features.\n","authors":["Haoling Li","Jiuniu Wang","Zhiwei Wei","Wenjia Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08080v1","updated":"2023-10-12T07:10:12Z","published":"2023-10-12T07:10:12Z","title":"RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and\n  Tumor Segmentation from Single X-Ray Projection","summary":"  Radiotherapy is one of the primary treatment methods for tumors, but the\norgan movement caused by respiratory motion limits its accuracy. Recently, 3D\nimaging from single X-ray projection receives extensive attentions as a\npromising way to address this issue. However, current methods can only\nreconstruct 3D image without direct location of the tumor and are only\nvalidated for fixed-angle imaging, which fails to fully meet the requirement of\nmotion control in radiotherapy. In this study, we propose a novel imaging\nmethod RT-SRTS which integrates 3D imaging and tumor segmentation into one\nnetwork based on the multi-task learning (MTL) and achieves real-time\nsimultaneous 3D reconstruction and tumor segmentation from single X-ray\nprojection at any angle. Futhermore, we propose the attention enhanced\ncalibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature\nextraction and improve segmentation accuracy. We evaluated the proposed method\non ten patient cases and compared it with two state-of-the-art methods. Our\napproach not only delivered superior 3D reconstruction but also demonstrated\ncommendable tumor segmentation results. The simultaneous reconstruction and\nsegmentation could be completed in approximately 70 ms, significantly faster\nthan the required time threshold for real-time tumor tracking. The efficacy of\nboth AEC and URE was also validated through ablation studies.\n","authors":["Miao Zhu","Qiming Fu","Bo Liu","Mengxi Zhang","Bojian Li","Xiaoyan Luo","Fugen Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08080v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2210.11318v2","updated":"2023-10-12T06:52:18Z","published":"2022-10-20T14:51:01Z","title":"A Survey of Computer Vision Technologies In Urban and\n  Controlled-environment Agriculture","summary":"  In the evolution of agriculture to its next stage, Agriculture 5.0,\nartificial intelligence will play a central role. Controlled-environment\nagriculture, or CEA, is a special form of urban and suburban agricultural\npractice that offers numerous economic, environmental, and social benefits,\nincluding shorter transportation routes to population centers, reduced\nenvironmental impact, and increased productivity. Due to its ability to control\nenvironmental factors, CEA couples well with computer vision (CV) in the\nadoption of real-time monitoring of the plant conditions and autonomous\ncultivation and harvesting. The objective of this paper is to familiarize CV\nresearchers with agricultural applications and agricultural practitioners with\nthe solutions offered by CV. We identify five major CV applications in CEA,\nanalyze their requirements and motivation, and survey the state of the art as\nreflected in 68 technical papers using deep learning methods. In addition, we\ndiscuss five key subareas of computer vision and how they related to these CEA\nproblems, as well as eleven vision-based CEA datasets. We hope the survey will\nhelp researchers quickly gain a bird-eye view of the striving research area and\nwill spark inspiration for new research and development.\n","authors":["Jiayun Luo","Boyang Li","Cyril Leung"],"pdf_url":"https://arxiv.org/pdf/2210.11318v2.pdf","comment":"1 overview figures, 37 pages, 8 tables, accepted by ACM Computing\n  Surveys"},{"id":"http://arxiv.org/abs/2310.08073v1","updated":"2023-10-12T06:50:43Z","published":"2023-10-12T06:50:43Z","title":"Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural\n  Networks","summary":"  Neural network pruning has shown to be an effective technique for reducing\nthe network size, trading desirable properties like generalization and\nrobustness to adversarial attacks for higher sparsity. Recent work has claimed\nthat adversarial pruning methods can produce sparse networks while also\npreserving robustness to adversarial examples. In this work, we first\nre-evaluate three state-of-the-art adversarial pruning methods, showing that\ntheir robustness was indeed overestimated. We then compare pruned and dense\nversions of the same models, discovering that samples on thin ice, i.e., closer\nto the unpruned model's decision boundary, are typically misclassified after\npruning. We conclude by discussing how this intuition may lead to designing\nmore effective adversarial pruning methods in future work.\n","authors":["Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08071v1","updated":"2023-10-12T06:36:41Z","published":"2023-10-12T06:36:41Z","title":"Learning Transferable Conceptual Prototypes for Interpretable\n  Unsupervised Domain Adaptation","summary":"  Despite the great progress of unsupervised domain adaptation (UDA) with the\ndeep neural networks, current UDA models are opaque and cannot provide\npromising explanations, limiting their applications in the scenarios that\nrequire safe and controllable model decisions. At present, a surge of work\nfocuses on designing deep interpretable methods with adequate data annotations\nand only a few methods consider the distributional shift problem. Most existing\ninterpretable UDA methods are post-hoc ones, which cannot facilitate the model\nlearning process for performance enhancement. In this paper, we propose an\ninherently interpretable method, named Transferable Conceptual Prototype\nLearning (TCPL), which could simultaneously interpret and improve the processes\nof knowledge transfer and decision-making in UDA. To achieve this goal, we\ndesign a hierarchically prototypical module that transfers categorical basic\nconcepts from the source domain to the target domain and learns domain-shared\nprototypes for explaining the underlying reasoning process. With the learned\ntransferable prototypes, a self-predictive consistent pseudo-label strategy\nthat fuses confidence, predictions, and prototype information, is designed for\nselecting suitable target samples for pseudo annotations and gradually\nnarrowing down the domain gap. Comprehensive experiments show that the proposed\nmethod can not only provide effective and intuitive explanations but also\noutperform previous state-of-the-arts.\n","authors":["Junyu Gao","Xinhong Ma","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08071v1.pdf","comment":"Submitted to IEEE TIP"},{"id":"http://arxiv.org/abs/2310.08068v1","updated":"2023-10-12T06:32:12Z","published":"2023-10-12T06:32:12Z","title":"Frequency-Aware Re-Parameterization for Over-Fitting Based Image\n  Compression","summary":"  Over-fitting-based image compression requires weights compactness for\ncompression and fast convergence for practical use, posing challenges for deep\nconvolutional neural networks (CNNs) based methods. This paper presents a\nsimple re-parameterization method to train CNNs with reduced weights storage\nand accelerated convergence. The convolution kernels are re-parameterized as a\nweighted sum of discrete cosine transform (DCT) kernels enabling direct\noptimization in the frequency domain. Combined with L1 regularization, the\nproposed method surpasses vanilla convolutions by achieving a significantly\nimproved rate-distortion with low computational cost. The proposed method is\nverified with extensive experiments of over-fitting-based image restoration on\nvarious datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200\niterations.\n","authors":["Yun Ye","Yanjie Pan","Qually Jiang","Ming Lu","Xiaoran Fang","Beryl Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08068v1.pdf","comment":"to be published at ICIP 2023, this version fixed a mistake in Eq. (1)\n  in the proceeding version"},{"id":"http://arxiv.org/abs/2310.08064v1","updated":"2023-10-12T06:26:39Z","published":"2023-10-12T06:26:39Z","title":"Age Estimation Based on Graph Convolutional Networks and Multi-head\n  Attention Mechanisms","summary":"  Age estimation technology is a part of facial recognition and has been\napplied to identity authentication. This technology achieves the development\nand application of a juvenile anti-addiction system by authenticating users in\nthe game. Convolutional Neural Network (CNN) and Transformer algorithms are\nwidely used in this application scenario. However, these two models cannot\nflexibly extract and model features of faces with irregular shapes, and they\nare ineffective in capturing key information. Furthermore, the above methods\nwill contain a lot of background information while extracting features, which\nwill interfere with the model. In consequence, it is easy to extract redundant\ninformation from images. In this paper, a new modeling idea is proposed to\nsolve this problem, which can flexibly model irregular objects. The Graph\nConvolutional Network (GCN) is used to extract features from irregular face\nimages effectively, and multi-head attention mechanisms are added to avoid\nredundant features and capture key region information in the image. This model\ncan effectively improve the accuracy of age estimation and reduce the MAE error\nvalue to about 3.64, which is better than the effect of today's age estimation\nmodel, to improve the accuracy of face recognition and identity authentication.\n","authors":["Miaomiao Yang","Changwei Yao","Shijin Yan"],"pdf_url":"https://arxiv.org/pdf/2310.08064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07763v2","updated":"2023-10-12T06:24:02Z","published":"2023-07-15T10:06:43Z","title":"Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile\n  Agents","summary":"  The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to\nprovide autonomous navigation and task execution in complex and unknown\nenvironments. However, it is hard to develop a dedicated algorithm for mobile\nrobots due to dynamic and challenging situations, such as poor lighting\nconditions and motion blur. To tackle this issue, we propose a tightly-coupled\nLiDAR-visual SLAM based on geometric features, which includes two sub-systems\n(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework\nassociates the depth and semantics of the multi-modal geometric features to\ncomplement the visual line landmarks and to add direction optimization in\nBundle Adjustment (BA). This further constrains visual odometry. On the other\nhand, the entire line segment detected by the visual subsystem overcomes the\nlimitation of the LiDAR subsystem, which can only perform the local calculation\nfor geometric features. It adjusts the direction of linear feature points and\nfilters out outliers, leading to a higher accurate odometry system. Finally, we\nemploy a module to detect the subsystem's operation, providing the LiDAR\nsubsystem's output as a complementary trajectory to our system while visual\nsubsystem tracking fails. The evaluation results on the public dataset M2DGR,\ngathered from ground robots across various indoor and outdoor scenarios, show\nthat our system achieves more accurate and robust pose estimation compared to\ncurrent state-of-the-art multi-modal methods.\n","authors":["Ke Cao","Ruiping Liu","Ze Wang","Kunyu Peng","Jiaming Zhang","Junwei Zheng","Zhifeng Teng","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2307.07763v2.pdf","comment":"Accepted to ROBIO 2023"},{"id":"http://arxiv.org/abs/2306.06599v4","updated":"2023-10-12T05:51:30Z","published":"2023-06-11T06:27:06Z","title":"Variational Imbalanced Regression: Fair Uncertainty Quantification via\n  Probabilistic Smoothing","summary":"  Existing regression models tend to fall short in both accuracy and\nuncertainty estimation when the label distribution is imbalanced. In this\npaper, we propose a probabilistic deep learning model, dubbed variational\nimbalanced regression (VIR), which not only performs well in imbalanced\nregression but naturally produces reasonable uncertainty estimation as a\nbyproduct. Different from typical variational autoencoders assuming I.I.D.\nrepresentations (a data point's representation is not directly affected by\nother data points), our VIR borrows data with similar regression labels to\ncompute the latent representation's variational distribution; furthermore,\ndifferent from deterministic regression models producing point estimates, VIR\npredicts the entire normal-inverse-gamma distributions and modulates the\nassociated conjugate distributions to impose probabilistic reweighting on the\nimbalanced data, thereby providing better uncertainty estimation. Experiments\nin several real-world datasets show that our VIR can outperform\nstate-of-the-art imbalanced regression models in terms of both accuracy and\nuncertainty estimation. Code will soon be available at\n\\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.\n","authors":["Ziyan Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2306.06599v4.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2302.08594v3","updated":"2023-10-12T05:43:45Z","published":"2023-02-16T21:38:36Z","title":"TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point\n  Cloud Semantic Segmentation","summary":"  Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS)\napproaches have bottlenecks resulting from the boundary-blurring problem of\nconvolution neural networks (CNNs) and quantitation loss of spherical\nprojection. In this work, we propose a transformer-based plug-and-play\nuncertain point refiner, i.e., TransUPR, to refine selected uncertain points in\na learnable manner, which leads to an improved segmentation performance.\nUncertain points are sampled from coarse semantic segmentation results of 2D\nimage segmentation where uncertain points are located close to the object\nboundaries in the 2D range image representation and 3D spherical projection\nbackground points. Following that, the geometry and coarse semantic features of\nuncertain points are aggregated by neighbor points in 3D space without adding\nexpensive computation and memory footprint. Finally, the transformer-based\nrefiner, which contains four stacked self-attention layers, along with an MLP\nmodule, is utilized for uncertain point classification on the concatenated\nfeatures of self-attention layers. As the proposed refiner is independent of 2D\nCNNs, our TransUPR can be easily integrated into any existing image-based LiDAR\nPCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves\nstate-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU)\non the Semantic KITTI benchmark, which provides a performance improvement of\n0.6% on the mIoU compared to the original CENet.\n","authors":["Zifan Yu","Meida Chen","Zhikang Zhang","Suya You","Raghuveer Rao","Sanjeev Agarwal","Fengbo Ren"],"pdf_url":"https://arxiv.org/pdf/2302.08594v3.pdf","comment":"6 pages; Accepted by 2023 IROS"},{"id":"http://arxiv.org/abs/2310.08044v1","updated":"2023-10-12T05:34:45Z","published":"2023-10-12T05:34:45Z","title":"EC-Depth: Exploring the consistency of self-supervised monocular depth\n  estimation under challenging scenes","summary":"  Self-supervised monocular depth estimation holds significant importance in\nthe fields of autonomous driving and robotics. However, existing methods are\ntypically designed to train and test on clear and pristine datasets,\noverlooking the impact of various adverse conditions prevalent in real-world\nscenarios. As a result, it is commonly observed that most self-supervised\nmonocular depth estimation methods struggle to perform adequately under\nchallenging conditions. To address this issue, we present EC-Depth, a novel\nself-supervised two-stage training framework to achieve a robust depth\nestimation, starting from the foundation of depth prediction consistency under\ndifferent perturbations. Leveraging the proposed perturbation-invariant depth\nconsistency constraint module and the consistency-based pseudo-label selection\nmodule, our model attains accurate and consistent depth predictions in both\nstandard and challenging scenarios. Extensive experiments substantiate the\neffectiveness of the proposed method. Moreover, our method surpasses existing\nstate-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks,\ndemonstrating its potential for enhancing the reliability of self-supervised\nmonocular depth estimation models in real-world applications.\n","authors":["Ruijie Zhu","Ziyang Song","Chuxin Wang","Jianfeng He","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08042v1","updated":"2023-10-12T05:33:25Z","published":"2023-10-12T05:33:25Z","title":"X-HRNet: Towards Lightweight Human Pose Estimation with Spatially\n  Unidimensional Self-Attention","summary":"  High-resolution representation is necessary for human pose estimation to\nachieve high performance, and the ensuing problem is high computational\ncomplexity. In particular, predominant pose estimation methods estimate human\njoints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and\nvertically projected to and reconstructed by a pair of 1D heat vectors.\nInspired by this observation, we introduce a lightweight and powerful\nalternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise\n(1x1) convolution that is the main computational bottleneck in the depthwise\nseparable 3c3 convolution. Our SUSA reduces the computational complexity of the\npointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore,\nwe use the SUSA as the main module to build our lightweight pose estimation\nbackbone X-HRNet, where `X' represents the estimated cross-shape attention\nvectors. Extensive experiments on the COCO benchmark demonstrate the\nsuperiority of our X-HRNet, and comprehensive ablation studies show the\neffectiveness of the SUSA modules. The code is publicly available at\nhttps://github.com/cool-xuan/x-hrnet.\n","authors":["Yixuan Zhou","Xuanhan Wang","Xing Xu","Lei Zhao","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2310.08042v1.pdf","comment":"Accepted by ICME 2022"},{"id":"http://arxiv.org/abs/2310.05624v2","updated":"2023-10-12T05:33:19Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.08038v1","updated":"2023-10-12T05:09:27Z","published":"2023-10-12T05:09:27Z","title":"Continual Learning via Manifold Expansion Replay","summary":"  In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.\n","authors":["Zihao Xu","Xuan Tang","Yufei Shi","Jianfeng Zhang","Jian Yang","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2310.08038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08035v1","updated":"2023-10-12T05:03:19Z","published":"2023-10-12T05:03:19Z","title":"BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic\n  Segmentation","summary":"  Active learning strives to reduce the need for costly data annotation, by\nrepeatedly querying an annotator to label the most informative samples from a\npool of unlabeled data and retraining a model from these samples. We identify\ntwo problems with existing active learning methods for LiDAR semantic\nsegmentation. First, they ignore the severe class imbalance inherent in LiDAR\nsemantic segmentation datasets. Second, to bootstrap the active learning loop,\nthey train their initial model from randomly selected data samples, which leads\nto low performance and is referred to as the cold start problem. To address\nthese problems we propose BaSAL, a size-balanced warm start active learning\nmodel, based on the observation that each object class has a characteristic\nsize. By sampling object clusters according to their size, we can thus create a\nsize-balanced dataset that is also more class-balanced. Furthermore, in\ncontrast to existing information measures like entropy or CoreSet, size-based\nsampling does not require an already trained model and thus can be used to\naddress the cold start problem. Results show that we are able to improve the\nperformance of the initial model by a large margin. Combining size-balanced\nsampling and warm start with established information measures, our approach\nachieves a comparable performance to training on the entire SemanticKITTI\ndataset, despite using only 5% of the annotations, which outperforms existing\nactive learning methods. We also match the existing state-of-the-art in active\nlearning on nuScenes. Our code will be made available upon paper acceptance.\n","authors":["Jiarong Wei","Yancong Lin","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2310.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08027v1","updated":"2023-10-12T04:14:28Z","published":"2023-10-12T04:14:28Z","title":"Exploring Large Language Models for Multi-Modal Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.\n","authors":["Yi Dai","Hao Lang","Kaisheng Zeng","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.08027v1.pdf","comment":"EMNLP2023 Findings Long Paper"},{"id":"http://arxiv.org/abs/2310.08026v1","updated":"2023-10-12T04:12:43Z","published":"2023-10-12T04:12:43Z","title":"Beyond Sharing Weights in Decoupling Feature Learning Network for UAV\n  RGB-Infrared Vehicle Re-Identification","summary":"  Owing to the capacity of performing full-time target search, cross-modality\nvehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is\ngaining more attention in both video surveillance and public security. However,\nthis promising and innovative research has not been studied sufficiently due to\nthe data inadequacy issue. Meanwhile, the cross-modality discrepancy and\norientation discrepancy challenges further aggravate the difficulty of this\ntask. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named\nUAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with\n16015 RGB and 13913 infrared images. Moreover, to meet cross-modality\ndiscrepancy and orientation discrepancy challenges, we present a hybrid weights\ndecoupling network (HWDNet) to learn the shared discriminative\norientation-invariant features. For the first challenge, we proposed a hybrid\nweights siamese network with a well-designed weight restrainer and its\ncorresponding objective function to learn both modality-specific and modality\nshared information. In terms of the second challenge, three effective\ndecoupling structures with two pretext tasks are investigated to learn\norientation-invariant feature. Comprehensive experiments are carried out to\nvalidate the effectiveness of the proposed method. The dataset and codes will\nbe released at https://github.com/moonstarL/UAV-CM-VeID.\n","authors":["Xingyue Liu","Jiahao Qi","Chen Chen","Kangcheng Bin","Ping Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.08026v1.pdf","comment":"13 pages, 10 figures, 64 citations, submitted to TMM"},{"id":"http://arxiv.org/abs/2210.15889v4","updated":"2023-10-12T04:05:41Z","published":"2022-10-28T04:38:10Z","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on\n  Neuro-Symbolic Computing","summary":"  Neural-symbolic computing (NeSy), which pursues the integration of the\nsymbolic and statistical paradigms of cognition, has been an active research\narea of Artificial Intelligence (AI) for many years. As NeSy shows promise of\nreconciling the advantages of reasoning and interpretability of symbolic\nrepresentation and robust learning in neural networks, it may serve as a\ncatalyst for the next generation of AI. In the present paper, we provide a\nsystematic overview of the recent developments and important contributions of\nNeSy research. Firstly, we introduce study history of this area, covering early\nwork and foundations. We further discuss background concepts and identify key\ndriving factors behind the development of NeSy. Afterward, we categorize recent\nlandmark approaches along several main characteristics that underline this\nresearch paradigm, including neural-symbolic integration, knowledge\nrepresentation, knowledge embedding, and functionality. Next, we briefly\ndiscuss the successful application of modern NeSy approaches in several\ndomains. Then, we benchmark several NeSy methods on three representative\napplication tasks. Finally, we identify the open problems together with\npotential future research directions. This survey is expected to help new\nresearchers enter this rapidly evolving field and accelerate the progress\ntowards data-and knowledge-driven AI.\n","authors":["Wenguan Wang","Yi Yang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2210.15889v4.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2308.03807v2","updated":"2023-10-12T03:36:17Z","published":"2023-08-06T15:47:03Z","title":"Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS\n  Image Reconstruction","summary":"  Proximal gradient-based optimization is one of the most common strategies to\nsolve inverse problem of images, and it is easy to implement. However, these\ntechniques often generate heavy artifacts in image reconstruction. One of the\nmost popular refinement methods is to fine-tune the regularization parameter to\nalleviate such artifacts, but it may not always be sufficient or applicable due\nto increased computational costs. In this work, we propose a deep geometric\nincremental learning framework based on the second Nesterov proximal gradient\noptimization. The proposed end-to-end network not only has the powerful\nlearning ability for high-/low-frequency image features, but also can\ntheoretically guarantee that geometric texture details will be reconstructed\nfrom preliminary linear reconstruction. Furthermore, it can avoid the risk of\nintermediate reconstruction results falling outside the geometric decomposition\ndomains and achieve fast convergence. Our reconstruction framework is\ndecomposed into four modules including general linear reconstruction, cascade\ngeometric incremental restoration, Nesterov acceleration, and post-processing.\nIn the image restoration step, a cascade geometric incremental learning module\nis designed to compensate for missing texture information from different\ngeometric spectral decomposition domains. Inspired by the overlap-tile\nstrategy, we also develop a post-processing module to remove the block effect\nin patch-wise-based natural image reconstruction. All parameters in the\nproposed model are learnable, an adaptive initialization technique of physical\nparameters is also employed to make model flexibility and ensure converging\nsmoothly. We compare the reconstruction performance of the proposed method with\nexisting state-of-the-art methods to demonstrate its superiority. Our source\ncodes are available at https://github.com/fanxiaohong/Nest-DGIL.\n","authors":["Xiaohong Fan","Yin Yang","Ke Chen","Yujie Feng","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.03807v2.pdf","comment":"15 pages,our source codes are available at\n  https://github.com/fanxiaohong/Nest-DGIL"},{"id":"http://arxiv.org/abs/2310.06488v2","updated":"2023-10-12T03:23:40Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08009v1","updated":"2023-10-12T03:21:12Z","published":"2023-10-12T03:21:12Z","title":"Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video\n  Retrieval","summary":"  Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.\n","authors":["Pandeng Li","Hongtao Xie","Jiannan Ge","Lei Zhang","Shaobo Min","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08009v1.pdf","comment":"17 pages, 8 figures, ECCV 2022"},{"id":"http://arxiv.org/abs/2310.08002v1","updated":"2023-10-12T03:14:02Z","published":"2023-10-12T03:14:02Z","title":"MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera\n  snapshot hyperspectral imaging","summary":"  Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages\nover traditional methods in dynamically acquiring Hyper-Spectral Image (HSI),\nbut there are the following problems. 1) Traditional mask relies on random\npatterns or analytical design, both of which limit the performance improvement\nof CASSI. 2) Existing high-quality reconstruction algorithms are slow in\nreconstruction and can only reconstruct scene information offline. To address\nthe above two problems, this paper designs the AMDC-CASSI system, introducing\nRGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the\nreconstruction quality. The existing SOTA reconstruction schemes are based on\ntransformer, but the operation of self-attention pulls down the operation\nefficiency of the network. In order to improve the inference speed of the\nreconstruction network, this paper proposes An MLP Architecture for\nAdaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure\nof the network. Numerous experiments have shown that MLP performs no less well\nthan transformer-based structures for HSI reconstruction, while MLP greatly\nimproves the network inference speed and has less number of parameters and\noperations, our method has a 8 db improvement over SOTA and at least a 5-fold\nimprovement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)\n","authors":["Zeyu Cai","Can Zhang","Xunhao Chen","Shanghuan Liu","Chengqian Jin","Feipeng Da"],"pdf_url":"https://arxiv.org/pdf/2310.08002v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2308.01541"},{"id":"http://arxiv.org/abs/2209.05167v3","updated":"2023-10-12T03:09:25Z","published":"2022-09-12T11:51:20Z","title":"LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with\n  Negative Imaging Plane on Mobile Agents","summary":"  Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in\nthe fields of autonomous driving and robotics. One crucial component of visual\nSLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a\nwider range of surrounding elements and features to be perceived. However, when\nthe FoV of the camera reaches the negative half-plane, traditional methods for\nrepresenting image feature points using [u,v,1]^T become ineffective. While the\npanoramic FoV is advantageous for loop closure, its benefits are not easily\nrealized under large-attitude-angle differences where loop-closure frames\ncannot be easily matched by existing methods. As loop closure on wide-FoV\npanoramic data further comes with a large number of outliers, traditional\noutlier rejection methods are not directly applicable. To address these issues,\nwe propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with\nextremely Large FoV with loop closure. A three-dimensional vector with unit\nlength is introduced to effectively represent feature points even on the\nnegative half-plane. The attitude information of the SLAM system is leveraged\nto guide the feature point detection of the loop closure. Additionally, a new\noutlier rejection method based on the unit length representation is integrated\ninto the loop closure module. We collect the PALVIO dataset using a Panoramic\nAnnular Lens (PAL) system with an entire FoV of 360{\\deg}x(40{\\deg}~120{\\deg})\nand an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to\naddress the lack of panoramic SLAM datasets. Experiments on the established\nPALVIO and public datasets show that the proposed LF-VISLAM outperforms\nstate-of-the-art SLAM methods. Our code will be open-sourced at\nhttps://github.com/flysoaryun/LF-VISLAM.\n","authors":["Ze Wang","Kailun Yang","Hao Shi","Peng Li","Fei Gao","Jian Bai","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2209.05167v3.pdf","comment":"Accepted to IEEE Transactions on Automation Science and Engineering\n  (T-ASE). Extended version of IROS2022 paper arXiv:2202.12613. Code and\n  dataset will be open-sourced at https://github.com/flysoaryun/LF-SLAM"},{"id":"http://arxiv.org/abs/2310.07997v1","updated":"2023-10-12T02:52:33Z","published":"2023-10-12T02:52:33Z","title":"Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by\n  Volume Rendering","summary":"  Recently, learning neural implicit surface by volume rendering has been a\npromising way for multi-view reconstruction. However, limited accuracy and\nexcessive time complexity remain bottlenecks that current methods urgently need\nto overcome. To address these challenges, we propose a new method called\nPoint-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient\nreconstruction. Point modeling is organically embedded into the volume\nrendering to enhance and regularize the representation of implicit surface.\nSpecifically, to achieve precise point guidance and noise robustness, aleatoric\nuncertainty of the point cloud is modeled to capture the distribution of noise\nand estimate the reliability of points. Additionally, a Neural Projection\nmodule connecting points and images is introduced to add geometric constraints\nto the Signed Distance Function (SDF). To better compensate for geometric bias\nbetween volume rendering and point modeling, high-fidelity points are filtered\ninto an Implicit Displacement Network to improve the representation of SDF.\nBenefiting from our effective point guidance, lightweight networks are employed\nto achieve an impressive 11x speedup compared to NeuS. Extensive experiments\nshow that our method yields high-quality surfaces, especially for fine-grained\ndetails and smooth regions. Moreover, it exhibits strong robustness to both\nnoisy and sparse data.\n","authors":["Chen Zhang","Wanjuan Su","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2310.07997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07996v1","updated":"2023-10-12T02:52:14Z","published":"2023-10-12T02:52:14Z","title":"Reset It and Forget It: Relearning Last-Layer Weights Improves Continual\n  and Transfer Learning","summary":"  This work identifies a simple pre-training mechanism that leads to\nrepresentations exhibiting better continual and transfer learning. This\nmechanism -- the repeated resetting of weights in the last layer, which we\nnickname \"zapping\" -- was originally designed for a meta-continual-learning\nprocedure, yet we show it is surprisingly applicable in many settings beyond\nboth meta-learning and continual learning. In our experiments, we wish to\ntransfer a pre-trained image classifier to a new set of classes, in a few\nshots. We show that our zapping procedure results in improved transfer accuracy\nand/or more rapid adaptation in both standard fine-tuning and continual\nlearning settings, while being simple to implement and computationally\nefficient. In many cases, we achieve performance on par with state of the art\nmeta-learning without needing the expensive higher-order gradients, by using a\ncombination of zapping and sequential learning. An intuitive explanation for\nthe effectiveness of this zapping procedure is that representations trained\nwith repeated zapping learn features that are capable of rapidly adapting to\nnewly initialized classifiers. Such an approach may be considered a\ncomputationally cheaper type of, or alternative to, meta-learning rapidly\nadaptable features with higher-order gradients. This adds to recent work on the\nusefulness of resetting neural network parameters during training, and invites\nfurther investigation of this mechanism.\n","authors":["Lapo Frati","Neil Traft","Jeff Clune","Nick Cheney"],"pdf_url":"https://arxiv.org/pdf/2310.07996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07995v1","updated":"2023-10-12T02:49:00Z","published":"2023-10-12T02:49:00Z","title":"HeightFormer: A Multilevel Interaction and Image-adaptive\n  Classification-regression Network for Monocular Height Estimation with Aerial\n  Images","summary":"  Height estimation has long been a pivotal topic within measurement and remote\nsensing disciplines, proving critical for endeavours such as 3D urban\nmodelling, MR and autonomous driving. Traditional methods utilise stereo\nmatching or multisensor fusion, both well-established techniques that typically\nnecessitate multiple images from varying perspectives and adjunct sensors like\nSAR, leading to substantial deployment costs. Single image height estimation\nhas emerged as an attractive alternative, boasting a larger data source variety\nand simpler deployment. However, current methods suffer from limitations such\nas fixed receptive fields, a lack of global information interaction, leading to\nnoticeable instance-level height deviations. The inherent complexity of height\nprediction can result in a blurry estimation of object edge depth when using\nmainstream regression methods based on fixed height division. This paper\npresents a comprehensive solution for monocular height estimation in remote\nsensing, termed HeightFormer, combining multilevel interactions and\nimage-adaptive classification-regression. It features the Multilevel\nInteraction Backbone (MIB) and Image-adaptive Classification-regression Height\nGenerator (ICG). MIB supplements the fixed sample grid in CNN of the\nconventional backbone network with tokens of different interaction ranges. It\nis complemented by a pixel-, patch-, and feature map-level hierarchical\ninteraction mechanism, designed to relay spatial geometry information across\ndifferent scales and introducing a global receptive field to enhance the\nquality of instance-level height estimation. The ICG dynamically generates\nheight partition for each image and reframes the traditional regression task,\nusing a refinement from coarse to fine classification-regression that\nsignificantly mitigates the innate ill-posedness issue and drastically improves\nedge sharpness.\n","authors":["Zhan Chen","Yidan Zhang","Xiyu Qi","Yongqiang Mao","Xin Zhou","Lulu Niu","Hui Wu","Lei Wang","Yunping Ge"],"pdf_url":"https://arxiv.org/pdf/2310.07995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12535v2","updated":"2023-10-12T02:38:50Z","published":"2023-03-21T17:28:44Z","title":"An Effective Motion-Centric Paradigm for 3D Single Object Tracking in\n  Point Clouds","summary":"  3D single object tracking in LiDAR point clouds (LiDAR SOT) plays a crucial\nrole in autonomous driving. Current approaches all follow the Siamese paradigm\nbased on appearance matching. However, LiDAR point clouds are usually\ntextureless and incomplete, which hinders effective appearance matching.\nBesides, previous methods greatly overlook the critical motion clues among\ntargets. In this work, beyond 3D Siamese tracking, we introduce a\nmotion-centric paradigm to handle LiDAR SOT from a new perspective. Following\nthis paradigm, we propose a matching-free two-stage tracker M^2-Track. At the\n1st-stage, M^2-Track localizes the target within successive frames via motion\ntransformation. Then it refines the target box through motion-assisted shape\ncompletion at the 2nd-stage. Due to the motion-centric nature, our method shows\nits impressive generalizability with limited training labels and provides good\ndifferentiability for end-to-end cycle training. This inspires us to explore\nsemi-supervised LiDAR SOT by incorporating a pseudo-label-based motion\naugmentation and a self-supervised loss term. Under the fully-supervised\nsetting, extensive experiments confirm that M^2-Track significantly outperforms\nprevious state-of-the-arts on three large-scale datasets while running at 57FPS\n(~3%, ~11% and ~22% precision gains on KITTI, NuScenes, and Waymo Open Dataset\nrespectively). While under the semi-supervised setting, our method performs on\npar with or even surpasses its fully-supervised counterpart using fewer than\nhalf of the labels from KITTI. Further analysis verifies each component's\neffectiveness and shows the motion-centric paradigm's promising potential for\nauto-labeling and unsupervised domain adaptation.\n","authors":["Chaoda Zheng","Xu Yan","Haiming Zhang","Baoyuan Wang","Shenghui Cheng","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2303.12535v2.pdf","comment":"Accepted version of the journal extension of M^2-Track. Accepted by\n  TPAMI. arXiv admin note: substantial text overlap with arXiv:2203.01730"},{"id":"http://arxiv.org/abs/2207.12389v2","updated":"2023-10-12T02:01:50Z","published":"2022-07-25T17:55:28Z","title":"MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised\n  Domain Adaptation","summary":"  Practical real world datasets with plentiful categories introduce new\nchallenges for unsupervised domain adaptation like small inter-class\ndiscriminability, that existing approaches relying on domain invariance alone\ncannot handle sufficiently well. In this work we propose MemSAC, which exploits\nsample level similarity across source and target domains to achieve\ndiscriminative transfer, along with architectures that scale to a large number\nof categories. For this purpose, we first introduce a memory augmented approach\nto efficiently extract pairwise similarity relations between labeled source and\nunlabeled target domain instances, suited to handle an arbitrary number of\nclasses. Next, we propose and theoretically justify a novel variant of the\ncontrastive loss to promote local consistency among within-class cross domain\nsamples while enforcing separation between classes, thus preserving\ndiscriminative transfer from source to target. We validate the advantages of\nMemSAC with significant improvements over previous state-of-the-art on multiple\nchallenging transfer tasks designed for large-scale adaptation, such as\nDomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds\ndataset with 200 classes. We also provide in-depth analysis and insights into\nthe effectiveness of MemSAC.\n","authors":["Tarun Kalluri","Astuti Sharma","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2207.12389v2.pdf","comment":"Accepted at ECCV 2022. Project Webpage:\n  https://tarun005.github.io/MemSAC/"},{"id":"http://arxiv.org/abs/2310.07975v1","updated":"2023-10-12T01:47:55Z","published":"2023-10-12T01:47:55Z","title":"Self-supervised visual learning for analyzing firearms trafficking\n  activities on the Web","summary":"  Automated visual firearms classification from RGB images is an important\nreal-world task with applications in public space security, intelligence\ngathering and law enforcement investigations. When applied to images massively\ncrawled from the World Wide Web (including social media and dark Web sites), it\ncan serve as an important component of systems that attempt to identify\ncriminal firearms trafficking networks, by analyzing Big Data from open-source\nintelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology\nfor achieving this, with Convolutional Neural Networks (CNN) being typically\nemployed. The common transfer learning approach consists of pretraining on a\nlarge-scale, generic annotated dataset for whole-image classification, such as\nImageNet-1k, and then finetuning the DNN on a smaller, annotated,\ntask-specific, downstream dataset for visual firearms classification. Neither\nVisual Transformer (ViT) neural architectures nor Self-Supervised Learning\n(SSL) approaches have been so far evaluated on this critical task. SSL\nessentially consists of replacing the traditional supervised pretraining\nobjective with an unsupervised pretext task that does not require ground-truth\nlabels..\n","authors":["Sotirios Konstantakos","Despina Ioanna Chalkiadaki","Ioannis Mademlis","Adamantia Anna Rebolledo Chrysochoou","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.07975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01244v2","updated":"2023-10-12T01:44:33Z","published":"2022-10-03T21:50:14Z","title":"Event-based Temporally Dense Optical Flow Estimation with Sequential\n  Learning","summary":"  Event cameras provide an advantage over traditional frame-based cameras when\ncapturing fast-moving objects without a motion blur. They achieve this by\nrecording changes in light intensity (known as events), thus allowing them to\noperate at a much higher frequency and making them suitable for capturing\nmotions in a highly dynamic scene. Many recent studies have proposed methods to\ntrain neural networks (NNs) for predicting optical flow from events. However,\nthey often rely on a spatio-temporal representation constructed from events\nover a fixed interval, such as 10Hz used in training on the DSEC dataset. This\nlimitation restricts the flow prediction to the same interval (10Hz) whereas\nthe fast speed of event cameras, which can operate up to 3kHz, has not been\neffectively utilized. In this work, we show that a temporally dense flow\nestimation at 100Hz can be achieved by treating the flow estimation as a\nsequential problem using two different variants of recurrent networks -\nLong-short term memory (LSTM) and spiking neural network (SNN). First, We\nutilize the NN model constructed similar to the popular EV-FlowNet but with\nLSTM layers to demonstrate the efficiency of our training method. The model not\nonly produces 10x more frequent optical flow than the existing ones, but the\nestimated flows also have 13% lower errors than predictions from the baseline\nEV-FlowNet. Second, we construct an EV-FlowNet SNN but with leaky integrate and\nfire neurons to efficiently capture the temporal dynamics. We found that simple\ninherent recurrent dynamics of SNN lead to significant parameter reduction\ncompared to the LSTM model. In addition, because of its event-driven\ncomputation, the spiking model is estimated to consume only 1.5% energy of the\nLSTM model, highlighting the efficiency of SNN in processing events and the\npotential for achieving temporally dense flow.\n","authors":["Wachirawit Ponghiran","Chamika Mihiranga Liyanagedera","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2210.01244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07969v1","updated":"2023-10-12T01:25:21Z","published":"2023-10-12T01:25:21Z","title":"CleftGAN: Adapting A Style-Based Generative Adversarial Network To\n  Create Images Depicting Cleft Lip Deformity","summary":"  A major obstacle when attempting to train a machine learning system to\nevaluate facial clefts is the scarcity of large datasets of high-quality,\nethics board-approved patient images. In response, we have built a deep\nlearning-based cleft lip generator designed to produce an almost unlimited\nnumber of artificial images exhibiting high-fidelity facsimiles of cleft lip\nwith wide variation. We undertook a transfer learning protocol testing\ndifferent versions of StyleGAN-ADA (a generative adversarial network image\ngenerator incorporating adaptive data augmentation (ADA)) as the base model.\nTraining images depicting a variety of cleft deformities were pre-processed to\nadjust for rotation, scaling, color adjustment and background blurring. The ADA\nmodification of the primary algorithm permitted construction of our new\ngenerative model while requiring input of a relatively small number of training\nimages. Adversarial training was carried out using 514 unique frontal\nphotographs of cleft-affected faces to adapt a pre-trained model based on\n70,000 normal faces. The Frechet Inception Distance (FID) was used to measure\nthe similarity of the newly generated facial images to the cleft training\ndataset, while Perceptual Path Length (PPL) and the novel Divergence Index of\nSeverity Histograms (DISH) measures were also used to assess the performance of\nthe image generator that we dub CleftGAN. We found that StyleGAN3 with\ntranslation invariance (StyleGAN3-t) performed optimally as a base model.\nGenerated images achieved a low FID reflecting a close similarity to our\ntraining input dataset of genuine cleft images. Low PPL and DISH measures\nreflected a smooth and semantically valid interpolation of images through the\ntransfer learning process and a similar distribution of severity in the\ntraining and generated images, respectively.\n","authors":["Abdullah Hayajneh","Erchin Serpedin","Mohammad Shaqfeh","Graeme Glass","Mitchell A. Stotland"],"pdf_url":"https://arxiv.org/pdf/2310.07969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03270v3","updated":"2023-10-12T01:13:41Z","published":"2023-10-05T02:51:53Z","title":"EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit\n  Diffusion Models","summary":"  Diffusion models have demonstrated remarkable capabilities in image synthesis\nand related generative tasks. Nevertheless, their practicality for low-latency\nreal-world applications is constrained by substantial computational costs and\nlatency issues. Quantization is a dominant way to compress and accelerate\ndiffusion models, where post-training quantization (PTQ) and quantization-aware\ntraining (QAT) are two main approaches, each bearing its own properties. While\nPTQ exhibits efficiency in terms of both time and data usage, it may lead to\ndiminished performance in low bit-width. On the other hand, QAT can alleviate\nperformance degradation but comes with substantial demands on computational and\ndata resources. To capitalize on the advantages while avoiding their respective\ndrawbacks, we introduce a data-free and parameter-efficient fine-tuning\nframework for low-bit diffusion models, dubbed EfficientDM, to achieve\nQAT-level performance with PTQ-like efficiency. Specifically, we propose a\nquantization-aware variant of the low-rank adapter (QALoRA) that can be merged\nwith model weights and jointly quantized to low bit-width. The fine-tuning\nprocess distills the denoising capabilities of the full-precision model into\nits quantized counterpart, eliminating the requirement for training data. We\nalso introduce scale-aware optimization and employ temporal learned step-size\nquantization to further enhance performance. Extensive experimental results\ndemonstrate that our method significantly outperforms previous PTQ-based\ndiffusion models while maintaining similar time and data efficiency.\nSpecifically, there is only a marginal 0.05 sFID increase when quantizing both\nweights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to\nQAT-based methods, our EfficientDM also boasts a 16.2x faster quantization\nspeed with comparable generation quality.\n","authors":["Yefei He","Jing Liu","Weijia Wu","Hong Zhou","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.03270v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00613v3","updated":"2023-10-12T00:27:09Z","published":"2022-12-01T16:09:54Z","title":"NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and\n  Animation","summary":"  The capture and animation of human hair are two of the major challenges in\nthe creation of realistic avatars for the virtual reality. Both problems are\nhighly challenging, because hair has complex geometry and appearance, as well\nas exhibits challenging motion. In this paper, we present a two-stage approach\nthat models hair independently from the head to address these challenges in a\ndata-driven manner. The first stage, state compression, learns a\nlow-dimensional latent space of 3D hair states containing motion and\nappearance, via a novel autoencoder-as-a-tracker strategy. To better\ndisentangle the hair and head in appearance learning, we employ multi-view hair\nsegmentation masks in combination with a differentiable volumetric renderer.\nThe second stage learns a novel hair dynamics model that performs temporal hair\ntransfer based on the discovered latent codes. To enforce higher stability\nwhile driving our dynamics model, we employ the 3D point-cloud autoencoder from\nthe compression stage for de-noising of the hair state. Our model outperforms\nthe state of the art in novel view synthesis and is capable of creating novel\nhair animations without having to rely on hair observations as a driving\nsignal. Project page is here https://ziyanw1.github.io/neuwigs/.\n","authors":["Ziyan Wang","Giljoo Nam","Tuur Stuyck","Stephen Lombardi","Chen Cao","Jason Saragih","Michael Zollhoefer","Jessica Hodgins","Christoph Lassner"],"pdf_url":"https://arxiv.org/pdf/2212.00613v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.08319v1","updated":"2023-10-12T13:32:35Z","published":"2023-10-12T13:32:35Z","title":"Fine-Tuning LLaMA for Multi-Stage Text Retrieval","summary":"  The effectiveness of multi-stage text retrieval has been solidly demonstrated\nsince before the era of pre-trained language models. However, most existing\nstudies utilize models that predate recent advances in large language models\n(LLMs). This study seeks to explore potential improvements that\nstate-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning\nthe latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise\nreranker (RankLLaMA) for both passage retrieval and document retrieval using\nthe MS MARCO datasets. Our findings demonstrate that the effectiveness of large\nlanguage models indeed surpasses that of smaller models. Additionally, since\nLLMs can inherently handle longer contexts, they can represent entire documents\nholistically, obviating the need for traditional segmenting and pooling\nstrategies. Furthermore, evaluations on BEIR demonstrate that our\nRepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model\ncheckpoints from this study are available on HuggingFace.\n","authors":["Xueguang Ma","Liang Wang","Nan Yang","Furu Wei","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2310.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08083v1","updated":"2023-10-12T07:14:22Z","published":"2023-10-12T07:14:22Z","title":"On Using GUI Interaction Data to Improve Text Retrieval-based Bug\n  Localization","summary":"  One of the most important tasks related to managing bug reports is localizing\nthe fault so that a fix can be applied. As such, prior work has aimed to\nautomate this task of bug localization by formulating it as an information\nretrieval problem, where potentially buggy files are retrieved and ranked\naccording to their textual similarity with a given bug report. However, there\nis often a notable semantic gap between the information contained in bug\nreports and identifiers or natural language contained within source code files.\nFor user-facing software, there is currently a key source of information that\ncould aid in bug localization, but has not been thoroughly investigated -\ninformation from the GUI.\n  We investigate the hypothesis that, for end user-facing applications,\nconnecting information in a bug report with information from the GUI, and using\nthis to aid in retrieving potentially buggy files, can improve upon existing\ntechniques for bug localization. To examine this phenomenon, we conduct a\ncomprehensive empirical study that augments four baseline techniques for bug\nlocalization with GUI interaction information from a reproduction scenario to\n(i) filter out potentially irrelevant files, (ii) boost potentially relevant\nfiles, and (iii) reformulate text-retrieval queries. To carry out our study, we\nsource the current largest dataset of fully-localized and reproducible real\nbugs for Android apps, with corresponding bug reports, consisting of 80 bug\nreports from 39 popular open-source apps. Our results illustrate that\naugmenting traditional techniques with GUI information leads to a marked\nincrease in effectiveness across multiple metrics, including a relative\nincrease in Hits@10 of 13-18%. Additionally, through further analysis, we find\nthat our studied augmentations largely complement existing techniques.\n","authors":["Junayed Mahmud","Nadeeshan De Silva","Safwat Ali Khan","Seyed Hooman Mostafavi","SM Hasan Mansur","Oscar Chaparro","Andrian Marcus","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2310.08083v1.pdf","comment":"13 pages, to appear in the Proceedings of the 46th International\n  Conference on Software Engineering (ICSE'24)"},{"id":"http://arxiv.org/abs/2310.08069v1","updated":"2023-10-12T06:32:42Z","published":"2023-10-12T06:32:42Z","title":"Rethinking Negative Pairs in Code Search","summary":"  Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.\n","authors":["Haochen Li","Xin Zhou","Luu Anh Tuan","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2310.08069v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08039v1","updated":"2023-10-12T05:14:42Z","published":"2023-10-12T05:14:42Z","title":"Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain\n  Models","summary":"  Industrial systems such as recommender systems and online advertising, have\nbeen widely equipped with multi-stage architectures, which are divided into\nseveral cascaded modules, including matching, pre-ranking, ranking and\nre-ranking. As a critical bridge between matching and ranking, existing\npre-ranking approaches mainly endure sample selection bias (SSB) problem owing\nto ignoring the entire-chain data dependence, resulting in sub-optimal\nperformances. In this paper, we rethink pre-ranking system from the perspective\nof the entire sample space, and propose Entire-chain Cross-domain Models (ECM),\nwhich leverage samples from the whole cascaded stages to effectively alleviate\nSSB problem. Besides, we design a fine-grained neural structure named ECMM to\nfurther improve the pre-ranking accuracy. Specifically, we propose a\ncross-domain multi-tower neural network to comprehensively predict for each\nstage result, and introduce the sub-networking routing strategy with $L0$\nregularization to reduce computational costs. Evaluations on real-world\nlarge-scale traffic logs demonstrate that our pre-ranking models outperform\nSOTA methods while time consumption is maintained within an acceptable level,\nwhich achieves better trade-off between efficiency and effectiveness.\n","authors":["Jinbo Song","Ruoran Huang","Xinyang Wang","Wei Huang","Qian Yu","Mingming Chen","Yafei Yao","Chaosheng Fan","Changping Peng","Zhangang Lin","Jinghe Hu","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2310.08039v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.08038v1","updated":"2023-10-12T05:09:27Z","published":"2023-10-12T05:09:27Z","title":"Continual Learning via Manifold Expansion Replay","summary":"  In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.\n","authors":["Zihao Xu","Xuan Tang","Yufei Shi","Jianfeng Zhang","Jian Yang","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2310.08038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07990v1","updated":"2023-10-12T02:34:56Z","published":"2023-10-12T02:34:56Z","title":"Multi-View Variational Autoencoder for Missing Value Imputation in\n  Untargeted Metabolomics","summary":"  Background: Missing data is a common challenge in mass spectrometry-based\nmetabolomics, which can lead to biased and incomplete analyses. The integration\nof whole-genome sequencing (WGS) data with metabolomics data has emerged as a\npromising approach to enhance the accuracy of data imputation in metabolomics\nstudies. Method: In this study, we propose a novel method that leverages the\ninformation from WGS data and reference metabolites to impute unknown\nmetabolites. Our approach utilizes a multi-view variational autoencoder to\njointly model the burden score, polygenetic risk score (PGS), and linkage\ndisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature\nextraction and missing metabolomics data imputation. By learning the latent\nrepresentations of both omics data, our method can effectively impute missing\nmetabolomics values based on genomic information. Results: We evaluate the\nperformance of our method on empirical metabolomics datasets with missing\nvalues and demonstrate its superiority compared to conventional imputation\ntechniques. Using 35 template metabolites derived burden scores, PGS and\nLD-pruned SNPs, the proposed methods achieved r2-scores > 0.01 for 71.55% of\nmetabolites. Conclusion: The integration of WGS data in metabolomics imputation\nnot only improves data completeness but also enhances downstream analyses,\npaving the way for more comprehensive and accurate investigations of metabolic\npathways and disease associations. Our findings offer valuable insights into\nthe potential benefits of utilizing WGS data for metabolomics data imputation\nand underscore the importance of leveraging multi-modal data integration in\nprecision medicine research.\n","authors":["Chen Zhao","Kuan-Jui Su","Chong Wu","Xuewei Cao","Qiuying Sha","Wu Li","Zhe Luo","Tian Qin","Chuan Qiu","Lan Juan Zhao","Anqi Liu","Lindong Jiang","Xiao Zhang","Hui Shen","Weihua Zhou","Hong-Wen Deng"],"pdf_url":"https://arxiv.org/pdf/2310.07990v1.pdf","comment":"19 pages, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2310.08588v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback","summary":"  Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.\n","authors":["Jingkang Yang","Yuhao Dong","Shuai Liu","Bo Li","Ziyue Wang","Chencheng Jiang","Haoran Tan","Jiamu Kang","Yuanhan Zhang","Kaiyang Zhou","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08588v1.pdf","comment":"Project Page: https://choiszt.github.io/Octopus/, Codebase:\n  https://github.com/dongyh20/Octopus"},{"id":"http://arxiv.org/abs/2310.08582v1","updated":"2023-10-12T17:59:50Z","published":"2023-10-12T17:59:50Z","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models","summary":"  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections. Project page:\nhttps://tree-planner.github.io/\n","authors":["Mengkang Hu","Yao Mu","Xinmiao Yu","Mingyu Ding","Shiguang Wu","Wenqi Shao","Qiguang Chen","Bin Wang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08577v1","updated":"2023-10-12T17:59:30Z","published":"2023-10-12T17:59:30Z","title":"Visual Data-Type Understanding does not emerge from Scaling\n  Vision-Language Models","summary":"  Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of \\textit{Visual Data-Type Identification}, a basic perceptual\nskill with implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual \\textit{data-types}, spanning four broad categories. An extensive\nzero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a\nnuanced performance landscape. While VLMs are reasonably good at identifying\ncertain stylistic \\textit{data-types}, such as cartoons and sketches, they\nstruggle with simpler \\textit{data-types} arising from basic manipulations like\nimage rotations or additive noise. Our findings reveal that (i) model scaling\nalone yields marginal gains for contrastively-trained models like CLIP, and\n(ii) there is a pronounced drop in performance for the largest\nauto-regressively trained VLMs like OpenFlamingo. This finding points to a\nblind spot in current frontier VLMs: they excel in recognizing semantic content\nbut fail to acquire an understanding of visual \\textit{data-types} through\nscaling. By analyzing the pre-training distributions of these models and\nincorporating \\textit{data-type} information into the captions during\nfine-tuning, we achieve a significant enhancement in performance. By exploring\nthis previously uncharted task, we aim to set the stage for further advancing\nVLMs to equip them with visual data-type understanding. Code and datasets are\nreleased \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.\n","authors":["Vishaal Udandarao","Max F. Burg","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2310.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08576v1","updated":"2023-10-12T17:59:23Z","published":"2023-10-12T17:59:23Z","title":"Learning to Act from Actionless Videos through Dense Correspondences","summary":"  In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.\n","authors":["Po-Chen Ko","Jiayuan Mao","Yilun Du","Shao-Hua Sun","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2310.08576v1.pdf","comment":"Project page: https://flow-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2209.07481v2","updated":"2023-10-12T17:59:22Z","published":"2022-09-15T17:22:04Z","title":"Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman\n  Information","summary":"  Markov Chain Monte Carlo methods for sampling from complex distributions and\nestimating normalization constants often simulate samples from a sequence of\nintermediate distributions along an annealing path, which bridges between a\ntractable initial distribution and a target density of interest. Prior work has\nconstructed annealing paths using quasi-arithmetic means, and interpreted the\nresulting intermediate densities as minimizing an expected divergence to the\nendpoints. We provide a comprehensive analysis of this 'centroid' property\nusing Bregman divergences under a monotonic embedding of the density function,\nthereby associating common divergences such as Amari's and Renyi's\n${\\alpha}$-divergences, ${(\\alpha,\\beta)}$-divergences, and the Jensen-Shannon\ndivergence with intermediate densities along an annealing path. Our analysis\nhighlights the interplay between parametric families, quasi-arithmetic means,\nand divergence functions using the rho-tau Bregman divergence framework of\nZhang 2004,2013.\n","authors":["Rob Brekelmans","Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2209.07481v2.pdf","comment":"19 pages + appendix (rewritten + changed title in revision)"},{"id":"http://arxiv.org/abs/2310.08574v1","updated":"2023-10-12T17:57:57Z","published":"2023-10-12T17:57:57Z","title":"Jigsaw: Supporting Designers in Prototyping Multimodal Applications by\n  Assembling AI Foundation Models","summary":"  Recent advancements in AI foundation models have made it possible for them to\nbe utilized off-the-shelf for creative tasks, including ideating design\nconcepts or generating visual prototypes. However, integrating these models\ninto the creative process can be challenging as they often exist as standalone\napplications tailored to specific tasks. To address this challenge, we\nintroduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to\nrepresent foundation models. Jigsaw allows designers to combine different\nfoundation model capabilities across various modalities by assembling\ncompatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten\ndesigners and distilled design goals. In a user study, we showed that Jigsaw\nenhanced designers' understanding of available foundation model capabilities,\nprovided guidance on combining capabilities across different modalities and\ntasks, and served as a canvas to support design exploration, prototyping, and\ndocumentation.\n","authors":["David Chuan-En Lin","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2310.08574v1.pdf","comment":"Webpage: https://preview.jigsaw.to"},{"id":"http://arxiv.org/abs/2310.08571v1","updated":"2023-10-12T17:56:53Z","published":"2023-10-12T17:56:53Z","title":"Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders","summary":"  Machine Learning as a Service (MLaaS) APIs provide ready-to-use and\nhigh-utility encoders that generate vector representations for given inputs.\nSince these encoders are very costly to train, they become lucrative targets\nfor model stealing attacks during which an adversary leverages query access to\nthe API to replicate the encoder locally at a fraction of the original training\ncosts. We propose Bucks for Buckets (B4B), the first active defense that\nprevents stealing while the attack is happening without degrading\nrepresentation quality for legitimate API users. Our defense relies on the\nobservation that the representations returned to adversaries who try to steal\nthe encoder's functionality cover a significantly larger fraction of the\nembedding space than representations of legitimate users who utilize the\nencoder to solve a particular downstream task.vB4B leverages this to adaptively\nadjust the utility of the returned representations according to a user's\ncoverage of the embedding space. To prevent adaptive adversaries from eluding\nour defense by simply creating multiple user accounts (sybils), B4B also\nindividually transforms each user's representations. This prevents the\nadversary from directly aggregating representations over multiple accounts to\ncreate their stolen encoder copy. Our active defense opens a new path towards\nsecurely sharing and democratizing encoders over public APIs.\n","authors":["Jan Dubiski","Stanisaw Pawlak","Franziska Boenisch","Tomasz Trzciski","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2310.08571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08566v1","updated":"2023-10-12T17:55:02Z","published":"2023-10-12T17:55:02Z","title":"Transformers as Decision Makers: Provable In-Context Reinforcement\n  Learning via Supervised Pretraining","summary":"  Large transformer models pretrained on offline reinforcement learning\ndatasets have demonstrated remarkable in-context reinforcement learning (ICRL)\ncapabilities, where they can make good decisions when prompted with interaction\ntrajectories from unseen environments. However, when and how transformers can\nbe trained to perform ICRL have not been theoretically well-understood. In\nparticular, it is unclear which reinforcement-learning algorithms transformers\ncan perform in context, and how distribution mismatch in offline training data\naffects the learned algorithms. This paper provides a theoretical framework\nthat analyzes supervised pretraining for ICRL. This includes two recently\nproposed training methods -- algorithm distillation and decision-pretrained\ntransformers. First, assuming model realizability, we prove the\nsupervised-pretrained transformer will imitate the conditional expectation of\nthe expert algorithm given the observed trajectory. The generalization error\nwill scale with model capacity and a distribution divergence factor between the\nexpert and offline algorithms. Second, we show transformers with ReLU attention\ncan efficiently approximate near-optimal online reinforcement learning\nalgorithms like LinUCB and Thompson sampling for stochastic linear bandits, and\nUCB-VI for tabular Markov decision processes. This provides the first\nquantitative analysis of the ICRL capabilities of transformers pretrained from\noffline trajectories.\n","authors":["Licong Lin","Yu Bai","Song Mei"],"pdf_url":"https://arxiv.org/pdf/2310.08566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08558v1","updated":"2023-10-12T17:50:09Z","published":"2023-10-12T17:50:09Z","title":"Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate\n  Exploration Bias","summary":"  It is desirable for policies to optimistically explore new states and\nbehaviors during online reinforcement learning (RL) or fine-tuning, especially\nwhen prior offline data does not provide enough state coverage. However,\nexploration bonuses can bias the learned policy, and our experiments find that\nnaive, yet standard use of such bonuses can fail to recover a performant\npolicy. Concurrently, pessimistic training in offline RL has enabled recovery\nof performant policies from static datasets. Can we leverage offline RL to\nrecover better policies from online interaction? We make a simple observation\nthat a policy can be trained from scratch on all interaction data with\npessimistic objectives, thereby decoupling the policies used for data\ncollection and for evaluation. Specifically, we propose offline retraining, a\npolicy extraction step at the end of online fine-tuning in our\nOffline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL).\nAn optimistic (exploration) policy is used to interact with the environment,\nand a separate pessimistic (exploitation) policy is trained on all the observed\ndata for evaluation. Such decoupling can reduce any bias from online\ninteraction (intrinsic rewards, primacy bias) in the evaluation policy, and can\nallow more exploratory behaviors during online interaction which in turn can\ngenerate better data for exploitation. OOO is complementary to several\noffline-to-online RL and online RL methods, and improves their average\nperformance by 14% to 26% in our fine-tuning experiments, achieves\nstate-of-the-art performance on several environments in the D4RL benchmarks,\nand improves online RL performance by 165% on two OpenAI gym environments.\nFurther, OOO can enable fine-tuning from incomplete offline datasets where\nprior methods can fail to recover a performant policy. Implementation:\nhttps://github.com/MaxSobolMark/OOO\n","authors":["Max Sobol Mark","Archit Sharma","Fahim Tajwar","Rafael Rafailov","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2310.08558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04370v2","updated":"2023-10-12T17:48:22Z","published":"2023-09-08T15:02:46Z","title":"Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control","summary":"  Seeing-eye robots are very useful tools for guiding visually impaired people,\npotentially producing a huge societal impact given the low availability and\nhigh cost of real guide dogs. Although a few seeing-eye robot systems have\nalready been demonstrated, none considered external tugs from humans, which\nfrequently occur in a real guide dog setting. In this paper, we simultaneously\ntrain a locomotion controller that is robust to external tugging forces via\nReinforcement Learning (RL), and an external force estimator via supervised\nlearning. The controller ensures stable walking, and the force estimator\nenables the robot to respond to the external forces from the human. These\nforces are used to guide the robot to the global goal, which is unknown to the\nrobot, while the robot guides the human around nearby obstacles via a local\nplanner. Experimental results in simulation and on hardware show that our\ncontroller is robust to external forces, and our seeing-eye system can\naccurately detect force direction. We demonstrate our full seeing-eye robot\nsystem on a real quadruped robot with a blindfolded human. The video can be\nseen at our project page: https://bu-air-lab.github.io/guide_dog/\n","authors":["David DeFazio","Eisuke Hirota","Shiqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.04370v2.pdf","comment":"Accepted to CoRL 2023"},{"id":"http://arxiv.org/abs/2310.08549v1","updated":"2023-10-12T17:45:05Z","published":"2023-10-12T17:45:05Z","title":"Cross-Episodic Curriculum for Transformer Agents","summary":"  We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the\nlearning efficiency and generalization of Transformer agents. Central to CEC is\nthe placement of cross-episodic experiences into a Transformer's context, which\nforms the basis of a curriculum. By sequentially structuring online learning\ntrials and mixed-quality demonstrations, CEC constructs curricula that\nencapsulate learning progression and proficiency increase across episodes. Such\nsynergy combined with the potent pattern recognition capabilities of\nTransformer models delivers a powerful cross-episodic attention mechanism. The\neffectiveness of CEC is demonstrated under two representative scenarios: one\ninvolving multi-task reinforcement learning with discrete control, such as in\nDeepMind Lab, where the curriculum captures the learning progression in both\nindividual and progressively complex settings; and the other involving\nimitation learning with mixed-quality data for continuous control, as seen in\nRoboMimic, where the curriculum captures the improvement in demonstrators'\nexpertise. In all instances, policies resulting from CEC exhibit superior\nperformance and strong generalization. Code is open-sourced at\nhttps://cec-agent.github.io/ to facilitate research on Transformer agent\nlearning.\n","authors":["Lucy Xiaoyang Shi","Yunfan Jiang","Jake Grigsby","Linxi \"Jim\" Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.08549v1.pdf","comment":"To appear in NeurIPS 2023; The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2310.08548v1","updated":"2023-10-12T17:44:59Z","published":"2023-10-12T17:44:59Z","title":"Stronger Coreset Bounds for Kernel Density Estimators via Chaining","summary":"  We apply the discrepancy method and a chaining approach to give improved\nbounds on the coreset complexity of a wide class of kernel functions. Our\nresults give randomized polynomial time algorithms to produce coresets of size\n$O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$\nfor the Gaussian and Laplacian kernels in the case that the data set is\nuniformly bounded, an improvement that was not possible with previous\ntechniques. We also obtain coresets of size\n$O\\big(\\frac{1}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$ for the\nLaplacian kernel for $d$ constant. Finally, we give the best known bounds of\n$O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log(2\\max\\{1,\\alpha\\})}\\big)$ on the\ncoreset complexity of the exponential, Hellinger, and JS Kernels, where\n$1/\\alpha$ is the bandwidth parameter of the kernel.\n","authors":["Rainie Bozzai","Thomas Rothvoss"],"pdf_url":"https://arxiv.org/pdf/2310.08548v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2310.08540v1","updated":"2023-10-12T17:32:09Z","published":"2023-10-12T17:32:09Z","title":"Do pretrained Transformers Really Learn In-context by Gradient Descent?","summary":"  Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?\nSeveral recent works draw analogies between the dynamics of GD and the emergent\nbehavior of ICL in large language models. However, these works make assumptions\nfar from the realistic natural language setting in which language models are\ntrained. Such discrepancies between theory and practice, therefore, necessitate\nfurther investigation to validate their applicability.\n  We start by highlighting the weaknesses in prior works that construct\nTransformer weights to simulate gradient descent. Their experiments with\ntraining Transformers on ICL objective, inconsistencies in the order\nsensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity\nto parameter changes are some examples of a mismatch from the real-world\nsetting.\n  Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural\nsetting. We conduct comprehensive empirical analyses on language models\npretrained on natural data (LLaMa-7B). Our comparisons on various performance\nmetrics highlight the inconsistent behavior of ICL and GD as a function of\nvarious factors such as datasets, models, and number of demonstrations. We\nobserve that ICL and GD adapt the output distribution of language models\ndifferently. These results indicate that the equivalence between ICL and GD is\nan open hypothesis, requires nuanced considerations and calls for further\nstudies.\n","authors":["Lingfeng Shen","Aayush Mishra","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2310.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1910.09143v5","updated":"2023-10-12T17:27:48Z","published":"2019-10-21T04:24:29Z","title":"Dynamic Subgoal-based Exploration via Bayesian Optimization","summary":"  Reinforcement learning in sparse-reward navigation environments with\nexpensive and limited interactions is challenging and poses a need for\neffective exploration. Motivated by complex navigation tasks that require\nreal-world training (when cheap simulators are not available), we consider an\nagent that faces an unknown distribution of environments and must decide on an\nexploration strategy. It may leverage a series of training environments to\nimprove its policy before it is evaluated in a test environment drawn from the\nsame environment distribution. Most existing approaches focus on fixed\nexploration strategies, while the few that view exploration as a\nmeta-optimization problem tend to ignore the need for cost-efficient\nexploration. We propose a cost-aware Bayesian optimization approach that\nefficiently searches over a class of dynamic subgoal-based exploration\nstrategies. The algorithm adjusts a variety of levers -- the locations of the\nsubgoals, the length of each episode, and the number of replications per trial\n-- in order to overcome the challenges of sparse rewards, expensive\ninteractions, and noise. An experimental evaluation demonstrates that the new\napproach outperforms existing baselines across a number of problem domains. We\nalso provide a theoretical foundation and prove that the method asymptotically\nidentifies a near-optimal subgoal design.\n","authors":["Yijia Wang","Matthias Poloczek","Daniel R. Jiang"],"pdf_url":"https://arxiv.org/pdf/1910.09143v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05173v2","updated":"2023-10-12T17:25:44Z","published":"2023-09-11T00:02:05Z","title":"DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n","authors":["Zhengxiang Shi","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2309.05173v2.pdf","comment":"Code is available at https://github.com/ZhengxiangShi/DePT"},{"id":"http://arxiv.org/abs/1908.04628v3","updated":"2023-10-12T17:19:09Z","published":"2019-08-13T13:20:50Z","title":"L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes","summary":"  Many real-world prediction tasks have outcome variables that have\ncharacteristic heavy-tail distributions. Examples include copies of books sold,\nauction prices of art pieces, demand for commodities in warehouses, etc. By\nlearning heavy-tailed distributions, \"big and rare\" instances (e.g., the\nbest-sellers) will have accurate predictions. Most existing approaches are not\ndedicated to learning heavy-tailed distribution; thus, they heavily\nunder-predict such instances. To tackle this problem, we introduce Learning to\nPlace (L2P), which exploits the pairwise relationships between instances for\nlearning. In its training phase, L2P learns a pairwise preference classifier:\nis instance A > instance B? In its placing phase, L2P obtains a prediction by\nplacing the new instance among the known instances. Based on its placement, the\nnew instance is then assigned a value for its outcome variable. Experiments on\nreal data show that L2P outperforms competing approaches in terms of accuracy\nand ability to reproduce heavy-tailed outcome distribution. In addition, L2P\nprovides an interpretable model by placing each predicted instance in relation\nto its comparable neighbors. Interpretable models are highly desirable when\nlives and treasure are at stake.\n","authors":["Xindi Wang","Onur Varol","Tina Eliassi-Rad"],"pdf_url":"https://arxiv.org/pdf/1908.04628v3.pdf","comment":"9 pages, 6 figures, 2 tables Nature of changes from previous version:\n  1. Added complexity analysis in Section 2.2 2. Datasets change 3. Added\n  LambdaMART in the baseline methods, also a brief discussion on why LambdaMart\n  failed in our problem. 4. Figure updates"},{"id":"http://arxiv.org/abs/2310.05898v2","updated":"2023-10-12T17:16:37Z","published":"2023-10-09T17:41:29Z","title":"Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts","summary":"  Lion (Evolved Sign Momentum), a new optimizer discovered through program\nsearch, has shown promising results in training large AI models. It performs\ncomparably or favorably to AdamW but with greater memory efficiency. As we can\nexpect from the results of a random search program, Lion incorporates elements\nfrom several existing algorithms, including signed momentum, decoupled weight\ndecay, Polak, and Nesterov momentum, but does not fit into any existing\ncategory of theoretically grounded optimizers. Thus, even though Lion appears\nto perform well as a general-purpose optimizer for a wide range of tasks, its\ntheoretical basis remains uncertain. This lack of theoretical clarity limits\nopportunities to further enhance and expand Lion's efficacy.\n  This work aims to demystify Lion. Based on both continuous-time and\ndiscrete-time analysis, we demonstrate that Lion is a theoretically novel and\nprincipled approach for minimizing a general loss function $f(x)$ while\nenforcing a bound constraint $\\|x\\|_\\infty \\leq 1/\\lambda$. Lion achieves this\nthrough the incorporation of decoupled weight decay, where $\\lambda$ represents\nthe weight decay coefficient. Our analysis is made possible by the development\nof a new Lyapunov function for the Lion updates. It applies to a broader family\nof Lion-$\\kappa$ algorithms, where the $\\text{sign}(\\cdot)$ operator in Lion is\nreplaced by the subgradient of a convex function $\\kappa$, leading to the\nsolution of a general composite optimization problem of $\\min_x f(x) +\n\\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion\nand pave the way for further improvements and extensions of Lion-related\nalgorithms.\n","authors":["Lizhang Chen","Bo Liu","Kaizhao Liang","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05898v2.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.06225v2","updated":"2023-10-12T17:06:17Z","published":"2023-10-10T00:39:04Z","title":"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using\n  Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding across various domains, including healthcare and\nfinance. For some tasks, LLMs achieve similar or better performance than\ntrained human beings, therefore it is reasonable to employ human exams (e.g.,\ncertification tests) to assess the performance of LLMs. We present a\ncomprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their\nability to answer agriculture-related questions. In our evaluation, we also\nemploy RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)\ntechniques, which combine information retrieval, generation capabilities, and\nprompting strategies to improve the LLMs' performance. To demonstrate the\ncapabilities of LLMs, we selected agriculture exams and benchmark datasets from\nthree of the largest agriculture producer countries: Brazil, India, and the\nUSA. Our analysis highlights GPT-4's ability to achieve a passing score on\nexams to earn credits for renewing agronomist certifications, answering 93% of\nthe questions correctly and outperforming earlier general-purpose models, which\nachieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest\nperformance when compared to human subjects. This performance suggests that\nGPT-4 could potentially pass on major graduate education admission tests or\neven earn credits for renewing agronomy certificates. We also explore the\nmodels' capacity to address general agriculture-related questions and generate\ncrop management guidelines for Brazilian and Indian farmers, utilizing robust\ndatasets from the Brazilian Agency of Agriculture (Embrapa) and graduate\nprogram exams from India. The results suggest that GPT-4, ER, and RAG can\ncontribute meaningfully to agricultural education, assessment, and crop\nmanagement practice, offering valuable insights to farmers and agricultural\nprofessionals.\n","authors":["Bruno Silva","Leonardo Nunes","Roberto Estevo","Vijay Aski","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2310.06225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08501v1","updated":"2023-10-12T16:59:50Z","published":"2023-10-12T16:59:50Z","title":"Unsupervised Learning of Object-Centric Embeddings for Cell Instance\n  Segmentation in Microscopy Images","summary":"  Segmentation of objects in microscopy images is required for many biomedical\napplications. We introduce object-centric embeddings (OCEs), which embed image\npatches such that the spatial offsets between patches cropped from the same\nobject are preserved. Those learnt embeddings can be used to delineate\nindividual objects and thus obtain instance segmentations. Here, we show\ntheoretically that, under assumptions commonly found in microscopy images, OCEs\ncan be learnt through a self-supervised task that predicts the spatial offset\nbetween image patches. Together, this forms an unsupervised cell instance\nsegmentation method which we evaluate on nine diverse large-scale microscopy\ndatasets. Segmentations obtained with our method lead to substantially improved\nresults, compared to state-of-the-art baselines on six out of nine datasets,\nand perform on par on the remaining three datasets. If ground-truth annotations\nare available, our method serves as an excellent starting point for supervised\ntraining, reducing the required amount of ground-truth needed by one order of\nmagnitude, thus substantially increasing the practical applicability of our\nmethod. Source code is available at https://github.com/funkelab/cellulus.\n","authors":["Steffen Wolf","Manan Lalit","Henry Westmacott","Katie McDole","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2310.08501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08497v1","updated":"2023-10-12T16:56:37Z","published":"2023-10-12T16:56:37Z","title":"Impact of time and note duration tokenizations on deep learning symbolic\n  music modeling","summary":"  Symbolic music is widely used in various deep learning tasks, including\ngeneration, transcription, synthesis, and Music Information Retrieval (MIR). It\nis mostly employed with discrete models like Transformers, which require music\nto be tokenized, i.e., formatted into sequences of distinct elements called\ntokens. Tokenization can be performed in different ways. As Transformer can\nstruggle at reasoning, but capture more easily explicit information, it is\nimportant to study how the way the information is represented for such model\nimpact their performances. In this work, we analyze the common tokenization\nmethods and experiment with time and note duration representations. We compare\nthe performances of these two impactful criteria on several tasks, including\ncomposer and emotion classification, music generation, and sequence\nrepresentation learning. We demonstrate that explicit information leads to\nbetter results depending on the task.\n","authors":["Nathan Fradet","Nicolas Gutowski","Fabien Chhel","Jean-Pierre Briot"],"pdf_url":"https://arxiv.org/pdf/2310.08497v1.pdf","comment":"ISMIR 2023"},{"id":"http://arxiv.org/abs/2310.08495v1","updated":"2023-10-12T16:55:04Z","published":"2023-10-12T16:55:04Z","title":"Characterizing climate pathways using feature importance on echo state\n  networks","summary":"  The 2022 National Defense Strategy of the United States listed climate change\nas a serious threat to national security. Climate intervention methods, such as\nstratospheric aerosol injection, have been proposed as mitigation strategies,\nbut the downstream effects of such actions on a complex climate system are not\nwell understood. The development of algorithmic techniques for quantifying\nrelationships between source and impact variables related to a climate event\n(i.e., a climate pathway) would help inform policy decisions. Data-driven deep\nlearning models have become powerful tools for modeling highly nonlinear\nrelationships and may provide a route to characterize climate variable\nrelationships. In this paper, we explore the use of an echo state network (ESN)\nfor characterizing climate pathways. ESNs are a computationally efficient\nneural network variation designed for temporal data, and recent work proposes\nESNs as a useful tool for forecasting spatio-temporal climate data. Like other\nneural networks, ESNs are non-interpretable black-box models, which poses a\nhurdle for understanding variable relationships. We address this issue by\ndeveloping feature importance methods for ESNs in the context of\nspatio-temporal data to quantify variable relationships captured by the model.\nWe conduct a simulation study to assess and compare the feature importance\ntechniques, and we demonstrate the approach on reanalysis climate data. In the\nclimate application, we select a time period that includes the 1991 volcanic\neruption of Mount Pinatubo. This event was a significant stratospheric aerosol\ninjection, which we use as a proxy for an artificial stratospheric aerosol\ninjection. Using the proposed approach, we are able to characterize\nrelationships between pathway variables associated with this event.\n","authors":["Katherine Goode","Daniel Ries","Kellie McClernon"],"pdf_url":"https://arxiv.org/pdf/2310.08495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08491v1","updated":"2023-10-12T16:50:08Z","published":"2023-10-12T16:50:08Z","title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language\n  Models","summary":"  Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://github.com/kaistAI/Prometheus.\n","authors":["Seungone Kim","Jamin Shin","Yejin Cho","Joel Jang","Shayne Longpre","Hwaran Lee","Sangdoo Yun","Seongjin Shin","Sungdong Kim","James Thorne","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2310.08491v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2310.06823v2","updated":"2023-10-12T16:42:55Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Moun Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.08475v1","updated":"2023-10-12T16:32:44Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights\\footnote{Code\nand dataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08470v1","updated":"2023-10-12T16:28:25Z","published":"2023-10-12T16:28:25Z","title":"Strategies and impact of learning curve estimation for CNN-based image\n  classification","summary":"  Learning curves are a measure for how the performance of machine learning\nmodels improves given a certain volume of training data. Over a wide variety of\napplications and models it was observed that learning curves follow -- to a\nlarge extent -- a power law behavior. This makes the performance of different\nmodels for a given task somewhat predictable and opens the opportunity to\nreduce the training time for practitioners, who are exploring the space of\npossible models and hyperparameters for the problem at hand. By estimating the\nlearning curve of a model from training on small subsets of data only the best\nmodels need to be considered for training on the full dataset. How to choose\nsubset sizes and how often to sample models on these to obtain estimates is\nhowever not researched. Given that the goal is to reduce overall training time\nstrategies are needed that sample the performance in a time-efficient way and\nyet leads to accurate learning curve estimates. In this paper we formulate the\nframework for these strategies and propose several strategies. Further we\nevaluate the strategies for simulated learning curves and in experiments with\npopular datasets and models for image classification tasks.\n","authors":["Laura Didyk","Brayden Yarish","Michael A. Beck","Christopher P. Bidinosti","Christopher J. Henry"],"pdf_url":"https://arxiv.org/pdf/2310.08470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08461v1","updated":"2023-10-12T16:21:04Z","published":"2023-10-12T16:21:04Z","title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation","summary":"  Speculative decoding (SD) accelerates large language model inference by\nemploying a faster draft model for generating multiple tokens, which are then\nverified in parallel by the larger target model, resulting in the text\ngenerated according to the target model distribution. However, identifying a\ncompact draft model that is well-aligned with the target model is challenging.\nTo tackle this issue, we propose DistillSpec that uses knowledge distillation\nto better align the draft model with the target model, before applying SD.\nDistillSpec makes two key design choices, which we demonstrate via systematic\nstudy to be crucial to improving the draft and target alignment: utilizing\non-policy data generation from the draft model, and tailoring the divergence\nfunction to the task and decoding strategy. Notably, DistillSpec yields\nimpressive 10 - 45% speedups over standard SD on a range of standard\nbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combine\nDistillSpec with lossy SD to achieve fine-grained control over the latency vs.\ntask performance trade-off. Finally, in practical scenarios with models of\nvarying sizes, first using distillation to boost the performance of the target\nmodel and then applying DistillSpec to train a well-aligned draft model can\nreduce decoding latency by 6-10x with minimal performance drop, compared to\nstandard decoding without distillation.\n","authors":["Yongchao Zhou","Kaifeng Lyu","Ankit Singh Rawat","Aditya Krishna Menon","Afshin Rostamizadeh","Sanjiv Kumar","Jean-Franois Kagy","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2310.08461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08459v1","updated":"2023-10-12T16:19:58Z","published":"2023-10-12T16:19:58Z","title":"A Survey on Heterogeneous Transfer Learning","summary":"  The application of transfer learning, an approach utilizing knowledge from a\nsource domain to enhance model performance in a target domain, has seen a\ntremendous rise in recent years, underpinning many real-world scenarios. The\nkey to its success lies in the shared common knowledge between the domains, a\nprerequisite in most transfer learning methodologies. These methods typically\npresuppose identical feature spaces and label spaces in both domains, known as\nhomogeneous transfer learning, which, however, is not always a practical\nassumption. Oftentimes, the source and target domains vary in feature spaces,\ndata distributions, and label spaces, making it challenging or costly to secure\nsource domain data with identical feature and label spaces as the target\ndomain. Arbitrary elimination of these differences is not always feasible or\noptimal. Thus, heterogeneous transfer learning, acknowledging and dealing with\nsuch disparities, has emerged as a promising approach for a variety of tasks.\nDespite the existence of a survey in 2017 on this topic, the fast-paced\nadvances post-2017 necessitate an updated, in-depth review. We therefore\npresent a comprehensive survey of recent developments in heterogeneous transfer\nlearning methods, offering a systematic guide for future research. Our paper\nreviews methodologies for diverse learning scenarios, discusses the limitations\nof current studies, and covers various application contexts, including Natural\nLanguage Processing, Computer Vision, Multimodality, and Biomedicine, to foster\na deeper understanding and spur future research.\n","authors":["Runxue Bao","Yiming Sun","Yuhe Gao","Jindong Wang","Qiang Yang","Haifeng Chen","Zhi-Hong Mao","Xing Xie","Ye Ye"],"pdf_url":"https://arxiv.org/pdf/2310.08459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15363v3","updated":"2023-10-12T16:12:57Z","published":"2022-11-28T14:38:45Z","title":"On the Security Vulnerabilities of Text-to-SQL Models","summary":"  Although it has been demonstrated that Natural Language Processing (NLP)\nalgorithms are vulnerable to deliberate attacks, the question of whether such\nweaknesses can lead to software security threats is under-explored. To bridge\nthis gap, we conducted vulnerability tests on Text-to-SQL systems that are\ncommonly used to create natural language interfaces to databases. We showed\nthat the Text-to-SQL modules within six commercial applications can be\nmanipulated to produce malicious code, potentially leading to data breaches and\nDenial of Service attacks. This is the first demonstration that NLP models can\nbe exploited as attack vectors in the wild. In addition, experiments using four\nopen-source language models verified that straightforward backdoor attacks on\nText-to-SQL systems achieve a 100% success rate without affecting their\nperformance. The aim of this work is to draw the community's attention to\npotential software security issues associated with NLP algorithms and encourage\nexploration of methods to mitigate against them.\n","authors":["Xutan Peng","Yipeng Zhang","Jingfeng Yang","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2211.15363v3.pdf","comment":"ISSRE 2023: Best Paper Candidate"},{"id":"http://arxiv.org/abs/2305.14259v3","updated":"2023-10-12T16:10:51Z","published":"2023-05-23T17:12:08Z","title":"Learning to Generate Novel Scientific Directions with Contextualized\n  Literature-based Discovery","summary":"  Literature-Based Discovery (LBD) aims to discover new scientific knowledge by\nmining papers and generating hypotheses. Standard LBD is limited to predicting\npairwise relations between discrete concepts (e.g., drug-disease links), and\nignores critical contexts like experimental settings (e.g., a specific patient\npopulation where a drug is evaluated) and background motivations (e.g., to find\ndrugs without specific side effects). We address these limitations with a novel\nformulation of contextualized-LBD (C-LBD): generating scientific hypotheses in\nnatural language, while grounding them in a context that controls the\nhypothesis search space. We present a modeling framework using retrieval of\n``inspirations'' from past scientific papers. Our evaluations reveal that GPT-4\ntends to generate ideas with overall low technical depth and novelty, while our\ninspiration prompting approaches partially mitigate this issue. Our work\nrepresents a first step toward building language models that generate new ideas\nderived from scientific literature.\n","authors":["Qingyun Wang","Doug Downey","Heng Ji","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2305.14259v3.pdf","comment":"24 pages. Code and resource is available at\n  https://github.com/EagleW/CLBD"},{"id":"http://arxiv.org/abs/2310.08446v1","updated":"2023-10-12T16:06:18Z","published":"2023-10-12T16:06:18Z","title":"Towards Robust Multi-Modal Reasoning via Model Selection","summary":"  The reasoning capabilities of LLM (Large Language Model) are widely\nacknowledged in recent research, inspiring studies on tool learning and\nautonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple\ntools for collaborative multi-step task solving. Unlike methods invoking tools\nlike calculators or weather APIs for straightforward tasks, multi-modal agents\nexcel by integrating diverse AI models for complex challenges. However, current\nmulti-modal agents neglect the significance of model selection: they primarily\nfocus on the planning and execution phases, and will only invoke predefined\ntask-specific models for each subtask, making the execution fragile. Meanwhile,\nother traditional model selection methods are either incompatible with or\nsuboptimal for the multi-modal agent scenarios, due to ignorance of\ndependencies among subtasks arising by multi-step reasoning.\n  To this end, we identify the key challenges therein and propose the\n$\\textit{M}^3$ framework as a plug-in with negligible runtime overhead at\ntest-time. This framework improves model selection and bolsters the robustness\nof multi-modal agents in multi-step reasoning. In the absence of suitable\nbenchmarks, we create MS-GQA, a new dataset specifically designed to\ninvestigate the model selection challenge in multi-modal agents. Our\nexperiments reveal that our framework enables dynamic model selection,\nconsidering both user inputs and subtask dependencies, thereby robustifying the\noverall reasoning process. Our code and benchmark:\nhttps://github.com/LINs-lab/M3.\n","authors":["Xiangyan Liu","Rongxue Li","Wei Ji","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2310.08446v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2308.16198v2","updated":"2023-10-12T15:57:52Z","published":"2023-08-25T21:30:16Z","title":"Learning Collaborative Information Dissemination with Graph-based\n  Multi-Agent Reinforcement Learning","summary":"  In modern communication systems, efficient and reliable information\ndissemination is crucial for supporting critical operations across domains like\ndisaster response, autonomous vehicles, and sensor networks. This paper\nintroduces a Multi-Agent Reinforcement Learning (MARL) approach as a\nsignificant step forward in achieving more decentralized, efficient, and\ncollaborative solutions. We propose a Partially Observable Stochastic Game\n(POSG) formulation for information dissemination empowering each agent to\ndecide on message forwarding independently, based on their one-hop\nneighborhood. This constitutes a significant paradigm shift from traditional\nheuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses\nGraph Convolutional Reinforcement Learning, employing Graph Attention Networks\n(GAT) with dynamic attention to capture essential network features. We propose\ntwo approaches, L-DGN and HL-DGN, which differ in the information that is\nexchanged among agents. We evaluate the performance of our decentralized\napproaches, by comparing them with a widely-used MPR heuristic, and we show\nthat our trained policies are able to efficiently cover the network while\nbypassing the MPR set selection process. Our approach is a first step toward\nsupporting the resilience of real-world broadcast communication infrastructures\nvia learned, collaborative information dissemination.\n","authors":["Raffaele Galliera","Kristen Brent Venable","Matteo Bassani","Niranjan Suri"],"pdf_url":"https://arxiv.org/pdf/2308.16198v2.pdf","comment":"11 pages (2 of Supplementary Materials), 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.08431v1","updated":"2023-10-12T15:56:02Z","published":"2023-10-12T15:56:02Z","title":"Neural Sampling in Hierarchical Exponential-family Energy-based Models","summary":"  Bayesian brain theory suggests that the brain employs generative models to\nunderstand the external world. The sampling-based perspective posits that the\nbrain infers the posterior distribution through samples of stochastic neuronal\nresponses. Additionally, the brain continually updates its generative model to\napproach the true distribution of the external world. In this study, we\nintroduce the Hierarchical Exponential-family Energy-based (HEE) model, which\ncaptures the dynamics of inference and learning. In the HEE model, we decompose\nthe partition function into individual layers and leverage a group of neurons\nwith shorter time constants to sample the gradient of the decomposed\nnormalization term. This allows our model to estimate the partition function\nand perform inference simultaneously, circumventing the negative phase\nencountered in conventional energy-based models (EBMs). As a result, the\nlearning process is localized both in time and space, and the model is easy to\nconverge. To match the brain's rapid computation, we demonstrate that neural\nadaptation can serve as a momentum term, significantly accelerating the\ninference process. On natural image datasets, our model exhibits\nrepresentations akin to those observed in the biological visual system.\nFurthermore, for the machine learning community, our model can generate\nobservations through joint or marginal generation. We show that marginal\ngeneration outperforms joint generation and achieves performance on par with\nother EBMs.\n","authors":["Xingsi Dong","Si Wu"],"pdf_url":"https://arxiv.org/pdf/2310.08431v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.09663v2","updated":"2023-10-12T15:52:37Z","published":"2023-04-19T13:50:13Z","title":"Generative modeling of time-dependent densities via optimal transport\n  and projection pursuit","summary":"  Motivated by the computational difficulties incurred by popular deep learning\nalgorithms for the generative modeling of temporal densities, we propose a\ncheap alternative which requires minimal hyperparameter tuning and scales\nfavorably to high dimensional problems. In particular, we use a\nprojection-based optimal transport solver [Meng et al., 2019] to join\nsuccessive samples and subsequently use transport splines [Chewi et al., 2020]\nto interpolate the evolving density. When the sampling frequency is\nsufficiently high, the optimal maps are close to the identity and are thus\ncomputationally efficient to compute. Moreover, the training process is highly\nparallelizable as all optimal maps are independent and can thus be learned\nsimultaneously. Finally, the approach is based solely on numerical linear\nalgebra rather than minimizing a nonconvex objective function, allowing us to\neasily analyze and control the algorithm. We present several numerical\nexperiments on both synthetic and real-world datasets to demonstrate the\nefficiency of our method. In particular, these experiments show that the\nproposed approach is highly competitive compared with state-of-the-art\nnormalizing flows conditioned on time across a wide range of dimensionalities.\n","authors":["Jonah Botvinick-Greenhouse","Yunan Yang","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2304.09663v2.pdf","comment":"This article may be downloaded for personal use only. Any other use\n  requires prior permission of the author and AIP Publishing. This article\n  appeared in Chaos: An Interdisciplinary Journal of Nonlinear Science, Volume\n  33, Issue 10, October 2023 and may be found at\n  https://doi.org/10.1063/5.0155783"},{"id":"http://arxiv.org/abs/2310.08425v1","updated":"2023-10-12T15:48:14Z","published":"2023-10-12T15:48:14Z","title":"Differentially Private Non-convex Learning for Multi-layer Neural\n  Networks","summary":"  This paper focuses on the problem of Differentially Private Stochastic\nOptimization for (multi-layer) fully connected neural networks with a single\noutput node. In the first part, we examine cases with no hidden nodes,\nspecifically focusing on Generalized Linear Models (GLMs). We investigate the\nwell-specific model where the random noise possesses a zero mean, and the link\nfunction is both bounded and Lipschitz continuous. We propose several\nalgorithms and our analysis demonstrates the feasibility of achieving an excess\npopulation risk that remains invariant to the data dimension. We also delve\ninto the scenario involving the ReLU link function, and our findings mirror\nthose of the bounded link function. We conclude this section by contrasting\nwell-specified and misspecified models, using ReLU regression as a\nrepresentative example.\n  In the second part of the paper, we extend our ideas to two-layer neural\nnetworks with sigmoid or ReLU activation functions in the well-specified model.\nIn the third part, we study the theoretical guarantees of DP-SGD in Abadi et\nal. (2016) for fully connected multi-layer neural networks. By utilizing recent\nadvances in Neural Tangent Kernel theory, we provide the first excess\npopulation risk when both the sample size and the width of the network are\nsufficiently large. Additionally, we discuss the role of some parameters in\nDP-SGD regarding their utility, both theoretically and empirically.\n","authors":["Hanpu Shen","Cheng-Long Wang","Zihang Xiang","Yiming Ying","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12345v4","updated":"2023-10-12T15:44:42Z","published":"2022-11-22T15:34:59Z","title":"Understanding Sparse Feature Updates in Deep Networks using Iterative\n  Linearisation","summary":"  Larger and deeper networks generalise well despite their increased capacity\nto overfit. Understanding why this happens is theoretically and practically\nimportant. One recent approach looks at the infinitely wide limits of such\nnetworks and their corresponding kernels. However, these theoretical tools\ncannot fully explain finite networks as the empirical kernel changes\nsignificantly during gradient-descent-based training in contrast to infinite\nnetworks. In this work, we derive an iterative linearised training method as a\nnovel empirical tool to further investigate this distinction, allowing us to\ncontrol for sparse (i.e. infrequent) feature updates and quantify the frequency\nof feature learning needed to achieve comparable performance. We justify\niterative linearisation as an interpolation between a finite analog of the\ninfinite width regime, which does not learn features, and standard gradient\ndescent training, which does. Informally, we also show that it is analogous to\na damped version of the Gauss-Newton algorithm -- a second-order method. We\nshow that in a variety of cases, iterative linearised training surprisingly\nperforms on par with standard training, noting in particular how much less\nfrequent feature learning is required to achieve comparable performance. We\nalso show that feature learning is essential for good performance. Since such\nfeature learning inevitably causes changes in the NTK kernel, we provide direct\nnegative evidence for the NTK theory, which states the NTK kernel remains\nconstant during training.\n","authors":["Adrian Goldwaser","Hong Ge"],"pdf_url":"https://arxiv.org/pdf/2211.12345v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08419v1","updated":"2023-10-12T15:38:28Z","published":"2023-10-12T15:38:28Z","title":"Jailbreaking Black Box Large Language Models in Twenty Queries","summary":"  There is growing interest in ensuring that large language models (LLMs) align\nwith human values. However, the alignment of such models is vulnerable to\nadversarial jailbreaks, which coax LLMs into overriding their safety\nguardrails. The identification of these vulnerabilities is therefore\ninstrumental in understanding inherent weaknesses and preventing future misuse.\nTo this end, we propose Prompt Automatic Iterative Refinement (PAIR), an\nalgorithm that generates semantic jailbreaks with only black-box access to an\nLLM. PAIR -- which is inspired by social engineering attacks -- uses an\nattacker LLM to automatically generate jailbreaks for a separate targeted LLM\nwithout human intervention. In this way, the attacker LLM iteratively queries\nthe target LLM to update and refine a candidate jailbreak. Empirically, PAIR\noften requires fewer than twenty queries to produce a jailbreak, which is\norders of magnitude more efficient than existing algorithms. PAIR also achieves\ncompetitive jailbreaking success rates and transferability on open and\nclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.\n","authors":["Patrick Chao","Alexander Robey","Edgar Dobriban","Hamed Hassani","George J. Pappas","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2310.08419v1.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.10108v2","updated":"2023-10-12T15:24:28Z","published":"2023-03-17T16:39:21Z","title":"Data-Centric Learning from Unlabeled Graphs with Diffusion Model","summary":"  Graph property prediction tasks are important and numerous. While each task\noffers a small size of labeled examples, unlabeled graphs have been collected\nfrom various sources and at a large scale. A conventional approach is training\na model with the unlabeled graphs on self-supervised tasks and then fine-tuning\nthe model on the prediction tasks. However, the self-supervised task knowledge\ncould not be aligned or sometimes conflicted with what the predictions needed.\nIn this paper, we propose to extract the knowledge underlying the large set of\nunlabeled graphs as a specific set of useful data points to augment each\nproperty prediction model. We use a diffusion model to fully utilize the\nunlabeled graphs and design two new objectives to guide the model's denoising\nprocess with each task's labeled data to generate task-specific graph examples\nand their labels. Experiments demonstrate that our data-centric approach\nperforms significantly better than fifteen existing various methods on fifteen\ntasks. The performance improvement brought by unlabeled data is visible as the\ngenerated labeled examples unlike the self-supervised learning.\n","authors":["Gang Liu","Eric Inae","Tong Zhao","Jiaxin Xu","Tengfei Luo","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.10108v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.08394v1","updated":"2023-10-12T15:07:11Z","published":"2023-10-12T15:07:11Z","title":"Towards Better Evaluation of Instruction-Following: A Case-Study in\n  Summarization","summary":"  Despite recent advances, evaluating how well large language models (LLMs)\nfollow user instructions remains an open problem. While evaluation methods of\nlanguage models have seen a rise in prompt-based approaches, limited work on\nthe correctness of these methods has been conducted. In this work, we perform a\nmeta-evaluation of a variety of metrics to quantify how accurately they measure\nthe instruction-following abilities of LLMs. Our investigation is performed on\ngrounded query-based summarization by collecting a new short-form, real-world\ndataset riSum, containing $300$ document-instruction pairs with $3$ answers\neach. All $900$ answers are rated by $3$ human annotators. Using riSum, we\nanalyze agreement between evaluation methods and human judgment. Finally, we\npropose new LLM-based reference-free evaluation methods that improve upon\nestablished baselines and perform on-par with costly reference-based metrics\nwhich require high-quality summaries.\n","authors":["Ondrej Skopek","Rahul Aralikatte","Sian Gooding","Victor Carbune"],"pdf_url":"https://arxiv.org/pdf/2310.08394v1.pdf","comment":"Accepted to CoNLL 2023"},{"id":"http://arxiv.org/abs/2308.12243v2","updated":"2023-10-12T15:06:50Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Neural Network Training","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN.\n","authors":["S. S. Hotegni","S. Peitz","M. Berkemeier"],"pdf_url":"https://arxiv.org/pdf/2308.12243v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2304.02621v2","updated":"2023-10-12T15:05:15Z","published":"2023-04-05T17:43:57Z","title":"High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation","summary":"  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset show that our proposed add-on is well-suited for\nlarge-scale settings. Our code is available at https://github.com/arvijj/hfpl.\n","authors":["Arvi Jonnarth","Yushan Zhang","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2304.02621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08392v1","updated":"2023-10-12T15:03:50Z","published":"2023-10-12T15:03:50Z","title":"Introducing a Deep Neural Network-based Model Predictive Control\n  Framework for Rapid Controller Implementation","summary":"  Model Predictive Control (MPC) provides an optimal control solution based on\na cost function while allowing for the implementation of process constraints.\nAs a model-based optimal control technique, the performance of MPC strongly\ndepends on the model used where a trade-off between model computation time and\nprediction performance exists. One solution is the integration of MPC with a\nmachine learning (ML) based process model which are quick to evaluate online.\nThis work presents the experimental implementation of a deep neural network\n(DNN) based nonlinear MPC for Homogeneous Charge Compression Ignition (HCCI)\ncombustion control. The DNN model consists of a Long Short-Term Memory (LSTM)\nnetwork surrounded by fully connected layers which was trained using\nexperimental engine data and showed acceptable prediction performance with\nunder 5% error for all outputs. Using this model, the MPC is designed to track\nthe Indicated Mean Effective Pressure (IMEP) and combustion phasing\ntrajectories, while minimizing several parameters. Using the acados software\npackage to enable the real-time implementation of the MPC on an ARM Cortex A72,\nthe optimization calculations are completed within 1.4 ms. The external A72\nprocessor is integrated with the prototyping engine controller using a UDP\nconnection allowing for rapid experimental deployment of the NMPC. The IMEP\ntrajectory following of the developed controller was excellent, with a\nroot-mean-square error of 0.133 bar, in addition to observing process\nconstraints.\n","authors":["David C. Gordon","Alexander Winkler","Julian Bedei","Patrick Schaber","Jakob Andert","Charles R. Koch"],"pdf_url":"https://arxiv.org/pdf/2310.08392v1.pdf","comment":"Submitted to 2024 American Control Conference (ACC), July 8-12, 2024\n  in Toronto, Canada. ACC is the annual conference of the American Automatic\n  Control Council (AACC), the U.S. national member organization of the\n  International Federation for Automatic Control (IFAC)"},{"id":"http://arxiv.org/abs/2310.08391v1","updated":"2023-10-12T15:01:43Z","published":"2023-10-12T15:01:43Z","title":"How Many Pretraining Tasks Are Needed for In-Context Learning of Linear\n  Regression?","summary":"  Transformers pretrained on diverse tasks exhibit remarkable in-context\nlearning (ICL) capabilities, enabling them to solve unseen tasks solely based\non input contexts without adjusting model parameters. In this paper, we study\nICL in one of its simplest setups: pretraining a linearly parameterized\nsingle-layer linear attention model for linear regression with a Gaussian\nprior. We establish a statistical task complexity bound for the attention model\npretraining, showing that effective pretraining only requires a small number of\nindependent tasks. Furthermore, we prove that the pretrained model closely\nmatches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by\nachieving nearly Bayes optimal risk on unseen tasks under a fixed context\nlength. These theoretical findings complement prior experimental research and\nshed light on the statistical foundations of ICL.\n","authors":["Jingfeng Wu","Difan Zou","Zixiang Chen","Vladimir Braverman","Quanquan Gu","Peter L. Bartlett"],"pdf_url":"https://arxiv.org/pdf/2310.08391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15395v2","updated":"2023-10-12T15:00:55Z","published":"2023-09-27T04:33:09Z","title":"Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs","summary":"  This paper considers the best policy identification (BPI) problem in online\nConstrained Markov Decision Processes (CMDPs). We are interested in algorithms\nthat are model-free, have low regret, and identify an optimal policy with a\nhigh probability. Existing model-free algorithms for online CMDPs with\nsublinear regret and constraint violation do not provide any convergence\nguarantee to an optimal policy and provide only average performance guarantees\nwhen a policy is uniformly sampled at random from all previously used policies.\nIn this paper, we develop a new algorithm, named\nPruning-Refinement-Identification (PRI), based on a fundamental structural\nproperty of CMDPs we discover, called limited stochasticity. The property says\nfor a CMDP with $N$ constraints, there exists an optimal policy with at most\n$N$ stochastic decisions.\n  The proposed algorithm first identifies at which step and in which state a\nstochastic decision has to be taken and then fine-tunes the distributions of\nthese stochastic decisions. PRI achieves trio objectives: (i) PRI is a\nmodel-free algorithm; and (ii) it outputs a near-optimal policy with a high\nprobability at the end of learning; and (iii) in the tabular setting, PRI\nguarantees $\\tilde{\\mathcal{O}}(\\sqrt{K})$ regret and constraint violation,\nwhich significantly improves the best existing regret bound\n$\\tilde{\\mathcal{O}}(K^{\\frac{4}{5}})$ under a model-free algorithm, where $K$\nis the total number of episodes.\n","authors":["Zihan Zhou","Honghao Wei","Lei Ying"],"pdf_url":"https://arxiv.org/pdf/2309.15395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08387v1","updated":"2023-10-12T14:59:22Z","published":"2023-10-12T14:59:22Z","title":"MeanAP-Guided Reinforced Active Learning for Object Detection","summary":"  Active learning presents a promising avenue for training high-performance\nmodels with minimal labeled data, achieved by judiciously selecting the most\ninformative instances to label and incorporating them into the task learner.\nDespite notable advancements in active learning for image recognition, metrics\ndevised or learned to gauge the information gain of data, crucial for query\nstrategy design, do not consistently align with task model performance metrics,\nsuch as Mean Average Precision (MeanAP) in object detection tasks. This paper\nintroduces MeanAP-Guided Reinforced Active Learning for Object Detection\n(MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task\nmodel to devise a sampling strategy employing a reinforcement learning-based\nsampling agent. Built upon LSTM architecture, the agent efficiently explores\nand selects subsequent training instances, and optimizes the process through\npolicy gradient with MeanAP serving as reward. Recognizing the time-intensive\nnature of MeanAP computation at each step, we propose fast look-up tables to\nexpedite agent training. We assess MAGRAL's efficacy across popular benchmarks,\nPASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical\nfindings substantiate MAGRAL's superiority over recent state-of-the-art\nmethods, showcasing substantial performance gains. MAGRAL establishes a robust\nbaseline for reinforced active object detection, signifying its potential in\nadvancing the field.\n","authors":["Zhixuan Liang","Xingyu Zeng","Rui Zhao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08381v1","updated":"2023-10-12T14:55:31Z","published":"2023-10-12T14:55:31Z","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","summary":"  Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach\nto adapting pre-trained vision models to solve various downstream\nimage-classification tasks. However, there has hitherto been little systematic\nstudy of the design space of VP and no clear benchmark for evaluating its\nperformance. To bridge this gap, we propose AutoVP, an end-to-end expandable\nframework for automating VP design choices, along with 12 downstream\nimage-classification tasks that can serve as a holistic VP-performance\nbenchmark. Our design space covers 1) the joint optimization of the prompts; 2)\nthe selection of pre-trained models, including image classifiers and text-image\nencoders; and 3) model output mapping strategies, including nonparametric and\ntrainable label mapping. Our extensive experimental results show that AutoVP\noutperforms the best-known current VP methods by a substantial margin, having\nup to 6.7% improvement in accuracy; and attains a maximum performance increase\nof 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold\ncontribution: serving both as an efficient tool for hyperparameter tuning on VP\ndesign choices, and as a comprehensive benchmark that can reasonably be\nexpected to accelerate VP's development. The source code is available at\nhttps://github.com/IBM/AutoVP.\n","authors":["Hsi-Ai Tsao","Lei Hsiung","Pin-Yu Chen","Sijia Liu","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2310.08381v1.pdf","comment":"Preprint. The code is available at https://github.com/IBM/AutoVP"},{"id":"http://arxiv.org/abs/2306.16335v2","updated":"2023-10-12T14:46:15Z","published":"2023-06-28T16:11:50Z","title":"Emulating the dynamics of complex systems using autoregressive models on\n  manifolds (mNARX)","summary":"  We propose a novel surrogate modelling approach to efficiently and accurately\napproximate the response of complex dynamical systems driven by time-varying\nexogenous excitations over extended time periods. Our approach, namely manifold\nnonlinear autoregressive modelling with exogenous input (mNARX), involves\nconstructing a problem-specific exogenous input manifold that is optimal for\nconstructing autoregressive surrogates. The manifold, which forms the core of\nmNARX, is constructed incrementally by incorporating the physics of the system,\nas well as prior expert- and domain- knowledge. Because mNARX decomposes the\nfull problem into a series of smaller sub-problems, each with a lower\ncomplexity than the original, it scales well with the complexity of the\nproblem, both in terms of training and evaluation costs of the final surrogate.\nFurthermore, mNARX synergizes well with traditional dimensionality reduction\ntechniques, making it highly suitable for modelling dynamical systems with\nhigh-dimensional exogenous inputs, a class of problems that is typically\nchallenging to solve. Since domain knowledge is particularly abundant in\nphysical systems, such as those found in civil and mechanical engineering,\nmNARX is well suited for these applications. We demonstrate that mNARX\noutperforms traditional autoregressive surrogates in predicting the response of\na classical coupled spring-mass system excited by a one-dimensional random\nexcitation. Additionally, we show that mNARX is well suited for emulating very\nhigh-dimensional time- and state-dependent systems, even when affected by\nactive controllers, by surrogating the dynamics of a realistic\naero-servo-elastic onshore wind turbine simulator. In general, our results\ndemonstrate that mNARX offers promising prospects for modelling complex\ndynamical systems, in terms of accuracy and efficiency.\n","authors":["Styfen Schr","Stefano Marelli","Bruno Sudret"],"pdf_url":"https://arxiv.org/pdf/2306.16335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08367v1","updated":"2023-10-12T14:38:25Z","published":"2023-10-12T14:38:25Z","title":"MCU: A Task-centric Framework for Open-ended Agent Evaluation in\n  Minecraft","summary":"  To pursue the goal of creating an open-ended agent in Minecraft, an\nopen-ended game environment with unlimited possibilities, this paper introduces\na task-centric framework named MCU for Minecraft agent evaluation. The MCU\nframework leverages the concept of atom tasks as fundamental building blocks,\nenabling the generation of diverse or even arbitrary tasks. Within the MCU\nframework, each task is measured with six distinct difficulty scores (time\nconsumption, operational effort, planning complexity, intricacy, creativity,\nnovelty). These scores offer a multi-dimensional assessment of a task from\ndifferent angles, and thus can reveal an agent's capability on specific facets.\nThe difficulty scores also serve as the feature of each task, which creates a\nmeaningful task space and unveils the relationship between tasks. For efficient\nevaluation of Minecraft agents employing the MCU framework, we maintain a\nunified benchmark, namely SkillForge, which comprises representative tasks with\ndiverse categories and difficulty distribution. We also provide convenient\nfilters for users to select tasks to assess specific capabilities of agents. We\nshow that MCU has the high expressivity to cover all tasks used in recent\nliterature on Minecraft agent, and underscores the need for advancements in\nareas such as creativity, precise control, and out-of-distribution\ngeneralization under the goal of open-ended Minecraft agent development.\n","authors":["Haowei Lin","Zihao Wang","Jianzhu Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08358v1","updated":"2023-10-12T14:29:02Z","published":"2023-10-12T14:29:02Z","title":"Towards Demystifying the Generalization Behaviors When Neural Collapse\n  Emerges","summary":"  Neural Collapse (NC) is a well-known phenomenon of deep neural networks in\nthe terminal phase of training (TPT). It is characterized by the collapse of\nfeatures and classifier into a symmetrical structure, known as simplex\nequiangular tight frame (ETF). While there have been extensive studies on\noptimization characteristics showing the global optimality of neural collapse,\nlittle research has been done on the generalization behaviors during the\noccurrence of NC. Particularly, the important phenomenon of generalization\nimprovement during TPT has been remaining in an empirical observation and\nlacking rigorous theoretical explanation. In this paper, we establish the\nconnection between the minimization of CE and a multi-class SVM during TPT, and\nthen derive a multi-class margin generalization bound, which provides a\ntheoretical explanation for why continuing training can still lead to accuracy\nimprovement on test set, even after the train accuracy has reached 100%.\nAdditionally, our further theoretical results indicate that different alignment\nbetween labels and features in a simplex ETF can result in varying degrees of\ngeneralization improvement, despite all models reaching NC and demonstrating\nsimilar optimization performance on train set. We refer to this newly\ndiscovered property as \"non-conservative generalization\". In experiments, we\nalso provide empirical observations to verify the indications suggested by our\ntheoretical results.\n","authors":["Peifeng Gao","Qianqian Xu","Yibo Yang","Peisong Wen","Huiyang Shao","Zhiyong Yang","Bernard Ghanem","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2310.08358v1.pdf","comment":"20 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2304.08914"},{"id":"http://arxiv.org/abs/2310.06970v2","updated":"2023-10-12T14:21:52Z","published":"2023-10-10T19:47:58Z","title":"Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing","summary":"  Graph Neural Networks are a natural fit for learning algorithms. They can\ndirectly represent tasks through an abstract but versatile graph structure and\nhandle inputs of different sizes. This opens up the possibility for scaling and\nextrapolation to larger graphs, one of the most important advantages of an\nalgorithm. However, this raises two core questions i) How can we enable nodes\nto gather the required information in a given graph ($\\textit{information\nexchange}$), even if is far away and ii) How can we design an execution\nframework which enables this information exchange for extrapolation to larger\ngraph sizes ($\\textit{algorithmic alignment for extrapolation}$). We propose a\nnew execution framework that is inspired by the design principles of\ndistributed algorithms: Flood and Echo Net. It propagates messages through the\nentire graph in a wave like activation pattern, which naturally generalizes to\nlarger instances. Through its sparse but parallel activations it is provably\nmore efficient in terms of message complexity. We study the proposed model and\nprovide both empirical evidence and theoretical insights in terms of its\nexpressiveness, efficiency, information exchange and ability to extrapolate.\n","authors":["Jol Mathys","Florian Grtschla","Kalyan Varma Nadimpalli","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2310.06970v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2310.08348v1","updated":"2023-10-12T14:18:09Z","published":"2023-10-12T14:18:09Z","title":"LightZero: A Unified Benchmark for Monte Carlo Tree Search in General\n  Sequential Decision Scenarios","summary":"  Building agents based on tree-search planning capabilities with learned\nmodels has achieved remarkable success in classic decision-making problems,\nsuch as Go and Atari. However, it has been deemed challenging or even\ninfeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse\nreal-world applications, especially when these environments involve complex\naction spaces and significant simulation costs, or inherent stochasticity. In\nthis work, we introduce LightZero, the first unified benchmark for deploying\nMCTS/MuZero in general sequential decision scenarios. Specificially, we\nsummarize the most critical challenges in designing a general MCTS-style\ndecision-making solver, then decompose the tightly-coupled algorithm and system\ndesign of tree-search RL methods into distinct sub-modules. By incorporating\nmore appropriate exploration and optimization strategies, we can significantly\nenhance these sub-modules and construct powerful LightZero agents to tackle\ntasks across a wide range of domains, such as board games, Atari, MuJoCo,\nMiniGrid and GoBigger. Detailed benchmark results reveal the significant\npotential of such methods in building scalable and efficient decision\nintelligence. The code is available as part of OpenDILab at\nhttps://github.com/opendilab/LightZero.\n","authors":["Yazhe Niu","Yuan Pu","Zhenjie Yang","Xueyan Li","Tong Zhou","Jiyuan Ren","Shuai Hu","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08348v1.pdf","comment":"NeurIPS 2023 Spotlight"},{"id":"http://arxiv.org/abs/2306.14041v2","updated":"2023-10-12T14:16:15Z","published":"2023-06-24T19:22:01Z","title":"Smoothed $f$-Divergence Distributionally Robust Optimization","summary":"  In data-driven optimization, sample average approximation (SAA) is known to\nsuffer from the so-called optimizer's curse that causes an over-optimistic\nevaluation of the solution performance. We argue that a special type of\ndistributionallly robust optimization (DRO) formulation offers theoretical\nadvantages in correcting for this optimizer's curse compared to simple\n``margin'' adjustments to SAA and other DRO approaches: It attains a\nstatistical bound on the out-of-sample performance, for a wide class of\nobjective functions and distributions, that is nearly tightest in terms of\nexponential decay rate. This DRO uses an ambiguity set based on a Kullback\nLeibler (KL) divergence smoothed by the Wasserstein or L\\'evy-Prokhorov (LP)\ndistance via a suitable distance optimization. Computationally, we also show\nthat such a DRO, and its generalized versions using smoothed $f$-divergence,\nare not harder than DRO problems based on $f$-divergence or Wasserstein\ndistances, rendering our DRO formulations both statistically optimal and\ncomputationally viable.\n","authors":["Zhenyuan Liu","Bart P. G. Van Parys","Henry Lam"],"pdf_url":"https://arxiv.org/pdf/2306.14041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07579v2","updated":"2023-10-12T14:15:24Z","published":"2023-10-11T15:19:31Z","title":"In-Context Unlearning: Language Models as Few Shot Unlearners","summary":"  Machine unlearning, the study of efficiently removing the impact of specific\ntraining points on the trained model, has garnered increased attention of late,\ndriven by the need to comply with privacy regulations like the Right to be\nForgotten. Although unlearning is particularly relevant for LLMs in light of\nthe copyright issues they raise, achieving precise unlearning is\ncomputationally infeasible for very large models. To this end, recent work has\nproposed several algorithms which approximate the removal of training data\nwithout retraining the model. These algorithms crucially rely on access to the\nmodel parameters in order to update them, an assumption that may not hold in\npractice due to computational constraints or when the LLM is accessed via API.\nIn this work, we propose a new class of unlearning methods for LLMs we call\n''In-Context Unlearning'', providing inputs in context and without having to\nupdate model parameters. To unlearn a particular training instance, we provide\nthe instance alongside a flipped label and additional correctly labelled\ninstances which are prepended as inputs to the LLM at inference time. Our\nexperimental results demonstrate that these contexts effectively remove\nspecific information from the training set while maintaining performance levels\nthat are competitive with (or in some cases exceed) state-of-the-art unlearning\nmethods that require access to the LLM parameters.\n","authors":["Martin Pawelczyk","Seth Neel","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2310.07579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00152v4","updated":"2023-10-12T14:05:03Z","published":"2023-04-29T02:27:42Z","title":"Limits of Model Selection under Transfer Learning","summary":"  Theoretical studies on transfer learning or domain adaptation have so far\nfocused on situations with a known hypothesis class or model; however in\npractice, some amount of model selection is usually involved, often appearing\nunder the umbrella term of hyperparameter-tuning: for example, one may think of\nthe problem of tuning for the right neural network architecture towards a\ntarget task, while leveraging data from a related source task.\n  Now, in addition to the usual tradeoffs on approximation vs estimation errors\ninvolved in model selection, this problem brings in a new complexity term,\nnamely, the transfer distance between source and target distributions, which is\nknown to vary with the choice of hypothesis class.\n  We present a first study of this problem, focusing on classification; in\nparticular, the analysis reveals some remarkable phenomena: adaptive rates,\ni.e., those achievable with no distributional information, can be arbitrarily\nslower than oracle rates, i.e., when given knowledge on distances.\n","authors":["Steve Hanneke","Samory Kpotufe","Yasaman Mahdaviyeh"],"pdf_url":"https://arxiv.org/pdf/2305.00152v4.pdf","comment":"Accepted for presentation at the Conference on Learning Theory (COLT)\n  2023"},{"id":"http://arxiv.org/abs/2310.08339v1","updated":"2023-10-12T13:57:32Z","published":"2023-10-12T13:57:32Z","title":"A Generic Software Framework for Distributed Topological Analysis\n  Pipelines","summary":"  This system paper presents a software framework for the support of\ntopological analysis pipelines in a distributed-memory model. While several\nrecent papers introduced topology-based approaches for distributed-memory\nenvironments, these were reporting experiments obtained with tailored,\nmono-algorithm implementations. In contrast, we describe in this paper a\ngeneral-purpose, generic framework for topological analysis pipelines, i.e. a\nsequence of topological algorithms interacting together, possibly on distinct\nnumbers of processes. Specifically, we instantiated our framework with the MPI\nmodel, within the Topology ToolKit (TTK). While developing this framework, we\nfaced several algorithmic and software engineering challenges, which we\ndocument in this paper. We provide a taxonomy for the distributed-memory\ntopological algorithms supported by TTK, depending on their communication needs\nand provide examples of hybrid MPI+thread parallelizations. Detailed\nperformance analyses show that parallel efficiencies range from $20\\%$ to\n$80\\%$ (depending on the algorithms), and that the MPI-specific preconditioning\nintroduced by our framework induces a negligible computation time overhead. We\nillustrate the new distributed-memory capabilities of TTK with an example of\nadvanced analysis pipeline, combining multiple algorithms, run on the largest\npublicly available dataset we have found (120 billion vertices) on a standard\ncluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a\nroadmap for the completion of TTK's MPI extension, along with generic\nrecommendations for each algorithm communication category.\n","authors":["Eve Le Guillou","Michael Will","Pierre Guillou","Jonas Lukasczyk","Pierre Fortin","Christoph Garth","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2310.08339v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.08337v1","updated":"2023-10-12T13:54:55Z","published":"2023-10-12T13:54:55Z","title":"Neural Diffusion Models","summary":"  Diffusion models have shown remarkable performance on many generative tasks.\nDespite recent success, most diffusion models are restricted in that they only\nallow linear transformation of the data distribution. In contrast, broader\nfamily of transformations can potentially help train generative distributions\nmore efficiently, simplifying the reverse process and closing the gap between\nthe true negative log-likelihood and the variational approximation. In this\npaper, we present Neural Diffusion Models (NDMs), a generalization of\nconventional diffusion models that enables defining and learning time-dependent\nnon-linear transformations of data. We show how to optimise NDMs using a\nvariational bound in a simulation-free setting. Moreover, we derive a\ntime-continuous formulation of NDMs, which allows fast and reliable inference\nusing off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the\nutility of NDMs with learnable transformations through experiments on standard\nimage generation benchmarks, including CIFAR-10, downsampled versions of\nImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms\nof likelihood and produce high-quality samples.\n","authors":["Grigory Bartosh","Dmitry Vetrov","Christian A. Naesseth"],"pdf_url":"https://arxiv.org/pdf/2310.08337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08331v1","updated":"2023-10-12T13:45:33Z","published":"2023-10-12T13:45:33Z","title":"Impact of multi-armed bandit strategies on deep recurrent reinforcement\n  learning","summary":"  Incomplete knowledge of the environment leads an agent to make decisions\nunder uncertainty. One of the major dilemmas in Reinforcement Learning (RL)\nwhere an autonomous agent has to balance two contrasting needs in making its\ndecisions is: exploiting the current knowledge of the environment to maximize\nthe cumulative reward as well as exploring actions that allow improving the\nknowledge of the environment, hopefully leading to higher reward values\n(exploration-exploitation trade-off). Concurrently, another relevant issue\nregards the full observability of the states, which may not be assumed in all\napplications. Such as when only 2D images are considered as input in a RL\napproach used for finding the optimal action within a 3D simulation\nenvironment. In this work, we address these issues by deploying and testing\nseveral techniques to balance exploration and exploitation trade-off on\npartially observable systems for predicting steering wheels in autonomous\ndriving scenario. More precisely, the final aim is to investigate the effects\nof using both stochastic and deterministic multi-armed bandit strategies\ncoupled with a Deep Recurrent Q-Network. Additionally, we adapted and evaluated\nthe impact of an innovative method to improve the learning phase of the\nunderlying Convolutional Recurrent Neural Network. We aim to show that adaptive\nstochastic methods for exploration better approximate the trade-off between\nexploration and exploitation as, in general, Softmax and Max-Boltzmann\nstrategies are able to outperform epsilon-greedy techniques.\n","authors":["Valentina Zangirolami","Matteo Borrotti"],"pdf_url":"https://arxiv.org/pdf/2310.08331v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2309.03004v2","updated":"2023-10-12T13:36:41Z","published":"2023-09-06T13:48:40Z","title":"A Theoretical Explanation of Activation Sparsity through Flat Minima and\n  Adversarial Robustness","summary":"  A recent empirical observation (Li et al., 2022b) of activation sparsity in\nMLP blocks offers an opportunity to drastically reduce computation costs for\nfree. Although having attributed it to training dynamics, existing theoretical\nexplanations of activation sparsity are restricted to shallow networks, small\ntraining steps and special training, despite its emergence in deep models\nstandardly trained for a large number of steps. To fill these gaps, we propose\nthe notion of gradient sparsity as one source of activation sparsity and a\ntheoretical explanation based on it that sees sparsity a necessary step to\nadversarial robustness w.r.t. hidden features and parameters, which is\napproximately the flatness of minima for well-learned models. The theory\napplies to standardly trained LayerNorm-ed MLPs, and further to Transformers or\nother architectures trained with weight noises. Eliminating other sources of\nflatness except for sparsity, we discover the phenomenon that the ratio between\nthe largest and smallest non-zero singular values of weight matrices is small.\nWhen discussing the emergence of this spectral concentration, we use random\nmatrix theory (RMT) as a powerful tool to analyze stochastic gradient noises.\nValidational experiments are conducted to verify our gradient-sparsity-based\nexplanation. We propose two plug-and-play modules for both training and\nfinetuning for sparsity. Experiments on ImageNet-1k and C4 demonstrate their\n50% sparsity improvements, indicating further potential cost reduction in both\ntraining and inference.\n","authors":["Ze Peng","Lei Qi","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2309.03004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v1","updated":"2023-10-12T13:33:04Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08312v1","updated":"2023-10-12T13:20:17Z","published":"2023-10-12T13:20:17Z","title":"GePSAn: Generative Procedure Step Anticipation in Cooking Videos","summary":"  We study the problem of future step anticipation in procedural videos. Given\na video of an ongoing procedural activity, we predict a plausible next\nprocedure step described in rich natural language. While most previous work\nfocus on the problem of data scarcity in procedural video datasets, another\ncore challenge of future anticipation is how to account for multiple plausible\nfuture realizations in natural settings. This problem has been largely\noverlooked in previous work. To address this challenge, we frame future step\nprediction as modelling the distribution of all possible candidates for the\nnext step. Specifically, we design a generative model that takes a series of\nvideo clips as input, and generates multiple plausible and diverse candidates\n(in natural language) for the next step. Following previous work, we side-step\nthe video annotation scarcity by pretraining our model on a large text-based\ncorpus of procedural activities, and then transfer the model to the video\ndomain. Our experiments, both in textual and video domains, show that our model\ncaptures diversity in the next step prediction and generates multiple plausible\nfuture predictions. Moreover, our model establishes new state-of-the-art\nresults on YouCookII, where it outperforms existing baselines on the next step\nanticipation. Finally, we also show that our model can successfully transfer\nfrom text to the video domain zero-shot, ie, without fine-tuning or adaptation,\nand produces good-quality future step predictions from video.\n","authors":["Mohamed Ashraf Abdelsalam","Samrudhdhi B. Rangrej","Isma Hadji","Nikita Dvornik","Konstantinos G. Derpanis","Afsaneh Fazly"],"pdf_url":"https://arxiv.org/pdf/2310.08312v1.pdf","comment":"published at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.08304v1","updated":"2023-10-12T13:11:38Z","published":"2023-10-12T13:11:38Z","title":"CHIP: Contrastive Hierarchical Image Pretraining","summary":"  Few-shot object classification is the task of classifying objects in an image\nwith limited number of examples as supervision. We propose a one-shot/few-shot\nclassification model that can classify an object of any unseen class into a\nrelatively general category in an hierarchically based classification. Our\nmodel uses a three-level hierarchical contrastive loss based ResNet152\nclassifier for classifying an object based on its features extracted from Image\nembedding, not used during the training phase. For our experimentation, we have\nused a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal\nclasses for training our model and created our own dataset of unseen classes\nfor evaluating our trained model. Our model provides satisfactory results in\nclassifying the unknown objects into a generic category which has been later\ndiscussed in greater detail.\n","authors":["Arpit Mittal","Harshil Jhaveri","Swapnil Mallick","Abhishek Ajmera"],"pdf_url":"https://arxiv.org/pdf/2310.08304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07189v3","updated":"2023-10-12T12:47:22Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  We aim to optimize the binary detection of Chronic Obstructive Pulmonary\nDisease (COPD) based on emphysema presence in the lung with convolutional\nneural networks (CNN) by exploring manually adjusted versus automated\nwindow-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT\nimages (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with\nCOPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and\npreprocessed. For each image, intensity values were manually clipped to the\nemphysema window setting and a baseline 'full-range' window setting.\nClass-balanced train, validation, and test sets contained 3,392, 1,114, and\n2,688 images. The network backbone was optimized by comparing various CNN\narchitectures. Furthermore, automated WSO was implemented by adding a\ncustomized layer to the model. The image-level area under the Receiver\nOperating Characteristics curve (AUC) [lower, upper limit 95% confidence] was\nutilized to compare model variations. Repeated inference (n=7) on the test set\nshowed that the DenseNet was the most efficient backbone and achieved a mean\nAUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually\nadjusted to the emphysema window, the DenseNet model predicted COPD with a mean\nAUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an\noptimal window in the proximity of the emphysema window setting was learned\nautomatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of\nCOPD with DenseNet models was improved by WSO of CT data to the emphysema\nwindow setting range.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Johannes Thalhammer","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08287v1","updated":"2023-10-12T12:45:13Z","published":"2023-10-12T12:45:13Z","title":"A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors","summary":"  The distribution of the weights of modern deep neural networks (DNNs) -\ncrucial for uncertainty quantification and robustness - is an eminently complex\nobject due to its extremely high dimensionality. This paper proposes one of the\nfirst large-scale explorations of the posterior distribution of deep Bayesian\nNeural Networks (BNNs), expanding its study to real-world vision tasks and\narchitectures. Specifically, we investigate the optimal approach for\napproximating the posterior, analyze the connection between posterior quality\nand uncertainty quantification, delve into the impact of modes on the\nposterior, and explore methods for visualizing the posterior. Moreover, we\nuncover weight-space symmetries as a critical aspect for understanding the\nposterior. To this extent, we develop an in-depth assessment of the impact of\nboth permutation and scaling symmetries that tend to obfuscate the Bayesian\nposterior. While the first type of transformation is known for duplicating\nmodes, we explore the relationship between the latter and L2 regularization,\nchallenging previous misconceptions. Finally, to help the community improve our\nunderstanding of the Bayesian posterior, we will shortly release the first\nlarge-scale checkpoint dataset, including thousands of real-world models and\nour codes.\n","authors":["Olivier Laurent","Emanuel Aldea","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.08287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08282v1","updated":"2023-10-12T12:39:08Z","published":"2023-10-12T12:39:08Z","title":"Data driven modeling of self-similar dynamics","summary":"  Multiscale modeling of complex systems is crucial for understanding their\nintricacies. Data-driven multiscale modeling has emerged as a promising\napproach to tackle challenges associated with complex systems. On the other\nhand, self-similarity is prevalent in complex systems, hinting that large-scale\ncomplex systems can be modeled at a reduced cost. In this paper, we introduce a\nmultiscale neural network framework that incorporates self-similarity as prior\nknowledge, facilitating the modeling of self-similar dynamical systems. For\ndeterministic dynamics, our framework can discern whether the dynamics are\nself-similar. For uncertain dynamics, it can compare and determine which\nparameter set is closer to self-similarity. The framework allows us to extract\nscale-invariant kernels from the dynamics for modeling at any scale. Moreover,\nour method can identify the power law exponents in self-similar systems.\nPreliminary tests on the Ising model yielded critical exponents consistent with\ntheoretical expectations, providing valuable insights for addressing critical\nphase transitions in non-equilibrium systems.\n","authors":["Ruyi Tao","Ningning Tao","Yizhuang You","Jiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08282v1.pdf","comment":"10 pages,4 figures,1 table"},{"id":"http://arxiv.org/abs/2310.08278v1","updated":"2023-10-12T12:29:32Z","published":"2023-10-12T12:29:32Z","title":"Lag-Llama: Towards Foundation Models for Time Series Forecasting","summary":"  Aiming to build foundation models for time-series forecasting and study their\nscaling behavior, we present here our work-in-progress on Lag-Llama, a\ngeneral-purpose univariate probabilistic time-series forecasting model trained\non a large collection of time-series data. The model shows good zero-shot\nprediction capabilities on unseen \"out-of-distribution\" time-series datasets,\noutperforming supervised baselines. We use smoothly broken power-laws to fit\nand predict model scaling behavior. The open source code is made available at\nhttps://github.com/kashif/pytorch-transformer-ts.\n","authors":["Kashif Rasul","Arjun Ashok","Andrew Robert Williams","Arian Khorasani","George Adamopoulos","Rishika Bhagwatkar","Marin Bilo","Hena Ghonia","Nadhir Vincent Hassen","Anderson Schneider","Sahil Garg","Alexandre Drouin","Nicolas Chapados","Yuriy Nevmyvaka","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2310.08278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15394v2","updated":"2023-10-12T12:11:23Z","published":"2023-05-24T17:56:18Z","title":"Differentially-Private Decision Trees and Provable Robustness to Data\n  Poisoning","summary":"  Decision trees are interpretable models that are well-suited to non-linear\nlearning problems. Much work has been done on extending decision tree learning\nalgorithms with differential privacy, a system that guarantees the privacy of\nsamples within the training data. However, current state-of-the-art algorithms\nfor this purpose sacrifice much utility for a small privacy benefit. These\nsolutions create random decision nodes that reduce decision tree accuracy or\nspend an excessive share of the privacy budget on labeling leaves. Moreover,\nmany works do not support continuous features or leak information about them.\nWe propose a new method called PrivaTree based on private histograms that\nchooses good splits while consuming a small privacy budget. The resulting trees\nprovide a significantly better privacy-utility trade-off and accept mixed\nnumerical and categorical data without leaking information about numerical\nfeatures. Finally, while it is notoriously hard to give robustness guarantees\nagainst data poisoning attacks, we demonstrate bounds for the expected accuracy\nand success rates of backdoor attacks against differentially-private learners.\nBy leveraging the better privacy-utility trade-off of PrivaTree we are able to\ntrain decision trees with significantly better robustness against backdoor\nattacks compared to regular decision trees and with meaningful theoretical\nguarantees.\n","authors":["Danil Vos","Jelle Vos","Tianyu Li","Zekeriya Erkin","Sicco Verwer"],"pdf_url":"https://arxiv.org/pdf/2305.15394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08259v1","updated":"2023-10-12T12:05:51Z","published":"2023-10-12T12:05:51Z","title":"Invisible Threats: Backdoor Attack in OCR Systems","summary":"  Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.\n","authors":["Mauro Conti","Nicola Farronato","Stefanos Koffas","Luca Pajola","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2310.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19443v2","updated":"2023-10-12T12:02:34Z","published":"2023-05-30T22:34:48Z","title":"OWAdapt: An adaptive loss function for deep learning using OWA operators","summary":"  In this paper, we propose a fuzzy adaptive loss function for enhancing deep\nlearning performance in classification tasks. Specifically, we redefine the\ncross-entropy loss to effectively address class-level noise conditions,\nincluding the challenging problem of class imbalance. Our approach introduces\naggregation operators, leveraging the power of fuzzy logic to improve\nclassification accuracy. The rationale behind our proposed method lies in the\niterative up-weighting of class-level components within the loss function,\nfocusing on those with larger errors. To achieve this, we employ the ordered\nweighted average (OWA) operator and combine it with an adaptive scheme for\ngradient-based learning. Through extensive experimentation, our method\noutperforms other commonly used loss functions, such as the standard\ncross-entropy or focal loss, across various binary and multiclass\nclassification tasks. Furthermore, we explore the influence of hyperparameters\nassociated with the OWA operators and present a default configuration that\nperforms well across different experimental settings.\n","authors":["Sebastin Maldonado","Carla Vairetti","Katherine Jara","Miguel Carrasco","Julio Lpez"],"pdf_url":"https://arxiv.org/pdf/2305.19443v2.pdf","comment":"15 pages, 1 figure, published"},{"id":"http://arxiv.org/abs/2310.08256v1","updated":"2023-10-12T12:01:32Z","published":"2023-10-12T12:01:32Z","title":"Impact of Co-occurrence on Factual Knowledge of Large Language Models","summary":"  Large language models (LLMs) often make factually incorrect responses despite\ntheir success in various applications. In this paper, we hypothesize that\nrelying heavily on simple co-occurrence statistics of the pre-training corpora\nis one of the main factors that cause factual errors. Our results reveal that\nLLMs are vulnerable to the co-occurrence bias, defined as preferring frequently\nco-occurred words over the correct answer. Consequently, LLMs struggle to\nrecall facts whose subject and object rarely co-occur in the pre-training\ndataset although they are seen during finetuning. We show that co-occurrence\nbias remains despite scaling up model sizes or finetuning. Therefore, we\nsuggest finetuning on a debiased dataset to mitigate the bias by filtering out\nbiased samples whose subject-object co-occurrence count is high. Although\ndebiased finetuning allows LLMs to memorize rare facts in the training set, it\nis not effective in recalling rare facts unseen during finetuning. Further\nresearch in mitigation will help build reliable language models by preventing\npotential errors. The code is available at\n\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.\n","authors":["Cheongwoong Kang","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2310.08256v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2304.12233v3","updated":"2023-10-12T12:00:09Z","published":"2023-04-20T10:45:57Z","title":"Diffusion-based Generative AI for Exploring Transition States from 2D\n  Molecular Graphs","summary":"  The exploration of transition state (TS) geometries is crucial for\nelucidating chemical reaction mechanisms and modeling their kinetics. Recently,\nmachine learning (ML) models have shown remarkable performance for prediction\nof TS geometries. However, they require 3D conformations of reactants and\nproducts often with their appropriate orientations as input, which demands\nsubstantial efforts and computational cost. Here, we propose a generative\napproach based on the stochastic diffusion method, namely TSDiff, for\nprediction of TS geometries just from 2D molecular graphs. TSDiff outperformed\nthe existing ML models with 3D geometries in terms of both accuracy and\nefficiency. Moreover, it enables to sample various TS conformations, because it\nlearned the distribution of TS geometries for diverse reactions in training.\nThus, TSDiff was able to find more favorable reaction pathways with lower\nbarrier heights than those in the reference database. These results demonstrate\nthat TSDiff shows promising potential for an efficient and reliable TS\nexploration.\n","authors":["Seonghwan Kim","Jeheon Woo","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2304.12233v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00327v2","updated":"2023-10-12T11:55:28Z","published":"2023-09-30T10:06:05Z","title":"Memorization with neural nets: going beyond the worst case","summary":"  In practice, deep neural networks are often able to easily interpolate their\ntraining data. To understand this phenomenon, many works have aimed to quantify\nthe memorization capacity of a neural network architecture: the largest number\nof points such that the architecture can interpolate any placement of these\npoints with any assignment of labels. For real-world data, however, one\nintuitively expects the presence of a benign structure so that interpolation\nalready occurs at a smaller network size than suggested by memorization\ncapacity. In this paper, we investigate interpolation by adopting an\ninstance-specific viewpoint. We introduce a simple randomized algorithm that,\ngiven a fixed finite dataset with two classes, with high probability constructs\nan interpolating three-layer neural network in polynomial time. The required\nnumber of parameters is linked to geometric properties of the two classes and\ntheir mutual arrangement. As a result, we obtain guarantees that are\nindependent of the number of samples and hence move beyond worst-case\nmemorization capacity bounds. We illustrate the effectiveness of the algorithm\nin non-pathological situations with extensive numerical experiments and link\nthe insights back to the theoretical results.\n","authors":["Sjoerd Dirksen","Patrick Finke","Martin Genzel"],"pdf_url":"https://arxiv.org/pdf/2310.00327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08252v1","updated":"2023-10-12T11:55:17Z","published":"2023-10-12T11:55:17Z","title":"MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with\n  Reinforcement Learning","summary":"  Recently, Meta-Black-Box Optimization with Reinforcement Learning\n(MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to\nmitigate manual fine-tuning of low-level black-box optimizers. However, this\nfield is hindered by the lack of a unified benchmark. To fill this gap, we\nintroduce MetaBox, the first benchmark platform expressly tailored for\ndeveloping and evaluating MetaBBO-RL methods. MetaBox offers a flexible\nalgorithmic template that allows users to effortlessly implement their unique\ndesigns within the platform. Moreover, it provides a broad spectrum of over 300\nproblem instances, collected from synthetic to realistic scenarios, and an\nextensive library of 19 baseline methods, including both traditional black-box\noptimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three\nstandardized performance metrics, enabling a more thorough assessment of the\nmethods. In a bid to illustrate the utility of MetaBox for facilitating\nrigorous evaluation and in-depth analysis, we carry out a wide-ranging\nbenchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source\nand accessible at: https://github.com/GMC-DRL/MetaBox.\n","authors":["Zeyuan Ma","Hongshu Guo","Jiacheng Chen","Zhenrui Li","Guojun Peng","Yue-Jiao Gong","Yining Ma","Zhiguang Cao"],"pdf_url":"https://arxiv.org/pdf/2310.08252v1.pdf","comment":"Accepted at NuerIPS 2023"},{"id":"http://arxiv.org/abs/2110.03469v3","updated":"2023-10-12T11:53:59Z","published":"2021-10-07T13:49:23Z","title":"Federated Learning from Small Datasets","summary":"  Federated learning allows multiple parties to collaboratively train a joint\nmodel without sharing local data. This enables applications of machine learning\nin settings of inherently distributed, undisclosable data such as in the\nmedical domain. In practice, joint training is usually achieved by aggregating\nlocal models, for which local training objectives have to be in expectation\nsimilar to the joint (global) objective. Often, however, local datasets are so\nsmall that local objectives differ greatly from the global objective, resulting\nin federated learning to fail. We propose a novel approach that intertwines\nmodel aggregations with permutations of local models. The permutations expose\neach local model to a daisy chain of local datasets resulting in more efficient\ntraining in data-sparse domains. This enables training on extremely small local\ndatasets, such as patient data across hospitals, while retaining the training\nefficiency and privacy benefits of federated learning.\n","authors":["Michael Kamp","Jonas Fischer","Jilles Vreeken"],"pdf_url":"https://arxiv.org/pdf/2110.03469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04263v3","updated":"2023-10-12T11:42:59Z","published":"2023-08-08T13:59:56Z","title":"BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning","summary":"  This paper introduces BarlowRL, a data-efficient reinforcement learning agent\nthat combines the Barlow Twins self-supervised learning framework with DER\n(Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its\ncontrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids\ndimensional collapse by enforcing information spread to the whole space. This\nhelps RL algorithms to utilize uniformly spread state representation that\neventually results in a remarkable performance. The integration of Barlow Twins\nwith DER enhances data efficiency and achieves superior performance in the RL\ntasks. BarlowRL demonstrates the potential of incorporating self-supervised\nlearning techniques to improve RL algorithms.\n","authors":["Omer Veysel Cagatan","Baris Akgun"],"pdf_url":"https://arxiv.org/pdf/2308.04263v3.pdf","comment":"ACML 2023, Camera-Ready Version"},{"id":"http://arxiv.org/abs/2304.00457v3","updated":"2023-10-12T11:35:35Z","published":"2023-04-02T05:47:09Z","title":"LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the dataset, which often covers an\nentire field. This field-based evaluation, is problematic with respect to\ntransparency and model improvement. A stratified evaluation could instead\nreveal subfields, where hallucinations are more likely to occur and thus help\nto better assess LLMs' risks and guide their further development. To support\nsuch stratified evaluations, we propose LLMMaps as a novel visualization\ntechnique that enables users to evaluate LLMs' performance with respect to Q&A\ndatasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities\nin different subfields, by transforming Q&A datasets as well as LLM responses\ninto an internal knowledge structure. An extension for comparative\nvisualization furthermore, allows for the detailed comparison of multiple LLMs.\nTo assess LLMMaps we use them to conduct a comparative analysis of several\nstate-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as\nwell as two qualitative user evaluations. All necessary source code and data\nfor generating LLMMaps to be used in scientific publications and elsewhere is\navailable on GitHub: https://github.com/viscom-ulm/LLMMaps\n","authors":["Patrik Puchert","Poonam Poonam","Christian van Onzenoodt","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2304.00457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08237v1","updated":"2023-10-12T11:33:15Z","published":"2023-10-12T11:33:15Z","title":"Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift","summary":"  Covariate shift occurs prevalently in practice, where the input distributions\nof the source and target data are substantially different. Despite its\npractical importance in various learning problems, most of the existing methods\nonly focus on some specific learning tasks and are not well validated\ntheoretically and numerically. To tackle this problem, we propose a unified\nanalysis of general nonparametric methods in a reproducing kernel Hilbert space\n(RKHS) under covariate shift. Our theoretical results are established for a\ngeneral loss belonging to a rich loss function family, which includes many\ncommonly used methods as special cases, such as mean regression, quantile\nregression, likelihood-based classification, and margin-based classification.\nTwo types of covariate shift problems are the focus of this paper and the sharp\nconvergence rates are established for a general loss function to provide a\nunified theoretical analysis, which concurs with the optimal results in\nliterature where the squared loss is used. Extensive numerical studies on\nsynthetic and real examples confirm our theoretical findings and further\nillustrate the effectiveness of our proposed method.\n","authors":["Xingdong Feng","Xin He","Caixing Wang","Chao Wang","Jingnan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08237v1.pdf","comment":"Poster to appear in Thirty-seventh Conference on Neural Information\n  Processing Systems"},{"id":"http://arxiv.org/abs/2310.08235v1","updated":"2023-10-12T11:31:01Z","published":"2023-10-12T11:31:01Z","title":"GROOT: Learning to Follow Instructions by Watching Gameplay Videos","summary":"  We study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. Code and video can be\nfound on the website https://craftjarvis-groot.github.io.\n","authors":["Shaofei Cai","Bowei Zhang","Zihao Wang","Xiaojian Ma","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.14432v4","updated":"2023-10-12T11:25:26Z","published":"2021-07-30T05:33:43Z","title":"Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR\n  Prediction","summary":"  We develop a novel framework that adds the regularizers of the sparse group\nlasso to a family of adaptive optimizers in deep learning, such as Momentum,\nAdagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which\nare named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group\nAdaHessian, etc., accordingly. We establish theoretically proven convergence\nguarantees in the stochastic convex settings, based on primal-dual methods. We\nevaluate the regularized effect of our new optimizers on three large-scale\nreal-world ad click datasets with state-of-the-art deep learning models. The\nexperimental results reveal that compared with the original optimizers with the\npost-processing procedure which uses the magnitude pruning method, the\nperformance of the models can be significantly improved on the same sparsity\nlevel. Furthermore, in comparison to the cases without magnitude pruning, our\nmethods can achieve extremely high sparsity with significantly better or highly\ncompetitive performance. The code is available at\nhttps://github.com/intelligent-machine-learning/dlrover/blob/master/tfplus.\n","authors":["Yun Yue","Yongchao Liu","Suo Tong","Minghao Li","Zhen Zhang","Chunyang Wen","Huanjun Bao","Lihong Gu","Jinjie Gu","Yixiang Mu"],"pdf_url":"https://arxiv.org/pdf/2107.14432v4.pdf","comment":"24 pages. Published as a conference paper at ECML PKDD 2021. This\n  version includes Appendix which was not included in the published version\n  because of page limit"},{"id":"http://arxiv.org/abs/2310.08224v1","updated":"2023-10-12T11:16:57Z","published":"2023-10-12T11:16:57Z","title":"Emergence of Latent Binary Encoding in Deep Neural Network Classifiers","summary":"  We observe the emergence of binary encoding within the latent space of\ndeep-neural-network classifiers. Such binary encoding is induced by introducing\na linear penultimate layer, which is equipped during training with a loss\nfunction that grows as $\\exp(\\vec{x}^2)$, where $\\vec{x}$ are the coordinates\nin the latent space. The phenomenon we describe represents a specific instance\nof a well-documented occurrence known as \\textit{neural collapse}, which arises\nin the terminal phase of training and entails the collapse of latent class\nmeans to the vertices of a simplex equiangular tight frame (ETF). We show that\nbinary encoding accelerates convergence toward the simplex ETF and enhances\nclassification accuracy.\n","authors":["Luigi Sbail","Luca Ghiringhelli"],"pdf_url":"https://arxiv.org/pdf/2310.08224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08221v1","updated":"2023-10-12T11:11:54Z","published":"2023-10-12T11:11:54Z","title":"SimCKP: Simple Contrastive Learning of Keyphrase Representations","summary":"  Keyphrase generation (KG) aims to generate a set of summarizing words or\nphrases given a source document, while keyphrase extraction (KE) aims to\nidentify them from the text. Because the search space is much smaller in KE, it\nis often combined with KG to predict keyphrases that may or may not exist in\nthe corresponding document. However, current unified approaches adopt sequence\nlabeling and maximization-based generation that primarily operate at a token\nlevel, falling short in observing and scoring keyphrases as a whole. In this\nwork, we propose SimCKP, a simple contrastive learning framework that consists\nof two stages: 1) An extractor-generator that extracts keyphrases by learning\ncontext-aware phrase-level representations in a contrastive manner while also\ngenerating keyphrases that do not appear in the document; 2) A reranker that\nadapts scores for each generated phrase by likewise aligning their\nrepresentations with the corresponding document. Experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our proposed\napproach, which outperforms the state-of-the-art models by a significant\nmargin.\n","authors":["Minseok Choi","Chaeheon Gwak","Seho Kim","Si Hyeong Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2310.08221v1.pdf","comment":"Accepted to Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.00848v2","updated":"2023-10-12T11:11:23Z","published":"2023-09-02T07:17:43Z","title":"Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach","summary":"  This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using\nthe YOLOv8 model and innovative post-processing techniques. We tackle\nchallenges unique to the complex Bengali script by employing data augmentation\nfor model robustness. After meticulous validation set evaluation, we fine-tune\nour approach on the complete dataset, leading to a two-stage prediction\nstrategy for accurate element segmentation. Our ensemble model, combined with\npost-processing, outperforms individual base architectures, addressing issues\nidentified in the BaDLAD dataset. By leveraging this approach, we aim to\nadvance Bengali document analysis, contributing to improved OCR and document\ncomprehension and BaDLAD serves as a foundational resource for this endeavor,\naiding future research in the field. Furthermore, our experiments provided key\ninsights to incorporate new strategies into the established solution.\n","authors":["Nazmus Sakib Ahmed","Saad Sakib Noor","Ashraful Islam Shanto Sikder","Abhijit Paul"],"pdf_url":"https://arxiv.org/pdf/2309.00848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08217v1","updated":"2023-10-12T11:05:34Z","published":"2023-10-12T11:05:34Z","title":"TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge\n  Retention and Promotion","summary":"  Continual learning (CL) has remained a persistent challenge for deep neural\nnetworks due to catastrophic forgetting (CF) of previously learned tasks.\nSeveral techniques such as weight regularization, experience rehearsal, and\nparameter isolation have been proposed to alleviate CF. Despite their relative\nsuccess, these research directions have predominantly remained orthogonal and\nsuffer from several shortcomings, while missing out on the advantages of\ncompeting strategies. On the contrary, the brain continually learns,\naccommodates, and transfers knowledge across tasks by simultaneously leveraging\nseveral neurophysiological processes, including neurogenesis, active\nforgetting, neuromodulation, metaplasticity, experience rehearsal, and\ncontext-dependent gating, rarely resulting in CF. Inspired by how the brain\nexploits multiple mechanisms concurrently, we propose TriRE, a novel CL\nparadigm that encompasses retaining the most prominent neurons for each task,\nrevising and solidifying the extracted knowledge of current and past tasks, and\nactively promoting less active neurons for subsequent tasks through rewinding\nand relearning. Across CL settings, TriRE significantly reduces task\ninterference and surpasses different CL approaches considered in isolation.\n","authors":["Preetha Vijayan","Prashant Bhat","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2310.08217v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.08215v1","updated":"2023-10-12T11:04:17Z","published":"2023-10-12T11:04:17Z","title":"Trustworthy Machine Learning","summary":"  As machine learning technology gets applied to actual products and solutions,\nnew challenges have emerged. Models unexpectedly fail to generalize to small\nchanges in the distribution, tend to be confident on novel data they have never\nseen, or cannot communicate the rationale behind their decisions effectively\nwith the end users. Collectively, we face a trustworthiness issue with the\ncurrent machine learning technology. This textbook on Trustworthy Machine\nLearning (TML) covers a theoretical and technical background of four key topics\nin TML: Out-of-Distribution Generalization, Explainability, Uncertainty\nQuantification, and Evaluation of Trustworthiness. We discuss important\nclassical and contemporary research papers of the aforementioned fields and\nuncover and connect their underlying intuitions. The book evolved from the\nhomonymous course at the University of T\\\"ubingen, first offered in the Winter\nSemester of 2022/23. It is meant to be a stand-alone product accompanied by\ncode snippets and various pointers to further sources on topics of TML. The\ndedicated website of the book is https://trustworthyml.io/.\n","authors":["Blint Mucsnyi","Michael Kirchhof","Elisa Nguyen","Alexander Rubinstein","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2310.08215v1.pdf","comment":"373 pages, textbook at the University of T\\\"ubingen"},{"id":"http://arxiv.org/abs/2310.08209v1","updated":"2023-10-12T10:56:25Z","published":"2023-10-12T10:56:25Z","title":"Conformal inference for regression on Riemannian Manifolds","summary":"  Regression on manifolds, and, more broadly, statistics on manifolds, has\ngarnered significant importance in recent years due to the vast number of\napplications for this type of data. Circular data is a classic example, but so\nis data in the space of covariance matrices, data on the Grassmannian manifold\nobtained as a result of principal component analysis, among many others. In\nthis work we investigate prediction sets for regression scenarios when the\nresponse variable, denoted by $Y$, resides in a manifold, and the covariable,\ndenoted by X, lies in Euclidean space. This extends the concepts delineated in\n[Lei and Wasserman, 2014] to this novel context. Aligning with traditional\nprinciples in conformal inference, these prediction sets are distribution-free,\nindicating that no specific assumptions are imposed on the joint distribution\nof $(X, Y)$, and they maintain a non-parametric character. We prove the\nasymptotic almost sure convergence of the empirical version of these regions on\nthe manifold to their population counterparts. The efficiency of this method is\nshown through a comprehensive simulation study and an analysis involving\nreal-world data.\n","authors":["Alejandro Cholaquidis","Fabrice Gamboa","Leonardo Moreno"],"pdf_url":"https://arxiv.org/pdf/2310.08209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08204v1","updated":"2023-10-12T10:50:21Z","published":"2023-10-12T10:50:21Z","title":"Lifelong Audio-video Masked Autoencoder with Forget-robust Localized\n  Alignments","summary":"  We present a lifelong audio-video masked autoencoder that continually learns\nthe multimodal representations from a video stream containing audio-video\npairs, while its distribution continually shifts over time. Specifically, we\npropose two novel ideas to tackle the problem: (1) Localized Alignment: We\nintroduce a small trainable multimodal encoder that predicts the audio and\nvideo tokens that are well-aligned with each other. This allows the model to\nlearn only the highly correlated audiovisual patches with accurate multimodal\nrelationships. (2) Forget-robust multimodal patch selection: We compare the\nrelative importance of each audio-video patch between the current and past data\npair to mitigate unintended drift of the previously learned audio-video\nrepresentations. Our proposed method, FLAVA (Forget-robust Localized\nAudio-Video Alignment), therefore, captures the complex relationships between\nthe audio and video modalities during training on a sequence of pre-training\ntasks while alleviating the forgetting of learned audiovisual correlations. Our\nexperiments validate that FLAVA outperforms the state-of-the-art continual\nlearning methods on several benchmark datasets under continual audio-video\nrepresentation learning scenarios.\n","authors":["Jaewoo Lee","Jaehong Yoon","Wonjae Kim","Yunji Kim","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.08204v1.pdf","comment":"Preprint, project page: https://g-jwlee.github.io/FLAVA/"},{"id":"http://arxiv.org/abs/2310.08198v1","updated":"2023-10-12T10:44:47Z","published":"2023-10-12T10:44:47Z","title":"Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing\n  Experiments in Model Identification of Battery Dynamics","summary":"  Model identification of battery dynamics is a central problem in energy\nresearch; many energy management systems and design processes rely on accurate\nbattery models for efficiency optimization. The standard methodology for\nbattery modelling is traditional design of experiments (DoE), where the battery\ndynamics are excited with many different current profiles and the measured\noutputs are used to estimate the system dynamics. However, although it is\npossible to obtain useful models with the traditional approach, the process is\ntime consuming and expensive because of the need to sweep many different\ncurrent-profile configurations. In the present work, a novel DoE approach is\ndeveloped based on deep reinforcement learning, which alters the configuration\nof the experiments on the fly based on the statistics of past experiments.\nInstead of sticking to a library of predefined current profiles, the proposed\napproach modifies the current profiles dynamically by updating the output space\ncovered by past measurements, hence only the current profiles that are\ninformative for future experiments are applied. Simulations and real\nexperiments are used to show that the proposed approach gives models that are\nas accurate as those obtained with traditional DoE but by using 85\\% less\nresources.\n","authors":["Gokhan Budan","Francesca Damiani","Can Kurtulus","N. Kemal Ure"],"pdf_url":"https://arxiv.org/pdf/2310.08198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09033v2","updated":"2023-10-12T10:34:03Z","published":"2023-03-16T02:07:29Z","title":"Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling","summary":"  Most bandit algorithms assume that the reward variances or their upper bounds\nare known, and that they are the same for all arms. This naturally leads to\nsuboptimal performance and higher regret due to variance overestimation. On the\nother hand, underestimated reward variances may lead to linear regret due to\ncommitting early to a suboptimal arm. This motivated prior works on\nvariance-adaptive frequentist algorithms, which have strong instance-dependent\nregret bounds but cannot incorporate prior knowledge on reward variances. We\nlay foundations for the Bayesian setting, which incorporates prior knowledge.\nThis results in lower regret in practice, due to using the prior in the\nalgorithm design, and also improved regret guarantees. Specifically, we study\nGaussian bandits with {unknown heterogeneous reward variances}, and develop a\nThompson sampling algorithm with prior-dependent Bayes regret bounds. We\nachieve lower regret with lower reward variances and more informative priors on\nthem, which is precisely why we pay only for what is uncertain. This is the\nfirst result of its kind. Finally, we corroborate our theory with extensive\nexperiments, which show the superiority of our variance-adaptive Bayesian\nalgorithm over prior frequentist approaches. We also show that our approach is\nrobust to model misspecification and can be applied with estimated priors.\n","authors":["Aadirupa Saha","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2303.09033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07402v2","updated":"2023-10-12T10:30:35Z","published":"2023-10-11T11:38:18Z","title":"NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series\n  Pretraining","summary":"  Recent research on time-series self-supervised models shows great promise in\nlearning semantic representations. However, it has been limited to small-scale\ndatasets, e.g., thousands of temporal sequences. In this work, we make key\ntechnical contributions that are tailored to the numerical properties of\ntime-series data and allow the model to scale to large datasets, e.g., millions\nof temporal sequences. We adopt the Transformer architecture by first\npartitioning the input into non-overlapping windows. Each window is then\ncharacterized by its normalized shape and two scalar values denoting the mean\nand standard deviation within each window. To embed scalar values that may\npossess arbitrary numerical scales to high-dimensional vectors, we propose a\nnumerically multi-scaled embedding module enumerating all possible scales for\nthe scalar values. The model undergoes pretraining using the proposed\nnumerically multi-scaled embedding with a simple contrastive objective on a\nlarge-scale dataset containing over a million sequences. We study its transfer\nperformance on a number of univariate and multivariate classification\nbenchmarks. Our method exhibits remarkable improvement against previous\nrepresentation learning approaches and establishes the new state of the art,\neven compared with domain-specific non-learning-based methods.\n","authors":["Chenguo Lin","Xumeng Wen","Wei Cao","Congrui Huang","Jiang Bian","Stephen Lin","Zhirong Wu"],"pdf_url":"https://arxiv.org/pdf/2310.07402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08184v1","updated":"2023-10-12T10:20:36Z","published":"2023-10-12T10:20:36Z","title":"Learn From Model Beyond Fine-Tuning: A Survey","summary":"  Foundation models (FM) have demonstrated remarkable performance across a wide\nrange of tasks (especially in the fields of natural language processing and\ncomputer vision), primarily attributed to their ability to comprehend\ninstructions and access extensive, high-quality data. This not only showcases\ntheir current effectiveness but also sets a promising trajectory towards the\ndevelopment of artificial general intelligence. Unfortunately, due to multiple\nconstraints, the raw data of the model used for large model training are often\ninaccessible, so the use of end-to-end models for downstream tasks has become a\nnew research trend, which we call Learn From Model (LFM) in this article. LFM\nfocuses on the research, modification, and design of FM based on the model\ninterface, so as to better understand the model structure and weights (in a\nblack box environment), and to generalize the model to downstream tasks. The\nstudy of LFM techniques can be broadly categorized into five major areas: model\ntuning, model distillation, model reuse, meta learning and model editing. Each\ncategory encompasses a repertoire of methods and strategies that aim to enhance\nthe capabilities and performance of FM. This paper gives a comprehensive review\nof the current methods based on FM from the perspective of LFM, in order to\nhelp readers better understand the current research status and ideas. To\nconclude, we summarize the survey by highlighting several critical areas for\nfuture exploration and addressing open issues that require further attention\nfrom the research community. The relevant papers we investigated in this\narticle can be accessed at\n<https://github.com/ruthless-man/Awesome-Learn-from-Model>.\n","authors":["Hongling Zheng","Li Shen","Anke Tang","Yong Luo","Han Hu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2310.08184v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.08182v1","updated":"2023-10-12T10:17:40Z","published":"2023-10-12T10:17:40Z","title":"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness\n  Evaluation","summary":"  The lack of standardized robustness metrics and the widespread reliance on\nnumerous unrelated benchmark datasets for testing have created a gap between\nacademically validated robust models and their often problematic practical\nadoption. To address this, we introduce XIMAGENET-12, an explainable benchmark\ndataset with over 200K images and 15,600 manual semantic annotations. Covering\n12 categories from ImageNet to represent objects commonly encountered in\npractical life and simulating six diverse scenarios, including overexposure,\nblurring, color changing, etc., we further propose a novel robustness criterion\nthat extends beyond model generation ability assessment. This benchmark\ndataset, along with related code, is available at\nhttps://sites.google.com/view/ximagenet-12/home. Researchers and practitioners\ncan leverage this resource to evaluate the robustness of their visual models\nunder challenging conditions and ultimately benefit from the demands of\npractical computer vision systems.\n","authors":["Qiang Li","Dan Zhang","Shengzhao Lei","Xun Zhao","Shuyan Li","Porawit Kamnoedboon","WeiWei Li"],"pdf_url":"https://arxiv.org/pdf/2310.08182v1.pdf","comment":"UnderSubmission"},{"id":"http://arxiv.org/abs/2310.08177v1","updated":"2023-10-12T10:03:25Z","published":"2023-10-12T10:03:25Z","title":"Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization","summary":"  Evaluating the adversarial robustness of machine learning models using\ngradient-based attacks is challenging. In this work, we show that\nhyperparameter optimization can improve fast minimum-norm attacks by automating\nthe selection of the loss function, the optimizer and the step-size scheduler,\nalong with the corresponding hyperparameters. Our extensive evaluation\ninvolving several robust models demonstrates the improved efficacy of fast\nminimum-norm attacks when hyper-up with hyperparameter optimization. We release\nour open-source code at https://github.com/pralab/HO-FMN.\n","authors":["Giuseppe Floris","Raffaele Mura","Luca Scionis","Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08177v1.pdf","comment":"Accepted at ESANN23"},{"id":"http://arxiv.org/abs/2310.08176v1","updated":"2023-10-12T10:01:39Z","published":"2023-10-12T10:01:39Z","title":"Infinite Width Graph Neural Networks for Node Regression/ Classification","summary":"  This work analyzes Graph Neural Networks, a generalization of Fully-Connected\nDeep Neural Nets on Graph structured data, when their width, that is the number\nof nodes in each fullyconnected layer is increasing to infinity. Infinite Width\nNeural Networks are connecting Deep Learning to Gaussian Processes and Kernels,\nboth Machine Learning Frameworks with long traditions and extensive theoretical\nfoundations. Gaussian Processes and Kernels have much less hyperparameters then\nNeural Networks and can be used for uncertainty estimation, making them more\nuser friendly for applications. This works extends the increasing amount of\nresearch connecting Gaussian Processes and Kernels to Neural Networks. The\nKernel and Gaussian Process closed forms are derived for a variety of\narchitectures, namely the standard Graph Neural Network, the Graph Neural\nNetwork with Skip-Concatenate Connections and the Graph Attention Neural\nNetwork. All architectures are evaluated on a variety of datasets on the task\nof transductive Node Regression and Classification. Additionally, a Spectral\nSparsification method known as Effective Resistance is used to improve runtime\nand memory requirements. Extending the setting to inductive graph learning\ntasks (Graph Regression/ Classification) is straightforward and is briefly\ndiscussed in 3.5.\n","authors":["Yunus Cobanoglu"],"pdf_url":"https://arxiv.org/pdf/2310.08176v1.pdf","comment":"50 Pages, 2 Figures (with subfigures)o, multiple tables"},{"id":"http://arxiv.org/abs/2308.08469v3","updated":"2023-10-12T09:58:03Z","published":"2023-08-16T16:19:50Z","title":"LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with\n  Pre-Trained LLMs","summary":"  In this work, we leverage pre-trained Large Language Models (LLMs) to enhance\ntime-series forecasting. Mirroring the growing interest in unifying models for\nNatural Language Processing and Computer Vision, we envision creating an\nanalogous model for long-term time-series forecasting. Due to limited\nlarge-scale time-series data for building robust foundation models, our\napproach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By\ncombining time-series patching with temporal encoding, we have enhanced the\ncapability of LLMs to handle time-series data effectively. Inspired by the\nsupervised fine-tuning in chatbot domains, we prioritize a two-stage\nfine-tuning process: first conducting supervised fine-tuning to orient the LLM\ntowards time-series data, followed by task-specific downstream fine-tuning.\nFurthermore, to unlock the flexibility of pre-trained LLMs without extensive\nparameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT)\ntechniques. Drawing on these innovations, LLM4TS has yielded state-of-the-art\nresults in long-term forecasting. Our model has also shown exceptional\ncapabilities as both a robust representation learner and an effective few-shot\nlearner, thanks to the knowledge transferred from the pre-trained LLM.\n","authors":["Ching Chang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2308.08469v3.pdf","comment":"This paper is currently under review. The code will be made available\n  upon acceptance"},{"id":"http://arxiv.org/abs/2310.08165v1","updated":"2023-10-12T09:37:56Z","published":"2023-10-12T09:37:56Z","title":"COVID-19 Detection Using Swin Transformer Approach from Computed\n  Tomography Images","summary":"  The accurate and efficient diagnosis of COVID-19 is of paramount importance,\nparticularly in the context of large-scale medical imaging datasets. In this\npreprint paper, we propose a novel approach for COVID-19 diagnosis using CT\nimages that leverages the power of Swin Transformer models, state-of-the-art\nsolutions in computer vision tasks. Our method includes a systematic approach\nfor patient-level predictions, where individual CT slices are classified as\nCOVID-19 or non-COVID, and the patient's overall diagnosis is determined\nthrough majority voting. The application of the Swin Transformer in this\ncontext results in patient-level predictions that demonstrate exceptional\ndiagnostic accuracy. In terms of evaluation metrics, our approach consistently\noutperforms the baseline, as well as numerous competing methods, showcasing its\neffectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model\nexceeds the baseline and offers a robust solution for accurate diagnosis.\n","authors":["Kenan Morani"],"pdf_url":"https://arxiv.org/pdf/2310.08165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08164v1","updated":"2023-10-12T09:36:03Z","published":"2023-10-12T09:36:03Z","title":"Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse\n  Autoencoders","summary":"  Large language models (LLMs) aligned to human preferences via reinforcement\nlearning from human feedback (RLHF) underpin many commercial applications.\nHowever, how RLHF impacts LLM internals remains opaque. We propose a novel\nmethod to interpret learned reward functions in RLHF-tuned LLMs using sparse\nautoencoders. Our approach trains autoencoder sets on activations from a base\nLLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we\nidentify unique features that reflect the accuracy of the learned reward model.\nTo quantify this, we construct a scenario where the tuned LLM learns\ntoken-reward mappings to maximize reward. This is the first application of\nsparse autoencoders for interpreting learned rewards and broadly inspecting\nreward learning in LLMs. Our method provides an abstract approximation of\nreward integrity. This presents a promising technique for ensuring alignment\nbetween specified objectives and model behaviors.\n","authors":["Luke Marks","Amir Abdullah","Luna Mendez","Rauno Arike","Philip Torr","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2310.08164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04234v2","updated":"2023-10-12T09:20:00Z","published":"2023-04-09T13:20:19Z","title":"Variational operator learning: A unified paradigm marrying training\n  neural operators and solving partial differential equations","summary":"  Neural operators as novel neural architectures for fast approximating\nsolution operators of partial differential equations (PDEs), have shown\nconsiderable promise for future scientific computing. However, the mainstream\nof training neural operators is still data-driven, which needs an expensive\nground-truth dataset from various sources (e.g., solving PDEs' samples with the\nconventional solvers, real-world experiments) in addition to training stage\ncosts. From a computational perspective, marrying operator learning and\nspecific domain knowledge to solve PDEs is an essential step in reducing\ndataset costs and label-free learning. We propose a novel paradigm that\nprovides a unified framework of training neural operators and solving PDEs with\nthe variational form, which we refer to as the variational operator learning\n(VOL). Ritz and Galerkin approach with finite element discretization are\ndeveloped for VOL to achieve matrix-free approximation of system functional and\nresidual, then direct minimization and iterative update are proposed as two\noptimization strategies for VOL. Various types of experiments based on\nreasonable benchmarks about variable heat source, Darcy flow, and variable\nstiffness elasticity are conducted to demonstrate the effectiveness of VOL.\nWith a label-free training set and a 5-label-only shift set, VOL learns\nsolution operators with its test errors decreasing in a power law with respect\nto the amount of unlabeled data. To the best of the authors' knowledge, this is\nthe first study that integrates the perspectives of the weak form and efficient\niterative methods for solving sparse linear systems into the end-to-end\noperator learning task.\n","authors":["Tengfei Xu","Dachuan Liu","Peng Hao","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2304.04234v2.pdf","comment":"35 pages, 6 figures with 5 extended figures"},{"id":"http://arxiv.org/abs/2305.14133v2","updated":"2023-10-12T09:18:09Z","published":"2023-05-23T14:56:19Z","title":"Conditional Mutual Information for Disentangled Representations in\n  Reinforcement Learning","summary":"  Reinforcement Learning (RL) environments can produce training data with\nspurious correlations between features due to the amount of training data or\nits limited feature coverage. This can lead to RL agents encoding these\nmisleading correlations in their latent representation, preventing the agent\nfrom generalising if the correlation changes within the environment or when\ndeployed in the real world. Disentangled representations can improve\nrobustness, but existing disentanglement techniques that minimise mutual\ninformation between features require independent features, thus they cannot\ndisentangle correlated features. We propose an auxiliary task for RL algorithms\nthat learns a disentangled representation of high-dimensional observations with\ncorrelated features by minimising the conditional mutual information between\nfeatures in the representation. We demonstrate experimentally, using continuous\ncontrol tasks, that our approach improves generalisation under correlation\nshifts, as well as improving the training performance of RL algorithms in the\npresence of correlated features.\n","authors":["Mhairi Dunion","Trevor McInroe","Kevin Sebastian Luck","Josiah P. Hanna","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2305.14133v2.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS), 2023"},{"id":"http://arxiv.org/abs/2310.08150v1","updated":"2023-10-12T09:17:46Z","published":"2023-10-12T09:17:46Z","title":"On Extreme Value Asymptotics of Projected Sample Covariances in High\n  Dimensions with Applications in Finance and Convolutional Networks","summary":"  Maximum-type statistics of certain functions of the sample covariance matrix\nof high-dimensional vector time series are studied to statistically confirm or\nreject the null hypothesis that a data set has been collected under normal\nconditions. The approach generalizes the case of the maximal deviation of the\nsample autocovariances function from its assumed values. Within a linear time\nseries framework it is shown that Gumbel-type extreme value asymptotics holds\ntrue. As applications we discuss long-only mimimal-variance portfolio\noptimization and subportfolio analysis with respect to idiosyncratic risks, ETF\nindex tracking by sparse tracking portfolios, convolutional deep learners for\nimage analysis and the analysis of array-of-sensors data.\n","authors":["Ansgar Steland"],"pdf_url":"https://arxiv.org/pdf/2310.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08148v1","updated":"2023-10-12T09:12:50Z","published":"2023-10-12T09:12:50Z","title":"Open-Set Knowledge-Based Visual Question Answering with Inference Paths","summary":"  Given an image and an associated textual question, the purpose of\nKnowledge-Based Visual Question Answering (KB-VQA) is to provide a correct\nanswer to the question with the aid of external knowledge bases. Prior KB-VQA\nmodels are usually formulated as a retriever-classifier framework, where a\npre-trained retriever extracts textual or visual information from knowledge\ngraphs and then makes a prediction among the candidates. Despite promising\nprogress, there are two drawbacks with existing models. Firstly, modeling\nquestion-answering as multi-class classification limits the answer space to a\npreset corpus and lacks the ability of flexible reasoning. Secondly, the\nclassifier merely consider \"what is the answer\" without \"how to get the\nanswer\", which cannot ground the answer to explicit reasoning paths. In this\npaper, we confront the challenge of \\emph{explainable open-set} KB-VQA, where\nthe system is required to answer questions with entities at wild and retain an\nexplainable reasoning path. To resolve the aforementioned issues, we propose a\nnew retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for\nbrevity). Specifically, it contains graph constructing, pruning, and path-level\nranking, which not only retrieves accurate answers but also provides inference\npaths that explain the reasoning process. To comprehensively evaluate our\nmodel, we reformulate the benchmark dataset OK-VQA with manually corrected\nentity-level annotations and release it as ConceptVQA. Extensive experiments on\nreal-world questions demonstrate that our framework is not only able to perform\nopen-set question answering across the whole knowledge base but provide\nexplicit reasoning path.\n","authors":["Jingru Gan","Xinzhe Han","Shuhui Wang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2310.08148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16584v2","updated":"2023-10-12T09:03:48Z","published":"2023-09-28T16:44:18Z","title":"A Design Toolbox for the Development of Collaborative Distributed\n  Machine Learning Systems","summary":"  To leverage data for the sufficient training of machine learning (ML) models\nfrom multiple parties in a confidentiality-preserving way, various\ncollaborative distributed ML (CDML) system designs have been developed, for\nexample, to perform assisted learning, federated learning, and split learning.\nCDML system designs show different traits, including high agent autonomy, ML\nmodel confidentiality, and fault tolerance. Facing a wide variety of CDML\nsystem designs with different traits, it is difficult for developers to design\nCDML systems with traits that match use case requirements in a targeted way.\nHowever, inappropriate CDML system designs may result in CDML systems failing\ntheir envisioned purposes. We developed a CDML design toolbox that can guide\nthe development of CDML systems. Based on the CDML design toolbox, we present\nCDML system archetypes with distinct key traits that can support the design of\nCDML systems to meet use case requirements.\n","authors":["David Jin","Niclas Kannengieer","Sascha Rank","Ali Sunyaev"],"pdf_url":"https://arxiv.org/pdf/2309.16584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08138v1","updated":"2023-10-12T08:52:36Z","published":"2023-10-12T08:52:36Z","title":"Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow\n  Prediction","summary":"  Traffic flow prediction is one of the most fundamental tasks of intelligent\ntransportation systems. The complex and dynamic spatial-temporal dependencies\nmake the traffic flow prediction quite challenging. Although existing\nspatial-temporal graph neural networks hold prominent, they often encounter\nchallenges such as (1) ignoring the fixed graph that limits the predictive\nperformance of the model, (2) insufficiently capturing complex spatial-temporal\ndependencies simultaneously, and (3) lacking attention to spatial-temporal\ninformation at different time lengths. In this paper, we propose a Multi-Scale\nSpatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN,\nwhich consists of two different recurrent neural networks: the single-step gate\nrecurrent unit and the multi-step gate recurrent unit to fully capture the\ncomplex spatial-temporal information in the traffic data under different time\nwindows. Moreover, we propose a spatial-temporal synchronous attention\nmechanism that integrates adaptive position graph convolutions into the\nself-attention mechanism to achieve synchronous capture of spatial-temporal\ndependencies. We conducted extensive experiments on four real traffic datasets\nand demonstrated that our model achieves the best prediction accuracy with\nnon-trivial margins compared to all the twenty baseline methods.\n","authors":["Haiyang Liu","Chunjiang Zhu","Detian Zhang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2310.08138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08137v1","updated":"2023-10-12T08:51:59Z","published":"2023-10-12T08:51:59Z","title":"Counterfactual Explanations for Time Series Forecasting","summary":"  Among recent developments in time series forecasting methods, deep\nforecasting models have gained popularity as they can utilize hidden feature\npatterns in time series to improve forecasting performance. Nevertheless, the\nmajority of current deep forecasting models are opaque, hence making it\nchallenging to interpret the results. While counterfactual explanations have\nbeen extensively employed as a post-hoc approach for explaining classification\nmodels, their application to forecasting models still remains underexplored. In\nthis paper, we formulate the novel problem of counterfactual generation for\ntime series forecasting, and propose an algorithm, called ForecastCF, that\nsolves the problem by applying gradient-based perturbations to the original\ntime series. ForecastCF guides the perturbations by applying constraints to the\nforecasted values to obtain desired prediction outcomes. We experimentally\nevaluate ForecastCF using four state-of-the-art deep model architectures and\ncompare to two baselines. Our results show that ForecastCF outperforms the\nbaseline in terms of counterfactual validity and data manifold closeness.\nOverall, our findings suggest that ForecastCF can generate meaningful and\nrelevant counterfactual explanations for various forecasting tasks.\n","authors":["Zhendong Wang","Ioanna Miliou","Isak Samsten","Panagiotis Papapetrou"],"pdf_url":"https://arxiv.org/pdf/2310.08137v1.pdf","comment":"10 pages, 6 figures. Accepted by ICDM 2023"},{"id":"http://arxiv.org/abs/2310.00177v3","updated":"2023-10-12T08:51:28Z","published":"2023-09-29T22:49:47Z","title":"A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann\n  Boundary Conditions","summary":"  We introduce a neural-preconditioned iterative solver for Poisson equations\nwith mixed boundary conditions. The Poisson equation is ubiquitous in\nscientific computing: it governs a wide array of physical phenomena, arises as\na subproblem in many numerical algorithms, and serves as a model problem for\nthe broader class of elliptic PDEs. The most popular Poisson discretizations\nyield large sparse linear systems. At high resolution, and for\nperformance-critical applications, iterative solvers can be advantageous for\nthese -- but only when paired with powerful preconditioners. The core of our\nsolver is a neural network trained to approximate the inverse of a discrete\nstructured-grid Laplace operator for a domain of arbitrary shape and with mixed\nboundary conditions. The structure of this problem motivates a novel network\narchitecture that we demonstrate is highly effective as a preconditioner even\nfor boundary conditions outside the training set. We show that on challenging\ntest cases arising from an incompressible fluid simulation, our method\noutperforms state-of-the-art solvers like algebraic multigrid as well as some\nrecent neural preconditioners.\n","authors":["Kai Weixian Lan","Elias Gueidon","Ayano Kaneda","Julian Panetta","Joseph Teran"],"pdf_url":"https://arxiv.org/pdf/2310.00177v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06118v2","updated":"2023-10-12T08:46:40Z","published":"2023-05-10T13:09:57Z","title":"NeRF2: Neural Radio-Frequency Radiance Fields","summary":"  Although Maxwell discovered the physical laws of electromagnetic waves 160\nyears ago, how to precisely model the propagation of an RF signal in an\nelectrically large and complex environment remains a long-standing problem. The\ndifficulty is in the complex interactions between the RF signal and the\nobstacles (e.g., reflection, diffraction, etc.). Inspired by the great success\nof using a neural network to describe the optical field in computer vision, we\npropose a neural radio-frequency radiance field, NeRF$^\\textbf{2}$, which\nrepresents a continuous volumetric scene function that makes sense of an RF\nsignal's propagation. Particularly, after training with a few signal\nmeasurements, NeRF$^\\textbf{2}$ can tell how/what signal is received at any\nposition when it knows the position of a transmitter. As a physical-layer\nneural network, NeRF$^\\textbf{2}$ can take advantage of the learned statistic\nmodel plus the physical model of ray tracing to generate a synthetic dataset\nthat meets the training demands of application-layer artificial neural networks\n(ANNs). Thus, we can boost the performance of ANNs by the proposed\nturbo-learning, which mixes the true and synthetic datasets to intensify the\ntraining. Our experiment results show that turbo-learning can enhance\nperformance with an approximate 50% increase. We also demonstrate the power of\nNeRF$^\\textbf{2}$ in the field of indoor localization and 5G MIMO.\n","authors":["Xiaopeng Zhao","Zhenlin An","Qingrui Pan","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2305.06118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02286v3","updated":"2023-10-12T08:40:31Z","published":"2022-10-05T14:22:24Z","title":"Efficient probabilistic reconciliation of forecasts for real-valued and\n  count time series","summary":"  Hierarchical time series are common in several applied fields. The forecasts\nfor these time series are required to be coherent, that is, to satisfy the\nconstraints given by the hierarchy. The most popular technique to enforce\ncoherence is called reconciliation, which adjusts the base forecasts computed\nfor each time series. However, recent works on probabilistic reconciliation\npresent several limitations. In this paper, we propose a new approach based on\nconditioning to reconcile any type of forecast distribution. We then introduce\na new algorithm, called Bottom-Up Importance Sampling, to efficiently sample\nfrom the reconciled distribution. It can be used for any base forecast\ndistribution: discrete, continuous, or in the form of samples, providing a\nmajor speedup compared to the current methods. Experiments on several temporal\nhierarchies show a significant improvement over base probabilistic forecasts.\n","authors":["Lorenzo Zambon","Dario Azzimonti","Giorgio Corani"],"pdf_url":"https://arxiv.org/pdf/2210.02286v3.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08122v1","updated":"2023-10-12T08:24:02Z","published":"2023-10-12T08:24:02Z","title":"Core-sets for Fair and Diverse Data Summarization","summary":"  We study core-set construction algorithms for the task of Diversity\nMaximization under fairness/partition constraint. Given a set of points $P$ in\na metric space partitioned into $m$ groups, and given $k_1,\\ldots,k_m$, the\ngoal of this problem is to pick $k_i$ points from each group $i$ such that the\noverall diversity of the $k=\\sum_i k_i$ picked points is maximized. We consider\ntwo natural diversity measures: sum-of-pairwise distances and\nsum-of-nearest-neighbor distances, and show improved core-set construction\nalgorithms with respect to these measures. More precisely, we show the first\nconstant factor core-set w.r.t. sum-of-pairwise distances whose size is\nindependent of the size of the dataset and the aspect ratio. Second, we show\nthe first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we\nrun several experiments showing the effectiveness of our core-set approach. In\nparticular, we apply constrained diversity maximization to summarize a set of\ntimed messages that takes into account the messages' recency. Specifically, the\nsummary should include more recent messages compared to older ones. This is a\nreal task in one of the largest communication platforms, affecting the\nexperience of hundreds of millions daily active users. By utilizing our\ncore-set method for this task, we achieve a 100x speed-up while losing the\ndiversity by only a few percent. Moreover, our approach allows us to improve\nthe space usage of the algorithm in the streaming setting.\n","authors":["Sepideh Mahabadi","Stojan Trajanovski"],"pdf_url":"https://arxiv.org/pdf/2310.08122v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.08109v1","updated":"2023-10-12T08:10:31Z","published":"2023-10-12T08:10:31Z","title":"Overview of Physics-Informed Machine Learning Inversion of Geophysical\n  Data","summary":"  We review four types of algorithms for physics-informed machine learning\n(PIML) inversion of geophysical data. The unifying equation is given by the\njoint objective function $\\epsilon$:\n  \\begin{eqnarray} \\epsilon^{||-PIML}&=&\\lambda_1 \\overbrace{||{\\bf\nW}^{ML}({\\bf H}_{{\\bf w}} {\\bf d}^{obs}-{\\bf m})||^2}^{NN} + \\lambda_2\n\\overbrace{{||{\\bf W}^{FWI}({\\bf L} {\\bf m}-{\\bf d}^{obs})||^2}}^{FWI} ~+\n\\nonumber\\\\ \\nonumber\\\\ && + ~~Regularizer, \\label{PIML.eq120}\n\\end{eqnarray}where the optimal model ${\\bf m}^*$ and weights $\\bf w^*$\nminimize $\\epsilon$. Here, The matrix weights are given by the boldface symbol\n$\\bf W$, and full waveform inversion (FWI) is typically computed using a\nfinite-difference solution of the wave equation, where $\\bf L$ represents the\nforward modeling operation of the wave equation as a function of the model $\\bf\nm$. Also, a fully-connected neural network (NN) is used to compute the model\n${\\bf H_w}{\\bf d}^{obs} \\approx \\bf m$ from the observed input data ${\\bf\nd}^{obs}$. The selection of weights $\\lambda_i$ and the NN operations determine\none of four different PIML algorithms.\n  PIML offers potential advantages over standard FWI through its enhanced\nability to avoid local minima and the option to locally train the inversion\noperator, minimizing the requirement for extensive training data for global\napplicability. However, the effectiveness of PIML relies on the similarity\nbetween the test and trained data. Nevertheless, a possible strategy to\novercome this limitation involves initial pretraining of a PIML architecture\nwith data from a broader region, followed by fine-tuning for specific data-a\nmethod reminiscent of the way large language models are pretrained and adapted\nfor various tasks.\n","authors":["Gerard T. Schuster","Shihang Feng"],"pdf_url":"https://arxiv.org/pdf/2310.08109v1.pdf","comment":"37 pages, 16 figures"},{"id":"http://arxiv.org/abs/2209.10404v3","updated":"2023-10-12T07:55:10Z","published":"2022-09-21T14:51:42Z","title":"GP-net: Flexible Viewpoint Grasp Proposal","summary":"  We present the Grasp Proposal Network (GP-net), a Convolutional Neural\nNetwork model which can generate 6-DoF grasps from flexible viewpoints, e.g. as\nexperienced by mobile manipulators. To train GP-net, we synthetically generate\na dataset containing depth-images and ground-truth grasp information. In\nreal-world experiments, we use the EGAD evaluation benchmark to evaluate GP-net\nagainst two commonly used algorithms, the Volumetric Grasping Network (VGN) and\nthe Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In\ncontrast to the state-of-the-art methods in robotic grasping, GP-net can be\nused for grasping objects from flexible, unknown viewpoints without the need to\ndefine the workspace and achieves a grasp success of 54.4% compared to 51.6%\nfor VGN and 44.2% for GPD. We provide a ROS package along with our code and\npre-trained models at https://aucoroboticsmu.github.io/GP-net/.\n","authors":["Anna Konrad","John McDonald","Rudi Villing"],"pdf_url":"https://arxiv.org/pdf/2209.10404v3.pdf","comment":"Accepted to ICAR 2023"},{"id":"http://arxiv.org/abs/2309.15505v2","updated":"2023-10-12T07:55:05Z","published":"2023-09-27T09:13:40Z","title":"Finite Scalar Quantization: VQ-VAE Made Simple","summary":"  We propose to replace vector quantization (VQ) in the latent representation\nof VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where\nwe project the VAE representation down to a few dimensions (typically less than\n10). Each dimension is quantized to a small set of fixed values, leading to an\n(implicit) codebook given by the product of these sets. By appropriately\nchoosing the number of dimensions and values each dimension can take, we obtain\nthe same codebook size as in VQ. On top of such discrete representations, we\ncan train the same models that have been trained on VQ-VAE representations. For\nexample, autoregressive and masked transformer models for image generation,\nmultimodal generation, and dense prediction computer vision tasks. Concretely,\nwe employ FSQ with MaskGIT for image generation, and with UViM for depth\nestimation, colorization, and panoptic segmentation. Despite the much simpler\ndesign of FSQ, we obtain competitive performance in all these tasks. We\nemphasize that FSQ does not suffer from codebook collapse and does not need the\ncomplex machinery employed in VQ (commitment losses, codebook reseeding, code\nsplitting, entropy penalties, etc.) to learn expressive discrete\nrepresentations.\n","authors":["Fabian Mentzer","David Minnen","Eirikur Agustsson","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2309.15505v2.pdf","comment":"Code:\n  https://github.com/google-research/google-research/tree/master/fsq"},{"id":"http://arxiv.org/abs/2310.08100v1","updated":"2023-10-12T07:50:37Z","published":"2023-10-12T07:50:37Z","title":"Generative Intrinsic Optimization: Intrisic Control with Model Learning","summary":"  Future sequence represents the outcome after executing the action into the\nenvironment. When driven by the information-theoretic concept of mutual\ninformation, it seeks maximally informative consequences. Explicit outcomes may\nvary across state, return, or trajectory serving different purposes such as\ncredit assignment or imitation learning. However, the inherent nature of\nincorporating intrinsic motivation with reward maximization is often neglected.\nIn this work, we propose a variational approach to jointly learn the necessary\nquantity for estimating the mutual information and the dynamics model,\nproviding a general framework for incorporating different forms of outcomes of\ninterest. Integrated into a policy iteration scheme, our approach guarantees\nconvergence to the optimal policy. While we mainly focus on theoretical\nanalysis, our approach opens the possibilities of leveraging intrinsic control\nwith model learning to enhance sample efficiency and incorporate uncertainty of\nthe environment into decision-making.\n","authors":["Jianfei Ma"],"pdf_url":"https://arxiv.org/pdf/2310.08100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08096v1","updated":"2023-10-12T07:43:27Z","published":"2023-10-12T07:43:27Z","title":"ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction\n  Targets","summary":"  Public and private actors struggle to assess the vast amounts of information\nabout sustainability commitments made by various institutions. To address this\nproblem, we create a novel tool for automatically detecting corporate,\nnational, and regional net zero and reduction targets in three steps. First, we\nintroduce an expert-annotated data set with 3.5K text samples. Second, we train\nand release ClimateBERT-NetZero, a natural language classifier to detect\nwhether a text contains a net zero or reduction target. Third, we showcase its\nanalysis potential with two use cases: We first demonstrate how\nClimateBERT-NetZero can be combined with conventional question-answering (Q&A)\nmodels to analyze the ambitions displayed in net zero and reduction targets.\nFurthermore, we employ the ClimateBERT-NetZero model on quarterly earning call\ntranscripts and outline how communication patterns evolve over time. Our\nexperiments demonstrate promising pathways for extracting and analyzing net\nzero and emission reduction targets at scale.\n","authors":["Tobias Schimanski","Julia Bingler","Camilla Hyslop","Mathias Kraus","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2310.08096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08091v1","updated":"2023-10-12T07:38:10Z","published":"2023-10-12T07:38:10Z","title":"Discerning Temporal Difference Learning","summary":"  Temporal difference learning (TD) is a foundational concept in reinforcement\nlearning (RL), aimed at efficiently assessing a policy's value function.\nTD($\\lambda$), a potent variant, incorporates a memory trace to distribute the\nprediction error into the historical context. However, this approach often\nneglects the significance of historical states and the relative importance of\npropagating the TD error, influenced by challenges such as visitation imbalance\nor outcome noise. To address this, we propose a novel TD algorithm named\ndiscerning TD learning (DTD), which allows flexible emphasis\nfunctions$-$predetermined or adapted during training$-$to allocate efforts\neffectively across states. We establish the convergence properties of our\nmethod within a specific class of emphasis functions and showcase its promising\npotential for adaptation to deep RL contexts. Empirical results underscore that\nemploying a judicious emphasis function not only improves value estimation but\nalso expedites learning across diverse scenarios.\n","authors":["Jianfei Ma"],"pdf_url":"https://arxiv.org/pdf/2310.08091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08088v1","updated":"2023-10-12T07:26:41Z","published":"2023-10-12T07:26:41Z","title":"Dealing with zero-inflated data: achieving SOTA with a two-fold machine\n  learning approach","summary":"  In many cases, a machine learning model must learn to correctly predict a few\ndata points with particular values of interest in a broader range of data where\nmany target values are zero. Zero-inflated data can be found in diverse\nscenarios, such as lumpy and intermittent demands, power consumption for home\nappliances being turned on and off, impurities measurement in distillation\nprocesses, and even airport shuttle demand prediction. The presence of zeroes\naffects the models' learning and may result in poor performance. Furthermore,\nzeroes also distort the metrics used to compute the model's prediction quality.\nThis paper showcases two real-world use cases (home appliances classification\nand airport shuttle demand prediction) where a hierarchical model applied in\nthe context of zero-inflated data leads to excellent results. In particular,\nfor home appliances classification, the weighted average of Precision, Recall,\nF1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively.\nFurthermore, it is estimated that the proposed approach is also four times more\nenergy efficient than the SOTA approach against which it was compared to.\nTwo-fold models performed best in all cases when predicting airport shuttle\ndemand, and the difference against other models has been proven to be\nstatistically significant.\n","authors":["Joe M. Roanec","Gaper Petelin","Joo Costa","Bla Bertalani","Gregor Cerar","Marko Guek","Gregor Papa","Dunja Mladeni"],"pdf_url":"https://arxiv.org/pdf/2310.08088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08087v1","updated":"2023-10-12T07:20:03Z","published":"2023-10-12T07:20:03Z","title":"A Carbon Tracking Model for Federated Learning: Impact of Quantization\n  and Sparsification","summary":"  Federated Learning (FL) methods adopt efficient communication technologies to\ndistribute machine learning tasks across edge devices, reducing the overhead in\nterms of data storage and computational complexity compared to centralized\nsolutions. Rather than moving large data volumes from producers (sensors,\nmachines) to energy-hungry data centers, raising environmental concerns due to\nresource demands, FL provides an alternative solution to mitigate the energy\ndemands of several learning tasks while enabling new Artificial Intelligence of\nThings (AIoT) applications. This paper proposes a framework for real-time\nmonitoring of the energy and carbon footprint impacts of FL systems. The carbon\ntracking tool is evaluated for consensus (fully decentralized) and classical FL\npolicies. For the first time, we present a quantitative evaluation of different\ncomputationally and communication efficient FL methods from the perspectives of\nenergy consumption and carbon equivalent emissions, suggesting also general\nguidelines for energy-efficient design. Results indicate that consensus-driven\nFL implementations should be preferred for limiting carbon emissions when the\nenergy efficiency of the communication is low (i.e., < 25 Kbit/Joule). Besides,\nquantization and sparsification operations are shown to strike a balance\nbetween learning performances and energy consumption, leading to sustainable FL\ndesigns.\n","authors":["Luca Barbieri","Stefano Savazzi","Sanaz Kianoush","Monica Nicoli","Luigi Serio"],"pdf_url":"https://arxiv.org/pdf/2310.08087v1.pdf","comment":"accepted for presentation at IEEE CAMAD 2023"},{"id":"http://arxiv.org/abs/2307.03486v2","updated":"2023-10-12T07:13:32Z","published":"2023-07-07T09:47:15Z","title":"Discovering Hierarchical Achievements in Reinforcement Learning via\n  Contrastive Learning","summary":"  Discovering achievements with a hierarchical structure in procedurally\ngenerated environments presents a significant challenge. This requires an agent\nto possess a broad range of abilities, including generalization and long-term\nreasoning. Many prior methods have been built upon model-based or hierarchical\napproaches, with the belief that an explicit module for long-term planning\nwould be advantageous for learning hierarchical dependencies. However, these\nmethods demand an excessive number of environment interactions or large model\nsizes, limiting their practicality. In this work, we demonstrate that proximal\npolicy optimization (PPO), a simple yet versatile model-free algorithm,\noutperforms previous methods when optimized with recent implementation\npractices. Moreover, we find that the PPO agent can predict the next\nachievement to be unlocked to some extent, albeit with limited confidence.\nBased on this observation, we introduce a novel contrastive learning method,\ncalled achievement distillation, which strengthens the agent's ability to\npredict the next achievement. Our method exhibits a strong capacity for\ndiscovering hierarchical achievements and shows state-of-the-art performance on\nthe challenging Crafter environment in a sample-efficient manner while\nutilizing fewer model parameters.\n","authors":["Seungyong Moon","Junyoung Yeom","Bumsoo Park","Hyun Oh Song"],"pdf_url":"https://arxiv.org/pdf/2307.03486v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.08078v1","updated":"2023-10-12T06:59:10Z","published":"2023-10-12T06:59:10Z","title":"To token or not to token: A Comparative Study of Text Representations\n  for Cross-Lingual Transfer","summary":"  Choosing an appropriate tokenization scheme is often a bottleneck in\nlow-resource cross-lingual transfer. To understand the downstream implications\nof text representation choices, we perform a comparative analysis on language\nmodels having diverse text representation modalities including 2\nsegmentation-based models (\\texttt{BERT}, \\texttt{mBERT}), 1 image-based model\n(\\texttt{PIXEL}), and 1 character-level model (\\texttt{CANINE}). First, we\npropose a scoring Language Quotient (LQ) metric capable of providing a weighted\nrepresentation of both zero-shot and few-shot evaluation combined. Utilizing\nthis metric, we perform experiments comprising 19 source languages and 133\ntarget languages on three tasks (POS tagging, Dependency parsing, and NER). Our\nanalysis reveals that image-based models excel in cross-lingual transfer when\nlanguages are closely related and share visually similar scripts. However, for\ntasks biased toward word meaning (POS, NER), segmentation-based models prove to\nbe superior. Furthermore, in dependency parsing tasks where word relationships\nplay a crucial role, models with their character-level focus, outperform\nothers. Finally, we propose a recommendation scheme based on our findings to\nguide model selection according to task and language requirements.\n","authors":["Md Mushfiqur Rahman","Fardin Ahsan Sakib","Fahim Faisal","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.08078v1.pdf","comment":"Accepted at 3RD MULTILINGUAL REPRESENTATION LEARNING (MRL) WORKSHOP,\n  2023"},{"id":"http://arxiv.org/abs/2310.07312v2","updated":"2023-10-12T06:57:51Z","published":"2023-10-11T08:57:59Z","title":"WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models","summary":"  Innovative foundation models, such as GPT-3 and stable diffusion models, have\nmade a paradigm shift in the realm of artificial intelligence (AI) towards\ngenerative AI-based systems. In unison, from data communication and networking\nperspective, AI and machine learning (AI/ML) algorithms are envisioned to be\npervasively incorporated into the future generations of wireless communications\nsystems, highlighting the need for novel AI-native solutions for the emergent\ncommunication scenarios. In this article, we outline the applications of\ngenerative AI in wireless communication systems to lay the foundations for\nresearch in this field. Diffusion-based generative models, as the new\nstate-of-the-art paradigm of generative models, are introduced, and their\napplications in wireless communication systems are discussed. Two case studies\nare also presented to showcase how diffusion models can be exploited for the\ndevelopment of resilient AI-native communication systems. Specifically, we\npropose denoising diffusion probabilistic models (DDPM) for a wireless\ncommunication scheme with non-ideal transceivers, where 30% improvement is\nachieved in terms of bit error rate. As the second application, DDPMs are\nemployed at the transmitter to shape the constellation symbols, highlighting a\nrobust out-of-distribution performance. Finally, future directions and open\nissues for the development of generative AI-based wireless systems are\ndiscussed to promote future research endeavors towards wireless generative AI\n(WiGenAI).\n","authors":["Mehdi Letafati","Samad Ali","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2310.07312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08073v1","updated":"2023-10-12T06:50:43Z","published":"2023-10-12T06:50:43Z","title":"Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural\n  Networks","summary":"  Neural network pruning has shown to be an effective technique for reducing\nthe network size, trading desirable properties like generalization and\nrobustness to adversarial attacks for higher sparsity. Recent work has claimed\nthat adversarial pruning methods can produce sparse networks while also\npreserving robustness to adversarial examples. In this work, we first\nre-evaluate three state-of-the-art adversarial pruning methods, showing that\ntheir robustness was indeed overestimated. We then compare pruned and dense\nversions of the same models, discovering that samples on thin ice, i.e., closer\nto the unpruned model's decision boundary, are typically misclassified after\npruning. We conclude by discussing how this intuition may lead to designing\nmore effective adversarial pruning methods in future work.\n","authors":["Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02010v2","updated":"2023-10-12T06:46:25Z","published":"2023-06-03T05:45:29Z","title":"Memorization Capacity of Multi-Head Attention in Transformers","summary":"  Transformers have become the go-to architecture for language and vision\ntasks, yet their theoretical properties, especially memorization capacity,\nremain elusive. This paper investigates the memorization abilities of\nmulti-head attention mechanisms, examining how many example sequences they can\nmemorize, as a function of the number of heads and sequence length. Motivated\nby experimental findings on vision transformers, we introduce novel assumptions\nabout the linear independence of input data, distinct from the commonly used\ngeneral-position assumption. Under these assumptions, we demonstrate that an\nattention layer with $H$ heads, dimension $d$, and context size $n < d$,\nfeaturing $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our\nanalysis sheds light on how different attention heads handle various example\nsequences, aided by the softmax operator's saturation property. We validate our\nfindings through experiments on synthetic data.\n","authors":["Sadegh Mahdavi","Renjie Liao","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2306.02010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08071v1","updated":"2023-10-12T06:36:41Z","published":"2023-10-12T06:36:41Z","title":"Learning Transferable Conceptual Prototypes for Interpretable\n  Unsupervised Domain Adaptation","summary":"  Despite the great progress of unsupervised domain adaptation (UDA) with the\ndeep neural networks, current UDA models are opaque and cannot provide\npromising explanations, limiting their applications in the scenarios that\nrequire safe and controllable model decisions. At present, a surge of work\nfocuses on designing deep interpretable methods with adequate data annotations\nand only a few methods consider the distributional shift problem. Most existing\ninterpretable UDA methods are post-hoc ones, which cannot facilitate the model\nlearning process for performance enhancement. In this paper, we propose an\ninherently interpretable method, named Transferable Conceptual Prototype\nLearning (TCPL), which could simultaneously interpret and improve the processes\nof knowledge transfer and decision-making in UDA. To achieve this goal, we\ndesign a hierarchically prototypical module that transfers categorical basic\nconcepts from the source domain to the target domain and learns domain-shared\nprototypes for explaining the underlying reasoning process. With the learned\ntransferable prototypes, a self-predictive consistent pseudo-label strategy\nthat fuses confidence, predictions, and prototype information, is designed for\nselecting suitable target samples for pseudo annotations and gradually\nnarrowing down the domain gap. Comprehensive experiments show that the proposed\nmethod can not only provide effective and intuitive explanations but also\noutperform previous state-of-the-arts.\n","authors":["Junyu Gao","Xinhong Ma","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08071v1.pdf","comment":"Submitted to IEEE TIP"},{"id":"http://arxiv.org/abs/2310.08070v1","updated":"2023-10-12T06:36:31Z","published":"2023-10-12T06:36:31Z","title":"Tight Time-Space Lower Bounds for Constant-Pass Learning","summary":"  In his breakthrough paper, Raz showed that any parity learning algorithm\nrequires either quadratic memory or an exponential number of samples [FOCS'16,\nJACM'19]. A line of work that followed extended this result to a large class of\nlearning problems. Until recently, all these results considered learning in the\nstreaming model, where each sample is drawn independently, and the learner is\nallowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19]\nconsidered a stronger model, allowing multiple passes over the stream. In the\n$2$-pass model, they showed that learning parities of size $n$ requires either\na memory of size $n^{1.5}$ or at least $2^{\\sqrt{n}}$ samples. (Their result\nalso generalizes to other learning problems.)\n  In this work, for any constant $q$, we prove tight memory-sample lower bounds\nfor any parity learning algorithm that makes $q$ passes over the stream of\nsamples. We show that such a learner requires either $\\Omega(n^{2})$ memory\nsize or at least $2^{\\Omega(n)}$ samples. Beyond establishing a tight lower\nbound, this is the first non-trivial lower bound for $q$-pass learning for any\n$q\\ge 3$. Similar to prior work, our results extend to any learning problem\nwith many nearly-orthogonal concepts.\n  We complement the lower bound with an upper bound, showing that parity\nlearning with $q$ passes can be done efficiently with $O(n^2/\\log q)$ memory.\n","authors":["Xin Lyu","Avishay Tal","Hongxun Wu","Junzhao Yang"],"pdf_url":"https://arxiv.org/pdf/2310.08070v1.pdf","comment":"To appear at FOCS 2023"},{"id":"http://arxiv.org/abs/2310.08069v1","updated":"2023-10-12T06:32:42Z","published":"2023-10-12T06:32:42Z","title":"Rethinking Negative Pairs in Code Search","summary":"  Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.\n","authors":["Haochen Li","Xin Zhou","Luu Anh Tuan","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2310.08069v1.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.03084v2","updated":"2023-10-12T06:24:33Z","published":"2023-09-04T09:16:49Z","title":"Pure Monte Carlo Counterfactual Regret Minimization","summary":"  Counterfactual Regret Minimization (CFR) and its variants are the best\nalgorithms so far for solving large-scale incomplete information games.\nHowever, we believe that there are two problems with CFR: First, matrix\nmultiplication is required in CFR iteration, and the time complexity of one\niteration is too high; Secondly, the game characteristics in the real world are\ndifferent. Just using one CFR algorithm will not be perfectly suitable for all\ngame problems.\n  For these two problems, this paper proposes a new algorithm called Pure CFR\n(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious\nPlay (FP), inheriting the concept of counterfactual regret (value) from CFR,\nand using the best response strategy instead of the regret matching strategy\nfor the next iteration. This algorithm has three advantages. First, PCFR can be\ncombined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can\nsignificantly reduce the time and space complexity of one iteration. Secondly,\nour experiments show that the convergence speed of the PMCCFR is 2$\\sim$3 times\nthat of the MCCFR. Finally, there is a type of game that is very suitable for\nPCFR, we call this type of game clear-game, which is characterized by a high\nproportion of dominated strategies. Experiments show that in clear-game, the\nconvergence rate of PMCCFR is two orders of magnitude higher than that of\nMCCFR.\n","authors":["Ju Qi","Ting Feng","Falun Hei","Zhemei Fang","Yunfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2309.03084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08061v1","updated":"2023-10-12T06:23:12Z","published":"2023-10-12T06:23:12Z","title":"ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking","summary":"  Predicting the docking between proteins and ligands is a crucial and\nchallenging task for drug discovery. However, traditional docking methods\nmainly rely on scoring functions, and deep learning-based docking approaches\nusually neglect the 3D spatial information of proteins and ligands, as well as\nthe graph-level features of ligands, which limits their performance. To address\nthese limitations, we propose an equivariant transformer neural network for\nprotein-ligand docking pose prediction. Our approach involves the fusion of\nligand graph-level features by feature processing, followed by the learning of\nligand and protein representations using our proposed TAMformer module.\nAdditionally, we employ an iterative optimization approach based on the\npredicted distance matrix to generate refined ligand poses. The experimental\nresults on real datasets show that our model can achieve state-of-the-art\nperformance.\n","authors":["Yiqiang Yi","Xu Wan","Yatao Bian","Le Ou-Yang","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08056v1","updated":"2023-10-12T06:09:26Z","published":"2023-10-12T06:09:26Z","title":"Learning from Label Proportions: Bootstrapping Supervised Learners via\n  Belief Propagation","summary":"  Learning from Label Proportions (LLP) is a learning problem where only\naggregate level labels are available for groups of instances, called bags,\nduring training, and the aim is to get the best performance at the\ninstance-level on the test data. This setting arises in domains like\nadvertising and medicine due to privacy considerations. We propose a novel\nalgorithmic framework for this problem that iteratively performs two main\nsteps. For the first step (Pseudo Labeling) in every iteration, we define a\nGibbs distribution over binary instance labels that incorporates a) covariate\ninformation through the constraint that instances with similar covariates\nshould have similar labels and b) the bag level aggregated label. We then use\nBelief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo\nlabels. In the second step (Embedding Refinement), we use the pseudo labels to\nprovide supervision for a learner that yields a better embedding. Further, we\niterate on the two steps again by using the second step's embeddings as new\ncovariates for the next iteration. In the final iteration, a classifier is\ntrained using the pseudo labels. Our algorithm displays strong gains against\nseveral SOTA baselines (up to 15%) for the LLP Binary Classification problem on\nvarious dataset types - tabular and Image. We achieve these improvements with\nminimal computational overhead above standard supervised learning due to Belief\nPropagation, for large bag sizes, even for a million samples.\n","authors":["Shreyas Havaldar","Navodita Sharma","Shubhi Sareen","Karthikeyan Shanmugam","Aravindan Raghuveer"],"pdf_url":"https://arxiv.org/pdf/2310.08056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07365v2","updated":"2023-10-12T06:00:52Z","published":"2023-10-11T10:30:49Z","title":"GraphControl: Adding Conditional Control to Universal Graph Pre-trained\n  Models for Graph Domain Transfer Learning","summary":"  Graph-structured data is ubiquitous in the world which models complex\nrelationships between objects, enabling various Web applications. Daily\ninfluxes of unlabeled graph data on the Web offer immense potential for these\napplications. Graph self-supervised algorithms have achieved significant\nsuccess in acquiring generic knowledge from abundant unlabeled graph data.\nThese pre-trained models can be applied to various downstream Web applications,\nsaving training time and improving downstream (target) performance. However,\ndifferent graphs, even across seemingly similar domains, can differ\nsignificantly in terms of attribute semantics, posing difficulties, if not\ninfeasibility, for transferring the pre-trained models to downstream tasks.\nConcretely speaking, for example, the additional task-specific node information\nin downstream tasks (specificity) is usually deliberately omitted so that the\npre-trained representation (transferability) can be leveraged. The trade-off as\nsuch is termed as \"transferability-specificity dilemma\" in this work. To\naddress this challenge, we introduce an innovative deployment module coined as\nGraphControl, motivated by ControlNet, to realize better graph domain transfer\nlearning. Specifically, by leveraging universal structural pre-trained models\nand GraphControl, we align the input space across various graphs and\nincorporate unique characteristics of target data as conditional inputs. These\nconditions will be progressively integrated into the model during fine-tuning\nor prompt tuning through ControlNet, facilitating personalized deployment.\nExtensive experiments show that our method significantly enhances the\nadaptability of pre-trained models on target attributed datasets, achieving\n1.4-3x performance gain. Furthermore, it outperforms training-from-scratch\nmethods on target data with a comparable margin and exhibits faster\nconvergence.\n","authors":["Yun Zhu","Yaoke Wang","Haizhou Shi","Zhenshuo Zhang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07365v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.08051v1","updated":"2023-10-12T05:52:54Z","published":"2023-10-12T05:52:54Z","title":"LGL-BCI: A Lightweight Geometric Learning Framework for Motor\n  Imagery-Based Brain-Computer Interfaces","summary":"  Brain-Computer Interfaces (BCIs) are a groundbreaking technology for\ninteracting with external devices using brain signals. Despite advancements,\nelectroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like\namplitude and phase variability, and complex spatial correlations, with a need\nfor smaller model size and faster inference. This study introduces the LGL-BCI\nframework, employing a Geometric Deep Learning Framework for EEG processing in\nnon-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD)\nManifold space. LGL-BCI offers robust EEG data representation and captures\nspatial correlations. We propose an EEG channel selection solution via a\nfeature decomposition algorithm to reduce SPD matrix dimensionality, with a\nlossless transformation boosting inference speed. Extensive experiments show\nLGL-BCI's superior accuracy and efficiency compared to current solutions,\nhighlighting geometric deep learning's potential in MI-BCI applications. The\nefficiency, assessed on two public EEG datasets and two real-world EEG devices,\nsignificantly outperforms the state-of-the-art solution in accuracy ($82.54\\%$\nversus $62.22\\%$) with fewer parameters (64.9M compared to 183.7M).\n","authors":["Jianchao Lu","Yuzhe Tian","Yang Zhang","Jiaqi Ge","Quan Z. Sheng","Xi Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.08051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06599v4","updated":"2023-10-12T05:51:30Z","published":"2023-06-11T06:27:06Z","title":"Variational Imbalanced Regression: Fair Uncertainty Quantification via\n  Probabilistic Smoothing","summary":"  Existing regression models tend to fall short in both accuracy and\nuncertainty estimation when the label distribution is imbalanced. In this\npaper, we propose a probabilistic deep learning model, dubbed variational\nimbalanced regression (VIR), which not only performs well in imbalanced\nregression but naturally produces reasonable uncertainty estimation as a\nbyproduct. Different from typical variational autoencoders assuming I.I.D.\nrepresentations (a data point's representation is not directly affected by\nother data points), our VIR borrows data with similar regression labels to\ncompute the latent representation's variational distribution; furthermore,\ndifferent from deterministic regression models producing point estimates, VIR\npredicts the entire normal-inverse-gamma distributions and modulates the\nassociated conjugate distributions to impose probabilistic reweighting on the\nimbalanced data, thereby providing better uncertainty estimation. Experiments\nin several real-world datasets show that our VIR can outperform\nstate-of-the-art imbalanced regression models in terms of both accuracy and\nuncertainty estimation. Code will soon be available at\n\\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.\n","authors":["Ziyan Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2306.06599v4.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.08049v1","updated":"2023-10-12T05:43:06Z","published":"2023-10-12T05:43:06Z","title":"Exploring the Relationship Between Model Architecture and In-Context\n  Learning Ability","summary":"  What is the relationship between model architecture and the ability to\nperform in-context learning? In this empirical study, we take the first steps\ntowards answering this question. In particular, we evaluate fifteen model\narchitectures across a suite of synthetic in-context learning tasks. The\nselected architectures represent a broad range of paradigms, including\nrecurrent and convolution-based neural networks, transformers, and emerging\nattention alternatives. We discover that all considered architectures can\nperform in-context learning under certain conditions. However, contemporary\narchitectures are found to be the best performing, especially as task\ncomplexity grows. Additionally, our follow-up experiments delve into various\nfactors that influence in-context learning. We observe varied sensitivities\namong architectures with respect to hyperparameter settings. Our study of\ntraining dynamics reveals that certain architectures exhibit a smooth,\nprogressive learning trajectory, while others demonstrate periods of stagnation\nfollowed by abrupt mastery of the task. Finally, and somewhat surprisingly, we\nfind that several emerging attention alternatives are more robust in-context\nlearners than transformers; since such approaches have constant-sized memory\nfootprints at inference time, this result opens the future possibility of\nscaling up in-context learning to vastly larger numbers of in-context examples.\n","authors":["Ivan Lee","Nan Jiang","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2310.08049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05624v2","updated":"2023-10-12T05:33:19Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.08041v1","updated":"2023-10-12T05:25:49Z","published":"2023-10-12T05:25:49Z","title":"QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large\n  Language Models","summary":"  Large Language Models (LLMs) excel in NLP, but their demands hinder their\nwidespread deployment. While Quantization-Aware Training (QAT) offers a\nsolution, its extensive training costs make Post-Training Quantization (PTQ) a\nmore practical approach for LLMs. In existing studies, activation outliers in\nparticular channels are identified as the bottleneck to PTQ accuracy. They\npropose to transform the magnitudes from activations to weights, which however\noffers limited alleviation or suffers from unstable gradients, resulting in a\nsevere performance drop at low-bitwidth. In this paper, we propose QLLM, an\naccurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM\nintroduces an adaptive channel reassembly technique that reallocates the\nmagnitude of outliers to other channels, thereby mitigating their impact on the\nquantization range. This is achieved by channel disassembly and channel\nassembly, which first breaks down the outlier channels into several\nsub-channels to ensure a more balanced distribution of activation magnitudes.\nThen similar channels are merged to maintain the original channel number for\nefficiency. Additionally, an adaptive strategy is designed to autonomously\ndetermine the optimal number of sub-channels for channel disassembly. To\nfurther compensate for the performance loss caused by quantization, we propose\nan efficient tuning method that only learns a small number of low-rank weights\nwhile freezing the pre-trained quantized model. After training, these low-rank\nparameters can be fused into the frozen weights without affecting inference.\nExtensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate\nquantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B\nwithin 10 hours on a single A100-80G GPU, outperforming the previous\nstate-of-the-art method by 7.89% on the average accuracy across five zero-shot\ntasks.\n","authors":["Jing Liu","Ruihao Gong","Xiuying Wei","Zhiwei Dong","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.08041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10902v2","updated":"2023-10-12T05:21:21Z","published":"2023-01-26T02:22:46Z","title":"Efficient Hyperdimensional Computing","summary":"  Hyperdimensional computing (HDC) is a method to perform classification that\nuses binary vectors with high dimensions and the majority rule. This approach\nhas the potential to be energy-efficient and hence deemed suitable for\nresource-limited platforms due to its simplicity and massive parallelism.\nHowever, in order to achieve high accuracy, HDC sometimes uses hypervectors\nwith tens of thousands of dimensions. This potentially negates its efficiency\nadvantage. In this paper, we examine the necessity of such high dimensions and\nconduct a detailed theoretical analysis of the relationship between hypervector\ndimensions and accuracy. Our results demonstrate that as the dimension of the\nhypervectors increases, the worst-case/average-case HDC prediction accuracy\nwith the majority rule decreases. Building on this insight, we develop HDC\nmodels that use binary hypervectors with dimensions orders of magnitude lower\nthan those of state-of-the-art HDC models while maintaining equivalent or even\nimproved accuracy and efficiency. For instance, on the MNIST dataset, we\nachieve 91.12% HDC accuracy in image classification with a dimension of only\n64. Our methods perform operations that are only 0.35% of other HDC models with\ndimensions of 10,000. Furthermore, we evaluate our methods on ISOLET, UCI-HAR,\nand Fashion-MNIST datasets and investigate the limits of HDC computing.\n","authors":["Zhanglu Yan","Shida Wang","Kaiwen Tang","Weng-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2301.10902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08040v1","updated":"2023-10-12T05:20:18Z","published":"2023-10-12T05:20:18Z","title":"SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution\n  Detection","summary":"  Current techniques for Out-of-Distribution (OoD) detection predominantly rely\non quantifying predictive uncertainty and incorporating model regularization\nduring the training phase, using either real or synthetic OoD samples. However,\nmethods that utilize real OoD samples lack exploration and are prone to overfit\nthe OoD samples at hand. Whereas synthetic samples are often generated based on\nfeatures extracted from training data, rendering them less effective when the\ntraining and OoD data are highly overlapped in the feature space. In this work,\nwe propose a Wasserstein-score-based generative adversarial training scheme to\nenhance OoD detection accuracy, which, for the first time, performs data\naugmentation and exploration simultaneously under the supervision of limited\nOoD samples. Specifically, the generator explores OoD spaces and generates\nsynthetic OoD samples using feedback from the discriminator, while the\ndiscriminator exploits both the observed and synthesized samples for OoD\ndetection using a predefined Wasserstein score. We provide theoretical\nguarantees that the optimal solutions of our generative scheme are\nstatistically achievable through adversarial training in empirical settings. We\nthen demonstrate that the proposed method outperforms state-of-the-art\ntechniques on various computer vision datasets and exhibits superior\ngeneralizability to unseen OoD data.\n","authors":["Xiaoyang Song","Wenbo Sun","Maher Nouiehed","Raed Al Kontar","Judy Jin"],"pdf_url":"https://arxiv.org/pdf/2310.08040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07297v2","updated":"2023-10-12T05:15:51Z","published":"2023-10-11T08:31:26Z","title":"Score Regularized Policy Optimization through Diffusion Behavior","summary":"  Recent developments in offline reinforcement learning have uncovered the\nimmense potential of diffusion modeling, which excels at representing\nheterogeneous behavior policies. However, sampling from diffusion policies is\nconsiderably slow because it necessitates tens to hundreds of iterative\ninference steps for one action. To address this issue, we propose to extract an\nefficient deterministic inference policy from critic models and pretrained\ndiffusion behavior models, leveraging the latter to directly regularize the\npolicy gradient with the behavior distribution's score function during\noptimization. Our method enjoys powerful generative capabilities of diffusion\nmodeling while completely circumventing the computationally intensive and\ntime-consuming diffusion sampling scheme, both during training and evaluation.\nExtensive results on D4RL tasks show that our method boosts action sampling\nspeed by more than 25 times compared with various leading diffusion-based\nmethods in locomotion tasks, while still maintaining state-of-the-art\nperformance.\n","authors":["Huayu Chen","Cheng Lu","Zhengyi Wang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07297v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.08039v1","updated":"2023-10-12T05:14:42Z","published":"2023-10-12T05:14:42Z","title":"Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain\n  Models","summary":"  Industrial systems such as recommender systems and online advertising, have\nbeen widely equipped with multi-stage architectures, which are divided into\nseveral cascaded modules, including matching, pre-ranking, ranking and\nre-ranking. As a critical bridge between matching and ranking, existing\npre-ranking approaches mainly endure sample selection bias (SSB) problem owing\nto ignoring the entire-chain data dependence, resulting in sub-optimal\nperformances. In this paper, we rethink pre-ranking system from the perspective\nof the entire sample space, and propose Entire-chain Cross-domain Models (ECM),\nwhich leverage samples from the whole cascaded stages to effectively alleviate\nSSB problem. Besides, we design a fine-grained neural structure named ECMM to\nfurther improve the pre-ranking accuracy. Specifically, we propose a\ncross-domain multi-tower neural network to comprehensively predict for each\nstage result, and introduce the sub-networking routing strategy with $L0$\nregularization to reduce computational costs. Evaluations on real-world\nlarge-scale traffic logs demonstrate that our pre-ranking models outperform\nSOTA methods while time consumption is maintained within an acceptable level,\nwhich achieves better trade-off between efficiency and effectiveness.\n","authors":["Jinbo Song","Ruoran Huang","Xinyang Wang","Wei Huang","Qian Yu","Mingming Chen","Yafei Yao","Chaosheng Fan","Changping Peng","Zhangang Lin","Jinghe Hu","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2310.08039v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.08038v1","updated":"2023-10-12T05:09:27Z","published":"2023-10-12T05:09:27Z","title":"Continual Learning via Manifold Expansion Replay","summary":"  In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.\n","authors":["Zihao Xu","Xuan Tang","Yufei Shi","Jianfeng Zhang","Jian Yang","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2310.08038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08036v1","updated":"2023-10-12T05:08:21Z","published":"2023-10-12T05:08:21Z","title":"ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device\n  Classification","summary":"  Recent research works have proposed machine learning models for classifying\nIoT devices connected to a network. However, there is still a practical\nchallenge of not having all devices (and hence their traffic) available during\nthe training of a model. This essentially means, during the operational phase,\nwe need to classify new devices not seen during the training phase. To address\nthis challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based\non self-attention for classifying both seen and unseen devices. ZEST consists\nof i) a self-attention based network feature extractor, termed SANE, for\nextracting latent space representations of IoT traffic, ii) a generative model\nthat trains a decoder using latent features to generate pseudo data, and iii) a\nsupervised model that is trained on the generated pseudo data for classifying\ndevices. We carry out extensive experiments on real IoT traffic data; our\nexperiments demonstrate i) ZEST achieves significant improvement (in terms of\naccuracy) over the baselines; ii) ZEST is able to better extract meaningful\nrepresentations than LSTM which has been commonly used for modeling network\ntraffic.\n","authors":["Binghui Wu","Philipp Gysel","Dinil Mon Divakaran","Mohan Gurusamy"],"pdf_url":"https://arxiv.org/pdf/2310.08036v1.pdf","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2306.03410v2","updated":"2023-10-12T04:57:23Z","published":"2023-06-06T05:17:02Z","title":"Learning to Simulate Tree-Branch Dynamics for Manipulation","summary":"  We propose to use a simulation driven inverse inference approach to model the\ndynamics of tree branches under manipulation. Learning branch dynamics and\ngaining the ability to manipulate deformable vegetation can help with\nocclusion-prone tasks, such as fruit picking in dense foliage, as well as\nmoving overhanging vines and branches for navigation in dense vegetation. The\nunderlying deformable tree geometry is encapsulated as coarse spring\nabstractions executed on parallel, non-differentiable simulators. The implicit\nstatistical model defined by the simulator, reference trajectories obtained by\nactively probing the ground truth, and the Bayesian formalism, together guide\nthe spring parameter posterior density estimation. Our non-parametric inference\nalgorithm, based on Stein Variational Gradient Descent, incorporates\nbiologically motivated assumptions into the inference process as neural network\ndriven learnt joint priors; moreover, it leverages the finite difference scheme\nfor gradient approximations. Real and simulated experiments confirm that our\nmodel can predict deformation trajectories, quantify the estimation\nuncertainty, and it can perform better when base-lined against other inference\nalgorithms, particularly from the Monte Carlo family. The model displays strong\nrobustness properties in the presence of heteroscedastic sensor noise;\nfurthermore, it can generalise to unseen grasp locations.\n","authors":["Jayadeep Jacob","Tirthankar Bandyopadhyay","Jason Williams","Paulo Borges","Fabio Ramos"],"pdf_url":"https://arxiv.org/pdf/2306.03410v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.08031v1","updated":"2023-10-12T04:37:15Z","published":"2023-10-12T04:37:15Z","title":"Local Graph Clustering with Noisy Labels","summary":"  The growing interest in machine learning problems over graphs with additional\nnode information such as texts, images, or labels has popularized methods that\nrequire the costly operation of processing the entire graph. Yet, little effort\nhas been made to the development of fast local methods (i.e. without accessing\nthe entire graph) that extract useful information from such data. To that end,\nwe propose a study of local graph clustering using noisy node labels as a proxy\nfor additional node information. In this setting, nodes receive initial binary\nlabels based on cluster affiliation: 1 if they belong to the target cluster and\n0 otherwise. Subsequently, a fraction of these labels is flipped. We\ninvestigate the benefits of incorporating noisy labels for local graph\nclustering. By constructing a weighted graph with such labels, we study the\nperformance of graph diffusion-based local clustering method on both the\noriginal and the weighted graphs. From a theoretical perspective, we consider\nrecovering an unknown target cluster with a single seed node in a random graph\nwith independent noisy node labels. We provide sufficient conditions on the\nlabel noise under which, with high probability, using diffusion in the weighted\ngraph yields a more accurate recovery of the target cluster. This approach\nproves more effective than using the given labels alone or using diffusion in\nthe label-free original graph. Empirically, we show that reliable node labels\ncan be obtained with just a few samples from an attributed graph. Moreover,\nutilizing these labels via diffusion in the weighted graph leads to\nsignificantly better local clustering performance across several real-world\ndatasets, improving F1 scores by up to 13%.\n","authors":["Artur Back de Luca","Kimon Fountoulakis","Shenghao Yang"],"pdf_url":"https://arxiv.org/pdf/2310.08031v1.pdf","comment":"26 pages, 5 figures, 14 tables"},{"id":"http://arxiv.org/abs/2309.10691v2","updated":"2023-10-12T04:07:56Z","published":"2023-09-19T15:25:42Z","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback","summary":"  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n","authors":["Xingyao Wang","Zihan Wang","Jiateng Liu","Yangyi Chen","Lifan Yuan","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.10691v2.pdf","comment":"Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench"},{"id":"http://arxiv.org/abs/2307.02484v5","updated":"2023-10-12T04:06:48Z","published":"2023-07-05T17:58:21Z","title":"Elastic Decision Transformer","summary":"  This paper introduces Elastic Decision Transformer (EDT), a significant\nadvancement over the existing Decision Transformer (DT) and its variants.\nAlthough DT purports to generate an optimal trajectory, empirical evidence\nsuggests it struggles with trajectory stitching, a process involving the\ngeneration of an optimal or near-optimal trajectory from the best parts of a\nset of sub-optimal trajectories. The proposed EDT differentiates itself by\nfacilitating trajectory stitching during action inference at test time,\nachieved by adjusting the history length maintained in DT. Further, the EDT\noptimizes the trajectory by retaining a longer history when the previous\ntrajectory is optimal and a shorter one when it is sub-optimal, enabling it to\n\"stitch\" with a more optimal trajectory. Extensive experimentation demonstrates\nEDT's ability to bridge the performance gap between DT-based and Q\nLearning-based approaches. In particular, the EDT outperforms Q Learning-based\nmethods in a multi-task regime on the D4RL locomotion benchmark and Atari\ngames. Videos are available at: https://kristery.github.io/edt/\n","authors":["Yueh-Hua Wu","Xiaolong Wang","Masashi Hamaya"],"pdf_url":"https://arxiv.org/pdf/2307.02484v5.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2210.15889v4","updated":"2023-10-12T04:05:41Z","published":"2022-10-28T04:38:10Z","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on\n  Neuro-Symbolic Computing","summary":"  Neural-symbolic computing (NeSy), which pursues the integration of the\nsymbolic and statistical paradigms of cognition, has been an active research\narea of Artificial Intelligence (AI) for many years. As NeSy shows promise of\nreconciling the advantages of reasoning and interpretability of symbolic\nrepresentation and robust learning in neural networks, it may serve as a\ncatalyst for the next generation of AI. In the present paper, we provide a\nsystematic overview of the recent developments and important contributions of\nNeSy research. Firstly, we introduce study history of this area, covering early\nwork and foundations. We further discuss background concepts and identify key\ndriving factors behind the development of NeSy. Afterward, we categorize recent\nlandmark approaches along several main characteristics that underline this\nresearch paradigm, including neural-symbolic integration, knowledge\nrepresentation, knowledge embedding, and functionality. Next, we briefly\ndiscuss the successful application of modern NeSy approaches in several\ndomains. Then, we benchmark several NeSy methods on three representative\napplication tasks. Finally, we identify the open problems together with\npotential future research directions. This survey is expected to help new\nresearchers enter this rapidly evolving field and accelerate the progress\ntowards data-and knowledge-driven AI.\n","authors":["Wenguan Wang","Yi Yang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2210.15889v4.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2302.02931v2","updated":"2023-10-12T03:47:00Z","published":"2023-02-06T17:07:16Z","title":"Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group\n  Shifts","summary":"  Training machine learning models robust to distribution shifts is critical\nfor real-world applications. Some robust training algorithms (e.g., Group DRO)\nspecialize to group shifts and require group information on all training\npoints. Other methods (e.g., CVaR DRO) that do not need group annotations can\nbe overly conservative, since they naively upweight high loss points which may\nform a contrived set that does not correspond to any meaningful group in the\nreal world (e.g., when the high loss points are randomly mislabeled training\npoints). In this work, we address limitations in prior approaches by assuming a\nmore nuanced form of group shift: conditioned on the label, we assume that the\ntrue group function (indicator over group) is simple. For example, we may\nexpect that group shifts occur along low bitrate features (e.g., image\nbackground, lighting). Thus, we aim to learn a model that maintains high\naccuracy on simple group functions realized by these low bitrate features, that\nneed not spend valuable model capacity achieving high accuracy on contrived\ngroups of examples. Based on this, we consider the two-player game formulation\nof DRO where the adversary's capacity is bitrate-constrained. Our resulting\npractical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group\ninformation on training samples yet matches the performance of Group DRO on\ndatasets that have training group annotations and that of CVaR DRO on\nlong-tailed distributions. Our theoretical analysis reveals that in some\nsettings BR-DRO objective can provably yield statistically efficient and less\nconservative solutions than unconstrained CVaR DRO.\n","authors":["Amrith Setlur","Don Dennis","Benjamin Eysenbach","Aditi Raghunathan","Chelsea Finn","Virginia Smith","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2302.02931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08019v1","updated":"2023-10-12T03:41:32Z","published":"2023-10-12T03:41:32Z","title":"Robust 1-bit Compressed Sensing with Iterative Hard Thresholding","summary":"  In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector\n$x\\in S^{n-1}$ within an $\\epsilon$ error (in $\\ell_2$) from minimal number of\nlinear measurements that are quantized to just their signs, i.e., from\nmeasurements of the form $y = \\mathrm{Sign}(\\langle a, x\\rangle).$ In this\npaper, we study a noisy version where a fraction of the measurements can be\nflipped, potentially by an adversary. In particular, we analyze the Binary\nIterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a\nproperly defined loss function used for 1-bit compressed sensing, in this noisy\nsetting. It is known from recent results that, with\n$\\tilde{O}(\\frac{k}{\\epsilon})$ noiseless measurements, BIHT provides an\nestimate within $\\epsilon$ error. This result is optimal and universal, meaning\none set of measurements work for all sparse vectors. In this paper, we show\nthat BIHT also provides better results than all known methods for the noisy\nsetting. We show that when up to $\\tau$-fraction of the sign measurements are\nincorrect (adversarial error), with the same number of measurements as before,\nBIHT agnostically provides an estimate of $x$ within an\n$\\tilde{O}(\\epsilon+\\tau)$ error, maintaining the universality of measurements.\nThis establishes stability of iterative hard thresholding in the presence of\nmeasurement error. To obtain the result, we use the restricted approximate\ninvertibility of Gaussian matrices, as well as a tight analysis of the\nhigh-dimensional geometry of the adversarially corrupted measurements.\n","authors":["Namiko Matsumoto","Arya Mazumdar"],"pdf_url":"https://arxiv.org/pdf/2310.08019v1.pdf","comment":"Accepted to appear in ACM-SIAM Symposium on Discrete Algorithms\n  (SODA) 2024"},{"id":"http://arxiv.org/abs/2308.03807v2","updated":"2023-10-12T03:36:17Z","published":"2023-08-06T15:47:03Z","title":"Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS\n  Image Reconstruction","summary":"  Proximal gradient-based optimization is one of the most common strategies to\nsolve inverse problem of images, and it is easy to implement. However, these\ntechniques often generate heavy artifacts in image reconstruction. One of the\nmost popular refinement methods is to fine-tune the regularization parameter to\nalleviate such artifacts, but it may not always be sufficient or applicable due\nto increased computational costs. In this work, we propose a deep geometric\nincremental learning framework based on the second Nesterov proximal gradient\noptimization. The proposed end-to-end network not only has the powerful\nlearning ability for high-/low-frequency image features, but also can\ntheoretically guarantee that geometric texture details will be reconstructed\nfrom preliminary linear reconstruction. Furthermore, it can avoid the risk of\nintermediate reconstruction results falling outside the geometric decomposition\ndomains and achieve fast convergence. Our reconstruction framework is\ndecomposed into four modules including general linear reconstruction, cascade\ngeometric incremental restoration, Nesterov acceleration, and post-processing.\nIn the image restoration step, a cascade geometric incremental learning module\nis designed to compensate for missing texture information from different\ngeometric spectral decomposition domains. Inspired by the overlap-tile\nstrategy, we also develop a post-processing module to remove the block effect\nin patch-wise-based natural image reconstruction. All parameters in the\nproposed model are learnable, an adaptive initialization technique of physical\nparameters is also employed to make model flexibility and ensure converging\nsmoothly. We compare the reconstruction performance of the proposed method with\nexisting state-of-the-art methods to demonstrate its superiority. Our source\ncodes are available at https://github.com/fanxiaohong/Nest-DGIL.\n","authors":["Xiaohong Fan","Yin Yang","Ke Chen","Yujie Feng","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.03807v2.pdf","comment":"15 pages,our source codes are available at\n  https://github.com/fanxiaohong/Nest-DGIL"},{"id":"http://arxiv.org/abs/2310.07644v2","updated":"2023-10-12T03:32:32Z","published":"2023-10-11T16:40:57Z","title":"Rethinking the BERT-like Pretraining for DNA Sequences","summary":"  With the success of large-scale pretraining in NLP, there is an increasing\ntrend of applying it to the domain of life sciences. In particular, pretraining\nmethods based on DNA sequences have garnered growing attention due to their\npotential to capture generic information about genes. However, existing\npretraining methods for DNA sequences largely rely on direct adoptions of BERT\npretraining from NLP, lacking a comprehensive understanding and a specifically\ntailored approach. To address this research gap, we first conducted a series of\nexploratory experiments and gained several insightful observations: 1) In the\nfine-tuning phase of downstream tasks, when using K-mer overlapping\ntokenization instead of K-mer non-overlapping tokenization, both overlapping\nand non-overlapping pretraining weights show consistent performance\nimprovement.2) During the pre-training process, using K-mer overlapping\ntokenization quickly produces clear K-mer embeddings and reduces the loss to a\nvery low level, while using K-mer non-overlapping tokenization results in less\ndistinct embeddings and continuously decreases the loss. 3) Using overlapping\ntokenization causes the self-attention in the intermediate layers of\npre-trained models to tend to overly focus on certain tokens, reflecting that\nthese layers are not adequately optimized. In summary, overlapping tokenization\ncan benefit the fine-tuning of downstream tasks but leads to inadequate\npretraining with fast convergence. To unleash the pretraining potential, we\nintroduce a novel approach called RandomMask, which gradually increases the\ntask difficulty of BERT-like pretraining by continuously expanding its mask\nboundary, forcing the model to learn more knowledge. RandomMask is simple but\neffective, achieving top-tier performance across 26 datasets of 28 datasets\nspanning 7 downstream tasks.\n","authors":["Chaoqi Liang","Weiqiang Bai","Lifeng Qiao","Yuchen Ren","Jianle Sun","Peng Ye","Hongliang Yan","Xinzhu Ma","Wangmeng Zuo","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.07644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08015v1","updated":"2023-10-12T03:29:53Z","published":"2023-10-12T03:29:53Z","title":"Why Train More? Effective and Efficient Membership Inference via\n  Memorization","summary":"  Membership Inference Attacks (MIAs) aim to identify specific data samples\nwithin the private training dataset of machine learning models, leading to\nserious privacy violations and other sophisticated threats. Many practical\nblack-box MIAs require query access to the data distribution (the same\ndistribution where the private data is drawn) to train shadow models. By doing\nso, the adversary obtains models trained \"with\" or \"without\" samples drawn from\nthe distribution, and analyzes the characteristics of the samples under\nconsideration. The adversary is often required to train more than hundreds of\nshadow models to extract the signals needed for MIAs; this becomes the\ncomputational overhead of MIAs. In this paper, we propose that by strategically\nchoosing the samples, MI adversaries can maximize their attack success while\nminimizing the number of shadow models. First, our motivational experiments\nsuggest memorization as the key property explaining disparate sample\nvulnerability to MIAs. We formalize this through a theoretical bound that\nconnects MI advantage with memorization. Second, we show sample complexity\nbounds that connect the number of shadow models needed for MIAs with\nmemorization. Lastly, we confirm our theoretical arguments with comprehensive\nexperiments; by utilizing samples with high memorization scores, the adversary\ncan (a) significantly improve its efficacy regardless of the MIA used, and (b)\nreduce the number of shadow models by nearly two orders of magnitude compared\nto state-of-the-art approaches.\n","authors":["Jihye Choi","Shruti Tople","Varun Chandrasekaran","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2310.08015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08012v1","updated":"2023-10-12T03:28:14Z","published":"2023-10-12T03:28:14Z","title":"AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE","summary":"  Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS\ninvolves polynomial approximation of unsupported non-linear activation\nfunctions. However, existing approaches have three main limitations: 1)\nInflexibility: The polynomial approximation and associated homomorphic\nevaluation architecture are customized manually for each CNN architecture and\ndo not generalize to other networks. 2) Suboptimal Approximation: Each\nactivation function is approximated instead of the function represented by the\nCNN. 3) Restricted Design: Either high-degree or low-degree polynomial\napproximations are used. The former retains high accuracy but slows down\ninference due to bootstrapping operations, while the latter accelerates\nciphertext inference but compromises accuracy. To address these limitations, we\npresent AutoFHE, which automatically adapts standard CNNs for secure inference\nunder RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial\nactivation functions, which are optimized jointly with the homomorphic\nevaluation architecture in terms of the placement of bootstrapping operations.\nThe problem is modeled within a multi-objective optimization framework to\nmaximize accuracy and minimize the number of bootstrapping operations. AutoFHE\ncan be applied flexibly on any CNN architecture, and it provides diverse\nsolutions that span the trade-off between accuracy and latency. Experimental\nevaluation over RNS-CKKS encrypted CIFAR datasets shows that AutoFHE\naccelerates secure inference by $1.32\\times$ to $1.8\\times$ compared to methods\nemploying high-degree polynomials. It also improves accuracy by up to 2.56%\ncompared to methods using low-degree polynomials. Lastly, AutoFHE accelerates\ninference and improves accuracy by $103\\times$ and 3.46%, respectively,\ncompared to CNNs under TFHE.\n","authors":["Wei Ao","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2310.08012v1.pdf","comment":"USENIX Security Symposium 2024"},{"id":"http://arxiv.org/abs/2310.06763v2","updated":"2023-10-12T03:24:43Z","published":"2023-10-10T16:39:47Z","title":"FABind: Fast and Accurate Protein-Ligand Binding","summary":"  Modeling the interaction between proteins and ligands and accurately\npredicting their binding structures is a critical yet challenging task in drug\ndiscovery. Recent advancements in deep learning have shown promise in\naddressing this challenge, with sampling-based and regression-based methods\nemerging as two prominent approaches. However, these methods have notable\nlimitations. Sampling-based methods often suffer from low efficiency due to the\nneed for generating multiple candidate structures for selection. On the other\nhand, regression-based methods offer fast predictions but may experience\ndecreased accuracy. Additionally, the variation in protein sizes often requires\nexternal modules for selecting suitable binding pockets, further impacting\nefficiency. In this work, we propose $\\mathbf{FABind}$, an end-to-end model\nthat combines pocket prediction and docking to achieve accurate and fast\nprotein-ligand binding. $\\mathbf{FABind}$ incorporates a unique ligand-informed\npocket prediction module, which is also leveraged for docking pose estimation.\nThe model further enhances the docking process by incrementally integrating the\npredicted pocket to optimize protein-ligand binding, reducing discrepancies\nbetween training and inference. Through extensive experiments on benchmark\ndatasets, our proposed $\\mathbf{FABind}$ demonstrates strong advantages in\nterms of effectiveness and efficiency compared to existing methods. Our code is\navailable at $\\href{https://github.com/QizhiPei/FABind}{Github}$.\n","authors":["Qizhi Pei","Kaiyuan Gao","Lijun Wu","Jinhua Zhu","Yingce Xia","Shufang Xie","Tao Qin","Kun He","Tie-Yan Liu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06763v2.pdf","comment":"Neural Information Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.06488v2","updated":"2023-10-12T03:23:40Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02285v2","updated":"2023-10-12T03:05:36Z","published":"2023-09-05T14:45:27Z","title":"PromptTTS 2: Describing and Generating Voices with Text Prompt","summary":"  Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.\n","authors":["Yichong Leng","Zhifang Guo","Kai Shen","Xu Tan","Zeqian Ju","Yanqing Liu","Yufei Liu","Dongchao Yang","Leying Zhang","Kaitao Song","Lei He","Xiang-Yang Li","Sheng Zhao","Tao Qin","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2309.02285v2.pdf","comment":"Demo page: https://speechresearch.github.io/prompttts2"}],"Multimedia":[{"id":"http://arxiv.org/abs/2112.09726v2","updated":"2023-10-12T17:57:51Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Cristbal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v2.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: https://soundify.cc"},{"id":"http://arxiv.org/abs/2310.08475v1","updated":"2023-10-12T16:32:44Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights\\footnote{Code\nand dataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08205v1","updated":"2023-10-12T10:51:17Z","published":"2023-10-12T10:51:17Z","title":"LiveVV: Human-Centered Live Volumetric Video Streaming System","summary":"  Volumetric video has emerged as a prominent medium within the realm of\neXtended Reality (XR) with the advancements in computer graphics and depth\ncapture hardware. Users can fully immersive themselves in volumetric video with\nthe ability to switch their viewport in six degree-of-freedom (DOF), including\nthree rotational dimensions (yaw, pitch, roll) and three translational\ndimensions (X, Y, Z). Different from traditional 2D videos that are composed of\npixel matrices, volumetric videos employ point clouds, meshes, or voxels to\nrepresent a volumetric scene, resulting in significantly larger data sizes.\nWhile previous works have successfully achieved volumetric video streaming in\nvideo-on-demand scenarios, the live streaming of volumetric video remains an\nunresolved challenge due to the limited network bandwidth and stringent latency\nconstraints. In this paper, we for the first time propose a holistic live\nvolumetric video streaming system, LiveVV, which achieves multi-view capture,\nscene segmentation \\& reuse, adaptive transmission, and rendering. LiveVV\ncontains multiple lightweight volumetric video capture modules that are capable\nof being deployed without prior preparation. To reduce bandwidth consumption,\nLiveVV processes static and dynamic volumetric content separately by reusing\nstatic data with low disparity and decimating data with low visual saliency.\nBesides, to deal with network fluctuation, LiveVV integrates a volumetric video\nadaptive bitrate streaming algorithm (VABR) to enable fluent playback with the\nmaximum quality of experience. Extensive real-world experiment shows that\nLiveVV can achieve live volumetric video streaming at a frame rate of 24 fps\nwith a latency of less than 350ms.\n","authors":["Kaiyuan Hu","Yongting Chen","Kaiying Han","Junhua Liu","Haowen Yang","Yili Jin","Boyan Li","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08006v1","updated":"2023-10-12T03:19:13Z","published":"2023-10-12T03:19:13Z","title":"MCPNS: A Macropixel Collocated Position and Its Neighbors Search for\n  Plenoptic 2.0 Video Coding","summary":"  Recently, it was demonstrated that a newly focused plenoptic 2.0 camera can\ncapture much higher spatial resolution owing to its effective light field\nsampling, as compared to a traditional unfocused plenoptic 1.0 camera. However,\ndue to the nature difference of the optical structure between the plenoptic 1.0\nand 2.0 cameras, the existing fast motion estimation (ME) method for plenoptic\n1.0 videos is expected to be sub-optimal for encoding plenoptic 2.0 videos. In\nthis paper, we point out the main motion characteristic differences between\nplenoptic 1.0 and 2.0 videos and then propose a new fast ME, called macropixel\ncollocated position and its neighbors search (MCPNS) for plenoptic 2.0 videos.\nIn detail, we propose to reduce the number of macropixel collocated position\n(MCP) search candidates based on the new observation of center-biased motion\nvector distribution at macropixel resolution. After that, due to large motion\ndeviation behavior around each MCP location in plenoptic 2.0 videos, we propose\nto select a certain number of key MCP locations with the lowest matching cost\nto perform the neighbors MCP search to improve the motion search accuracy.\nDifferent from existing methods, our method can achieve better performance\nwithout requiring prior knowledge of microlens array orientations. Our\nsimulation results confirmed the effectiveness of the proposed algorithm in\nterms of both bitrate savings and computational costs compared to existing\nmethods.\n","authors":["Vinh Van Duong","Thuc Nguyen Huu","Jonghoon Yim","Byeungwoo Jeon"],"pdf_url":"https://arxiv.org/pdf/2310.08006v1.pdf","comment":null}]}}